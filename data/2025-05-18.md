<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.CR](#cs.CR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: 这篇文章是一篇立场论文，批判性地综述了人类与AI智能体协作的广泛实证进展，强调其技术成就及持续存在的差距。文中提出了一种新的概念架构（分层探索-利用网络），以系统地整合多智能体协调、知识管理、控制论反馈回路和高级控制机制等技术细节。该框架有助于修订传统方法并激发融合定性和定量范式的新工作，推动人类认知与AI能力的共同进化。文章结构灵活，可从任何部分开始阅读，既是对技术实现的批判性回顾，也是设计或扩展人机共生系统的前瞻性参考。


<details>
  <summary>Details</summary>
Motivation: 当前关于人类与AI智能体协作的研究缺乏统一的理论框架，尤其是在处理开放性复杂任务时，各研究之间难以整合。因此，需要一种能够连贯集成这些多样化研究的新型框架。

Method: 作者提出了一种名为“分层探索-利用网络”的概念架构，将现有的研究成果（包括符号AI技术、基于连接主义的大语言模型智能体以及混合组织实践）映射到这一框架中。通过这种方法，可以系统地连接多智能体协调、知识管理、控制论反馈回路和高级控制机制等技术领域。

Result: 该框架不仅帮助重新审视传统的研究方法，还为未来融合定性和定量范式的创新工作提供了灵感，促进了人类认知与AI能力的深度协同进化。

Conclusion: 通过提出“分层探索-利用网络”概念架构，本文为深化人类认知与AI能力的共同进化奠定了基础，并为设计或扩展人机共生系统提供了重要的理论支持和实践指导。

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [2] [Automated Meta Prompt Engineering for Alignment with the Theory of Mind](https://arxiv.org/abs/2505.09024)
*Aaron Baughman,Rahul Agarwal,Eduardo Morales,Gozde Akay*

Main category: cs.AI

TL;DR: 文章介绍了一种元提示（meta-prompting）方法，通过联合生成流畅文本和优化人类心理预期与大型语言模型（LLM）神经处理状态之间的相似性，来解决复杂任务。具体来说，该方法使用代理强化学习技术，让一个作为'评判者'的LLM（LLMaaJ）通过上下文学习指导另一个LLM生成内容，并预测人类编辑的行为。实验结果表明，在2024年美国网球公开赛的应用中，AI生成的内容有53.8%的时间完全符合人类评审的期望，平均迭代次数为4.38次。此外，通过在希尔伯特向量空间中对内容特质（如事实性、新颖性、重复性和相关性）进行几何解释，LLMaaJ能够更好地优化人类的心理理论（ToM），从而提高内容质量。这项技术已扩展到其他体育和娱乐现场活动。


<details>
  <summary>Details</summary>
Motivation: 动机在于提升大型语言模型生成内容的质量，使其更符合人类的心理预期和编辑需求。同时，希望通过引入'评判者'角色，利用代理强化学习技术改进LLM生成内容的能力，特别是在复杂任务中实现更高的准确性和一致性。

Method: 研究采用了元提示方法结合代理强化学习技术。具体步骤包括：1) 使用一个LLM作为'评判者'（LLMaaJ），通过上下文学习指导另一个LLM生成内容；2) 在生成过程中，考虑并预测人类可能的编辑行为；3) 通过在希尔伯特向量空间中对内容特质进行几何解释，优化LLM生成内容以匹配人类的心理理论（ToM）。

Result: 实验结果显示，在2024年美国网球公开赛上，AI生成的内容有53.8%的时间完全符合人类评审的期望，平均需要迭代4.38次才能达到最终结果。此外，通过对内容特质的几何解释，LLMaaJ显著提高了生成内容的质量，扩展了对网球赛事的覆盖范围。

Conclusion: 本研究成功展示了元提示方法结合代理强化学习技术在生成高质量内容方面的潜力。通过优化LLM与人类心理预期的一致性，可以显著提高生成内容的质量和适用性。这一技术不仅适用于体育赛事报道，还可在其他实时场景中推广使用。

Abstract: We introduce a method of meta-prompting that jointly produces fluent text for
complex tasks while optimizing the similarity of neural states between a
human's mental expectation and a Large Language Model's (LLM) neural
processing. A technique of agentic reinforcement learning is applied, in which
an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,
how to produce content by interpreting the intended and unintended generated
text traits. To measure human mental beliefs around content production, users
modify long form AI-generated text articles before publication at the US Open
2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)
alignment problem by anticipating and including human edits within the creation
of text from an LLM. Throughout experimentation and by interpreting the results
of a live production system, the expectations of human content reviewers had
100% of alignment with AI 53.8% of the time with an average iteration count of
4.38. The geometric interpretation of content traits such as factualness,
novelty, repetitiveness, and relevancy over a Hilbert vector space combines
spatial volume (all trait importance) with vertices alignment (individual trait
relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an
increase in content quality by extending the coverage of tennis action. Our
work that was deployed at the US Open 2024 has been used across other live
events within sports and entertainment.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs](https://arxiv.org/abs/2505.10425)
*Jingyao Wang,Wenwen Qiang,Zeen Song,Changwen Zheng,Hui Xiong*

Main category: cs.LG

TL;DR: 提出了一种名为Learning to Think (L2T)的信息理论强化微调框架，以减少大型语言模型在推理任务中的token使用量，同时保持或提升推理效果。通过引入分层会话机制和普适密集过程奖励，优化了模型的计算效率和推理效果，并通过理论分析和实验证明了其优势。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在提高推理能力时忽视了推理效果与计算效率之间的权衡，导致不必要的长推理链和token浪费。因此需要一种方法来优化大型语言模型（LLMs）的推理过程，使其能够在更少的token下实现最优推理。

Method: 提出了L2T框架，将每次查询-响应交互视为多阶段的分层会话，并引入了一个普适密集过程奖励，用于量化每阶段的信息增益。此外，还提出了一种基于PAC-Bayes边界和Fisher信息矩阵的快速奖励估计方法，以降低计算复杂度并保持高估计精度。通过立即奖励每阶段的贡献并惩罚过度更新，L2T利用强化学习优化模型。

Result: 在多个推理基准和基础模型上的实验证明，L2T在不同任务中均表现出优势，提升了推理的有效性和效率。

Conclusion: L2T框架为LLMs提供了一种新的优化方法，能够在减少token使用的同时提高推理性能，具有广泛的适用性。

Abstract: Large language models (LLMs) excel at complex tasks thanks to advances in
reasoning abilities. However, existing methods overlook the trade-off between
reasoning effectiveness and computational efficiency, often encouraging
unnecessarily long reasoning chains and wasting tokens. To address this, we
propose Learning to Think (L2T), an information-theoretic reinforcement
fine-tuning framework for LLMs to make the models achieve optimal reasoning
with fewer tokens. Specifically, L2T treats each query-response interaction as
a hierarchical session of multiple episodes and proposes a universal dense
process reward, i.e., quantifies the episode-wise information gain in
parameters, requiring no extra annotations or task-specific evaluators. We
propose a method to quickly estimate this reward based on PAC-Bayes bounds and
the Fisher information matrix. Theoretical analyses show that it significantly
reduces computational complexity with high estimation accuracy. By immediately
rewarding each episode's contribution and penalizing excessive updates, L2T
optimizes the model via reinforcement learning to maximize the use of each
episode and achieve effective updates. Empirical results on various reasoning
benchmarks and base models demonstrate the advantage of L2T across different
tasks, boosting both reasoning effectiveness and efficiency.

</details>


### [4] [ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts](https://arxiv.org/abs/2505.10010)
*Jing-Cheng Pang,Kaiyuan Li,Yidi Wang,Si-Hang Yang,Shengyi Jiang,Yang Yu*

Main category: cs.LG

TL;DR: 本文介绍了ImagineBench，一个用于评估结合实际和想象轨迹的离线强化学习算法的基准测试平台。通过系统评估现有算法，发现单纯使用现有算法在未见任务上的表现不佳，强调了改进算法以更好利用LLM生成轨迹的重要性，并指出了未来研究的关键方向。


<details>
  <summary>Details</summary>
Motivation: 强化学习依赖大量真实世界交互数据来学习特定任务策略，而大型语言模型可以生成合成经验（想象轨迹）以减少这种依赖。但由于缺乏标准基准测试，难以衡量该领域进展。

Method: 构建了一个名为ImagineBench的综合基准测试平台，包含：实际与LLM生成的轨迹数据集、涵盖多种任务类型的环境、以及具有不同复杂度级别的自然语言指令。并以此对当前最先进的离线RL算法进行了系统性评估。

Result: 现有离线RL算法在未见任务上表现欠佳，在困难任务中的成功率为35.44%，远低于基于真实轨迹训练的方法的成功率64.37%。

Conclusion: 需要改进算法以更好地利用LLM生成的轨迹数据，并提出了未来研究的几个关键方向，如快速在线适应、持续学习及多模态任务扩展等。

Abstract: A central challenge in reinforcement learning (RL) is its dependence on
extensive real-world interaction data to learn task-specific policies. While
recent work demonstrates that large language models (LLMs) can mitigate this
limitation by generating synthetic experience (noted as imaginary rollouts) for
mastering novel tasks, progress in this emerging field is hindered due to the
lack of a standard benchmark. To bridge this gap, we introduce ImagineBench,
the first comprehensive benchmark for evaluating offline RL algorithms that
leverage both real rollouts and LLM-imaginary rollouts. The key features of
ImagineBench include: (1) datasets comprising environment-collected and
LLM-imaginary rollouts; (2) diverse domains of environments covering
locomotion, robotic manipulation, and navigation tasks; and (3) natural
language task instructions with varying complexity levels to facilitate
language-conditioned policy learning. Through systematic evaluation of
state-of-the-art offline RL algorithms, we observe that simply applying
existing offline RL algorithms leads to suboptimal performance on unseen tasks,
achieving 35.44% success rate in hard tasks in contrast to 64.37% of method
training on real rollouts for hard tasks. This result highlights the need for
algorithm advancements to better leverage LLM-imaginary rollouts. Additionally,
we identify key opportunities for future research: including better utilization
of imaginary rollouts, fast online adaptation and continual learning, and
extension to multi-modal tasks. Our code is publicly available at
https://github.com/LAMDA-RL/ImagineBench.

</details>


### [5] [Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback](https://arxiv.org/abs/2505.09925)
*Yutao Yang,Jie Zhou,Junsong Li,Qianjun Pan,Bihao Zhan,Qin Chen,Xipeng Qiu,Liang He*

Main category: cs.LG

TL;DR: 本文介绍了一种交互式的持续学习范式，其中AI模型在保留先前知识的同时，能够从实时的人类反馈中动态地学习新技能。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习存在两个主要局限性：1）使用静态数据集而非流式、实时的人类标注数据进行动态模型更新；2）假设标签干净，而未处理现实交互中常见的噪声反馈。

Method: 提出了RiCL框架，利用大语言模型（LLMs）从动态反馈中有效学习新技能。该框架包含三个关键组件：1）时间一致性感知净化器，用于自动区分数据流中的干净样本和噪声样本；2）交互感知直接偏好优化策略，通过协调AI生成的反馈与人类提供的反馈，使模型行为与人类意图对齐；3）抗噪声对比学习模块，通过利用数据内在关系捕捉鲁棒表示，避免依赖潜在不可靠标签。

Result: 在两个基准数据集（FewRel和TACRED）上的广泛实验表明，RiCL方法在包含现实噪声模式的数据上显著优于现有的最先进的在线持续学习和噪声标签学习方法组合。

Conclusion: RiCL框架有效地解决了传统持续学习中的问题，在处理实时人类反馈和噪声标签方面表现出色。

Abstract: This paper introduces an interactive continual learning paradigm where AI
models dynamically learn new skills from real-time human feedback while
retaining prior knowledge. This paradigm distinctively addresses two major
limitations of traditional continual learning: (1) dynamic model updates using
streaming, real-time human-annotated data, rather than static datasets with
fixed labels, and (2) the assumption of clean labels, by explicitly handling
the noisy feedback common in real-world interactions. To tackle these problems,
we propose RiCL, a Reinforced interactive Continual Learning framework
leveraging Large Language Models (LLMs) to learn new skills effectively from
dynamic feedback. RiCL incorporates three key components: a temporal
consistency-aware purifier to automatically discern clean from noisy samples in
data streams; an interaction-aware direct preference optimization strategy to
align model behavior with human intent by reconciling AI-generated and
human-provided feedback; and a noise-resistant contrastive learning module that
captures robust representations by exploiting inherent data relationships, thus
avoiding reliance on potentially unreliable labels. Extensive experiments on
two benchmark datasets (FewRel and TACRED), contaminated with realistic noise
patterns, demonstrate that our RiCL approach substantially outperforms existing
combinations of state-of-the-art online continual learning and noisy-label
learning methods.

</details>


### [6] [Adversarial Attack on Large Language Models using Exponentiated Gradient Descent](https://arxiv.org/abs/2505.09820)
*Sajib Biswas,Mao Nishino,Samuel Jacob Chacko,Xiuwen Liu*

Main category: cs.LG

TL;DR: 文章提出了一种基于指数梯度下降和Bregman投影方法的内在优化技术，用于攻击大型语言模型（LLMs）的越狱问题。该方法在五个开源LLM和四个公开数据集上进行了测试，相较于其他三种最先进的越狱技术，具有更高的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管许多LLMs通过如RLHF等技术进行了对齐，但它们仍然容易受到越狱攻击。现有的对抗攻击方法要么在离散空间中搜索可能越狱目标模型的标记，要么尝试优化由模型词汇表中的标记表示的连续空间。然而，这些方法各有不足：基于离散空间的技术可能效率低下，而连续标记嵌入的优化需要投影以生成离散标记，这可能会使它们失效。

Method: 作者开发了一种内在优化技术，使用指数梯度下降与Bregman投影方法，确保优化的一热编码始终保留在概率单纯形内。这种方法充分利用了空间的约束和结构。此外，作者证明了该技术的收敛性，并实现了一个高效的算法。

Result: 在五个开源LLM和四个公开数据集上的实验结果表明，所提出的技术相较于其他三种最先进的越狱技术，能够以更高的成功率和极大的效率进行攻击。

Conclusion: 提出的内在优化技术为LLM的越狱攻击提供了一种新的有效方法，其高效率和成功率展示了该技术的优势。

Abstract: As Large Language Models (LLMs) are widely used, understanding them
systematically is key to improving their safety and realizing their full
potential. Although many models are aligned using techniques such as
reinforcement learning from human feedback (RLHF), they are still vulnerable to
jailbreaking attacks. Some of the existing adversarial attack methods search
for discrete tokens that may jailbreak a target model while others try to
optimize the continuous space represented by the tokens of the model's
vocabulary. While techniques based on the discrete space may prove to be
inefficient, optimization of continuous token embeddings requires projections
to produce discrete tokens, which might render them ineffective. To fully
utilize the constraints and the structures of the space, we develop an
intrinsic optimization technique using exponentiated gradient descent with the
Bregman projection method to ensure that the optimized one-hot encoding always
stays within the probability simplex. We prove the convergence of the technique
and implement an efficient algorithm that is effective in jailbreaking several
widely used LLMs. We demonstrate the efficacy of the proposed technique using
five open-source LLMs on four openly available datasets. The results show that
the technique achieves a higher success rate with great efficiency compared to
three other state-of-the-art jailbreaking techniques. The source code for our
implementation is available at:
https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack

</details>


### [7] [Self Rewarding Self Improving](https://arxiv.org/abs/2505.08827)
*Toby Simonds,Kevin Lopez,Akira Yoshiyama,Dominique Garmier*

Main category: cs.LG

TL;DR: 通过自我评判机制，大型语言模型能够在无需参考答案的情况下实现自我改进。实验表明，在 Countdown puzzles 和 MIT Integration Bee 问题上，模型可以提供可靠的奖励信号，从而在以前不可能的领域实现强化学习。结合合成问题生成，建立了完整的自我改进循环，显著提升了性能并超越了 GPT-4 的表现。


<details>
  <summary>Details</summary>
Motivation: 当前许多强化学习环境受限于难以创建程序化奖励信号，而生成和验证解决方案之间存在固有的不对称性，这为模型的自我评判提供了可能性。

Method: 利用大型语言模型的自我评判能力，在无需真实答案的情况下，通过强化学习提升模型性能。具体方法包括：生成实践问题、解决问题并评估自身表现，形成自我改进闭环。

Result: 实现了8%的性能提升（以Qwen 2.5 7B为例），并在积分任务上超越了GPT-4的表现。证明了LLM评判者可以为模型训练提供有效的奖励信号。

Conclusion: 这项研究展示了通过自我指导学习实现AI系统持续改进的潜力，可能加速那些训练数据稀缺或评估要求复杂的领域的进步。

Abstract: We demonstrate that large language models can effectively self-improve
through self-judging without requiring reference solutions, leveraging the
inherent asymmetry between generating and verifying solutions. Our experiments
on Countdown puzzles and MIT Integration Bee problems show that models can
provide reliable reward signals without ground truth answers, enabling
reinforcement learning in domains previously not possible. By implementing
self-judging, we achieve significant performance gains maintaining alignment
with formal verification. When combined with synthetic question generation, we
establish a complete self-improvement loop where models generate practice
problems, solve them, and evaluate their own performance-achieving an 8%
improvement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on
integration tasks. Our findings demonstrate that LLM judges can provide
effective reward signals for training models, unlocking many reinforcement
learning environments previously limited by the difficulty of creating
programmatic rewards. This suggests a potential paradigm shift toward AI
systems that continuously improve through self-directed learning rather than
human-guided training, potentially accelerating progress in domains with scarce
training data or complex evaluation requirements.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [8] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 在大规模语言模型（LLMs）中，通过重复采样来扩展推理计算，随着样本数量的增加，覆盖率（解决问题的比例）持续增加。本文推测这种改善部分是由于标准评估基准的答案分布偏向于相对少量的常见答案。实验表明，在数学推理和事实知识两个领域，根据训练集中的答案频率枚举答案的基线方法对于某些LLMs优于重复模型采样，而对于其他模型则与混合策略相当。该基线方法有助于更准确地衡量重复采样在提示无关猜测之外的覆盖率提升。


<details>
  <summary>Details</summary>
Motivation: 研究者观察到在大语言模型中，通过重复采样可以提高问题解决的覆盖率，并推测这可能与评估基准中答案分布的偏斜有关，因此希望验证这一假设并提供一个更准确的测量方式。

Method: 定义了一个基于训练集中答案频率枚举答案的基线方法，并在数学推理和事实知识两个领域进行实验，比较该基线方法与重复模型采样以及混合策略的表现。

Result: 实验结果表明，对于某些LLMs，基线方法优于重复模型采样；对于其他模型，其表现与混合策略相当。这说明重复采样在特定条件下的效果可能被高估。

Conclusion: 通过引入基于训练集答案频率的基线方法，可以更准确地评估重复采样对覆盖率的实际提升，揭示了重复采样效果的部分原因在于评估基准答案分布的偏斜。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [9] [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320)
*Chenxi Whitehouse,Tianlu Wang,Ping Yu,Xian Li,Jason Weston,Ilia Kulikov,Swarnadeep Saha*

Main category: cs.CL

TL;DR: 文章介绍了一种名为J1的强化学习方法，用于训练判断模型。该方法通过可验证奖励激励思考并减轻判断偏差，在多个基准测试中表现优于其他模型。


<details>
  <summary>Details</summary>
Motivation: AI的进步受限于评估质量，强大的LLM-as-a-Judge模型被认为是核心解决方案，因此需要找到最佳方法来训练这些模型以增强其推理能力。

Method: 引入了J1，一种强化学习方法，将可验证和不可验证的提示转换为具有可验证奖励的判断任务，从而激励思考并减轻判断偏差。

Result: J1在8B或70B规模的训练中，表现优于所有其他现有模型，包括从DeepSeek-R1蒸馏出的模型，并且在一些基准上超过了o1-mini和R1，尽管训练的是较小的模型。

Conclusion: 通过学习概述评估标准、与自动生成的参考答案进行比较以及重新评估模型响应的正确性，J1模型能够做出更好的判断。

Abstract: The progress of AI is bottlenecked by the quality of evaluation, and powerful
LLM-as-a-Judge models have proved to be a core solution. Improved judgment
ability is enabled by stronger chain-of-thought reasoning, motivating the need
to find the best recipes for training such models to think. In this work we
introduce J1, a reinforcement learning approach to training such models. Our
method converts both verifiable and non-verifiable prompts to judgment tasks
with verifiable rewards that incentivize thinking and mitigate judgment bias.
In particular, our approach outperforms all other existing 8B or 70B models
when trained at those sizes, including models distilled from DeepSeek-R1. J1
also outperforms o1-mini, and even R1 on some benchmarks, despite training a
smaller model. We provide analysis and ablations comparing Pairwise-J1 vs
Pointwise-J1 models, offline vs online training recipes, reward strategies,
seed prompts, and variations in thought length and content. We find that our
models make better judgments by learning to outline evaluation criteria,
comparing against self-generated reference answers, and re-evaluating the
correctness of model responses.

</details>


### [10] [Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning](https://arxiv.org/abs/2505.10182)
*Yoichi Ishibashi,Taro Yano,Masafumi Oyamada*

Main category: cs.CL

TL;DR: 本文研究了使用合成数据进行持续预训练（Reasoning CPT）对大语言模型推理能力的影响，发现其能有效提升跨领域推理性能，并且随着问题难度增加优势更加明显。


<details>
  <summary>Details</summary>
Motivation: 现有的监督微调和强化学习方法主要适用于特定领域（如数学和编程），限制了训练数据的广度和可扩展性。而持续预训练（CPT）无需任务特定信号，但如何有效生成用于推理的训练数据及其对广泛领域的影响尚未被充分探索。

Method: 通过Reasoning CPT方法，利用从STEM和法律语料库中提取的隐藏思维过程生成合成数据，对Gemma2-9B模型进行训练，并与标准CPT方法在MMLU基准上进行比较。

Result: Reasoning CPT在所有评估领域中均提高了性能，尤其是在最难的问题上表现出了高达8分的提升；并且在一个领域中学到的推理技能可以有效迁移到其他领域。

Conclusion: Reasoning CPT能够显著提升模型的推理能力，并且模型可以根据问题难度调整推理深度，为生成更广泛的推理训练数据提供了新思路。

Abstract: Large Language Models (LLMs) have demonstrated significant improvements in
reasoning capabilities through supervised fine-tuning and reinforcement
learning. However, when training reasoning models, these approaches are
primarily applicable to specific domains such as mathematics and programming,
which imposes fundamental constraints on the breadth and scalability of
training data. In contrast, continual pretraining (CPT) offers the advantage of
not requiring task-specific signals. Nevertheless, how to effectively
synthesize training data for reasoning and how such data affect a wide range of
domains remain largely unexplored. This study provides a detailed evaluation of
Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden
thought processes underlying texts, based on the premise that texts are the
result of the author's thinking process. Specifically, we apply Reasoning CPT
to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and
Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis
reveals that Reasoning CPT consistently improves performance across all
evaluated domains. Notably, reasoning skills acquired in one domain transfer
effectively to others; the performance gap with conventional methods widens as
problem difficulty increases, with gains of up to 8 points on the most
challenging problems. Furthermore, models trained with hidden thoughts learn to
adjust the depth of their reasoning according to problem difficulty.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [11] [Improved Algorithms for Differentially Private Language Model Alignment](https://arxiv.org/abs/2505.08849)
*Keyu Chen,Hao Tang,Qinglin Liu,Yizhao Xu*

Main category: cs.CR

TL;DR: 提出新的隐私保护对齐算法，适用于DPO和RLHF，在适度隐私预算下性能优越。


<details>
  <summary>Details</summary>
Motivation: 确保大语言模型与人类偏好一致，同时解决敏感用户数据带来的隐私问题。

Method: 开发新型隐私保护对齐算法，并在不同隐私预算和模型上分析其有效性；结合DPO和RLHF框架进行实验。

Result: 在适度隐私预算下，DP-AdamW与DPO结合的方法比现有技术提升高达15%的对齐质量。

Conclusion: 提供优化隐私、对齐效果和计算需求之间权衡的实际指导。

Abstract: Language model alignment is crucial for ensuring that large language models
(LLMs) align with human preferences, yet it often involves sensitive user data,
raising significant privacy concerns. While prior work has integrated
differential privacy (DP) with alignment techniques, their performance remains
limited. In this paper, we propose novel algorithms for privacy-preserving
alignment and rigorously analyze their effectiveness across varying privacy
budgets and models. Our framework can be deployed on two celebrated alignment
techniques, namely direct preference optimization (DPO) and reinforcement
learning from human feedback (RLHF). Through systematic experiments on
large-scale language models, we demonstrate that our approach achieves
state-of-the-art performance. Notably, one of our algorithms, DP-AdamW,
combined with DPO, surpasses existing methods, improving alignment quality by
up to 15% under moderate privacy budgets ({\epsilon}=2-5). We further
investigate the interplay between privacy guarantees, alignment efficacy, and
computational demands, providing practical guidelines for optimizing these
trade-offs.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [12] [Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting](https://arxiv.org/abs/2505.09395)
*Chen-Yu Liu,Kuan-Cheng Chen,Yi-Chien Chen,Samuel Yen-Chi Chen,Wei-Hao Huang,Wei-Jia Huang,Yen-Jui Chang*

Main category: quant-ph

TL;DR: 提出了一种名为Quantum Parameter Adaptation (QPA)的新方法，用于高效的台风轨迹预测模型学习。该方法结合了基于注意力机制的Multi-ConvGRU模型，首次将量子机器学习应用于大规模台风轨迹预测中，大幅减少了可训练参数的数量，同时保持了预测精度。


<details>
  <summary>Details</summary>
Motivation: 台风轨迹预测对于灾害准备至关重要，但由于大气动力学的复杂性和深度学习模型对资源的需求，计算成本仍然很高。为了解决这一问题，研究者引入了量子机器学习技术。

Method: 使用Quantum Parameter Adaptation (QPA) 方法与Attention-based Multi-ConvGRU模型相结合，仅在训练期间利用量子神经网络生成可训练参数，推断时无需量子硬件。

Result: 实验结果表明，QPA显著减少了可训练参数的数量，同时保持了预测性能，提供了一种可扩展且节能的气候建模方法。

Conclusion: QPA为高性能量子经典混合学习的台风预报提供了更便捷和可持续的方法，标志着量子机器学习在气候科学中的一个重要应用。

Abstract: Typhoon trajectory forecasting is essential for disaster preparedness but
remains computationally demanding due to the complexity of atmospheric dynamics
and the resource requirements of deep learning models. Quantum-Train (QT), a
hybrid quantum-classical framework that leverages quantum neural networks
(QNNs) to generate trainable parameters exclusively during training,
eliminating the need for quantum hardware at inference time. Building on QT's
success across multiple domains, including image classification, reinforcement
learning, flood prediction, and large language model (LLM) fine-tuning, we
introduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting
model learning. Integrated with an Attention-based Multi-ConvGRU model, QPA
enables parameter-efficient training while maintaining predictive accuracy.
This work represents the first application of quantum machine learning (QML) to
large-scale typhoon trajectory prediction, offering a scalable and
energy-efficient approach to climate modeling. Our results demonstrate that QPA
significantly reduces the number of trainable parameters while preserving
performance, making high-performance forecasting more accessible and
sustainable through hybrid quantum-classical learning.

</details>
