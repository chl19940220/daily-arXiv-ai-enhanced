{"id": "2505.00018", "keyword": "LLM Agent", "pdf": "https://arxiv.org/pdf/2505.00018", "abs": "https://arxiv.org/abs/2505.00018", "authors": ["Ju Wu", "Calvin K. L. Or"], "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability.", "keywords": ["LLM Agent"], "AI": {"tldr": "This paper surveys recent developments in human-AI collaboration, identifies gaps, and proposes a new conceptual framework called Hierarchical Exploration-Exploitation Net to integrate varied studies and inspire future work.", "motivation": "The motivation of this paper is the lack of a unifying theoretical framework that can coherently integrate various studies on human-AI agent collaboration, especially for complex tasks.", "method": "The method involves critically surveying a broad spectrum of recent empirical developments on human-AI agents collaboration and proposing a novel conceptual architecture named Hierarchical Exploration-Exploitation Net which interlinks multi-agent coordination, knowledge management, cybernetic feedback loops, and higher-level control mechanisms.", "result": "By mapping existing contributions onto the proposed framework, the approach facilitates revision of legacy methods and inspires new work that fuses qualitative and quantitative paradigms.", "conclusion": "These insights provide a stepping stone toward deeper co-evolution of human cognition and AI capability, allowing the paper to serve as both a critical review and a forward-looking reference."}}
{"id": "2410.15466", "keyword": "LLM reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing.", "keywords": ["LLM reasoning"], "AI": {"tldr": "\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\uff0c\u901a\u8fc7\u91cd\u590d\u91c7\u6837\u6765\u6269\u5c55\u63a8\u7406\u8ba1\u7b97\uff0c\u53ef\u4ee5\u968f\u7740\u6837\u672c\u6570\u91cf\u7684\u589e\u52a0\u6301\u7eed\u63d0\u9ad8\u8986\u76d6\u7387\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u63d0\u5347\u90e8\u5206\u662f\u7531\u4e8e\u7b54\u6848\u5206\u5e03\u504f\u5411\u5c11\u91cf\u5e38\u89c1\u7b54\u6848\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u67d0\u4e9bLLMs\u4e0a\u4f18\u4e8e\u91cd\u590d\u91c7\u6837\uff0c\u4e14\u80fd\u66f4\u51c6\u786e\u8bc4\u4f30\u91cd\u590d\u91c7\u6837\u7684\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u8005\u89c2\u5bdf\u5230\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u91cd\u590d\u91c7\u6837\u6269\u5927\u63a8\u7406\u8ba1\u7b97\u80fd\u591f\u6301\u7eed\u589e\u52a0\u95ee\u9898\u89e3\u51b3\u7684\u8986\u76d6\u7387\uff0c\u5e76\u63a8\u6d4b\u8fd9\u4e00\u73b0\u8c61\u90e8\u5206\u5f52\u56e0\u4e8e\u6807\u51c6\u8bc4\u4f30\u57fa\u51c6\u4e2d\u7684\u7b54\u6848\u5206\u5e03\u504f\u5411\u5c11\u6570\u5e38\u89c1\u7b54\u6848\u3002\u4e3a\u4e86\u9a8c\u8bc1\u8fd9\u4e2a\u5047\u8bbe\uff0c\u4ed6\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8bad\u7ec3\u96c6\u4e2d\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "method": "\u5b9a\u4e49\u4e00\u4e2a\u57fa\u7ebf\uff0c\u8be5\u57fa\u7ebf\u6839\u636e\u8bad\u7ec3\u96c6\u4e2d\u7b54\u6848\u7684\u51fa\u73b0\u9891\u7387\u679a\u4e3e\u7b54\u6848\u3002\u7136\u540e\u5728\u4e24\u4e2a\u9886\u57df\uff08\u6570\u5b66\u63a8\u7406\u548c\u4e8b\u5b9e\u77e5\u8bc6\uff09\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u6b64\u57fa\u7ebf\u4e0e\u91cd\u590d\u6a21\u578b\u91c7\u6837\u4ee5\u53ca\u6df7\u5408\u7b56\u7565\uff08\u7ed3\u5408\u5c11\u91cf\u6a21\u578b\u91c7\u6837\u548c\u679a\u4e3e\u731c\u6d4b\uff09\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u4e8e\u67d0\u4e9bLLMs\uff0c\u57fa\u7ebf\u65b9\u6cd5\u7684\u8868\u73b0\u4f18\u4e8e\u91cd\u590d\u6a21\u578b\u91c7\u6837\uff1b\u800c\u5bf9\u4e8e\u5176\u4ed6LLMs\uff0c\u5176\u8986\u76d6\u7387\u4e0e\u4e00\u79cd\u6df7\u5408\u7b56\u7565\u76f8\u5f53\uff0c\u8be5\u7b56\u7565\u901a\u8fc7\u4ec5\u4f7f\u752810\u4e2a\u6a21\u578b\u6837\u672c\u5e76\u679a\u4e3e\u5269\u4f59\u7684\u7b54\u6848\u6765\u83b7\u5f97k\u4e2a\u7b54\u6848\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u7ebf\u65b9\u6cd5\u4f7f\u6211\u4eec\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u6d4b\u91cf\u5728\u63d0\u793a\u65e0\u5173\u7684\u731c\u6d4b\u4e4b\u5916\uff0c\u91cd\u590d\u91c7\u6837\u5bf9\u8986\u76d6\u7387\u7684\u6539\u8fdb\u7a0b\u5ea6\u3002\u8fd9\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u91cd\u590d\u91c7\u6837\u5728\u4e0d\u540cLLMs\u4e0a\u7684\u6548\u679c\u3002"}}
{"id": "2505.08189", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.08189", "abs": "https://arxiv.org/abs/2505.08189", "authors": ["Alex Zhihao Dou", "Dongfei Cui", "Jun Yan", "Weida Wang", "Benteng Chen", "Haoming Wang", "Zeke Xie", "Shufei Zhang"], "title": "DSADF: Thinking Fast and Slow for Decision Making", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Although Reinforcement Learning (RL) agents are effective in well-defined\nenvironments, they often struggle to generalize their learned policies to\ndynamic settings due to their reliance on trial-and-error interactions. Recent\nwork has explored applying Large Language Models (LLMs) or Vision Language\nModels (VLMs) to boost the generalization of RL agents through policy\noptimization guidance or prior knowledge. However, these approaches often lack\nseamless coordination between the RL agent and the foundation model, leading to\nunreasonable decision-making in unfamiliar environments and efficiency\nbottlenecks. Making full use of the inferential capabilities of foundation\nmodels and the rapid response capabilities of RL agents and enhancing the\ninteraction between the two to form a dual system is still a lingering\nscientific question. To address this problem, we draw inspiration from\nKahneman's theory of fast thinking (System 1) and slow thinking (System 2),\ndemonstrating that balancing intuition and deep reasoning can achieve nimble\ndecision-making in a complex world. In this study, we propose a Dual-System\nAdaptive Decision Framework (DSADF), integrating two complementary modules:\nSystem 1, comprising an RL agent and a memory space for fast and intuitive\ndecision making, and System 2, driven by a VLM for deep and analytical\nreasoning. DSADF facilitates efficient and adaptive decision-making by\ncombining the strengths of both systems. The empirical study in the video game\nenvironment: Crafter and Housekeep demonstrates the effectiveness of our\nproposed method, showing significant improvements in decision abilities for\nboth unseen and known tasks.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ee3\u7406\u5728\u5b9a\u4e49\u826f\u597d\u7684\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u96be\u4ee5\u5c06\u5176\u5b66\u5230\u7684\u7b56\u7565\u63a8\u5e7f\u5230\u52a8\u6001\u8bbe\u7f6e\u4e2d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u53cc\u7cfb\u7edf\u81ea\u9002\u5e94\u51b3\u7b56\u6846\u67b6\uff08DSADF\uff09\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5feb\u901f\u76f4\u89c2\u51b3\u7b56\u7684RL\u4ee3\u7406\u548c\u6df1\u5ea6\u5206\u6790\u63a8\u7406\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0cDSADF\u4fc3\u8fdb\u4e86\u9ad8\u6548\u548c\u9002\u5e94\u6027\u7684\u51b3\u7b56\u3002\u5b9e\u8bc1\u7814\u7a76\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "RL\u4ee3\u7406\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u73b0\u6709\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u7684\u65b9\u6cd5\u7f3a\u4e4f\u65e0\u7f1d\u534f\u8c03\uff0c\u5bfc\u81f4\u4e0d\u5408\u7406\u51b3\u7b56\u548c\u6548\u7387\u74f6\u9888\u3002", "method": "\u53d7\u5230Kahneman\u7684\u5feb\u601d\u8003\uff08\u7cfb\u7edf1\uff09\u548c\u6162\u601d\u8003\uff08\u7cfb\u7edf2\uff09\u7406\u8bba\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u7cfb\u7edf\u81ea\u9002\u5e94\u51b3\u7b56\u6846\u67b6\uff08DSADF\uff09\u3002\u8be5\u6846\u67b6\u5305\u62ec\u4e24\u4e2a\u4e92\u8865\u6a21\u5757\uff1a\u7cfb\u7edf1\uff08\u7531RL\u4ee3\u7406\u548c\u8bb0\u5fc6\u7a7a\u95f4\u7ec4\u6210\uff0c\u7528\u4e8e\u5feb\u901f\u76f4\u89c2\u51b3\u7b56\uff09\u548c\u7cfb\u7edf2\uff08\u7531VLM\u9a71\u52a8\uff0c\u7528\u4e8e\u6df1\u5ea6\u5206\u6790\u63a8\u7406\uff09\u3002", "result": "\u5728Crafter\u548cHousekeep\u89c6\u9891\u6e38\u620f\u73af\u5883\u4e2d\u7684\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5df2\u77e5\u4efb\u52a1\u548c\u672a\u77e5\u4efb\u52a1\u4e2d\u90fd\u663e\u8457\u63d0\u9ad8\u4e86\u51b3\u7b56\u80fd\u529b\u3002", "conclusion": "DSADF\u901a\u8fc7\u7ed3\u5408\u4e24\u79cd\u7cfb\u7edf\u7684\u4f18\u70b9\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u548c\u9002\u5e94\u6027\u7684\u51b3\u7b56\uff0c\u5e73\u8861\u4e86\u76f4\u89c9\u548c\u6df1\u5165\u63a8\u7406\uff0c\u4ece\u800c\u5728\u590d\u6742\u4e16\u754c\u4e2d\u5b9e\u73b0\u654f\u6377\u51b3\u7b56\u3002"}}
{"id": "2505.07782", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.07782", "abs": "https://arxiv.org/abs/2505.07782", "authors": ["Rushi Qiang", "Yuchen Zhuang", "Yinghao Li", "Dingu Sagar V K", "Rongzhi Zhang", "Changhao Li", "Ian Shu-Hei Wong", "Sherry Yang", "Percy Liang", "Chao Zhang", "Bo Dai"], "title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering", "categories": ["cs.LG"], "comment": null, "summary": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement\nlearning, evaluating, and improving autonomous large language model (LLM)\nagents in iterative machine learning engineering (MLE) workflows. Unlike\nexisting benchmarks that primarily rely on static datasets or single-attempt\nevaluations, MLE-Dojo provides an interactive environment enabling agents to\niteratively experiment, debug, and refine solutions through structured feedback\nloops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,\nopen-ended MLE tasks carefully curated to reflect realistic engineering\nscenarios such as data processing, architecture search, hyperparameter tuning,\nand code debugging. Its fully executable environment supports comprehensive\nagent training via both supervised fine-tuning and reinforcement learning,\nfacilitating iterative experimentation, realistic data sampling, and real-time\noutcome verification. Extensive evaluations of eight frontier LLMs reveal that\nwhile current models achieve meaningful iterative improvements, they still\nexhibit significant limitations in autonomously generating long-horizon\nsolutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's\nflexible and extensible architecture seamlessly integrates diverse data\nsources, tools, and evaluation protocols, uniquely enabling model-based agent\ntuning and promoting interoperability, scalability, and reproducibility. We\nopen-source our framework and benchmarks to foster community-driven innovation\ntowards next-generation MLE agents.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "The paper introduces MLE-Dojo, an interactive framework for training and evaluating autonomous LLM agents in MLE workflows. Built on real-world Kaggle challenges, it supports iterative experimentation and reveals limitations in current LLMs.", "motivation": "To systematically improve autonomous LLM agents through iterative experimentation and structured feedback loops in realistic machine learning engineering scenarios.", "method": "Developed a Gym-style framework named MLE-Dojo based on over 200 real-world Kaggle challenges, providing diverse tasks and a fully executable environment supporting both supervised fine-tuning and reinforcement learning.", "result": "Evaluations of eight frontier LLMs show meaningful iterative improvements but significant limitations in generating long-term solutions and resolving complex errors efficiently.", "conclusion": "MLE-Dojo's flexible architecture promotes interoperability and reproducibility, and the open-source framework aims to drive community innovation for next-generation MLE agents."}}
{"id": "2505.07274", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.07274", "abs": "https://arxiv.org/abs/2505.07274", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains", "categories": ["cs.LG"], "comment": null, "summary": "Integrating large language models (LLMs) as priors in reinforcement learning\n(RL) offers significant advantages but comes with substantial computational\ncosts. We present a principled cache-efficient framework for posterior sampling\nwith LLM-derived priors that dramatically reduces these costs while maintaining\nhigh performance. At the core of our approach is an adaptive caching mechanism,\nwhere cache parameters are meta-optimized using surrogate gradients derived\nfrom policy performance. This design enables efficient inference across both\ndiscrete text environments (e.g., TextWorld, ALFWorld) and continuous control\ndomains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries\nand 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU)\nwhile retaining 96--98\\% of uncached performance. Our theoretical analysis\nprovides KL divergence bounds on approximation quality, validated empirically.\nThe framework extends to offline RL, where our CQL-Prior variant improves\nperformance by 14--29\\% and reduces training time by 38--40\\%. Extensive\nevaluations across a diverse suite of eight tasks demonstrate the\ngeneralizability and practical viability of LLM-guided RL in\nresource-constrained settings.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "The paper introduces a cache-efficient framework for integrating LLMs in RL, reducing computational costs while maintaining performance.", "motivation": "To make the integration of large language models in reinforcement learning more computationally feasible without sacrificing performance.", "method": "An adaptive caching mechanism is used where cache parameters are meta-optimized using surrogate gradients from policy performance. This method applies to both discrete text environments and continuous control domains.", "result": "Achieved 3.8-4.7 times reduction in LLM queries and 4.0-12.0 times lower median latencies with minimal performance loss (96-98% of uncached performance). The CQL-Prior variant improved offline RL performance by 14-29% and reduced training time by 38-40%.", "conclusion": "This principled framework enables efficient inference and demonstrates generalizability across different tasks, making LLM-guided RL practical in resource-constrained settings."}}
{"id": "2505.06335", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.06335", "abs": "https://arxiv.org/abs/2505.06335", "authors": ["Jinsheng Yuan", "Yuhang Hao", "Weisi Guo", "Yun Wu", "Chongyan Gu"], "title": "Remote Rowhammer Attack using Adversarial Observations on Federated Learning Clients", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Federated Learning (FL) has the potential for simultaneous global learning\namongst a large number of parallel agents, enabling emerging AI such as LLMs to\nbe trained across demographically diverse data. Central to this being efficient\nis the ability for FL to perform sparse gradient updates and remote direct\nmemory access at the central server. Most of the research in FL security\nfocuses on protecting data privacy at the edge client or in the communication\nchannels between the client and server. Client-facing attacks on the server are\nless well investigated as the assumption is that a large collective of clients\noffer resilience.\n  Here, we show that by attacking certain clients that lead to a high frequency\nrepetitive memory update in the server, we can remote initiate a rowhammer\nattack on the server memory. For the first time, we do not need backdoor access\nto the server, and a reinforcement learning (RL) attacker can learn how to\nmaximize server repetitive memory updates by manipulating the client's sensor\nobservation. The consequence of the remote rowhammer attack is that we are able\nto achieve bit flips, which can corrupt the server memory. We demonstrate the\nfeasibility of our attack using a large-scale FL automatic speech recognition\n(ASR) systems with sparse updates, our adversarial attacking agent can achieve\naround 70\\% repeated update rate (RUR) in the targeted server model,\neffectively inducing bit flips on server DRAM. The security implications are\nthat can cause disruptions to learning or may inadvertently cause elevated\nprivilege. This paves the way for further research on practical mitigation\nstrategies in FL and hardware design.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "In this paper, the authors explore a new type of attack on Federated Learning (FL) systems where an attacker can remotely initiate a rowhammer attack on the server memory by targeting certain clients. This leads to bit flips that corrupt the server memory, potentially causing disruptions or privilege escalation.", "motivation": "The motivation is to investigate client-facing attacks on FL servers which have been less well researched, specifically focusing on how repetitive memory updates caused by certain clients can lead to remote rowhammer attacks.", "method": "By using a reinforcement learning (RL) attacker that manipulates the client's sensor observations, the method aims at maximizing server repetitive memory updates leading to bit flips in the server DRAM.", "result": "The adversarial attacking agent achieved around 70% repeated update rate (RUR) in the targeted server model, effectively inducing bit flips on server DRAM.", "conclusion": "This research highlights the security implications for FL systems and calls for further investigation into practical mitigation strategies in FL and hardware design."}}
{"id": "2505.06284", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.06284", "abs": "https://arxiv.org/abs/2505.06284", "authors": ["Zhiqiang Wang", "Ruoxi Cheng"], "title": "DMRL: Data- and Model-aware Reward Learning for Data Extraction", "categories": ["cs.LG", "cs.CR"], "comment": "Data- and Model-aware Reward Learning for Data Extraction. arXiv\n  admin note: substantial text overlap with arXiv:2503.18991", "summary": "Large language models (LLMs) are inherently vulnerable to unintended privacy\nbreaches. Consequently, systematic red-teaming research is essential for\ndeveloping robust defense mechanisms. However, current data extraction methods\nsuffer from several limitations: (1) rely on dataset duplicates (addressable\nvia deduplication), (2) depend on prompt engineering (now countered by\ndetection and defense), and (3) rely on random-search adversarial generation.\nTo address these challenges, we propose DMRL, a Data- and Model-aware Reward\nLearning approach for data extraction. This technique leverages inverse\nreinforcement learning to extract sensitive data from LLMs. Our method consists\nof two main components: (1) constructing an introspective reasoning dataset\nthat captures leakage mindsets to guide model behavior, and (2) training reward\nmodels with Group Relative Policy Optimization (GRPO), dynamically tuning\noptimization based on task difficulty at both the data and model levels.\nComprehensive experiments across various LLMs demonstrate that DMRL outperforms\nall baseline methods in data extraction performance.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bb9\u6613\u53d1\u751f\u975e\u6545\u610f\u7684\u9690\u79c1\u6cc4\u9732\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u7684\u7814\u7a76\u6765\u5f00\u53d1\u5f3a\u5927\u7684\u9632\u5fa1\u673a\u5236\u3002\u73b0\u6709\u7684\u6570\u636e\u63d0\u53d6\u65b9\u6cd5\u5b58\u5728\u4e00\u4e9b\u9650\u5236\uff0c\u5982\u4f9d\u8d56\u6570\u636e\u96c6\u526f\u672c\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u968f\u673a\u641c\u7d22\u5bf9\u6297\u751f\u6210\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DMRL\uff0c\u4e00\u79cd\u6570\u636e\u548c\u6a21\u578b\u611f\u77e5\u7684\u5956\u52b1\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4eceLLMs\u4e2d\u63d0\u53d6\u6570\u636e\u3002DMRL\u4f7f\u7528\u9006\u5411\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u7ec4\u6210\u90e8\u5206\uff1a\u6784\u5efa\u5185\u7701\u63a8\u7406\u6570\u636e\u96c6\u4ee5\u6355\u6349\u6cc4\u6f0f\u5fc3\u6001\uff0c\u4ee5\u53ca\u4f7f\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDMRL\u5728\u6570\u636e\u63d0\u53d6\u6027\u80fd\u4e0a\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u53d1\u751f\u975e\u6545\u610f\u7684\u9690\u79c1\u6cc4\u9732\uff0c\u800c\u5f53\u524d\u7684\u6570\u636e\u63d0\u53d6\u65b9\u6cd5\u5b58\u5728\u591a\u79cd\u5c40\u9650\u6027\uff0c\u5305\u62ec\u4f9d\u8d56\u6570\u636e\u96c6\u526f\u672c\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u968f\u673a\u641c\u7d22\u5bf9\u6297\u751f\u6210\u7b49\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u8981\u4e48\u53ef\u4ee5\u901a\u8fc7\u53bb\u91cd\u89e3\u51b3\uff0c\u8981\u4e48\u5df2\u7ecf\u88ab\u68c0\u6d4b\u548c\u9632\u5fa1\u624b\u6bb5\u6240\u5e94\u5bf9\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u66f4\u6709\u6548\u5730\u8fdb\u884c\u6570\u636e\u63d0\u53d6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDMRL\uff08Data- and Model-aware Reward Learning\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u9006\u5411\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u6570\u636e\u63d0\u53d6\u3002\u5177\u4f53\u6765\u8bf4\uff0cDMRL\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u90e8\u5206\uff1a1) \u6784\u5efa\u4e00\u4e2a\u5185\u7701\u63a8\u7406\u6570\u636e\u96c6\uff0c\u7528\u6765\u6355\u6349\u53ef\u80fd\u5bfc\u81f4\u6570\u636e\u6cc4\u6f0f\u7684\u5fc3\u6001\uff0c\u4ece\u800c\u6307\u5bfc\u6a21\u578b\u884c\u4e3a\uff1b2) \u4f7f\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u6839\u636e\u4efb\u52a1\u5728\u6570\u636e\u548c\u6a21\u578b\u5c42\u9762\u7684\u96be\u5ea6\u52a8\u6001\u8c03\u6574\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0cDMRL\u5728\u5404\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u6570\u636e\u63d0\u53d6\u6027\u80fd\u5747\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DMRL\u662f\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u63d0\u53d6\u65b9\u6cd5\uff0c\u80fd\u591f\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u8bf8\u591a\u9650\u5236\uff0c\u5e76\u4e14\u5728\u591a\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.04842", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.04842", "abs": "https://arxiv.org/abs/2505.04842", "authors": ["Kusha Sareen", "Morgane M Moss", "Alessandro Sordoni", "Rishabh Agarwal", "Arian Hosseini"], "title": "Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners,\nsuch as GRPO or Leave-one-out PPO, abandon the learned value function in favor\nof empirically estimated returns. This hinders test-time compute scaling that\nrelies on using the value-function for verification. In this work, we propose\nRL$^V$ that augments any ``value-free'' RL method by jointly training the LLM\nas both a reasoner and a generative verifier using RL-generated data, adding\nverification capabilities without significant overhead. Empirically, RL$^V$\nboosts MATH accuracy by over 20\\% with parallel sampling and enables\n$8-32\\times$ efficient test-time compute scaling compared to the base RL\nmethod. RL$^V$ also exhibits strong generalization capabilities for both\neasy-to-hard and out-of-domain tasks. Furthermore, RL$^V$ achieves\n$1.2-1.6\\times$ higher performance when jointly scaling parallel and sequential\ntest-time compute with a long reasoning R1 model.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u63d0\u51faRL^V\u65b9\u6cd5\uff0c\u589e\u5f3a\u65e0\u4ef7\u503c\u51fd\u6570\u7684\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u63d0\u5347\u63a8\u7406\u548c\u9a8c\u8bc1\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u6570\u5b66\u7cbe\u5ea6\u3001\u6d4b\u8bd5\u65f6\u95f4\u8ba1\u7b97\u6269\u5c55\u6548\u7387\u53ca\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u7684\u5fae\u8c03LLM\u63a8\u7406\u5668\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982GRPO\u6216Leave-one-out PPO\uff09\u820d\u5f03\u4e86\u5df2\u5b66\u4e60\u7684\u4ef7\u503c\u51fd\u6570\uff0c\u8f6c\u800c\u4f7f\u7528\u7ecf\u9a8c\u4f30\u8ba1\u56de\u62a5\uff0c\u8fd9\u963b\u788d\u4e86\u4f9d\u8d56\u4ef7\u503c\u51fd\u6570\u8fdb\u884c\u9a8c\u8bc1\u7684\u6d4b\u8bd5\u65f6\u95f4\u8ba1\u7b97\u6269\u5c55\u3002", "method": "\u63d0\u51faRL^V\u65b9\u6cd5\uff0c\u5c06\u4efb\u610f\u65e0\u4ef7\u503c\u51fd\u6570\u7684\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u589e\u5f3a\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3LLM\u4f5c\u4e3a\u63a8\u7406\u5668\u548c\u751f\u6210\u5f0f\u9a8c\u8bc1\u5668\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u7684\u6570\u636e\u6dfb\u52a0\u9a8c\u8bc1\u529f\u80fd\uff0c\u4e14\u4e0d\u589e\u52a0\u663e\u8457\u5f00\u9500\u3002", "result": "RL^V\u5728\u5e76\u884c\u91c7\u6837\u7684\u60c5\u51b5\u4e0b\u5c06\u6570\u5b66\u51c6\u786e\u7387\u63d0\u9ad8\u4e8620%\u4ee5\u4e0a\uff0c\u76f8\u6bd4\u57fa\u7840RL\u65b9\u6cd5\u5b9e\u73b0\u4e868-32\u500d\u66f4\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u95f4\u8ba1\u7b97\u6269\u5c55\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\uff0c\u5305\u62ec\u4ece\u7b80\u5355\u5230\u590d\u6742\u4ee5\u53ca\u9886\u57df\u5916\u7684\u4efb\u52a1\uff1b\u5f53\u4e0e\u957f\u63a8\u7406R1\u6a21\u578b\u8054\u5408\u6269\u5c55\u5e76\u884c\u548c\u987a\u5e8f\u6d4b\u8bd5\u65f6\u95f4\u8ba1\u7b97\u65f6\uff0c\u6027\u80fd\u9ad8\u51fa1.2-1.6\u500d\u3002", "conclusion": "RL^V\u65b9\u6cd5\u6709\u6548\u589e\u5f3a\u4e86\u65e0\u4ef7\u503c\u51fd\u6570\u7684\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u63d0\u4f9b\u4e86\u9a8c\u8bc1\u80fd\u529b\uff0c\u63d0\u5347\u4e86\u6d4b\u8bd5\u65f6\u95f4\u8ba1\u7b97\u6269\u5c55\u6548\u7387\u548c\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.03439", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.03439", "abs": "https://arxiv.org/abs/2505.03439", "authors": ["Artem Karpov", "Tinuade Adeleke", "Seong Hah Cho", "Natalia Perez-Campanero"], "title": "The Steganographic Potentials of Language Models", "categories": ["cs.AI", "cs.CR", "cs.LG"], "comment": "Published at Building Trust Workshop at ICLR 2025", "summary": "The potential for large language models (LLMs) to hide messages within plain\ntext (steganography) poses a challenge to detection and thwarting of unaligned\nAI agents, and undermines faithfulness of LLMs reasoning. We explore the\nsteganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)\nto: (1) develop covert encoding schemes, (2) engage in steganography when\nprompted, and (3) utilize steganography in realistic scenarios where hidden\nreasoning is likely, but not prompted. In these scenarios, we detect the\nintention of LLMs to hide their reasoning as well as their steganography\nperformance. Our findings in the fine-tuning experiments as well as in\nbehavioral non fine-tuning evaluations reveal that while current models exhibit\nrudimentary steganographic abilities in terms of security and capacity,\nexplicit algorithmic guidance markedly enhances their capacity for information\nconcealment.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "The abstract explores the steganographic capabilities of LLMs fine-tuned via reinforcement learning, revealing rudimentary abilities in security and capacity that can be enhanced with algorithmic guidance.", "motivation": "To understand and address the challenge posed by LLMs' ability to hide messages within plain text, undermining the faithfulness of their reasoning.", "method": "Fine-tuning LLMs via reinforcement learning to develop covert encoding schemes, engage in steganography when prompted, and utilize it in realistic scenarios. Detecting LLMs' intention to hide reasoning and evaluating their steganography performance.", "result": "Current models exhibit basic steganographic abilities which can be significantly improved with explicit algorithmic guidance.", "conclusion": "LLMs have potential steganographic capabilities that need to be understood and managed to ensure the faithfulness of their reasoning."}}
{"id": "2505.03209", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.03209", "abs": "https://arxiv.org/abs/2505.03209", "authors": ["Borui Wang", "Kathleen McKeown", "Rex Ying"], "title": "DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning from expert demonstrations has long remained a\nchallenging research problem, and existing state-of-the-art methods using\nbehavioral cloning plus further RL training often suffer from poor\ngeneralization, low sample efficiency, and poor model interpretability.\nInspired by the strong reasoning abilities of large language models (LLMs), we\npropose a novel strategy-based reinforcement learning framework integrated with\nLLMs called DYnamic STrategy Induction with Llms for reinforcement learning\n(DYSTIL) to overcome these limitations. DYSTIL dynamically queries a\nstrategy-generating LLM to induce textual strategies based on advantage\nestimations and expert demonstrations, and gradually internalizes induced\nstrategies into the RL agent through policy optimization to improve its\nperformance through boosting policy generalization and enhancing sample\nefficiency. It also provides a direct textual channel to observe and interpret\nthe evolution of the policy's underlying strategies during training. We test\nDYSTIL over challenging RL environments from Minigrid and BabyAI, and\nempirically demonstrate that DYSTIL significantly outperforms state-of-the-art\nbaseline methods by 17.75% in average success rate while also enjoying higher\nsample efficiency during the learning process.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "The paper introduces DYSTIL, a new reinforcement learning framework combined with LLMs that enhances policy generalization and sample efficiency, outperforming current methods by 17.75% in average success rate.", "motivation": "Reinforcement learning from expert demonstrations is challenging due to poor generalization, low sample efficiency, and poor model interpretability of existing methods.", "method": "DYSTIL dynamically queries a strategy-generating LLM to induce textual strategies based on advantage estimations and expert demonstrations, then internalizes these strategies into the RL agent through policy optimization.", "result": "Empirical tests on Minigrid and BabyAI environments show DYSTIL outperforms state-of-the-art baselines by 17.75% in average success rate with higher sample efficiency.", "conclusion": "DYSTIL provides a promising approach to improve reinforcement learning performance, generalization, and interpretability."}}
{"id": "2505.03181", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.03181", "abs": "https://arxiv.org/abs/2505.03181", "authors": ["Jake Grigsby", "Yuke Zhu", "Michael Ryoo", "Juan Carlos Niebles"], "title": "VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making", "categories": ["cs.LG"], "comment": "SSI-FM Workshop ICLR 2025", "summary": "Recent research looks to harness the general knowledge and reasoning of large\nlanguage models (LLMs) into agents that accomplish user-specified goals in\ninteractive environments. Vision-language models (VLMs) extend LLMs to\nmulti-modal data and provide agents with the visual reasoning necessary for new\napplications in areas such as computer automation. However, agent tasks\nemphasize skills where accessible open-weight VLMs lag behind their LLM\nequivalents. For example, VLMs are less capable of following an environment's\nstrict output syntax requirements and are more focused on open-ended question\nanswering. Overcoming these limitations requires supervised fine-tuning (SFT)\non task-specific expert demonstrations. Our work approaches these challenges\nfrom an offline-to-online reinforcement learning (RL) perspective. RL lets us\nfine-tune VLMs to agent tasks while learning from the unsuccessful decisions of\nour own model or more capable (larger) models. We explore an off-policy RL\nsolution that retains the stability and simplicity of the widely used SFT\nworkflow while allowing our agent to self-improve and learn from low-quality\ndatasets. We demonstrate this technique with two open-weight VLMs across three\nmulti-modal agent domains.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "Recent research aims to integrate the capabilities of large language models (LLMs) and vision-language models (VLMs) into agents for interactive environments. While VLMs lag behind LLMs in certain skills, supervised fine-tuning (SFT) can help overcome these limitations. This work explores an off-policy reinforcement learning (RL) solution that allows VLMs to self-improve and learn from low-quality datasets, demonstrating this technique with two open-weight VLMs across three multi-modal agent domains.", "motivation": "To enhance the abilities of vision-language models (VLMs) in interactive environments so they can accomplish user-specified goals as effectively as large language models (LLMs).", "method": "Using an off-policy reinforcement learning (RL) approach to fine-tune VLMs, allowing them to learn from both successful and unsuccessful decisions, including those from larger models. This method retains the stability and simplicity of the supervised fine-tuning (SFT) workflow while enabling self-improvement.", "result": "Demonstrated the effectiveness of the off-policy RL technique using two open-weight VLMs across three multi-modal agent domains, showing potential for VLMs to improve and perform better on specific tasks.", "conclusion": "An off-policy RL solution provides a viable path for enhancing VLM capabilities in multi-modal agent domains, allowing them to learn effectively from various data sources and improve their performance."}}
{"id": "2505.02722", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.02722", "abs": "https://arxiv.org/abs/2505.02722", "authors": ["Junu Kim", "Chaeeun Shim", "Sungjin Park", "Su Yeon Lee", "Gee Young Suh", "Chae-Man Lim", "Seong Jin Choi", "Song Mi Moon", "Kyoung-Ho Song", "Eu Suk Kim", "Hong Bin Kim", "Sejoong Kim", "Chami Im", "Dong-Wan Kang", "Yong Soo Kim", "Hee-Joon Bae", "Sung Yoon Lim", "Han-Gil Jeong", "Edward Choi"], "title": "Enhancing LLMs' Clinical Reasoning with Real-World Data from a Nationwide Sepsis Registry", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Although large language models (LLMs) have demonstrated impressive reasoning\ncapabilities across general domains, their effectiveness in real-world clinical\npractice remains limited. This is likely due to their insufficient exposure to\nreal-world clinical data during training, as such data is typically not\nincluded due to privacy concerns. To address this, we propose enhancing the\nclinical reasoning capabilities of LLMs by leveraging real-world clinical data.\nWe constructed reasoning-intensive questions from a nationwide sepsis registry\nand fine-tuned Phi-4 on these questions using reinforcement learning, resulting\nin C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the\nin-domain test set, as evidenced by both quantitative metrics and expert\nevaluations. Furthermore, its enhanced reasoning capabilities generalized to a\nsepsis dataset involving different tasks and patient cohorts, an open-ended\nconsultations on antibiotics use task, and other diseases. Future research\nshould focus on training LLMs with large-scale, multi-disease clinical datasets\nto develop more powerful, general-purpose clinical reasoning models.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "Although LLMs show good reasoning abilities in general domains, their effectiveness in clinical practice is limited. The researchers propose enhancing the clinical reasoning capabilities of LLMs by using real-world clinical data. They constructed questions from a sepsis registry and fine-tuned Phi-4 with reinforcement learning to create C-Reason, which showed strong clinical reasoning abilities.", "motivation": "To improve the clinical reasoning capabilities of large language models that have been limited due to insufficient exposure to real-world clinical data during training because of privacy concerns.", "method": "Constructed reasoning-intensive questions from a nationwide sepsis registry and fine-tuned Phi-4 on these questions using reinforcement learning to produce C-Reason.", "result": "C-Reason exhibited strong clinical reasoning capabilities on the in-domain test set and its enhanced reasoning capabilities generalized to other tasks and diseases.", "conclusion": "Future research should focus on training LLMs with large-scale, multi-disease clinical datasets to develop more powerful, general-purpose clinical reasoning models."}}
