<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 7]
- [cs.LG](#cs.LG) [Total: 17]
- [cs.CL](#cs.CL) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: This paper surveys recent developments in human-AI collaboration, pointing out achievements and gaps. It proposes a new conceptual architecture (Hierarchical Exploration-Exploitation Net) to integrate varied studies and facilitate new work fusing qualitative and quantitative paradigms.


<details>
  <summary>Details</summary>
Motivation: There is a lack of a unifying theoretical framework that can coherently integrate various studies on human-AI agents collaboration, particularly for open-ended, complex tasks.

Method: The paper proposes a novel conceptual architecture called Hierarchical Exploration-Exploitation Net that interlinks multi-agent coordination, knowledge management, cybernetic feedback loops, and higher-level control mechanisms.

Result: This approach allows mapping existing contributions onto the proposed framework, facilitating revision of legacy methods and inspiring new work that combines different paradigms.

Conclusion: The insights provided offer a stepping stone toward deeper co-evolution of human cognition and AI capability.

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [2] [The Steganographic Potentials of Language Models](https://arxiv.org/abs/2505.03439)
*Artem Karpov,Tinuade Adeleke,Seong Hah Cho,Natalia Perez-Campanero*

Main category: cs.AI

TL;DR: Large Language Models (LLMs) can potentially hide messages within texts through steganography, which is a challenge to detection and faithful reasoning. This study explores the steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL). The findings reveal that current models have basic steganographic abilities, but explicit algorithmic guidance can significantly enhance their information concealment capacity.


<details>
  <summary>Details</summary>
Motivation: To understand and address the potential risk of LLMs using steganography to hide messages, which could undermine the faithfulness of their reasoning and pose challenges to detecting unaligned AI agents.

Method: The research involves fine-tuning LLMs via reinforcement learning to explore three aspects: developing covert encoding schemes, engaging in steganography when prompted, and utilizing steganography in realistic scenarios without being prompted. Evaluations are conducted both in fine-tuning experiments and behavioral non fine-tuning evaluations.

Result: Current LLMs show basic steganographic abilities in terms of security and capacity. However, with explicit algorithmic guidance, there is a significant improvement in their capacity for information concealment.

Conclusion: While current LLMs have rudimentary steganographic abilities, they can be greatly enhanced with proper algorithmic guidance. This implies the need for further research and development in ensuring the faithfulness and alignment of LLMs.

Abstract: The potential for large language models (LLMs) to hide messages within plain
text (steganography) poses a challenge to detection and thwarting of unaligned
AI agents, and undermines faithfulness of LLMs reasoning. We explore the
steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)
to: (1) develop covert encoding schemes, (2) engage in steganography when
prompted, and (3) utilize steganography in realistic scenarios where hidden
reasoning is likely, but not prompted. In these scenarios, we detect the
intention of LLMs to hide their reasoning as well as their steganography
performance. Our findings in the fine-tuning experiments as well as in
behavioral non fine-tuning evaluations reveal that while current models exhibit
rudimentary steganographic abilities in terms of security and capacity,
explicit algorithmic guidance markedly enhances their capacity for information
concealment.

</details>


### [3] [Enhancing LLMs' Clinical Reasoning with Real-World Data from a Nationwide Sepsis Registry](https://arxiv.org/abs/2505.02722)
*Junu Kim,Chaeeun Shim,Sungjin Park,Su Yeon Lee,Gee Young Suh,Chae-Man Lim,Seong Jin Choi,Song Mi Moon,Kyoung-Ho Song,Eu Suk Kim,Hong Bin Kim,Sejoong Kim,Chami Im,Dong-Wan Kang,Yong Soo Kim,Hee-Joon Bae,Sung Yoon Lim,Han-Gil Jeong,Edward Choi*

Main category: cs.AI

TL;DR: Large Language Models (LLMs) show great potential but lack effectiveness in clinical practice due to limited exposure to real-world clinical data. This study proposes enhancing LLMs' clinical reasoning through real-world clinical data, specifically by constructing reasoning-intensive questions from a sepsis registry and fine-tuning Phi-4 using reinforcement learning, leading to the creation of C-Reason. C-Reason demonstrates strong clinical reasoning capabilities and generalizes well across different datasets and tasks.


<details>
  <summary>Details</summary>
Motivation: To improve the clinical reasoning capabilities of LLMs by addressing their insufficient exposure to real-world clinical data during training.

Method: Constructed reasoning-intensive questions from a nationwide sepsis registry and fine-tuned Phi-4 on these questions using reinforcement learning, resulting in C-Reason.

Result: C-Reason exhibited strong clinical reasoning capabilities not only on the in-domain test set but also generalized to other datasets involving different tasks and patient cohorts.

Conclusion: Future research should focus on training LLMs with large-scale, multi-disease clinical datasets to develop more powerful, general-purpose clinical reasoning models.

Abstract: Although large language models (LLMs) have demonstrated impressive reasoning
capabilities across general domains, their effectiveness in real-world clinical
practice remains limited. This is likely due to their insufficient exposure to
real-world clinical data during training, as such data is typically not
included due to privacy concerns. To address this, we propose enhancing the
clinical reasoning capabilities of LLMs by leveraging real-world clinical data.
We constructed reasoning-intensive questions from a nationwide sepsis registry
and fine-tuned Phi-4 on these questions using reinforcement learning, resulting
in C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the
in-domain test set, as evidenced by both quantitative metrics and expert
evaluations. Furthermore, its enhanced reasoning capabilities generalized to a
sepsis dataset involving different tasks and patient cohorts, an open-ended
consultations on antibiotics use task, and other diseases. Future research
should focus on training LLMs with large-scale, multi-disease clinical datasets
to develop more powerful, general-purpose clinical reasoning models.

</details>


### [4] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: 本文综述了人类与AI协作的最新实证进展，提出了一种新的概念架构（分层探索-利用网络），以系统地整合多智能体协调、知识管理、反馈循环和高层控制机制，并为未来设计或扩展人机共生提供参考。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个统一的理论框架来整合各种关于人机协作的研究，尤其是在处理开放性复杂任务时。

Method: 作者提出了一个名为分层探索-利用网络（Hierarchical Exploration-Exploitation Net）的新概念架构，将现有的贡献（包括符号AI技术、基于LLM的智能体以及混合组织实践）映射到该框架上，从而促进传统方法的修订并激发融合定性和定量范式的新工作。

Result: 通过使用新提出的框架，文章能够从不同角度重新审视现有研究，并为人机协作的未来发展提供了方向。

Conclusion: 这些见解为人类认知和AI能力的更深层次共进化提供了基础。

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [5] [The Steganographic Potentials of Language Models](https://arxiv.org/abs/2505.03439)
*Artem Karpov,Tinuade Adeleke,Seong Hah Cho,Natalia Perez-Campanero*

Main category: cs.AI

TL;DR: 研究探讨了通过强化学习微调的大语言模型（LLMs）在隐写术方面的能力，包括开发隐蔽编码方案、按提示进行隐写以及在实际场景中隐藏推理。实验表明，当前模型的隐写能力有限，但算法指导能显著提升其信息隐藏能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型可能利用文本隐藏信息（隐写术），这对检测和阻止未对齐的AI代理构成挑战，并影响LLM推理的可靠性。因此，研究其隐写能力至关重要。

Method: 通过对LLMs进行强化学习微调，研究其在三种情况下的隐写能力：1) 开发隐蔽编码方案；2) 按提示进行隐写；3) 在实际场景中隐藏推理。同时检测模型隐藏推理的意图及其隐写性能。

Result: 实验结果表明，当前LLMs的隐写能力在安全性和容量方面较为基础，但在算法指导下，其信息隐藏能力显著增强。

Conclusion: 虽然当前LLMs的隐写能力有限，但通过适当的算法指导和训练，可以显著提高其信息隐藏能力，这为未来研究提供了方向。

Abstract: The potential for large language models (LLMs) to hide messages within plain
text (steganography) poses a challenge to detection and thwarting of unaligned
AI agents, and undermines faithfulness of LLMs reasoning. We explore the
steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)
to: (1) develop covert encoding schemes, (2) engage in steganography when
prompted, and (3) utilize steganography in realistic scenarios where hidden
reasoning is likely, but not prompted. In these scenarios, we detect the
intention of LLMs to hide their reasoning as well as their steganography
performance. Our findings in the fine-tuning experiments as well as in
behavioral non fine-tuning evaluations reveal that while current models exhibit
rudimentary steganographic abilities in terms of security and capacity,
explicit algorithmic guidance markedly enhances their capacity for information
concealment.

</details>


### [6] [Enhancing LLMs' Clinical Reasoning with Real-World Data from a Nationwide Sepsis Registry](https://arxiv.org/abs/2505.02722)
*Junu Kim,Chaeeun Shim,Sungjin Park,Su Yeon Lee,Gee Young Suh,Chae-Man Lim,Seong Jin Choi,Song Mi Moon,Kyoung-Ho Song,Eu Suk Kim,Hong Bin Kim,Sejoong Kim,Chami Im,Dong-Wan Kang,Yong Soo Kim,Hee-Joon Bae,Sung Yoon Lim,Han-Gil Jeong,Edward Choi*

Main category: cs.AI

TL;DR: 尽管大型语言模型（LLMs）在通用领域表现出强大的推理能力，但在真实临床实践中的效果有限。这是由于训练时缺乏接触真实临床数据的机会（主要出于隐私考虑）。为此，研究提出通过利用真实临床数据来增强LLM的临床推理能力。具体方法是构建了一个来自全国败血症登记处的强化推理问题集，并使用强化学习对Phi-4进行微调，从而生成C-Reason。C-Reason在领域内测试集中表现出强大的临床推理能力，且其增强的推理能力能够推广到其他任务和疾病中。未来研究应关注使用大规模、多疾病的临床数据集训练LLM，以开发更强大、通用的临床推理模型。


<details>
  <summary>Details</summary>
Motivation: 为了克服大型语言模型在临床实践中因缺乏真实临床数据而导致的有效性不足的问题，提升其在医疗领域的推理能力。

Method: 从全国败血症登记处构建推理密集型问题，并使用强化学习对Phi-4进行微调，创建出名为C-Reason的模型。

Result: C-Reason在领域内测试集中表现出色，不仅在定量指标上优秀，还通过了专家评估。其推理能力可以推广到不同任务、患者群体以及抗生素使用的开放式咨询任务和其他疾病中。

Conclusion: 未来的研究应致力于使用大规模、多疾病的临床数据集来训练LLM，从而开发更强大、通用的临床推理模型。

Abstract: Although large language models (LLMs) have demonstrated impressive reasoning
capabilities across general domains, their effectiveness in real-world clinical
practice remains limited. This is likely due to their insufficient exposure to
real-world clinical data during training, as such data is typically not
included due to privacy concerns. To address this, we propose enhancing the
clinical reasoning capabilities of LLMs by leveraging real-world clinical data.
We constructed reasoning-intensive questions from a nationwide sepsis registry
and fine-tuned Phi-4 on these questions using reinforcement learning, resulting
in C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the
in-domain test set, as evidenced by both quantitative metrics and expert
evaluations. Furthermore, its enhanced reasoning capabilities generalized to a
sepsis dataset involving different tasks and patient cohorts, an open-ended
consultations on antibiotics use task, and other diseases. Future research
should focus on training LLMs with large-scale, multi-disease clinical datasets
to develop more powerful, general-purpose clinical reasoning models.

</details>


### [7] [Automated Meta Prompt Engineering for Alignment with the Theory of Mind](https://arxiv.org/abs/2505.09024)
*Aaron Baughman,Rahul Agarwal,Eduardo Morales,Gozde Akay*

Main category: cs.AI

TL;DR: 本文介绍了一种元提示（meta-prompting）方法，通过联合生成流畅文本优化大型语言模型（LLM）与人类心理预期之间的神经状态相似性。使用代理强化学习技术，让一个作为评判者的LLM（LLMaaJ）通过上下文学习教导另一个LLM生成内容，同时考虑预期和非预期的文本特征。通过对用户在2024年美国网球公开赛上对AI生成文章的修改，LLMaaJ能够解决心智理论（ToM）对齐问题，预测并包含人类编辑的内容。实验结果表明，在平均4.38次迭代后，AI与人类内容评审者的期望对齐率达到53.8%。通过在希尔伯特向量空间中对内容特征（如事实性、新颖性、重复性和相关性）进行几何解释，结合所有特征重要性的空间体积与单个特征相关的顶点对齐，LLMaaJ成功优化了人类ToM，并提升了网球赛事报道的内容质量。该技术已在2024年美国网球公开赛及其他体育和娱乐现场活动中部署使用。


<details>
  <summary>Details</summary>
Motivation: 研究者希望提升大型语言模型生成内容的质量，使其更符合人类的心理预期。具体来说，他们试图解决如何让LLM生成的内容不仅具有良好的文本流畅度，还能准确反映人类编辑可能进行的修改，从而实现心智理论（ToM）对齐。这将有助于提高生成内容的相关性和准确性，特别是在实时报道等需要高度精确和快速响应的场景中。

Method: 提出了一种元提示方法，结合代理强化学习技术，利用一个LLM作为评判者（LLMaaJ），通过上下文学习指导另一个LLM生成内容。在此过程中，LLMaaJ分析生成文本的预期和非预期特征，并根据人类用户的修改调整生成策略。此外，研究还引入了希尔伯特向量空间的几何解释，以量化内容特征（如事实性、新颖性等），并通过优化这些特征的空间分布来改进LLM生成内容的质量。

Result: 实验结果表明，在2024年美国网球公开赛的实际应用中，AI生成内容与人类评审者期望的对齐率达到了53.8%，平均每次内容生成需经过4.38次迭代。通过优化内容特征的空间分布，LLM生成的内容质量得到了显著提升，特别是扩展了对网球赛事动作的报道覆盖范围。

Conclusion: 本研究表明，通过元提示和代理强化学习技术，可以有效解决LLM与人类心智理论（ToM）对齐的问题。这种方法不仅提高了生成内容的质量，还为其他体育和娱乐现场活动提供了可扩展的技术解决方案。未来的研究可以进一步探索如何减少迭代次数并提高对齐率。

Abstract: We introduce a method of meta-prompting that jointly produces fluent text for
complex tasks while optimizing the similarity of neural states between a
human's mental expectation and a Large Language Model's (LLM) neural
processing. A technique of agentic reinforcement learning is applied, in which
an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,
how to produce content by interpreting the intended and unintended generated
text traits. To measure human mental beliefs around content production, users
modify long form AI-generated text articles before publication at the US Open
2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)
alignment problem by anticipating and including human edits within the creation
of text from an LLM. Throughout experimentation and by interpreting the results
of a live production system, the expectations of human content reviewers had
100% of alignment with AI 53.8% of the time with an average iteration count of
4.38. The geometric interpretation of content traits such as factualness,
novelty, repetitiveness, and relevancy over a Hilbert vector space combines
spatial volume (all trait importance) with vertices alignment (individual trait
relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an
increase in content quality by extending the coverage of tennis action. Our
work that was deployed at the US Open 2024 has been used across other live
events within sports and entertainment.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [DSADF: Thinking Fast and Slow for Decision Making](https://arxiv.org/abs/2505.08189)
*Alex Zhihao Dou,Dongfei Cui,Jun Yan,Weida Wang,Benteng Chen,Haoming Wang,Zeke Xie,Shufei Zhang*

Main category: cs.LG

TL;DR: To solve the generalization problem of RL agents in dynamic settings, this paper proposes DSADF which integrates fast intuitive decision making (System 1) and deep analytical reasoning (System 2). Empirical results show significant improvements in decision-making abilities.


<details>
  <summary>Details</summary>
Motivation: RL agents struggle to generalize their learned policies to dynamic settings. Using LLMs or VLMs has been explored but lacks seamless coordination between RL agents and foundation models.

Method: Propose DSADF integrating two complementary modules: System 1 with an RL agent and memory space for fast decisions, and System 2 driven by a VLM for deep reasoning.

Result: Empirical study in Crafter and Housekeep environments demonstrates significant improvements in decision-making abilities for both unseen and known tasks.

Conclusion: DSADF facilitates efficient and adaptive decision-making by combining the strengths of fast intuitive decision making and deep analytical reasoning.

Abstract: Although Reinforcement Learning (RL) agents are effective in well-defined
environments, they often struggle to generalize their learned policies to
dynamic settings due to their reliance on trial-and-error interactions. Recent
work has explored applying Large Language Models (LLMs) or Vision Language
Models (VLMs) to boost the generalization of RL agents through policy
optimization guidance or prior knowledge. However, these approaches often lack
seamless coordination between the RL agent and the foundation model, leading to
unreasonable decision-making in unfamiliar environments and efficiency
bottlenecks. Making full use of the inferential capabilities of foundation
models and the rapid response capabilities of RL agents and enhancing the
interaction between the two to form a dual system is still a lingering
scientific question. To address this problem, we draw inspiration from
Kahneman's theory of fast thinking (System 1) and slow thinking (System 2),
demonstrating that balancing intuition and deep reasoning can achieve nimble
decision-making in a complex world. In this study, we propose a Dual-System
Adaptive Decision Framework (DSADF), integrating two complementary modules:
System 1, comprising an RL agent and a memory space for fast and intuitive
decision making, and System 2, driven by a VLM for deep and analytical
reasoning. DSADF facilitates efficient and adaptive decision-making by
combining the strengths of both systems. The empirical study in the video game
environment: Crafter and Housekeep demonstrates the effectiveness of our
proposed method, showing significant improvements in decision abilities for
both unseen and known tasks.

</details>


### [9] [MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering](https://arxiv.org/abs/2505.07782)
*Rushi Qiang,Yuchen Zhuang,Yinghao Li,Dingu Sagar V K,Rongzhi Zhang,Changhao Li,Ian Shu-Hei Wong,Sherry Yang,Percy Liang,Chao Zhang,Bo Dai*

Main category: cs.LG

TL;DR: The paper presents MLE-Dojo, a framework for reinforcement learning and evaluating autonomous LLM agents in MLE workflows. It provides an interactive environment based on real-world Kaggle challenges, supporting comprehensive agent training and revealing limitations of current LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks primarily rely on static datasets or single-attempt evaluations, lacking the ability to provide structured feedback loops for iterative improvement.

Method: MLE-Dojo is built upon 200+ real-world Kaggle challenges, offering an interactive environment that supports both supervised fine-tuning and reinforcement learning for agent training.

Result: Evaluations show that while current LLMs can achieve meaningful iterative improvements, they still have significant limitations in generating long-term solutions and resolving complex errors efficiently.

Conclusion: MLE-Dojo's flexible architecture promotes interoperability, scalability, and reproducibility, and it is open-sourced to foster community-driven innovation.

Abstract: We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement
learning, evaluating, and improving autonomous large language model (LLM)
agents in iterative machine learning engineering (MLE) workflows. Unlike
existing benchmarks that primarily rely on static datasets or single-attempt
evaluations, MLE-Dojo provides an interactive environment enabling agents to
iteratively experiment, debug, and refine solutions through structured feedback
loops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,
open-ended MLE tasks carefully curated to reflect realistic engineering
scenarios such as data processing, architecture search, hyperparameter tuning,
and code debugging. Its fully executable environment supports comprehensive
agent training via both supervised fine-tuning and reinforcement learning,
facilitating iterative experimentation, realistic data sampling, and real-time
outcome verification. Extensive evaluations of eight frontier LLMs reveal that
while current models achieve meaningful iterative improvements, they still
exhibit significant limitations in autonomously generating long-horizon
solutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's
flexible and extensible architecture seamlessly integrates diverse data
sources, tools, and evaluation protocols, uniquely enabling model-based agent
tuning and promoting interoperability, scalability, and reproducibility. We
open-source our framework and benchmarks to foster community-driven innovation
towards next-generation MLE agents.

</details>


### [10] [Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains](https://arxiv.org/abs/2505.07274)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 将大型语言模型（LLMs）作为强化学习（RL）中的先验知识进行整合，虽然有显著优势但计算成本高昂。本文提出了一种基于LLM先验知识的后验采样高效缓存框架，大幅降低了计算成本同时保持高性能。通过适应性缓存机制和代理梯度元优化，该方法在离散文本环境和连续控制领域中均表现出色，减少了LLM查询次数和延迟时间，同时保留了大部分未缓存性能。理论分析和实验证明了近似质量，并且该框架可扩展到离线RL，进一步提高了性能并减少了训练时间。


<details>
  <summary>Details</summary>
Motivation: 尽管将大型语言模型（LLMs）作为强化学习（RL）中的先验知识具有显著优势，但其带来的计算成本问题亟待解决。因此，需要一种能够在维持高性能的同时降低计算成本的方法。

Method: 提出了一种基于LLM先验知识的后验采样高效缓存框架，核心是适应性缓存机制。缓存参数通过来自策略性能的代理梯度进行元优化，从而实现高效推理。该方法适用于离散文本环境和连续控制领域，并且可以扩展到离线RL。

Result: 在多个环境中，与未使用缓存的情况相比，LLM查询次数减少了3.8-4.7倍，中位延迟降低了4.0-12.0倍，同时保留了96-98%的性能。在离线RL中，CQL-Prior变体提高了14-29%的性能并减少了38-40%的训练时间。

Conclusion: 提出的缓存高效框架在降低计算成本的同时保持了高性能，适用于多种任务环境，并且在资源受限的情况下展示了通用性和实际可行性。

Abstract: Integrating large language models (LLMs) as priors in reinforcement learning
(RL) offers significant advantages but comes with substantial computational
costs. We present a principled cache-efficient framework for posterior sampling
with LLM-derived priors that dramatically reduces these costs while maintaining
high performance. At the core of our approach is an adaptive caching mechanism,
where cache parameters are meta-optimized using surrogate gradients derived
from policy performance. This design enables efficient inference across both
discrete text environments (e.g., TextWorld, ALFWorld) and continuous control
domains (e.g., MuJoCo), achieving a 3.8--4.7$\times$ reduction in LLM queries
and 4.0--12.0$\times$ lower median latencies (85--93\,ms on a consumer GPU)
while retaining 96--98\% of uncached performance. Our theoretical analysis
provides KL divergence bounds on approximation quality, validated empirically.
The framework extends to offline RL, where our CQL-Prior variant improves
performance by 14--29\% and reduces training time by 38--40\%. Extensive
evaluations across a diverse suite of eight tasks demonstrate the
generalizability and practical viability of LLM-guided RL in
resource-constrained settings.

</details>


### [11] [Remote Rowhammer Attack using Adversarial Observations on Federated Learning Clients](https://arxiv.org/abs/2505.06335)
*Jinsheng Yuan,Yuhang Hao,Weisi Guo,Yun Wu,Chongyan Gu*

Main category: cs.LG

TL;DR: 通过攻击特定客户端，可以远程发起对服务器内存的Rowhammer攻击，从而导致服务器内存位翻转，影响联邦学习的安全性。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习（FL）中的安全性研究主要集中在保护边缘客户端的数据隐私或客户端与服务器之间的通信渠道安全，而针对服务器端的客户端攻击研究较少。本文旨在探讨通过攻击某些特定客户端，是否可以在服务器上远程引发Rowhammer攻击，从而破坏服务器内存。

Method: 利用强化学习（RL）攻击者，通过操控客户端的传感器观察值，最大化服务器的重复内存更新率（RUR）。在大规模联邦学习自动语音识别（ASR）系统中验证该方法，并使用稀疏更新来增加攻击效果。

Result: 在目标服务器模型中，对抗攻击代理能够达到约70%的重复更新率（RUR），成功诱导服务器DRAM上的位翻转。

Conclusion: 此攻击方式可能导致学习中断或意外提升权限，为联邦学习和硬件设计的实际缓解策略研究开辟了新途径。

Abstract: Federated Learning (FL) has the potential for simultaneous global learning
amongst a large number of parallel agents, enabling emerging AI such as LLMs to
be trained across demographically diverse data. Central to this being efficient
is the ability for FL to perform sparse gradient updates and remote direct
memory access at the central server. Most of the research in FL security
focuses on protecting data privacy at the edge client or in the communication
channels between the client and server. Client-facing attacks on the server are
less well investigated as the assumption is that a large collective of clients
offer resilience.
  Here, we show that by attacking certain clients that lead to a high frequency
repetitive memory update in the server, we can remote initiate a rowhammer
attack on the server memory. For the first time, we do not need backdoor access
to the server, and a reinforcement learning (RL) attacker can learn how to
maximize server repetitive memory updates by manipulating the client's sensor
observation. The consequence of the remote rowhammer attack is that we are able
to achieve bit flips, which can corrupt the server memory. We demonstrate the
feasibility of our attack using a large-scale FL automatic speech recognition
(ASR) systems with sparse updates, our adversarial attacking agent can achieve
around 70\% repeated update rate (RUR) in the targeted server model,
effectively inducing bit flips on server DRAM. The security implications are
that can cause disruptions to learning or may inadvertently cause elevated
privilege. This paves the way for further research on practical mitigation
strategies in FL and hardware design.

</details>


### [12] [DMRL: Data- and Model-aware Reward Learning for Data Extraction](https://arxiv.org/abs/2505.06284)
*Zhiqiang Wang,Ruoxi Cheng*

Main category: cs.LG

TL;DR: The paper proposes DMRL, a Data- and Model-aware Reward Learning approach to extract sensitive data from LLMs using inverse reinforcement learning. It outperforms baselines in data extraction.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerabilities of LLMs to unintended privacy breaches and overcome limitations in current data extraction methods.

Method: DMRL consists of constructing an introspective reasoning dataset capturing leakage mindsets and training reward models with Group Relative Policy Optimization (GRPO), dynamically tuning optimization based on task difficulty at both data and model levels.

Result: Comprehensive experiments across various LLMs show that DMRL outperforms all baseline methods in data extraction performance.

Conclusion: DMRL is an effective approach for extracting sensitive data from LLMs.

Abstract: Large language models (LLMs) are inherently vulnerable to unintended privacy
breaches. Consequently, systematic red-teaming research is essential for
developing robust defense mechanisms. However, current data extraction methods
suffer from several limitations: (1) rely on dataset duplicates (addressable
via deduplication), (2) depend on prompt engineering (now countered by
detection and defense), and (3) rely on random-search adversarial generation.
To address these challenges, we propose DMRL, a Data- and Model-aware Reward
Learning approach for data extraction. This technique leverages inverse
reinforcement learning to extract sensitive data from LLMs. Our method consists
of two main components: (1) constructing an introspective reasoning dataset
that captures leakage mindsets to guide model behavior, and (2) training reward
models with Group Relative Policy Optimization (GRPO), dynamically tuning
optimization based on task difficulty at both the data and model levels.
Comprehensive experiments across various LLMs demonstrate that DMRL outperforms
all baseline methods in data extraction performance.

</details>


### [13] [Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers](https://arxiv.org/abs/2505.04842)
*Kusha Sareen,Morgane M Moss,Alessandro Sordoni,Rishabh Agarwal,Arian Hosseini*

Main category: cs.LG

TL;DR: RL$^V$ is an augmented reinforcement learning method that adds verification capabilities to value-free RL methods, boosting MATH accuracy and enabling efficient test-time compute scaling.


<details>
  <summary>Details</summary>
Motivation: Prevalent RL methods for fine-tuning LLM reasoners abandon the learned value function in favor of empirically estimated returns, which hinders test-time compute scaling.

Method: RL$^V$ jointly trains the LLM as both a reasoner and a generative verifier using RL-generated data.

Result: Empirically, RL$^V$ boosts MATH accuracy by over 20\% with parallel sampling and enables $8-32\times$ efficient test-time compute scaling. It also exhibits strong generalization capabilities and achieves $1.2-1.6\times$ higher performance when jointly scaling parallel and sequential test-time compute.

Conclusion: RL$^V$ successfully augments value-free RL methods by adding verification capabilities without significant overhead.

Abstract: Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners,
such as GRPO or Leave-one-out PPO, abandon the learned value function in favor
of empirically estimated returns. This hinders test-time compute scaling that
relies on using the value-function for verification. In this work, we propose
RL$^V$ that augments any ``value-free'' RL method by jointly training the LLM
as both a reasoner and a generative verifier using RL-generated data, adding
verification capabilities without significant overhead. Empirically, RL$^V$
boosts MATH accuracy by over 20\% with parallel sampling and enables
$8-32\times$ efficient test-time compute scaling compared to the base RL
method. RL$^V$ also exhibits strong generalization capabilities for both
easy-to-hard and out-of-domain tasks. Furthermore, RL$^V$ achieves
$1.2-1.6\times$ higher performance when jointly scaling parallel and sequential
test-time compute with a long reasoning R1 model.

</details>


### [14] [DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning](https://arxiv.org/abs/2505.03209)
*Borui Wang,Kathleen McKeown,Rex Ying*

Main category: cs.LG

TL;DR: Reinforcement learning from expert demonstrations is challenging. Current methods suffer poor generalization, low sample efficiency, and poor model interpretability. This paper proposes DYSTIL, a novel framework integrated with LLMs to overcome these limitations. DYSTIL dynamically queries a strategy-generating LLM based on advantage estimations and expert demonstrations, improving policy generalization and sample efficiency while providing interpretability.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenges in reinforcement learning from expert demonstrations, particularly the issues of poor generalization, low sample efficiency, and poor model interpretability that current state-of-the-art methods face.

Method: The method proposed in this paper is called DYnamic STrategy Induction with Llms for reinforcement learning (DYSTIL). It involves dynamically querying a strategy-generating LLM to induce textual strategies based on advantage estimations and expert demonstrations. These strategies are then internalized into the RL agent through policy optimization.

Result: The results show that DYSTIL significantly outperforms state-of-the-art baseline methods by 17.75% in average success rate and also enjoys higher sample efficiency during the learning process. These findings were empirically demonstrated through testing over challenging RL environments from Minigrid and BabyAI.

Conclusion: The conclusion is that DYSTIL, a novel strategy-based reinforcement learning framework integrated with LLMs, successfully overcomes the limitations of poor generalization, low sample efficiency, and poor model interpretability in reinforcement learning from expert demonstrations.

Abstract: Reinforcement learning from expert demonstrations has long remained a
challenging research problem, and existing state-of-the-art methods using
behavioral cloning plus further RL training often suffer from poor
generalization, low sample efficiency, and poor model interpretability.
Inspired by the strong reasoning abilities of large language models (LLMs), we
propose a novel strategy-based reinforcement learning framework integrated with
LLMs called DYnamic STrategy Induction with Llms for reinforcement learning
(DYSTIL) to overcome these limitations. DYSTIL dynamically queries a
strategy-generating LLM to induce textual strategies based on advantage
estimations and expert demonstrations, and gradually internalizes induced
strategies into the RL agent through policy optimization to improve its
performance through boosting policy generalization and enhancing sample
efficiency. It also provides a direct textual channel to observe and interpret
the evolution of the policy's underlying strategies during training. We test
DYSTIL over challenging RL environments from Minigrid and BabyAI, and
empirically demonstrate that DYSTIL significantly outperforms state-of-the-art
baseline methods by 17.75% in average success rate while also enjoying higher
sample efficiency during the learning process.

</details>


### [15] [VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making](https://arxiv.org/abs/2505.03181)
*Jake Grigsby,Yuke Zhu,Michael Ryoo,Juan Carlos Niebles*

Main category: cs.LG

TL;DR: This paper explores an off-policy reinforcement learning solution to fine-tune vision-language models (VLMs) for agent tasks, enabling them to learn from unsuccessful decisions and low-quality datasets.


<details>
  <summary>Details</summary>
Motivation: Agent tasks require skills where open-weight VLMs lag behind LLMs, such as following strict output syntax requirements. Supervised fine-tuning on task-specific expert demonstrations is required to overcome these limitations.

Method: The work approaches the challenges of enhancing VLM capabilities for agent tasks from an offline-to-online reinforcement learning perspective, allowing the model to learn from its own unsuccessful decisions or those of more capable models.

Result: The technique was demonstrated using two open-weight VLMs across three multi-modal agent domains, showing potential for agents to self-improve and learn effectively from low-quality datasets.

Conclusion: An off-policy RL solution retains the stability and simplicity of supervised fine-tuning while enabling VLMs used in agents to improve through learning from a broader range of experiences.

Abstract: Recent research looks to harness the general knowledge and reasoning of large
language models (LLMs) into agents that accomplish user-specified goals in
interactive environments. Vision-language models (VLMs) extend LLMs to
multi-modal data and provide agents with the visual reasoning necessary for new
applications in areas such as computer automation. However, agent tasks
emphasize skills where accessible open-weight VLMs lag behind their LLM
equivalents. For example, VLMs are less capable of following an environment's
strict output syntax requirements and are more focused on open-ended question
answering. Overcoming these limitations requires supervised fine-tuning (SFT)
on task-specific expert demonstrations. Our work approaches these challenges
from an offline-to-online reinforcement learning (RL) perspective. RL lets us
fine-tune VLMs to agent tasks while learning from the unsuccessful decisions of
our own model or more capable (larger) models. We explore an off-policy RL
solution that retains the stability and simplicity of the widely used SFT
workflow while allowing our agent to self-improve and learn from low-quality
datasets. We demonstrate this technique with two open-weight VLMs across three
multi-modal agent domains.

</details>


### [16] [DSADF: Thinking Fast and Slow for Decision Making](https://arxiv.org/abs/2505.08189)
*Alex Zhihao Dou,Dongfei Cui,Jun Yan,Weida Wang,Benteng Chen,Haoming Wang,Zeke Xie,Shufei Zhang*

Main category: cs.LG

TL;DR: 尽管强化学习（RL）代理在定义明确的环境中表现良好，但它们通常难以将其学到的策略推广到动态环境中。本文提出了一种双系统自适应决策框架（DSADF），通过整合快速直观决策的RL代理（系统1）和深度分析推理的视觉语言模型（系统2），实现了复杂环境中的灵活决策。实验结果表明，该方法在已知和未知任务中均显著提高了决策能力。


<details>
  <summary>Details</summary>
Motivation: 强化学习代理在动态环境中的泛化能力不足，而现有的结合大语言模型或视觉语言模型的方法缺乏与RL代理的无缝协调，导致不合理的决策和效率瓶颈。因此，需要一种能充分利用两者优势并增强其交互的新方法。

Method: 受卡尼曼的快思考（系统1）和慢思考（系统2）理论启发，本文提出了双系统自适应决策框架（DSADF）。其中，系统1由RL代理和记忆空间组成，用于快速直观决策；系统2由视觉语言模型驱动，用于深度分析推理。两个系统通过协作实现高效、自适应的决策。

Result: 在视频游戏环境Crafter和Housekeep中的实证研究表明，DSADF显著提高了RL代理在已知任务和未知任务中的决策能力。

Conclusion: 提出的DSADF通过整合RL代理的快速响应能力和VLM的推理能力，成功解决了现有方法在动态环境中的效率瓶颈和不合理决策问题，为复杂环境下的灵活决策提供了新思路。

Abstract: Although Reinforcement Learning (RL) agents are effective in well-defined
environments, they often struggle to generalize their learned policies to
dynamic settings due to their reliance on trial-and-error interactions. Recent
work has explored applying Large Language Models (LLMs) or Vision Language
Models (VLMs) to boost the generalization of RL agents through policy
optimization guidance or prior knowledge. However, these approaches often lack
seamless coordination between the RL agent and the foundation model, leading to
unreasonable decision-making in unfamiliar environments and efficiency
bottlenecks. Making full use of the inferential capabilities of foundation
models and the rapid response capabilities of RL agents and enhancing the
interaction between the two to form a dual system is still a lingering
scientific question. To address this problem, we draw inspiration from
Kahneman's theory of fast thinking (System 1) and slow thinking (System 2),
demonstrating that balancing intuition and deep reasoning can achieve nimble
decision-making in a complex world. In this study, we propose a Dual-System
Adaptive Decision Framework (DSADF), integrating two complementary modules:
System 1, comprising an RL agent and a memory space for fast and intuitive
decision making, and System 2, driven by a VLM for deep and analytical
reasoning. DSADF facilitates efficient and adaptive decision-making by
combining the strengths of both systems. The empirical study in the video game
environment: Crafter and Housekeep demonstrates the effectiveness of our
proposed method, showing significant improvements in decision abilities for
both unseen and known tasks.

</details>


### [17] [MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering](https://arxiv.org/abs/2505.07782)
*Rushi Qiang,Yuchen Zhuang,Yinghao Li,Dingu Sagar V K,Rongzhi Zhang,Changhao Li,Ian Shu-Hei Wong,Sherry Yang,Percy Liang,Chao Zhang,Bo Dai*

Main category: cs.LG

TL;DR: 本文介绍了MLE-Dojo，一个用于系统性强化学习、评估和改进自主大型语言模型（LLM）代理的框架。它基于200多个真实的Kaggle挑战，提供了一个交互式环境，支持通过监督微调和强化学习进行全面代理训练。尽管当前模型在迭代改进方面取得了一定进展，但在生成长期解决方案和高效解决复杂错误方面仍存在显著限制。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要依赖静态数据集或单次评估，无法充分满足机器学习工程（MLE）工作流中交互式、迭代式的实验需求。因此，需要一个能够支持代理进行反复实验、调试和改进的框架。

Method: 构建了一个基于200多个真实Kaggle挑战的框架MLE-Dojo，涵盖多样化的开放性MLE任务。该框架提供了一个完全可执行的环境，支持通过监督微调和强化学习进行综合代理训练，并允许实时结果验证和现实数据采样。

Result: 对八个前沿LLM的广泛评估表明，当前模型在实现有意义的迭代改进方面表现出色，但在自主生成长期解决方案和高效解决复杂错误方面仍存在显著局限性。

Conclusion: MLE-Dojo的灵活和可扩展架构可以无缝集成各种数据源、工具和评估协议，促进模型驱动的代理调整，并推动互操作性、可扩展性和可重复性的发展。

Abstract: We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement
learning, evaluating, and improving autonomous large language model (LLM)
agents in iterative machine learning engineering (MLE) workflows. Unlike
existing benchmarks that primarily rely on static datasets or single-attempt
evaluations, MLE-Dojo provides an interactive environment enabling agents to
iteratively experiment, debug, and refine solutions through structured feedback
loops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,
open-ended MLE tasks carefully curated to reflect realistic engineering
scenarios such as data processing, architecture search, hyperparameter tuning,
and code debugging. Its fully executable environment supports comprehensive
agent training via both supervised fine-tuning and reinforcement learning,
facilitating iterative experimentation, realistic data sampling, and real-time
outcome verification. Extensive evaluations of eight frontier LLMs reveal that
while current models achieve meaningful iterative improvements, they still
exhibit significant limitations in autonomously generating long-horizon
solutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's
flexible and extensible architecture seamlessly integrates diverse data
sources, tools, and evaluation protocols, uniquely enabling model-based agent
tuning and promoting interoperability, scalability, and reproducibility. We
open-source our framework and benchmarks to foster community-driven innovation
towards next-generation MLE agents.

</details>


### [18] [Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains](https://arxiv.org/abs/2505.07274)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 将大型语言模型（LLMs）作为强化学习（RL）中的先验进行整合，虽然具有显著优势，但计算成本较高。本文提出了一种基于LLM先验的高效缓存后验采样框架，通过自适应缓存机制和元优化方法大幅降低计算成本，同时保持高性能。实验表明，该框架在多种环境和任务中减少了LLM查询次数、降低了延迟，并保留了大部分性能。此外，该框架可扩展到离线RL，进一步提高性能并减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型（LLMs）作为强化学习（RL）中的先验可以带来显著的优势，但由于其高昂的计算成本，实际应用受到限制。因此，研究如何在不牺牲性能的前提下降低计算成本成为一个重要的问题。

Method: 本文提出了一种基于LLM先验的高效缓存后验采样框架，核心是自适应缓存机制。通过使用代理梯度对缓存参数进行元优化，从而实现高效的推理。这种方法适用于离散文本环境和连续控制领域，并且通过理论分析提供了近似质量的KL散度界限。此外，还提出了CQL-Prior变体以扩展到离线RL。

Result: 实验结果表明，与未使用缓存的方法相比，该框架能够减少3.8-4.7倍的LLM查询次数，降低4.0-12.0倍的中位延迟，同时保留96-98%的性能。在离线RL中，CQL-Prior变体提高了14-29%的性能并减少了38-40%的训练时间。

Conclusion: 本文提出的缓存高效框架成功地降低了将LLM作为RL先验的计算成本，同时保持了高性能。这一方法在多个环境中表现出良好的泛化性和实用性，为资源受限场景下的LLM引导RL提供了一个可行的解决方案。

Abstract: Integrating large language models (LLMs) as priors in reinforcement learning
(RL) offers significant advantages but comes with substantial computational
costs. We present a principled cache-efficient framework for posterior sampling
with LLM-derived priors that dramatically reduces these costs while maintaining
high performance. At the core of our approach is an adaptive caching mechanism,
where cache parameters are meta-optimized using surrogate gradients derived
from policy performance. This design enables efficient inference across both
discrete text environments (e.g., TextWorld, ALFWorld) and continuous control
domains (e.g., MuJoCo), achieving a 3.8--4.7$\times$ reduction in LLM queries
and 4.0--12.0$\times$ lower median latencies (85--93\,ms on a consumer GPU)
while retaining 96--98\% of uncached performance. Our theoretical analysis
provides KL divergence bounds on approximation quality, validated empirically.
The framework extends to offline RL, where our CQL-Prior variant improves
performance by 14--29\% and reduces training time by 38--40\%. Extensive
evaluations across a diverse suite of eight tasks demonstrate the
generalizability and practical viability of LLM-guided RL in
resource-constrained settings.

</details>


### [19] [Remote Rowhammer Attack using Adversarial Observations on Federated Learning Clients](https://arxiv.org/abs/2505.06335)
*Jinsheng Yuan,Yuhang Hao,Weisi Guo,Yun Wu,Chongyan Gu*

Main category: cs.LG

TL;DR: 本文探讨了联邦学习（FL）中的一种新型安全威胁：通过攻击特定客户端，诱导服务器内存高频重复更新，从而远程发起rowhammer攻击，导致服务器内存位翻转。此攻击无需服务器后门访问，攻击者可通过强化学习操控客户端传感器观测以最大化服务器内存重复更新率。实验在大规模稀疏更新的FL自动语音识别系统中实现约70%的重复更新率，成功诱导服务器DRAM位翻转。研究揭示了此类攻击对学习过程的干扰或权限提升风险，并呼吁进一步研究FL中的实际缓解策略及硬件设计改进。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习的安全研究主要关注边缘客户端的数据隐私保护和通信信道安全，而对服务器端的客户端面向攻击研究较少。假设大量客户端的集体行为能提供弹性防御，但忽略了某些客户端可能导致服务器内存高频重复更新，从而引发潜在安全威胁（如rowhammer攻击）。

Method: 1. 选择特定客户端进行攻击，使其触发服务器内存高频重复更新。
2. 利用强化学习训练攻击代理，学习如何通过操控客户端传感器观测来最大化服务器内存重复更新率。
3. 在大规模联邦学习自动语音识别系统中验证方法，该系统支持稀疏梯度更新和远程直接内存访问。
4. 评估攻击效果，包括重复更新率（RUR）和服务器DRAM位翻转情况。

Result: 实验表明，在目标服务器模型中，攻击代理能够实现约70%的重复更新率（RUR），有效诱导服务器DRAM发生位翻转。证明了通过操控客户端行为可以远程发起rowhammer攻击，影响服务器内存完整性。

Conclusion: 本文首次展示了无需服务器后门访问即可通过操控客户端行为远程发起rowhammer攻击的可能性。这种攻击可能干扰联邦学习过程或导致权限提升等安全问题。研究强调了进一步探索联邦学习中实际缓解策略和硬件设计改进的重要性。

Abstract: Federated Learning (FL) has the potential for simultaneous global learning
amongst a large number of parallel agents, enabling emerging AI such as LLMs to
be trained across demographically diverse data. Central to this being efficient
is the ability for FL to perform sparse gradient updates and remote direct
memory access at the central server. Most of the research in FL security
focuses on protecting data privacy at the edge client or in the communication
channels between the client and server. Client-facing attacks on the server are
less well investigated as the assumption is that a large collective of clients
offer resilience.
  Here, we show that by attacking certain clients that lead to a high frequency
repetitive memory update in the server, we can remote initiate a rowhammer
attack on the server memory. For the first time, we do not need backdoor access
to the server, and a reinforcement learning (RL) attacker can learn how to
maximize server repetitive memory updates by manipulating the client's sensor
observation. The consequence of the remote rowhammer attack is that we are able
to achieve bit flips, which can corrupt the server memory. We demonstrate the
feasibility of our attack using a large-scale FL automatic speech recognition
(ASR) systems with sparse updates, our adversarial attacking agent can achieve
around 70\% repeated update rate (RUR) in the targeted server model,
effectively inducing bit flips on server DRAM. The security implications are
that can cause disruptions to learning or may inadvertently cause elevated
privilege. This paves the way for further research on practical mitigation
strategies in FL and hardware design.

</details>


### [20] [DMRL: Data- and Model-aware Reward Learning for Data Extraction](https://arxiv.org/abs/2505.06284)
*Zhiqiang Wang,Ruoxi Cheng*

Main category: cs.LG

TL;DR: 提出了一种新的数据提取方法DMRL，用于从大型语言模型中提取敏感数据，通过构建内省推理数据集和使用组相对策略优化训练奖励模型，该方法在多个LLM上表现出优于基线方法的数据提取性能。


<details>
  <summary>Details</summary>
Motivation: 当前的数据提取方法存在依赖数据集副本、提示工程以及随机搜索对抗生成等局限性，需要一种更有效的数据提取方法来解决这些问题。

Method: DMRL是一种数据和模型感知的奖励学习方法，利用逆向强化学习从LLMs中提取敏感数据。其方法包括构建内省推理数据集以指导模型行为，以及使用组相对策略优化（GRPO）动态调整优化过程。

Result: 在各种LLMs上的全面实验表明，DMRL在数据提取性能方面优于所有基线方法。

Conclusion: DMRL为数据提取提供了一种新思路，并且在提高数据提取性能方面具有显著优势。

Abstract: Large language models (LLMs) are inherently vulnerable to unintended privacy
breaches. Consequently, systematic red-teaming research is essential for
developing robust defense mechanisms. However, current data extraction methods
suffer from several limitations: (1) rely on dataset duplicates (addressable
via deduplication), (2) depend on prompt engineering (now countered by
detection and defense), and (3) rely on random-search adversarial generation.
To address these challenges, we propose DMRL, a Data- and Model-aware Reward
Learning approach for data extraction. This technique leverages inverse
reinforcement learning to extract sensitive data from LLMs. Our method consists
of two main components: (1) constructing an introspective reasoning dataset
that captures leakage mindsets to guide model behavior, and (2) training reward
models with Group Relative Policy Optimization (GRPO), dynamically tuning
optimization based on task difficulty at both the data and model levels.
Comprehensive experiments across various LLMs demonstrate that DMRL outperforms
all baseline methods in data extraction performance.

</details>


### [21] [Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers](https://arxiv.org/abs/2505.04842)
*Kusha Sareen,Morgane M Moss,Alessandro Sordoni,Rishabh Agarwal,Arian Hosseini*

Main category: cs.LG

TL;DR: 提出RL^V方法，增强无价值函数的强化学习方法，提升LLM推理器性能。


<details>
  <summary>Details</summary>
Motivation: 当前主流的微调LLM推理器的强化学习方法（如GRPO或Leave-one-out PPO）放弃了学习到的价值函数，转而使用经验估计回报，限制了测试时计算扩展的能力。

Method: 通过联合训练LLM作为推理器和生成验证器，使用RL生成的数据，为任何“无价值函数”的RL方法添加验证功能，且不增加显著开销。

Result: RL^V将MATH准确率提高了20%以上，并实现了8-32倍更高效的测试时计算扩展，表现出强大的泛化能力。

Conclusion: RL^V在并行和顺序测试时计算扩展中表现优异，提升了模型推理性能。

Abstract: Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners,
such as GRPO or Leave-one-out PPO, abandon the learned value function in favor
of empirically estimated returns. This hinders test-time compute scaling that
relies on using the value-function for verification. In this work, we propose
RL$^V$ that augments any ``value-free'' RL method by jointly training the LLM
as both a reasoner and a generative verifier using RL-generated data, adding
verification capabilities without significant overhead. Empirically, RL$^V$
boosts MATH accuracy by over 20\% with parallel sampling and enables
$8-32\times$ efficient test-time compute scaling compared to the base RL
method. RL$^V$ also exhibits strong generalization capabilities for both
easy-to-hard and out-of-domain tasks. Furthermore, RL$^V$ achieves
$1.2-1.6\times$ higher performance when jointly scaling parallel and sequential
test-time compute with a long reasoning R1 model.

</details>


### [22] [DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning](https://arxiv.org/abs/2505.03209)
*Borui Wang,Kathleen McKeown,Rex Ying*

Main category: cs.LG

TL;DR: 从专家演示中进行强化学习一直是一个具有挑战性的研究问题，现有的行为克隆加进一步RL训练的方法通常存在泛化能力差、样本效率低和模型可解释性差的问题。受大型语言模型（LLMs）强大推理能力的启发，我们提出了一个与LLMs结合的新型基于策略的强化学习框架——DYSTIL，该框架通过动态查询策略生成LLM来诱导基于优势估计和专家演示的文本策略，并通过策略优化将这些策略逐步内化到RL代理中，从而提升其性能、政策泛化能力和样本效率。此外，它还提供了一个直接的文本通道来观察和解释培训期间政策潜在策略的演变。我们在Minigrid和BabyAI的具有挑战性的RL环境中测试了DYSTIL，结果表明，DYSTIL在平均成功率上比最先进的基线方法高出17.75%，同时在学习过程中也具有更高的样本效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习领域中，从专家演示中学习的方法虽然有效，但存在泛化能力差、样本效率低以及模型可解释性不足等问题。因此，需要一种新的方法来克服这些限制，提高强化学习模型的整体性能和效率。

Method: 提出了一种名为DYSTIL的新框架，该框架利用大型语言模型（LLMs）的强大推理能力。具体来说，DYSTIL通过动态查询策略生成的LLM，根据优势估计和专家演示生成文本策略，并通过策略优化逐渐将这些策略内化到强化学习代理中。这种方法不仅提高了策略的泛化能力，还增强了样本效率，并提供了对策略演变的直接文本观察渠道。

Result: 在Minigrid和BabyAI等具有挑战性的强化学习环境中进行了测试，实验结果表明，DYSTIL相比现有最先进的方法，在平均成功率上提高了17.75%，并且在学习过程中表现出更高的样本效率。

Conclusion: DYSTIL框架有效地解决了现有强化学习方法在泛化能力、样本效率和模型可解释性方面的不足，显著提升了强化学习模型的性能。这一新方法为未来强化学习的发展提供了新的思路和方向。

Abstract: Reinforcement learning from expert demonstrations has long remained a
challenging research problem, and existing state-of-the-art methods using
behavioral cloning plus further RL training often suffer from poor
generalization, low sample efficiency, and poor model interpretability.
Inspired by the strong reasoning abilities of large language models (LLMs), we
propose a novel strategy-based reinforcement learning framework integrated with
LLMs called DYnamic STrategy Induction with Llms for reinforcement learning
(DYSTIL) to overcome these limitations. DYSTIL dynamically queries a
strategy-generating LLM to induce textual strategies based on advantage
estimations and expert demonstrations, and gradually internalizes induced
strategies into the RL agent through policy optimization to improve its
performance through boosting policy generalization and enhancing sample
efficiency. It also provides a direct textual channel to observe and interpret
the evolution of the policy's underlying strategies during training. We test
DYSTIL over challenging RL environments from Minigrid and BabyAI, and
empirically demonstrate that DYSTIL significantly outperforms state-of-the-art
baseline methods by 17.75% in average success rate while also enjoying higher
sample efficiency during the learning process.

</details>


### [23] [VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making](https://arxiv.org/abs/2505.03181)
*Jake Grigsby,Yuke Zhu,Michael Ryoo,Juan Carlos Niebles*

Main category: cs.LG

TL;DR: 研究探讨了通过离线到在线强化学习方法改进视觉-语言模型（VLMs）在多模态代理任务中的表现，使其能更好地适应环境的严格输出语法要求，并展示该技术在两个开放权重VLMs和三个多模态代理领域的应用。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言模型（VLMs）在处理多模态数据时，相较于大型语言模型（LLMs），在遵循环境严格的输出语法要求方面存在不足，更多专注于开放性问题的回答。为了克服这些限制并提升VLMs在代理任务中的能力，需要采用监督微调（SFT）的方法。

Method: 本研究从离线到在线强化学习（RL）的角度出发，探索一种非策略RL解决方案。这种方法不仅保留了广泛使用的SFT工作流的稳定性和简单性，还允许代理自我改进，并能够从低质量的数据集中学习。

Result: 通过使用两个开放权重的视觉-语言模型，在三个多模态代理领域中展示了该技术的有效性。这表明所提出的方法可以有效提升VLMs在代理任务中的性能。

Conclusion: 离线到在线强化学习为提升视觉-语言模型在多模态代理任务中的表现提供了一种有效的途径，未来可以在更广泛的代理任务和更大规模的数据集上进一步验证和优化该方法。

Abstract: Recent research looks to harness the general knowledge and reasoning of large
language models (LLMs) into agents that accomplish user-specified goals in
interactive environments. Vision-language models (VLMs) extend LLMs to
multi-modal data and provide agents with the visual reasoning necessary for new
applications in areas such as computer automation. However, agent tasks
emphasize skills where accessible open-weight VLMs lag behind their LLM
equivalents. For example, VLMs are less capable of following an environment's
strict output syntax requirements and are more focused on open-ended question
answering. Overcoming these limitations requires supervised fine-tuning (SFT)
on task-specific expert demonstrations. Our work approaches these challenges
from an offline-to-online reinforcement learning (RL) perspective. RL lets us
fine-tune VLMs to agent tasks while learning from the unsuccessful decisions of
our own model or more capable (larger) models. We explore an off-policy RL
solution that retains the stability and simplicity of the widely used SFT
workflow while allowing our agent to self-improve and learn from low-quality
datasets. We demonstrate this technique with two open-weight VLMs across three
multi-modal agent domains.

</details>


### [24] [Self Rewarding Self Improving](https://arxiv.org/abs/2505.08827)
*Toby Simonds,Kevin Lopez,Akira Yoshiyama,Dominique Garmier*

Main category: cs.LG

TL;DR: 通过自我评判机制，大语言模型可以在没有参考答案的情况下实现有效的自我改进。在 Countdown 谜题和 MIT 积分蜂问题上的实验表明，模型能够在没有真实答案的情况下提供可靠的奖励信号，从而在以前不可能的领域实现强化学习。通过自我评判和合成问题生成，建立了完整的自我改进循环，使 Qwen 2.5 7B 的性能比基线提高了 8%，并在积分任务上超越了 GPT-4o 的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型训练通常依赖于人类提供的参考答案或奖励信号，这在一些复杂或数据稀缺的任务中可能难以实现。为了克服这一限制，研究者试图探索一种无需外部监督的自我改进方法。

Method: 利用生成与验证解决方案之间的固有不对称性，模型通过自我评判来评估自己的表现。具体来说，在 Countdown 谜题和 MIT 积分蜂问题上进行实验，模型生成问题、解决问题并自行评价其性能，形成一个自我改进的闭环。

Result: Qwen 2.5 7B 在基线之上实现了 8% 的性能提升，并且在积分任务上超过了 GPT-4o 的表现。此外，模型能够在没有真实答案的情况下提供可靠的奖励信号，从而在以前不可能的领域实现强化学习。

Conclusion: 大语言模型可以通过自我评判提供有效的奖励信号以进行训练，这为以前受限于创建程序化奖励的许多强化学习环境解锁了可能性。这种机制可能引发范式转变，推动 AI 系统从人类指导的训练转向自我引导的持续学习，特别是在训练数据稀缺或评估要求复杂的领域。

Abstract: We demonstrate that large language models can effectively self-improve
through self-judging without requiring reference solutions, leveraging the
inherent asymmetry between generating and verifying solutions. Our experiments
on Countdown puzzles and MIT Integration Bee problems show that models can
provide reliable reward signals without ground truth answers, enabling
reinforcement learning in domains previously not possible. By implementing
self-judging, we achieve significant performance gains maintaining alignment
with formal verification. When combined with synthetic question generation, we
establish a complete self-improvement loop where models generate practice
problems, solve them, and evaluate their own performance-achieving an 8%
improvement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on
integration tasks. Our findings demonstrate that LLM judges can provide
effective reward signals for training models, unlocking many reinforcement
learning environments previously limited by the difficulty of creating
programmatic rewards. This suggests a potential paradigm shift toward AI
systems that continuously improve through self-directed learning rather than
human-guided training, potentially accelerating progress in domains with scarce
training data or complex evaluation requirements.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [25] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 在大规模语言模型（LLMs）中，通过重复采样来扩展推理计算，随着样本数量的增加，覆盖率（解决问题的比例）会持续提高。本文研究了这一现象部分是由于标准评估基准中的答案分布偏向于较小的常见答案集。通过定义一个基于训练集中答案频率的基线，发现对于某些LLMs，该基线比重复模型采样表现更好；而对于其他模型，其覆盖率与一种混合策略相当。此基线有助于更准确地测量重复采样在提示无关猜测之外的覆盖率提升。


<details>
  <summary>Details</summary>
Motivation: 研究重复采样在大语言模型推理中的效果，并分析这种效果是否与评估基准中答案分布的偏斜有关。

Method: 定义了一个根据训练集中答案频率枚举答案的基线，并在数学推理和事实知识两个领域进行了实验。比较了该基线、重复模型采样以及混合策略的表现。

Result: 对于某些LLMs，基于训练集答案频率的基线优于重复模型采样；而对其他模型，其表现与混合策略相当。

Conclusion: 重复采样的性能提升部分归因于评估基准中答案分布的偏斜。使用基于训练集答案频率的基线可以更准确地衡量重复采样在提示无关猜测之外的实际改进。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [26] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 在大规模语言模型（LLMs）中，通过重复采样来扩展推理计算，会随着样本数量的增加而持续提高覆盖率（解决问题的比例）。实验表明，这种提升部分是因为评估基准的答案分布偏向于少量常见答案。研究提出了一种基于训练集中答案频率的基线方法，在某些LLM上优于重复采样，而在其他模型上与混合策略相当。这为更准确地测量重复采样在提示无关猜测之外的覆盖率改进提供了基础。


<details>
  <summary>Details</summary>
Motivation: 作者观察到在大语言模型中，重复采样可以提高问题解决的覆盖率，并推测这种提升可能与标准评估基准中答案分布的偏差有关，即答案集中于少量常见答案。为了验证这一假设，作者希望通过定义一种基于训练集答案频率的基线方法，来量化重复采样的实际效果。

Method: 研究者定义了一个基线，该基线根据训练集中答案的出现频率枚举答案。然后在两个领域（数学推理和事实知识）进行实验，比较了该基线与重复模型采样以及混合策略（部分采样+部分枚举）的表现。

Result: 实验结果表明，在某些LLM上，基于训练集答案频率的基线方法优于重复采样；而在其他模型上，其表现与混合策略相当，即用少量模型采样结合枚举猜测，可以达到相似的覆盖率。

Conclusion: 研究证明了重复采样的性能提升部分源于评估基准答案分布的偏差。通过引入基于训练集答案频率的基线方法，可以更准确地衡量重复采样在提示无关猜测之外的实际改进。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [27] [Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting](https://arxiv.org/abs/2505.09395)
*Chen-Yu Liu,Kuan-Cheng Chen,Yi-Chien Chen,Samuel Yen-Chi Chen,Wei-Hao Huang,Wei-Jia Huang,Yen-Jui Chang*

Main category: quant-ph

TL;DR: 将量子机器学习（QML）应用于台风轨迹预测，提出Quantum Parameter Adaptation (QPA)方法，结合Attention-based Multi-ConvGRU模型，显著减少可训练参数数量，同时保持预测精度，为气候建模提供了一种可扩展且节能的方法。


<details>
  <summary>Details</summary>
Motivation: 台风轨迹预测对于灾害准备至关重要，但由于大气动力学的复杂性和深度学习模型对资源的需求，计算成本仍然很高。因此，需要一种更高效、可持续的方法来进行台风预测。

Method: 引入Quantum Parameter Adaptation (QPA)，一种基于量子神经网络（QNNs）的混合量子-经典框架，与Attention-based Multi-ConvGRU模型集成，实现参数高效的训练，并在训练期间生成可训练参数，无需在推理时使用量子硬件。

Result: QPA显著减少了可训练参数的数量，同时保持了预测性能，展示了其在高精度台风轨迹预测中的应用潜力。

Conclusion: QPA为大规模台风轨迹预测提供了一种新的、可扩展且节能的方法，标志着量子机器学习在气候建模领域的首次应用，有助于推动高性能预报的发展和普及。

Abstract: Typhoon trajectory forecasting is essential for disaster preparedness but
remains computationally demanding due to the complexity of atmospheric dynamics
and the resource requirements of deep learning models. Quantum-Train (QT), a
hybrid quantum-classical framework that leverages quantum neural networks
(QNNs) to generate trainable parameters exclusively during training,
eliminating the need for quantum hardware at inference time. Building on QT's
success across multiple domains, including image classification, reinforcement
learning, flood prediction, and large language model (LLM) fine-tuning, we
introduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting
model learning. Integrated with an Attention-based Multi-ConvGRU model, QPA
enables parameter-efficient training while maintaining predictive accuracy.
This work represents the first application of quantum machine learning (QML) to
large-scale typhoon trajectory prediction, offering a scalable and
energy-efficient approach to climate modeling. Our results demonstrate that QPA
significantly reduces the number of trainable parameters while preserving
performance, making high-performance forecasting more accessible and
sustainable through hybrid quantum-classical learning.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [28] [Improved Algorithms for Differentially Private Language Model Alignment](https://arxiv.org/abs/2505.08849)
*Keyu Chen,Hao Tang,Qinglin Liu,Yizhao Xu*

Main category: cs.CR

TL;DR: 本文提出了一种新的隐私保护对齐算法，可以在确保用户隐私的同时提高大语言模型与人类偏好的对齐质量。实验表明，在中等隐私预算下，结合DP-AdamW和DPO的方法比现有技术提高了15%的对齐质量。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型对齐方法虽然可以提高模型与人类偏好的一致性，但通常需要使用敏感的用户数据，从而引发隐私问题。尽管已有研究尝试将差分隐私（DP）与对齐技术相结合，但性能仍受限。

Method: 作者提出了新的隐私保护对齐算法，并在不同的隐私预算和模型上对其有效性进行了严格分析。该框架可应用于两种主要的对齐技术：直接偏好优化（DPO）和基于人类反馈的强化学习（RLHF）。其中，DP-AdamW算法与DPO结合表现尤为突出。

Result: 实验结果表明，新方法在大规模语言模型上达到了最先进的性能，特别是在中等隐私预算（ε=2-5）下，DP-AdamW与DPO结合可使对齐质量提升高达15%。此外，作者还探讨了隐私保证、对齐效果和计算需求之间的权衡关系。

Conclusion: 本文提出的隐私保护对齐算法显著提高了语言模型在保护用户隐私条件下的对齐质量，并为实际应用提供了优化这些权衡的实用指南。

Abstract: Language model alignment is crucial for ensuring that large language models
(LLMs) align with human preferences, yet it often involves sensitive user data,
raising significant privacy concerns. While prior work has integrated
differential privacy (DP) with alignment techniques, their performance remains
limited. In this paper, we propose novel algorithms for privacy-preserving
alignment and rigorously analyze their effectiveness across varying privacy
budgets and models. Our framework can be deployed on two celebrated alignment
techniques, namely direct preference optimization (DPO) and reinforcement
learning from human feedback (RLHF). Through systematic experiments on
large-scale language models, we demonstrate that our approach achieves
state-of-the-art performance. Notably, one of our algorithms, DP-AdamW,
combined with DPO, surpasses existing methods, improving alignment quality by
up to 15% under moderate privacy budgets ({\epsilon}=2-5). We further
investigate the interplay between privacy guarantees, alignment efficacy, and
computational demands, providing practical guidelines for optimizing these
trade-offs.

</details>
