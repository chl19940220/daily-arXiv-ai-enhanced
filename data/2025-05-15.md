<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: This paper surveys recent developments in human-AI collaboration, pointing out achievements and gaps. It proposes a new conceptual architecture (Hierarchical Exploration-Exploitation Net) to integrate varied studies and facilitate new work fusing qualitative and quantitative paradigms.


<details>
  <summary>Details</summary>
Motivation: There is a lack of a unifying theoretical framework that can coherently integrate various studies on human-AI agents collaboration, particularly for open-ended, complex tasks.

Method: The paper proposes a novel conceptual architecture called Hierarchical Exploration-Exploitation Net that interlinks multi-agent coordination, knowledge management, cybernetic feedback loops, and higher-level control mechanisms.

Result: This approach allows mapping existing contributions onto the proposed framework, facilitating revision of legacy methods and inspiring new work that combines different paradigms.

Conclusion: The insights provided offer a stepping stone toward deeper co-evolution of human cognition and AI capability.

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [2] [The Steganographic Potentials of Language Models](https://arxiv.org/abs/2505.03439)
*Artem Karpov,Tinuade Adeleke,Seong Hah Cho,Natalia Perez-Campanero*

Main category: cs.AI

TL;DR: Large Language Models (LLMs) can potentially hide messages within texts through steganography, which is a challenge to detection and faithful reasoning. This study explores the steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL). The findings reveal that current models have basic steganographic abilities, but explicit algorithmic guidance can significantly enhance their information concealment capacity.


<details>
  <summary>Details</summary>
Motivation: To understand and address the potential risk of LLMs using steganography to hide messages, which could undermine the faithfulness of their reasoning and pose challenges to detecting unaligned AI agents.

Method: The research involves fine-tuning LLMs via reinforcement learning to explore three aspects: developing covert encoding schemes, engaging in steganography when prompted, and utilizing steganography in realistic scenarios without being prompted. Evaluations are conducted both in fine-tuning experiments and behavioral non fine-tuning evaluations.

Result: Current LLMs show basic steganographic abilities in terms of security and capacity. However, with explicit algorithmic guidance, there is a significant improvement in their capacity for information concealment.

Conclusion: While current LLMs have rudimentary steganographic abilities, they can be greatly enhanced with proper algorithmic guidance. This implies the need for further research and development in ensuring the faithfulness and alignment of LLMs.

Abstract: The potential for large language models (LLMs) to hide messages within plain
text (steganography) poses a challenge to detection and thwarting of unaligned
AI agents, and undermines faithfulness of LLMs reasoning. We explore the
steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)
to: (1) develop covert encoding schemes, (2) engage in steganography when
prompted, and (3) utilize steganography in realistic scenarios where hidden
reasoning is likely, but not prompted. In these scenarios, we detect the
intention of LLMs to hide their reasoning as well as their steganography
performance. Our findings in the fine-tuning experiments as well as in
behavioral non fine-tuning evaluations reveal that while current models exhibit
rudimentary steganographic abilities in terms of security and capacity,
explicit algorithmic guidance markedly enhances their capacity for information
concealment.

</details>


### [3] [Enhancing LLMs' Clinical Reasoning with Real-World Data from a Nationwide Sepsis Registry](https://arxiv.org/abs/2505.02722)
*Junu Kim,Chaeeun Shim,Sungjin Park,Su Yeon Lee,Gee Young Suh,Chae-Man Lim,Seong Jin Choi,Song Mi Moon,Kyoung-Ho Song,Eu Suk Kim,Hong Bin Kim,Sejoong Kim,Chami Im,Dong-Wan Kang,Yong Soo Kim,Hee-Joon Bae,Sung Yoon Lim,Han-Gil Jeong,Edward Choi*

Main category: cs.AI

TL;DR: Large Language Models (LLMs) show great potential but lack effectiveness in clinical practice due to limited exposure to real-world clinical data. This study proposes enhancing LLMs' clinical reasoning through real-world clinical data, specifically by constructing reasoning-intensive questions from a sepsis registry and fine-tuning Phi-4 using reinforcement learning, leading to the creation of C-Reason. C-Reason demonstrates strong clinical reasoning capabilities and generalizes well across different datasets and tasks.


<details>
  <summary>Details</summary>
Motivation: To improve the clinical reasoning capabilities of LLMs by addressing their insufficient exposure to real-world clinical data during training.

Method: Constructed reasoning-intensive questions from a nationwide sepsis registry and fine-tuned Phi-4 on these questions using reinforcement learning, resulting in C-Reason.

Result: C-Reason exhibited strong clinical reasoning capabilities not only on the in-domain test set but also generalized to other datasets involving different tasks and patient cohorts.

Conclusion: Future research should focus on training LLMs with large-scale, multi-disease clinical datasets to develop more powerful, general-purpose clinical reasoning models.

Abstract: Although large language models (LLMs) have demonstrated impressive reasoning
capabilities across general domains, their effectiveness in real-world clinical
practice remains limited. This is likely due to their insufficient exposure to
real-world clinical data during training, as such data is typically not
included due to privacy concerns. To address this, we propose enhancing the
clinical reasoning capabilities of LLMs by leveraging real-world clinical data.
We constructed reasoning-intensive questions from a nationwide sepsis registry
and fine-tuned Phi-4 on these questions using reinforcement learning, resulting
in C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the
in-domain test set, as evidenced by both quantitative metrics and expert
evaluations. Furthermore, its enhanced reasoning capabilities generalized to a
sepsis dataset involving different tasks and patient cohorts, an open-ended
consultations on antibiotics use task, and other diseases. Future research
should focus on training LLMs with large-scale, multi-disease clinical datasets
to develop more powerful, general-purpose clinical reasoning models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [DSADF: Thinking Fast and Slow for Decision Making](https://arxiv.org/abs/2505.08189)
*Alex Zhihao Dou,Dongfei Cui,Jun Yan,Weida Wang,Benteng Chen,Haoming Wang,Zeke Xie,Shufei Zhang*

Main category: cs.LG

TL;DR: To solve the generalization problem of RL agents in dynamic settings, this paper proposes DSADF which integrates fast intuitive decision making (System 1) and deep analytical reasoning (System 2). Empirical results show significant improvements in decision-making abilities.


<details>
  <summary>Details</summary>
Motivation: RL agents struggle to generalize their learned policies to dynamic settings. Using LLMs or VLMs has been explored but lacks seamless coordination between RL agents and foundation models.

Method: Propose DSADF integrating two complementary modules: System 1 with an RL agent and memory space for fast decisions, and System 2 driven by a VLM for deep reasoning.

Result: Empirical study in Crafter and Housekeep environments demonstrates significant improvements in decision-making abilities for both unseen and known tasks.

Conclusion: DSADF facilitates efficient and adaptive decision-making by combining the strengths of fast intuitive decision making and deep analytical reasoning.

Abstract: Although Reinforcement Learning (RL) agents are effective in well-defined
environments, they often struggle to generalize their learned policies to
dynamic settings due to their reliance on trial-and-error interactions. Recent
work has explored applying Large Language Models (LLMs) or Vision Language
Models (VLMs) to boost the generalization of RL agents through policy
optimization guidance or prior knowledge. However, these approaches often lack
seamless coordination between the RL agent and the foundation model, leading to
unreasonable decision-making in unfamiliar environments and efficiency
bottlenecks. Making full use of the inferential capabilities of foundation
models and the rapid response capabilities of RL agents and enhancing the
interaction between the two to form a dual system is still a lingering
scientific question. To address this problem, we draw inspiration from
Kahneman's theory of fast thinking (System 1) and slow thinking (System 2),
demonstrating that balancing intuition and deep reasoning can achieve nimble
decision-making in a complex world. In this study, we propose a Dual-System
Adaptive Decision Framework (DSADF), integrating two complementary modules:
System 1, comprising an RL agent and a memory space for fast and intuitive
decision making, and System 2, driven by a VLM for deep and analytical
reasoning. DSADF facilitates efficient and adaptive decision-making by
combining the strengths of both systems. The empirical study in the video game
environment: Crafter and Housekeep demonstrates the effectiveness of our
proposed method, showing significant improvements in decision abilities for
both unseen and known tasks.

</details>


### [5] [MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering](https://arxiv.org/abs/2505.07782)
*Rushi Qiang,Yuchen Zhuang,Yinghao Li,Dingu Sagar V K,Rongzhi Zhang,Changhao Li,Ian Shu-Hei Wong,Sherry Yang,Percy Liang,Chao Zhang,Bo Dai*

Main category: cs.LG

TL;DR: The paper presents MLE-Dojo, a framework for reinforcement learning and evaluating autonomous LLM agents in MLE workflows. It provides an interactive environment based on real-world Kaggle challenges, supporting comprehensive agent training and revealing limitations of current LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks primarily rely on static datasets or single-attempt evaluations, lacking the ability to provide structured feedback loops for iterative improvement.

Method: MLE-Dojo is built upon 200+ real-world Kaggle challenges, offering an interactive environment that supports both supervised fine-tuning and reinforcement learning for agent training.

Result: Evaluations show that while current LLMs can achieve meaningful iterative improvements, they still have significant limitations in generating long-term solutions and resolving complex errors efficiently.

Conclusion: MLE-Dojo's flexible architecture promotes interoperability, scalability, and reproducibility, and it is open-sourced to foster community-driven innovation.

Abstract: We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement
learning, evaluating, and improving autonomous large language model (LLM)
agents in iterative machine learning engineering (MLE) workflows. Unlike
existing benchmarks that primarily rely on static datasets or single-attempt
evaluations, MLE-Dojo provides an interactive environment enabling agents to
iteratively experiment, debug, and refine solutions through structured feedback
loops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,
open-ended MLE tasks carefully curated to reflect realistic engineering
scenarios such as data processing, architecture search, hyperparameter tuning,
and code debugging. Its fully executable environment supports comprehensive
agent training via both supervised fine-tuning and reinforcement learning,
facilitating iterative experimentation, realistic data sampling, and real-time
outcome verification. Extensive evaluations of eight frontier LLMs reveal that
while current models achieve meaningful iterative improvements, they still
exhibit significant limitations in autonomously generating long-horizon
solutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's
flexible and extensible architecture seamlessly integrates diverse data
sources, tools, and evaluation protocols, uniquely enabling model-based agent
tuning and promoting interoperability, scalability, and reproducibility. We
open-source our framework and benchmarks to foster community-driven innovation
towards next-generation MLE agents.

</details>


### [6] [Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains](https://arxiv.org/abs/2505.07274)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 将大型语言模型（LLMs）作为强化学习（RL）中的先验知识进行整合，虽然有显著优势但计算成本高昂。本文提出了一种基于LLM先验知识的后验采样高效缓存框架，大幅降低了计算成本同时保持高性能。通过适应性缓存机制和代理梯度元优化，该方法在离散文本环境和连续控制领域中均表现出色，减少了LLM查询次数和延迟时间，同时保留了大部分未缓存性能。理论分析和实验证明了近似质量，并且该框架可扩展到离线RL，进一步提高了性能并减少了训练时间。


<details>
  <summary>Details</summary>
Motivation: 尽管将大型语言模型（LLMs）作为强化学习（RL）中的先验知识具有显著优势，但其带来的计算成本问题亟待解决。因此，需要一种能够在维持高性能的同时降低计算成本的方法。

Method: 提出了一种基于LLM先验知识的后验采样高效缓存框架，核心是适应性缓存机制。缓存参数通过来自策略性能的代理梯度进行元优化，从而实现高效推理。该方法适用于离散文本环境和连续控制领域，并且可以扩展到离线RL。

Result: 在多个环境中，与未使用缓存的情况相比，LLM查询次数减少了3.8-4.7倍，中位延迟降低了4.0-12.0倍，同时保留了96-98%的性能。在离线RL中，CQL-Prior变体提高了14-29%的性能并减少了38-40%的训练时间。

Conclusion: 提出的缓存高效框架在降低计算成本的同时保持了高性能，适用于多种任务环境，并且在资源受限的情况下展示了通用性和实际可行性。

Abstract: Integrating large language models (LLMs) as priors in reinforcement learning
(RL) offers significant advantages but comes with substantial computational
costs. We present a principled cache-efficient framework for posterior sampling
with LLM-derived priors that dramatically reduces these costs while maintaining
high performance. At the core of our approach is an adaptive caching mechanism,
where cache parameters are meta-optimized using surrogate gradients derived
from policy performance. This design enables efficient inference across both
discrete text environments (e.g., TextWorld, ALFWorld) and continuous control
domains (e.g., MuJoCo), achieving a 3.8--4.7$\times$ reduction in LLM queries
and 4.0--12.0$\times$ lower median latencies (85--93\,ms on a consumer GPU)
while retaining 96--98\% of uncached performance. Our theoretical analysis
provides KL divergence bounds on approximation quality, validated empirically.
The framework extends to offline RL, where our CQL-Prior variant improves
performance by 14--29\% and reduces training time by 38--40\%. Extensive
evaluations across a diverse suite of eight tasks demonstrate the
generalizability and practical viability of LLM-guided RL in
resource-constrained settings.

</details>


### [7] [Remote Rowhammer Attack using Adversarial Observations on Federated Learning Clients](https://arxiv.org/abs/2505.06335)
*Jinsheng Yuan,Yuhang Hao,Weisi Guo,Yun Wu,Chongyan Gu*

Main category: cs.LG

TL;DR: 通过攻击特定客户端，可以远程发起对服务器内存的Rowhammer攻击，从而导致服务器内存位翻转，影响联邦学习的安全性。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习（FL）中的安全性研究主要集中在保护边缘客户端的数据隐私或客户端与服务器之间的通信渠道安全，而针对服务器端的客户端攻击研究较少。本文旨在探讨通过攻击某些特定客户端，是否可以在服务器上远程引发Rowhammer攻击，从而破坏服务器内存。

Method: 利用强化学习（RL）攻击者，通过操控客户端的传感器观察值，最大化服务器的重复内存更新率（RUR）。在大规模联邦学习自动语音识别（ASR）系统中验证该方法，并使用稀疏更新来增加攻击效果。

Result: 在目标服务器模型中，对抗攻击代理能够达到约70%的重复更新率（RUR），成功诱导服务器DRAM上的位翻转。

Conclusion: 此攻击方式可能导致学习中断或意外提升权限，为联邦学习和硬件设计的实际缓解策略研究开辟了新途径。

Abstract: Federated Learning (FL) has the potential for simultaneous global learning
amongst a large number of parallel agents, enabling emerging AI such as LLMs to
be trained across demographically diverse data. Central to this being efficient
is the ability for FL to perform sparse gradient updates and remote direct
memory access at the central server. Most of the research in FL security
focuses on protecting data privacy at the edge client or in the communication
channels between the client and server. Client-facing attacks on the server are
less well investigated as the assumption is that a large collective of clients
offer resilience.
  Here, we show that by attacking certain clients that lead to a high frequency
repetitive memory update in the server, we can remote initiate a rowhammer
attack on the server memory. For the first time, we do not need backdoor access
to the server, and a reinforcement learning (RL) attacker can learn how to
maximize server repetitive memory updates by manipulating the client's sensor
observation. The consequence of the remote rowhammer attack is that we are able
to achieve bit flips, which can corrupt the server memory. We demonstrate the
feasibility of our attack using a large-scale FL automatic speech recognition
(ASR) systems with sparse updates, our adversarial attacking agent can achieve
around 70\% repeated update rate (RUR) in the targeted server model,
effectively inducing bit flips on server DRAM. The security implications are
that can cause disruptions to learning or may inadvertently cause elevated
privilege. This paves the way for further research on practical mitigation
strategies in FL and hardware design.

</details>


### [8] [DMRL: Data- and Model-aware Reward Learning for Data Extraction](https://arxiv.org/abs/2505.06284)
*Zhiqiang Wang,Ruoxi Cheng*

Main category: cs.LG

TL;DR: The paper proposes DMRL, a Data- and Model-aware Reward Learning approach to extract sensitive data from LLMs using inverse reinforcement learning. It outperforms baselines in data extraction.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerabilities of LLMs to unintended privacy breaches and overcome limitations in current data extraction methods.

Method: DMRL consists of constructing an introspective reasoning dataset capturing leakage mindsets and training reward models with Group Relative Policy Optimization (GRPO), dynamically tuning optimization based on task difficulty at both data and model levels.

Result: Comprehensive experiments across various LLMs show that DMRL outperforms all baseline methods in data extraction performance.

Conclusion: DMRL is an effective approach for extracting sensitive data from LLMs.

Abstract: Large language models (LLMs) are inherently vulnerable to unintended privacy
breaches. Consequently, systematic red-teaming research is essential for
developing robust defense mechanisms. However, current data extraction methods
suffer from several limitations: (1) rely on dataset duplicates (addressable
via deduplication), (2) depend on prompt engineering (now countered by
detection and defense), and (3) rely on random-search adversarial generation.
To address these challenges, we propose DMRL, a Data- and Model-aware Reward
Learning approach for data extraction. This technique leverages inverse
reinforcement learning to extract sensitive data from LLMs. Our method consists
of two main components: (1) constructing an introspective reasoning dataset
that captures leakage mindsets to guide model behavior, and (2) training reward
models with Group Relative Policy Optimization (GRPO), dynamically tuning
optimization based on task difficulty at both the data and model levels.
Comprehensive experiments across various LLMs demonstrate that DMRL outperforms
all baseline methods in data extraction performance.

</details>


### [9] [Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers](https://arxiv.org/abs/2505.04842)
*Kusha Sareen,Morgane M Moss,Alessandro Sordoni,Rishabh Agarwal,Arian Hosseini*

Main category: cs.LG

TL;DR: RL$^V$ is an augmented reinforcement learning method that adds verification capabilities to value-free RL methods, boosting MATH accuracy and enabling efficient test-time compute scaling.


<details>
  <summary>Details</summary>
Motivation: Prevalent RL methods for fine-tuning LLM reasoners abandon the learned value function in favor of empirically estimated returns, which hinders test-time compute scaling.

Method: RL$^V$ jointly trains the LLM as both a reasoner and a generative verifier using RL-generated data.

Result: Empirically, RL$^V$ boosts MATH accuracy by over 20\% with parallel sampling and enables $8-32\times$ efficient test-time compute scaling. It also exhibits strong generalization capabilities and achieves $1.2-1.6\times$ higher performance when jointly scaling parallel and sequential test-time compute.

Conclusion: RL$^V$ successfully augments value-free RL methods by adding verification capabilities without significant overhead.

Abstract: Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners,
such as GRPO or Leave-one-out PPO, abandon the learned value function in favor
of empirically estimated returns. This hinders test-time compute scaling that
relies on using the value-function for verification. In this work, we propose
RL$^V$ that augments any ``value-free'' RL method by jointly training the LLM
as both a reasoner and a generative verifier using RL-generated data, adding
verification capabilities without significant overhead. Empirically, RL$^V$
boosts MATH accuracy by over 20\% with parallel sampling and enables
$8-32\times$ efficient test-time compute scaling compared to the base RL
method. RL$^V$ also exhibits strong generalization capabilities for both
easy-to-hard and out-of-domain tasks. Furthermore, RL$^V$ achieves
$1.2-1.6\times$ higher performance when jointly scaling parallel and sequential
test-time compute with a long reasoning R1 model.

</details>


### [10] [DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning](https://arxiv.org/abs/2505.03209)
*Borui Wang,Kathleen McKeown,Rex Ying*

Main category: cs.LG

TL;DR: Reinforcement learning from expert demonstrations is challenging. Current methods suffer poor generalization, low sample efficiency, and poor model interpretability. This paper proposes DYSTIL, a novel framework integrated with LLMs to overcome these limitations. DYSTIL dynamically queries a strategy-generating LLM based on advantage estimations and expert demonstrations, improving policy generalization and sample efficiency while providing interpretability.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenges in reinforcement learning from expert demonstrations, particularly the issues of poor generalization, low sample efficiency, and poor model interpretability that current state-of-the-art methods face.

Method: The method proposed in this paper is called DYnamic STrategy Induction with Llms for reinforcement learning (DYSTIL). It involves dynamically querying a strategy-generating LLM to induce textual strategies based on advantage estimations and expert demonstrations. These strategies are then internalized into the RL agent through policy optimization.

Result: The results show that DYSTIL significantly outperforms state-of-the-art baseline methods by 17.75% in average success rate and also enjoys higher sample efficiency during the learning process. These findings were empirically demonstrated through testing over challenging RL environments from Minigrid and BabyAI.

Conclusion: The conclusion is that DYSTIL, a novel strategy-based reinforcement learning framework integrated with LLMs, successfully overcomes the limitations of poor generalization, low sample efficiency, and poor model interpretability in reinforcement learning from expert demonstrations.

Abstract: Reinforcement learning from expert demonstrations has long remained a
challenging research problem, and existing state-of-the-art methods using
behavioral cloning plus further RL training often suffer from poor
generalization, low sample efficiency, and poor model interpretability.
Inspired by the strong reasoning abilities of large language models (LLMs), we
propose a novel strategy-based reinforcement learning framework integrated with
LLMs called DYnamic STrategy Induction with Llms for reinforcement learning
(DYSTIL) to overcome these limitations. DYSTIL dynamically queries a
strategy-generating LLM to induce textual strategies based on advantage
estimations and expert demonstrations, and gradually internalizes induced
strategies into the RL agent through policy optimization to improve its
performance through boosting policy generalization and enhancing sample
efficiency. It also provides a direct textual channel to observe and interpret
the evolution of the policy's underlying strategies during training. We test
DYSTIL over challenging RL environments from Minigrid and BabyAI, and
empirically demonstrate that DYSTIL significantly outperforms state-of-the-art
baseline methods by 17.75% in average success rate while also enjoying higher
sample efficiency during the learning process.

</details>


### [11] [VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making](https://arxiv.org/abs/2505.03181)
*Jake Grigsby,Yuke Zhu,Michael Ryoo,Juan Carlos Niebles*

Main category: cs.LG

TL;DR: This paper explores an off-policy reinforcement learning solution to fine-tune vision-language models (VLMs) for agent tasks, enabling them to learn from unsuccessful decisions and low-quality datasets.


<details>
  <summary>Details</summary>
Motivation: Agent tasks require skills where open-weight VLMs lag behind LLMs, such as following strict output syntax requirements. Supervised fine-tuning on task-specific expert demonstrations is required to overcome these limitations.

Method: The work approaches the challenges of enhancing VLM capabilities for agent tasks from an offline-to-online reinforcement learning perspective, allowing the model to learn from its own unsuccessful decisions or those of more capable models.

Result: The technique was demonstrated using two open-weight VLMs across three multi-modal agent domains, showing potential for agents to self-improve and learn effectively from low-quality datasets.

Conclusion: An off-policy RL solution retains the stability and simplicity of supervised fine-tuning while enabling VLMs used in agents to improve through learning from a broader range of experiences.

Abstract: Recent research looks to harness the general knowledge and reasoning of large
language models (LLMs) into agents that accomplish user-specified goals in
interactive environments. Vision-language models (VLMs) extend LLMs to
multi-modal data and provide agents with the visual reasoning necessary for new
applications in areas such as computer automation. However, agent tasks
emphasize skills where accessible open-weight VLMs lag behind their LLM
equivalents. For example, VLMs are less capable of following an environment's
strict output syntax requirements and are more focused on open-ended question
answering. Overcoming these limitations requires supervised fine-tuning (SFT)
on task-specific expert demonstrations. Our work approaches these challenges
from an offline-to-online reinforcement learning (RL) perspective. RL lets us
fine-tune VLMs to agent tasks while learning from the unsuccessful decisions of
our own model or more capable (larger) models. We explore an off-policy RL
solution that retains the stability and simplicity of the widely used SFT
workflow while allowing our agent to self-improve and learn from low-quality
datasets. We demonstrate this technique with two open-weight VLMs across three
multi-modal agent domains.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 在大规模语言模型（LLMs）中，通过重复采样来扩展推理计算，随着样本数量的增加，覆盖率（解决问题的比例）会持续提高。本文研究了这一现象部分是由于标准评估基准中的答案分布偏向于较小的常见答案集。通过定义一个基于训练集中答案频率的基线，发现对于某些LLMs，该基线比重复模型采样表现更好；而对于其他模型，其覆盖率与一种混合策略相当。此基线有助于更准确地测量重复采样在提示无关猜测之外的覆盖率提升。


<details>
  <summary>Details</summary>
Motivation: 研究重复采样在大语言模型推理中的效果，并分析这种效果是否与评估基准中答案分布的偏斜有关。

Method: 定义了一个根据训练集中答案频率枚举答案的基线，并在数学推理和事实知识两个领域进行了实验。比较了该基线、重复模型采样以及混合策略的表现。

Result: 对于某些LLMs，基于训练集答案频率的基线优于重复模型采样；而对其他模型，其表现与混合策略相当。

Conclusion: 重复采样的性能提升部分归因于评估基准中答案分布的偏斜。使用基于训练集答案频率的基线可以更准确地衡量重复采样在提示无关猜测之外的实际改进。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>
