{"id": "2410.15466", "keyword": "LLM reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing."}
{"id": "2505.00018", "keyword": "LLM Agent", "pdf": "https://arxiv.org/pdf/2505.00018", "abs": "https://arxiv.org/abs/2505.00018", "authors": ["Ju Wu", "Calvin K. L. Or"], "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability."}
{"id": "2505.11475", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.11475", "abs": "https://arxiv.org/abs/2505.11475", "authors": ["Zhilin Wang", "Jiaqi Zeng", "Olivier Delalleau", "Hoo-Chang Shin", "Felipe Soares", "Alexander Bukharin", "Ellie Evans", "Yi Dong", "Oleksii Kuchaiev"], "title": "HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "38 pages, 2 figures", "summary": "Preference datasets are essential for training general-domain,\ninstruction-following language models with Reinforcement Learning from Human\nFeedback (RLHF). Each subsequent data release raises expectations for future\ndata collection, meaning there is a constant need to advance the quality and\ndiversity of openly available preference data. To address this need, we\nintroduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0),\nhigh-quality, human-annotated preference dataset comprising of over 40,000\nsamples. These samples span diverse real-world applications of large language\nmodels (LLMs), including tasks relating to STEM, coding and multilingual\nscenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that\nachieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This\nrepresents a substantial improvement (~10% absolute) over the previously\nbest-reported results from existing RMs. We demonstrate HelpSteer3-Preference\ncan also be applied to train Generative RMs and how policy models can be\naligned with RLHF using our RMs. Dataset (CC-BY-4.0):\nhttps://huggingface.co/datasets/nvidia/HelpSteer3#preference"}
{"id": "2505.11409", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.11409", "abs": "https://arxiv.org/abs/2505.11409", "authors": ["Yi Xu", "Chengzu Li", "Han Zhou", "Xingchen Wan", "Caiqi Zhang", "Anna Korhonen", "Ivan VuliÄ‡"], "title": "Visual Planning: Let's Think Only with Images", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "10 pages, 6 figures, 1 table (26 pages, 12 figures, 8 tables\n  including references and appendices)", "summary": "Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference."}
{"id": "2505.11227", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.11227", "abs": "https://arxiv.org/abs/2505.11227", "authors": ["Zhangying Feng", "Qianglong Chen", "Ning Lu", "Yongqian Li", "Siqi Cheng", "Shuangmu Peng", "Duyu Tang", "Shengcai Liu", "Zhirui Zhang"], "title": "Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "The development of reasoning capabilities represents a critical frontier in\nlarge language models (LLMs) research, where reinforcement learning (RL) and\nprocess reward models (PRMs) have emerged as predominant methodological\nframeworks. Contrary to conventional wisdom, empirical evidence from\nDeepSeek-R1 demonstrates that pure RL training focused on mathematical\nproblem-solving can progressively enhance reasoning abilities without PRM\nintegration, challenging the perceived necessity of process supervision. In\nthis study, we conduct a systematic investigation of the relationship between\nRL training and PRM capabilities. Our findings demonstrate that problem-solving\nproficiency and process supervision capabilities represent complementary\ndimensions of reasoning that co-evolve synergistically during pure RL training.\nIn particular, current PRMs underperform simple baselines like majority voting\nwhen applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To\naddress this limitation, we propose Self-PRM, an introspective framework in\nwhich models autonomously evaluate and rerank their generated solutions through\nself-reward mechanisms. Although Self-PRM consistently improves the accuracy of\nthe benchmark (particularly with larger sample sizes), analysis exposes\npersistent challenges: The approach exhibits low precision (<10\\%) on difficult\nproblems, frequently misclassifying flawed solutions as valid. These analyses\nunderscore the need for continued RL scaling to improve reward alignment and\nintrospective accuracy. Overall, our findings suggest that PRM may not be\nessential for enhancing complex reasoning, as pure RL not only improves\nproblem-solving skills but also inherently fosters robust PRM capabilities. We\nhope these findings provide actionable insights for building more reliable and\nself-aware complex reasoning models."}
{"id": "2505.11081", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.11081", "abs": "https://arxiv.org/abs/2505.11081", "authors": ["Pierre Clavier", "Nathan Grinsztajn", "Raphael Avalos", "Yannis Flet-Berliac", "Irem Ergun", "Omar D. Domingues", "Eugene Tarassov", "Olivier Pietquin", "Pierre H. Richemond", "Florian Strub", "Matthieu Geist"], "title": "ShiQ: Bringing back Bellman to LLMs", "categories": ["cs.LG"], "comment": null, "summary": "The fine-tuning of pre-trained large language models (LLMs) using\nreinforcement learning (RL) is generally formulated as direct policy\noptimization. This approach was naturally favored as it efficiently improves a\npretrained LLM, seen as an initial policy. Another RL paradigm, Q-learning\nmethods, has received far less attention in the LLM community while\ndemonstrating major success in various non-LLM RL tasks. In particular,\nQ-learning effectiveness comes from its sample efficiency and ability to learn\noffline, which is particularly valuable given the high computational cost of\nsampling with LLMs. However, naively applying a Q-learning-style update to the\nmodel's logits is ineffective due to the specificity of LLMs. Our core\ncontribution is to derive theoretically grounded loss functions from Bellman\nequations to adapt Q-learning methods to LLMs. To do so, we carefully adapt\ninsights from the RL literature to account for LLM-specific characteristics,\nensuring that the logits become reliable Q-value estimates. We then use this\nloss to build a practical algorithm, ShiQ for Shifted-Q, that supports\noff-policy, token-wise learning while remaining simple to implement. Finally,\nwe evaluate ShiQ on both synthetic data and real-world benchmarks, e.g.,\nUltraFeedback and BFCL-V3, demonstrating its effectiveness in both single-turn\nand multi-turn LLM settings"}
{"id": "2505.10992", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.10992", "abs": "https://arxiv.org/abs/2505.10992", "authors": ["Feiran You", "Hongyang Du"], "title": "ReaCritic: Large Reasoning Transformer-based DRL Critic-model Scaling For Heterogeneous Networks", "categories": ["cs.LG", "cs.NI"], "comment": null, "summary": "Heterogeneous Networks (HetNets) pose critical challenges for intelligent\nmanagement due to the diverse user requirements and time-varying wireless\nconditions. These factors introduce significant decision complexity, which\nlimits the adaptability of existing Deep Reinforcement Learning (DRL) methods.\nIn many DRL algorithms, especially those involving value-based or actor-critic\nstructures, the critic component plays a key role in guiding policy learning by\nestimating value functions. However, conventional critic models often use\nshallow architectures that map observations directly to scalar estimates,\nlimiting their ability to handle multi-task complexity. In contrast, recent\nprogress in inference-time scaling of Large Language Models (LLMs) has shown\nthat generating intermediate reasoning steps can significantly improve decision\nquality. Motivated by this, we propose ReaCritic, a large reasoning\ntransformer-based criticmodel scaling scheme that brings reasoning ability into\nDRL. ReaCritic performs horizontal reasoning over parallel state-action inputs\nand vertical reasoning through deep transformer stacks. It is compatible with a\nbroad range of value-based and actor-critic DRL algorithms and enhances\ngeneralization in dynamic wireless environments. Extensive experiments\ndemonstrate that ReaCritic improves convergence speed and final performance\nacross various HetNet settings and standard OpenAI Gym control tasks."}
{"id": "2505.10978", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.10978", "abs": "https://arxiv.org/abs/2505.10978", "authors": ["Lang Feng", "Zhenghai Xue", "Tingcong Liu", "Bo An"], "title": "Group-in-Group Policy Optimization for LLM Agent Training", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "Recent advances in group-based reinforcement learning (RL) have driven\nfrontier large language models (LLMs) in single-turn tasks like mathematical\nreasoning. However, their scalability to long-horizon LLM agent training\nremains limited. Unlike static tasks, agent-environment interactions unfold\nover many steps and often yield sparse or delayed rewards, making credit\nassignment across individual steps significantly more challenging. In this\nwork, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL\nalgorithm that achieves fine-grained credit assignment for LLM agents while\npreserving the appealing properties of group-based RL: critic-free, low memory,\nand stable convergence. GiGPO introduces a two-level structure for estimating\nrelative advantage: (i) At the episode-level, GiGPO computes macro relative\nadvantages based on groups of complete trajectories; (ii) At the step-level,\nGiGPO introduces an anchor state grouping mechanism that retroactively\nconstructs step-level groups by identifying repeated environment states across\ntrajectories. Actions stemming from the same state are grouped together,\nenabling micro relative advantage estimation. This hierarchical structure\neffectively captures both global trajectory quality and local step\neffectiveness without relying on auxiliary models or additional rollouts. We\nevaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using\nQwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers\nfine-grained per-step credit signals and achieves performance gains of > 12\\%\non ALFWorld and > 9\\% on WebShop over the GRPO baseline: all while maintaining\nthe same GPU memory overhead, identical LLM rollout, and incurring little to no\nadditional time cost."}
{"id": "2505.10861", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.10861", "abs": "https://arxiv.org/abs/2505.10861", "authors": ["Thang Duong", "Minglai Yang", "Chicheng Zhang"], "title": "Improving the Data-efficiency of Reinforcement Learning by Warm-starting with LLM", "categories": ["cs.LG"], "comment": "31 pages (9 for the main paper), 27 figures, NeurIPS 25 submission", "summary": "We investigate the usage of Large Language Model (LLM) in collecting\nhigh-quality data to warm-start Reinforcement Learning (RL) algorithms for\nlearning in some classical Markov Decision Process (MDP) environments. In this\nwork, we focus on using LLM to generate an off-policy dataset that sufficiently\ncovers state-actions visited by optimal policies, then later using an RL\nalgorithm to explore the environment and improve the policy suggested by the\nLLM. Our algorithm, LORO, can both converge to an optimal policy and have a\nhigh sample efficiency thanks to the LLM's good starting policy. On multiple\nOpenAI Gym environments, such as CartPole and Pendulum, we empirically\ndemonstrate that LORO outperforms baseline algorithms such as pure LLM-based\npolicies, pure RL, and a naive combination of the two, achieving up to $4\n\\times$ the cumulative rewards of the pure RL baseline."}
{"id": "2505.10425", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.10425", "abs": "https://arxiv.org/abs/2505.10425", "authors": ["Jingyao Wang", "Wenwen Qiang", "Zeen Song", "Changwen Zheng", "Hui Xiong"], "title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) excel at complex tasks thanks to advances in\nreasoning abilities. However, existing methods overlook the trade-off between\nreasoning effectiveness and computational efficiency, often encouraging\nunnecessarily long reasoning chains and wasting tokens. To address this, we\npropose Learning to Think (L2T), an information-theoretic reinforcement\nfine-tuning framework for LLMs to make the models achieve optimal reasoning\nwith fewer tokens. Specifically, L2T treats each query-response interaction as\na hierarchical session of multiple episodes and proposes a universal dense\nprocess reward, i.e., quantifies the episode-wise information gain in\nparameters, requiring no extra annotations or task-specific evaluators. We\npropose a method to quickly estimate this reward based on PAC-Bayes bounds and\nthe Fisher information matrix. Theoretical analyses show that it significantly\nreduces computational complexity with high estimation accuracy. By immediately\nrewarding each episode's contribution and penalizing excessive updates, L2T\noptimizes the model via reinforcement learning to maximize the use of each\nepisode and achieve effective updates. Empirical results on various reasoning\nbenchmarks and base models demonstrate the advantage of L2T across different\ntasks, boosting both reasoning effectiveness and efficiency."}
{"id": "2505.10320", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.10320", "abs": "https://arxiv.org/abs/2505.10320", "authors": ["Chenxi Whitehouse", "Tianlu Wang", "Ping Yu", "Xian Li", "Jason Weston", "Ilia Kulikov", "Swarnadeep Saha"], "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 8 tables, 11 figures", "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses."}
{"id": "2505.10182", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.10182", "abs": "https://arxiv.org/abs/2505.10182", "authors": ["Yoichi Ishibashi", "Taro Yano", "Masafumi Oyamada"], "title": "Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant improvements in\nreasoning capabilities through supervised fine-tuning and reinforcement\nlearning. However, when training reasoning models, these approaches are\nprimarily applicable to specific domains such as mathematics and programming,\nwhich imposes fundamental constraints on the breadth and scalability of\ntraining data. In contrast, continual pretraining (CPT) offers the advantage of\nnot requiring task-specific signals. Nevertheless, how to effectively\nsynthesize training data for reasoning and how such data affect a wide range of\ndomains remain largely unexplored. This study provides a detailed evaluation of\nReasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden\nthought processes underlying texts, based on the premise that texts are the\nresult of the author's thinking process. Specifically, we apply Reasoning CPT\nto Gemma2-9B using synthetic data with hidden thoughts derived from STEM and\nLaw corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis\nreveals that Reasoning CPT consistently improves performance across all\nevaluated domains. Notably, reasoning skills acquired in one domain transfer\neffectively to others; the performance gap with conventional methods widens as\nproblem difficulty increases, with gains of up to 8 points on the most\nchallenging problems. Furthermore, models trained with hidden thoughts learn to\nadjust the depth of their reasoning according to problem difficulty."}
