{"id": "2402.00559", "keyword": "Chain of Thoughts", "pdf": "https://arxiv.org/pdf/2402.00559", "abs": "https://arxiv.org/abs/2402.00559", "authors": ["Alon Jacovi", "Yonatan Bitton", "Bernd Bohnet", "Jonathan Herzig", "Or Honovich", "Michael Tseng", "Michael Collins", "Roee Aharoni", "Mor Geva"], "title": "A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains", "categories": ["cs.CL"], "comment": "Accepted to ACL 2024", "summary": "Prompting language models to provide step-by-step answers (e.g.,\n\"Chain-of-Thought\") is the prominent approach for complex reasoning tasks,\nwhere more accurate reasoning chains typically improve downstream task\nperformance. Recent literature discusses automatic methods to verify reasoning\nto evaluate and improve their correctness. However, no fine-grained step-level\ndatasets are available to enable thorough evaluation of such verification\nmethods, hindering progress in this direction. We introduce REVEAL: Reasoning\nVerification Evaluation, a dataset to benchmark automatic verifiers of complex\nChain-of-Thought reasoning in open-domain question-answering settings. REVEAL\nincludes comprehensive labels for the relevance, attribution to evidence\npassages, and logical correctness of each reasoning step in a language model's\nanswer, across a variety of datasets and state-of-the-art language models.\nEvaluation on REVEAL shows that verifiers struggle at verifying reasoning\nchains - in particular, verifying logical correctness and detecting\ncontradictions. Available at https://reveal-dataset.github.io/ ."}
{"id": "2410.15466", "keyword": "LLM reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing."}
{"id": "2505.00018", "keyword": "LLM Agent", "pdf": "https://arxiv.org/pdf/2505.00018", "abs": "https://arxiv.org/abs/2505.00018", "authors": ["Ju Wu", "Calvin K. L. Or"], "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability."}
{"id": "2505.04493", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2505.04493", "abs": "https://arxiv.org/abs/2505.04493", "authors": ["Or Wertheim", "Ronen I. Brafman"], "title": "Model-Based AI planning and Execution Systems for Robotics", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Model-based planning and execution systems offer a principled approach to\nbuilding flexible autonomous robots that can perform diverse tasks by\nautomatically combining a host of basic skills. This idea is almost as old as\nmodern robotics. Yet, while diverse general-purpose reasoning architectures\nhave been proposed since, general-purpose systems that are integrated with\nmodern robotic platforms have emerged only recently, starting with the\ninfluential ROSPlan system. Since then, a growing number of model-based systems\nfor robot task-level control have emerged. In this paper, we consider the\ndiverse design choices and issues existing systems attempt to address, the\ndifferent solutions proposed so far, and suggest avenues for future\ndevelopment."}
{"id": "2412.02441", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2412.02441", "abs": "https://arxiv.org/abs/2412.02441", "authors": ["Shai Shalev-Shwartz", "Amnon Shashua", "Gal Beniamini", "Yoav Levine", "Or Sharir", "Noam Wies", "Ido Ben-Shaul", "Tomer Nussbaum", "Shir Granot Peled"], "title": "Artificial Expert Intelligence through PAC-reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Artificial Expert Intelligence (AEI) seeks to transcend the limitations of\nboth Artificial General Intelligence (AGI) and narrow AI by integrating\ndomain-specific expertise with critical, precise reasoning capabilities akin to\nthose of top human experts. Existing AI systems often excel at predefined tasks\nbut struggle with adaptability and precision in novel problem-solving. To\novercome this, AEI introduces a framework for ``Probably Approximately Correct\n(PAC) Reasoning\". This paradigm provides robust theoretical guarantees for\nreliably decomposing complex problems, with a practical mechanism for\ncontrolling reasoning precision. In reference to the division of human thought\ninto System 1 for intuitive thinking and System 2 for reflective\nreasoning~\\citep{tversky1974judgment}, we refer to this new type of reasoning\nas System 3 for precise reasoning, inspired by the rigor of the scientific\nmethod. AEI thus establishes a foundation for error-bounded, inference-time\nlearning."}
{"id": "2410.15466", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing."}
{"id": "2409.04397", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2409.04397", "abs": "https://arxiv.org/abs/2409.04397", "authors": ["Yotam Erel", "Or Kozlovsky-Mordenfeld", "Daisuke Iwai", "Kosuke Sato", "Amit H. Bermano"], "title": "Casper DPM: Cascaded Perceptual Dynamic Projection Mapping onto Hands", "categories": ["cs.GR"], "comment": "Project page: https://yoterel.github.io/casper-project-page/", "summary": "We present a technique for dynamically projecting 3D content onto human hands\nwith short perceived motion-to-photon latency. Computing the pose and shape of\nhuman hands accurately and quickly is a challenging task due to their\narticulated and deformable nature. We combine a slower 3D coarse estimation of\nthe hand pose with high speed 2D correction steps which improve the alignment\nof the projection to the hands, increase the projected surface area, and reduce\nperceived latency. Since our approach leverages a full 3D reconstruction of the\nhands, any arbitrary texture or reasonably performant effect can be applied,\nwhich was not possible before. We conducted two user studies to assess the\nbenefits of using our method. The results show subjects are less sensitive to\nlatency artifacts and perform faster and with more ease a given associated task\nover the naive approach of directly projecting rendered frames from the 3D pose\nestimation. We demonstrate several novel use cases and applications."}
{"id": "2505.21493", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.21493", "abs": "https://arxiv.org/abs/2505.21493", "authors": ["Xiangxin Zhou", "Zichen Liu", "Anya Sims", "Haonan Wang", "Tianyu Pang", "Chongxuan Li", "Liang Wang", "Min Lin", "Chao Du"], "title": "Reinforcing General Reasoning without Verifiers", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The recent paradigm shift towards training large language models (LLMs) using\nDeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has\nled to impressive advancements in code and mathematical reasoning. However,\nthis methodology is limited to tasks where rule-based answer verification is\npossible and does not naturally extend to real-world domains such as chemistry,\nhealthcare, engineering, law, biology, business, and economics. Current\npractical workarounds use an additional LLM as a model-based verifier; however,\nthis introduces issues such as reliance on a strong verifier LLM,\nsusceptibility to reward hacking, and the practical burden of maintaining the\nverifier model in memory during training. To address this and extend\nDeepSeek-R1-Zero-style training to general reasoning domains, we propose a\nverifier-free method (VeriFree) that bypasses answer verification and instead\nuses RL to directly maximize the probability of generating the reference\nanswer. We compare VeriFree with verifier-based methods and demonstrate that,\nin addition to its significant practical benefits and reduced compute\nrequirements, VeriFree matches and even surpasses verifier-based methods on\nextensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related\nbenchmarks. Moreover, we provide insights into this method from multiple\nperspectives: as an elegant integration of training both the policy and\nimplicit verifier in a unified model, and as a variational optimization\napproach. Code is available at https://github.com/sail-sg/VeriFree."}
{"id": "2408.08654", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2408.08654", "abs": "https://arxiv.org/abs/2408.08654", "authors": ["Li-Fang Zhu", "Fritz Koermann", "Qing Chen", "Malin Selleby", "Joerg Neugebauer", "and Blazej Grabowski"], "title": "Accelerating ab initio melting property calculations with machine learning: Application to the high entropy alloy TaVCrW", "categories": ["cond-mat.mtrl-sci"], "comment": "14 pages, 6 figures", "summary": "Melting properties are critical for designing novel materials, especially for\ndiscovering high-performance, high-melting refractory materials. Experimental\nmeasurements of these properties are extremely challenging due to their high\nmelting temperatures. Complementary theoretical predictions are, therefore,\nindispensable. The conventional free energy approach using density functional\ntheory (DFT) has been a gold standard for such purposes because of its high\naccuracy. However,it generally involves expensive thermodynamic integration\nusing ab initio molecular dynamic simulations. The high computational cost\nmakes high-throughput calculations infeasible. Here, we propose a highly\nefficient DFT-based method aided by a specially designed machine learning\npotential. As the machine learning potential can closely reproduce the ab\ninitio phase space, even for multi-component alloys, the costly thermodynamic\nintegration can be fully substituted with more efficient free energy\nperturbation calculations. The method achieves overall savings of computational\nresources by 80% compared to current alternatives. We apply the method to the\nhigh-entropy alloy TaVCrW and calculate its melting properties, including\nmelting temperature, entropy and enthalpy of fusion, and volume change at the\nmelting point. Additionally, the heat capacities of solid and liquid TaVCrW are\ncalculated. The results agree reasonably with the calphad extrapolated values."}
{"id": "2505.21444", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.21444", "abs": "https://arxiv.org/abs/2505.21444", "authors": ["Sheikh Shafayat", "Fahim Tajwar", "Ruslan Salakhutdinov", "Jeff Schneider", "Andrea Zanette"], "title": "Can Large Reasoning Models Self-Train?", "categories": ["cs.LG"], "comment": "Project website: https://self-rewarding-llm-training.github.io/", "summary": "Scaling the performance of large language models (LLMs) increasingly depends\non methods that reduce reliance on human supervision. Reinforcement learning\nfrom automated verification offers an alternative, but it incurs scalability\nlimitations due to dependency upon human-designed verifiers. Self-training,\nwhere the model's own judgment provides the supervisory signal, presents a\ncompelling direction. We propose an online self-training reinforcement learning\nalgorithm that leverages the model's self-consistency to infer correctness\nsignals and train without any ground-truth supervision. We apply the algorithm\nto challenging mathematical reasoning tasks and show that it quickly reaches\nperformance levels rivaling reinforcement-learning methods trained explicitly\non gold-standard answers. Additionally, we analyze inherent limitations of the\nalgorithm, highlighting how the self-generated proxy reward initially\ncorrelated with correctness can incentivize reward hacking, where confidently\nincorrect outputs are favored. Our results illustrate how self-supervised\nimprovement can achieve significant performance gains without external labels,\nwhile also revealing its fundamental challenges."}
{"id": "2403.11729", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2403.11729", "abs": "https://arxiv.org/abs/2403.11729", "authors": ["Kento Kawaharazuka", "Akihiro Miki", "Masahiro Bando", "Temma Suzuki", "Yoshimoto Ribayashi", "Yasunori Toshimitsu", "Yuya Nagamatsu", "Kei Okada", "and Masayuki Inaba"], "title": "Hardware Design and Learning-Based Software Architecture of Musculoskeletal Wheeled Robot Musashi-W for Real-World Applications", "categories": ["cs.RO"], "comment": "Accepted at Humanoids2022", "summary": "Various musculoskeletal humanoids have been developed so far. While these\nhumanoids have the advantage of their flexible and redundant bodies that mimic\nthe human body, they are still far from being applied to real-world tasks. One\nof the reasons for this is the difficulty of bipedal walking in a flexible\nbody. Thus, we developed a musculoskeletal wheeled robot, Musashi-W, by\ncombining a wheeled base and musculoskeletal upper limbs for real-world\napplications. Also, we constructed its software system by combining static and\ndynamic body schema learning, reflex control, and visual recognition. We show\nthat the hardware and software of Musashi-W can make the most of the advantages\nof the musculoskeletal upper limbs, through several tasks of cleaning by human\nteaching, carrying a heavy object considering muscle addition, and setting a\ntable through dynamic cloth manipulation with variable stiffness."}
{"id": "2505.21097", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.21097", "abs": "https://arxiv.org/abs/2505.21097", "authors": ["Stephen Chung", "Wenyu Du", "Jie Fu"], "title": "Thinker: Learning to Think Fast and Slow", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.6; I.2.8; I.5.1"], "comment": "21 pages", "summary": "Recent studies show that the reasoning capabilities of Large Language Models\n(LLMs) can be improved by applying Reinforcement Learning (RL) to\nquestion-answering (QA) tasks in areas such as math and coding. With a long\ncontext length, LLMs may learn to perform search, as indicated by the\nself-correction behavior observed in DeepSeek R1. However, this search behavior\nis often imprecise and lacks confidence, resulting in long, redundant responses\nand highlighting deficiencies in intuition and verification. Inspired by the\nDual Process Theory in psychology, we introduce a simple modification to the QA\ntask that includes four stages: Fast Thinking, where the LLM must answer within\na strict token budget; Verification, where the model evaluates its initial\nresponse; Slow Thinking, where it refines the initial response with more\ndeliberation; and Summarization, where it distills the refinement from the\nprevious stage into precise steps. Our proposed task improves average accuracy\nfrom 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for\nDeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone\nachieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial\ninference efficiency gains. These findings suggest that intuition and\ndeliberative reasoning are distinct, complementary systems benefiting from\ntargeted training."}
{"id": "2403.00458", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2403.00458", "abs": "https://arxiv.org/abs/2403.00458", "authors": ["Chung Yi See", "Vasco Rato Santos", "Lucas Woodley", "Megan Yeo", "Daniel Palmer", "Shuheng Zhang", "and Ashley Nunes"], "title": "Prices and preferences in the electric vehicle market", "categories": ["econ.EM"], "comment": "Main paper: 5 tables, 2 figures", "summary": "Although electric vehicles are less polluting than gasoline powered vehicles,\nadoption is challenged by higher procurement prices. Existing discourse\nemphasizes EV battery costs as being principally responsible for this price\ndifferential and widespread adoption is routinely conditioned upon battery\ncosts declining. We scrutinize such reasoning by sourcing data on EV attributes\nand market conditions between 2011 and 2023. Our findings are fourfold. First,\nEV prices are influenced principally by the number of amenities, additional\nfeatures, and dealer-installed accessories sold as standard on an EV, and to a\nlesser extent, by EV horsepower. Second, EV range is negatively correlated with\nEV price implying that range anxiety concerns may be less consequential than\nexisting discourse suggests. Third, battery capacity is positively correlated\nwith EV price, due to more capacity being synonymous with the delivery of more\nhorsepower. Collectively, this suggests that higher procurement prices for EVs\nreflects consumer preference for vehicles that are feature dense and more\npowerful. Fourth and finally, accommodating these preferences have produced\nvehicles with lower fuel economy, a shift that reduces envisioned lifecycle\nemissions benefits by at least 3.26 percent, subject to the battery pack\nchemistry leveraged and the carbon intensity of the electrical grid. These\nfindings warrant attention as decarbonization efforts increasingly emphasize\nelectrification as a pathway for complying with domestic and international\nclimate agreements."}
{"id": "2505.20732", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.20732", "abs": "https://arxiv.org/abs/2505.20732", "authors": ["Hanlin Wang", "Chak Tou Leong", "Jiashuo Wang", "Jian Wang", "Wenjie Li"], "title": "SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) holds significant promise for training LLM agents\nto handle complex, goal-oriented tasks that require multi-step interactions\nwith external environments. However, a critical challenge when applying RL to\nthese agentic tasks arises from delayed rewards: feedback signals are typically\navailable only after the entire task is completed. This makes it non-trivial to\nassign delayed rewards to earlier actions, providing insufficient guidance\nregarding environmental constraints and hindering agent training. In this work,\nwe draw on the insight that the ultimate completion of a task emerges from the\ncumulative progress an agent makes across individual steps. We propose Stepwise\nProgress Attribution (SPA), a general reward redistribution framework that\ndecomposes the final reward into stepwise contributions, each reflecting its\nincremental progress toward overall task completion. To achieve this, we train\na progress estimator that accumulates stepwise contributions over a trajectory\nto match the task completion. During policy optimization, we combine the\nestimated per-step contribution with a grounding signal for actions executed in\nthe environment as the fine-grained, intermediate reward for effective agent\ntraining. Extensive experiments on common agent benchmarks (including Webshop,\nALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the\nstate-of-the-art method in both success rate (+2.5\\% on average) and grounding\naccuracy (+1.9\\% on average). Further analyses demonstrate that our method\nremarkably provides more effective intermediate rewards for RL training. Our\ncode is available at https://github.com/WangHanLinHenry/SPA-RL-Agent."}
{"id": "2402.00559", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2402.00559", "abs": "https://arxiv.org/abs/2402.00559", "authors": ["Alon Jacovi", "Yonatan Bitton", "Bernd Bohnet", "Jonathan Herzig", "Or Honovich", "Michael Tseng", "Michael Collins", "Roee Aharoni", "Mor Geva"], "title": "A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains", "categories": ["cs.CL"], "comment": "Accepted to ACL 2024", "summary": "Prompting language models to provide step-by-step answers (e.g.,\n\"Chain-of-Thought\") is the prominent approach for complex reasoning tasks,\nwhere more accurate reasoning chains typically improve downstream task\nperformance. Recent literature discusses automatic methods to verify reasoning\nto evaluate and improve their correctness. However, no fine-grained step-level\ndatasets are available to enable thorough evaluation of such verification\nmethods, hindering progress in this direction. We introduce REVEAL: Reasoning\nVerification Evaluation, a dataset to benchmark automatic verifiers of complex\nChain-of-Thought reasoning in open-domain question-answering settings. REVEAL\nincludes comprehensive labels for the relevance, attribution to evidence\npassages, and logical correctness of each reasoning step in a language model's\nanswer, across a variety of datasets and state-of-the-art language models.\nEvaluation on REVEAL shows that verifiers struggle at verifying reasoning\nchains - in particular, verifying logical correctness and detecting\ncontradictions. Available at https://reveal-dataset.github.io/ ."}
{"id": "2505.20686", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.20686", "abs": "https://arxiv.org/abs/2505.20686", "authors": ["Kianté Brantley", "Mingyu Chen", "Zhaolin Gao", "Jason D. Lee", "Wen Sun", "Wenhao Zhan", "Xuezhou Zhang"], "title": "Accelerating RL for LLM Reasoning with Optimal Advantage Regression", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning\nlarge language models (LLMs) to improve complex reasoning abilities. However,\nstate-of-the-art policy optimization methods often suffer from high\ncomputational overhead and memory consumption, primarily due to the need for\nmultiple generations per prompt and the reliance on critic networks or\nadvantage estimates of the current policy. In this paper, we propose $A$*-PO, a\nnovel two-stage policy optimization framework that directly approximates the\noptimal advantage function and enables efficient training of LLMs for reasoning\ntasks. In the first stage, we leverage offline sampling from a reference policy\nto estimate the optimal value function $V$*, eliminating the need for costly\nonline value estimation. In the second stage, we perform on-policy updates\nusing a simple least-squares regression loss with only a single generation per\nprompt. Theoretically, we establish performance guarantees and prove that the\nKL-regularized RL objective can be optimized without requiring complex\nexploration strategies. Empirically, $A$*-PO achieves competitive performance\nacross a wide range of mathematical reasoning benchmarks, while reducing\ntraining time by up to 2$\\times$ and peak memory usage by over 30% compared to\nPPO, GRPO, and REBEL. Implementation of $A$*-PO can be found at\nhttps://github.com/ZhaolinGao/A-PO."}
{"id": "2208.08580", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2208.08580", "abs": "https://arxiv.org/abs/2208.08580", "authors": ["Gopal Sharma", "Kangxue Yin", "Subhransu Maji", "Evangelos Kalogerakis", "Or Litany", "Sanja Fidler"], "title": "MvDeCor: Multi-view Dense Correspondence Learning for Fine-grained 3D Segmentation", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "project page: https://nv-tlabs.github.io/MvDeCor/", "summary": "We propose to utilize self-supervised techniques in the 2D domain for\nfine-grained 3D shape segmentation tasks. This is inspired by the observation\nthat view-based surface representations are more effective at modeling\nhigh-resolution surface details and texture than their 3D counterparts based on\npoint clouds or voxel occupancy. Specifically, given a 3D shape, we render it\nfrom multiple views, and set up a dense correspondence learning task within the\ncontrastive learning framework. As a result, the learned 2D representations are\nview-invariant and geometrically consistent, leading to better generalization\nwhen trained on a limited number of labeled shapes compared to alternatives\nthat utilize self-supervision in 2D or 3D alone. Experiments on textured\n(RenderPeople) and untextured (PartNet) 3D datasets show that our method\noutperforms state-of-the-art alternatives in fine-grained part segmentation.\nThe improvements over baselines are greater when only a sparse set of views is\navailable for training or when shapes are textured, indicating that MvDeCor\nbenefits from both 2D processing and 3D geometric reasoning."}
{"id": "2505.20671", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.20671", "abs": "https://arxiv.org/abs/2505.20671", "authors": ["Heng Tan", "Hua Yan", "Yu Yang"], "title": "LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "While reinforcement learning (RL) has achieved notable success in various\ndomains, training effective policies for complex tasks remains challenging.\nAgents often converge to local optima and fail to maximize long-term rewards.\nExisting approaches to mitigate training bottlenecks typically fall into two\ncategories: (i) Automated policy refinement, which identifies critical states\nfrom past trajectories to guide policy updates, but suffers from costly and\nuncertain model training; and (ii) Human-in-the-loop refinement, where human\nfeedback is used to correct agent behavior, but this does not scale well to\nenvironments with large or continuous action spaces. In this work, we design a\nlarge language model-guided policy modulation framework that leverages LLMs to\nimprove RL training without additional model training or human intervention. We\nfirst prompt an LLM to identify critical states from a sub-optimal agent's\ntrajectories. Based on these states, the LLM then provides action suggestions\nand assigns implicit rewards to guide policy refinement. Experiments across\nstandard RL benchmarks demonstrate that our method outperforms state-of-the-art\nbaselines, highlighting the effectiveness of LLM-based explanations in\naddressing RL training bottlenecks."}
{"id": "2110.02210", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2110.02210", "abs": "https://arxiv.org/abs/2110.02210", "authors": ["Alexey Nekrasov", "Jonas Schult", "Or Litany", "Bastian Leibe", "Francis Engelmann"], "title": "Mix3D: Out-of-Context Data Augmentation for 3D Scenes", "categories": ["cs.CV"], "comment": "Accepted for publication at 3DV 2021. Camera-ready submission. Link\n  to code: https://github.com/kumuji/mix3d - Project page:\n  https://nekrasov.dev/mix3d/", "summary": "We present Mix3D, a data augmentation technique for segmenting large-scale 3D\nscenes. Since scene context helps reasoning about object semantics, current\nworks focus on models with large capacity and receptive fields that can fully\ncapture the global context of an input 3D scene. However, strong contextual\npriors can have detrimental implications like mistaking a pedestrian crossing\nthe street for a car. In this work, we focus on the importance of balancing\nglobal scene context and local geometry, with the goal of generalizing beyond\nthe contextual priors in the training set. In particular, we propose a \"mixing\"\ntechnique which creates new training samples by combining two augmented scenes.\nBy doing so, object instances are implicitly placed into novel out-of-context\nenvironments and therefore making it harder for models to rely on scene context\nalone, and instead infer semantics from local structure as well. We perform\ndetailed analysis to understand the importance of global context, local\nstructures and the effect of mixing scenes. In experiments, we show that models\ntrained with Mix3D profit from a significant performance boost on indoor\n(ScanNet, S3DIS) and outdoor datasets (SemanticKITTI). Mix3D can be trivially\nused with any existing method, e.g., trained with Mix3D, MinkowskiNet\noutperforms all prior state-of-the-art methods by a significant margin on the\nScanNet test benchmark 78.1 mIoU. Code is available at:\nhttps://nekrasov.dev/mix3d/"}
{"id": "2505.20622", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.20622", "abs": "https://arxiv.org/abs/2505.20622", "authors": ["Ting Xu", "Zhichao Huang", "Jiankai Sun", "Shanbo Cheng", "Wai Lam"], "title": "SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "We present Sequential Policy Optimization for Simultaneous Machine\nTranslation (SeqPO-SiMT), a new policy optimization framework that defines the\nsimultaneous machine translation (SiMT) task as a sequential decision making\nproblem, incorporating a tailored reward to enhance translation quality while\nreducing latency. In contrast to popular Reinforcement Learning from Human\nFeedback (RLHF) methods, such as PPO and DPO, which are typically applied in\nsingle-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task.\nThis intuitive framework allows the SiMT LLMs to simulate and refine the SiMT\nprocess using a tailored reward. We conduct experiments on six datasets from\ndiverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that\nSeqPO-SiMT consistently achieves significantly higher translation quality with\nlower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning\n(SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17\nin the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context\nthan offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly\nrival the offline translation of high-performing LLMs, including\nQwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct."}
{"id": "2505.20561", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.20561", "abs": "https://arxiv.org/abs/2505.20561", "authors": ["Shenao Zhang", "Yaqing Wang", "Yinxiao Liu", "Tianqi Liu", "Peter Grabowski", "Eugene Ie", "Zhaoran Wang", "Yunxuan Li"], "title": "Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Large Language Models (LLMs) trained via Reinforcement Learning (RL) have\nexhibited strong reasoning capabilities and emergent reflective behaviors, such\nas backtracking and error correction. However, conventional Markovian RL\nconfines exploration to the training phase to learn an optimal deterministic\npolicy and depends on the history contexts only through the current state.\nTherefore, it remains unclear whether reflective reasoning will emerge during\nMarkovian RL training, or why they are beneficial at test time. To remedy this,\nwe recast reflective exploration within the Bayes-Adaptive RL framework, which\nexplicitly optimizes the expected return under a posterior distribution over\nMarkov decision processes. This Bayesian formulation inherently incentivizes\nboth reward-maximizing exploitation and information-gathering exploration via\nbelief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and\nswitch strategies based on the observed outcomes, offering principled guidance\non when and how the model should reflectively explore. Empirical results on\nboth synthetic and mathematical reasoning tasks demonstrate that BARL\noutperforms standard Markovian RL approaches at test time, achieving superior\ntoken efficiency with improved exploration effectiveness. Our code is available\nat https://github.com/shenao-zhang/BARL."}
{"id": "2505.20196", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.20196", "abs": "https://arxiv.org/abs/2505.20196", "authors": ["Yuetai Li", "Zhangchen Xu", "Fengqing Jiang", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Xiang Yue", "Radha Poovendran"], "title": "Temporal Sampling for Forgotten Reasoning in LLMs", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Fine-tuning large language models (LLMs) is intended to improve their\nreasoning capabilities, yet we uncover a counterintuitive effect: models often\nforget how to solve problems they previously answered correctly during\ntraining. We term this phenomenon temporal forgetting and show that it is\nwidespread across model sizes, fine-tuning methods (both Reinforcement Learning\nand Supervised Fine-Tuning), and multiple reasoning benchmarks. To address this\ngap, we introduce Temporal Sampling, a simple decoding strategy that draws\noutputs from multiple checkpoints along the training trajectory. This approach\nrecovers forgotten solutions without retraining or ensembling, and leads to\nsubstantial improvements in reasoning performance, gains from 4 to 19 points in\nPass@k and consistent gains in Majority@k across several benchmarks. We further\nextend our method to LoRA-adapted models, demonstrating that storing only\nadapter weights across checkpoints achieves similar benefits with minimal\nstorage cost. By leveraging the temporal diversity inherent in training,\nTemporal Sampling offers a practical, compute-efficient way to surface hidden\nreasoning ability and rethink how we evaluate LLMs."}
{"id": "2505.20065", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.20065", "abs": "https://arxiv.org/abs/2505.20065", "authors": ["Geon-Hyeong Kim", "Youngsoo Jang", "Yu Jin Kim", "Byoungjip Kim", "Honglak Lee", "Kyunghoon Bae", "Moontae Lee"], "title": "SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety", "categories": ["cs.LG", "cs.AI"], "comment": "34 pages", "summary": "As Large Language Models (LLMs) continue to advance and find applications\nacross a growing number of fields, ensuring the safety of LLMs has become\nincreasingly critical. To address safety concerns, recent studies have proposed\nintegrating safety constraints into Reinforcement Learning from Human Feedback\n(RLHF). However, these approaches tend to be complex, as they encompass\ncomplicated procedures in RLHF along with additional steps required by the\nsafety constraints. Inspired by Direct Preference Optimization (DPO), we\nintroduce a new algorithm called SafeDPO, which is designed to directly\noptimize the safety alignment objective in a single stage of policy learning,\nwithout requiring relaxation. SafeDPO introduces only one additional\nhyperparameter to further enhance safety and requires only minor modifications\nto standard DPO. As a result, it eliminates the need to fit separate reward and\ncost models or to sample from the language model during fine-tuning, while\nstill enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO\nachieves competitive performance compared to state-of-the-art safety alignment\nalgorithms, both in terms of aligning with human preferences and improving\nsafety."}
