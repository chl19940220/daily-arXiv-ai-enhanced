{"id": "2505.00018", "keyword": "LLM Agent", "pdf": "https://arxiv.org/pdf/2505.00018", "abs": "https://arxiv.org/abs/2505.00018", "authors": ["Ju Wu", "Calvin K. L. Or"], "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability.", "keywords": ["LLM Agent"], "AI": {"tldr": "\u672c\u6587\u662f\u4e00\u7bc7\u7acb\u573a\u8bba\u6587\uff0c\u5bf9\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u534f\u4f5c\u7684\u8fd1\u671f\u7ecf\u9a8c\u6027\u8fdb\u5c55\u8fdb\u884c\u4e86\u6279\u5224\u6027\u7efc\u8ff0\u3002\u6587\u7ae0\u6307\u51fa\u5f53\u524d\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u6574\u5408\u8fd9\u4e9b\u591a\u6837\u5316\u7684\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6982\u5ff5\u67b6\u6784\u2014\u2014\u5206\u5c42\u63a2\u7d22-\u5229\u7528\u7f51\u7edc\uff08Hierarchical Exploration-Exploitation Net\uff09\uff0c\u4ee5\u7cfb\u7edf\u5730\u8fde\u63a5\u591a\u4ee3\u7406\u534f\u8c03\u3001\u77e5\u8bc6\u7ba1\u7406\u3001\u63a7\u5236\u53cd\u9988\u56de\u8def\u548c\u9ad8\u5c42\u63a7\u5236\u673a\u5236\u7b49\u6280\u672f\u7ec6\u8282\u3002\u901a\u8fc7\u5c06\u73b0\u6709\u8d21\u732e\u6620\u5c04\u5230\u8fd9\u4e00\u6846\u67b6\u4e2d\uff0c\u4f5c\u8005\u5e0c\u671b\u63a8\u52a8\u4f20\u7edf\u65b9\u6cd5\u7684\u6539\u8fdb\u5e76\u542f\u53d1\u878d\u5408\u5b9a\u6027\u548c\u5b9a\u91cf\u8303\u5f0f\u7684\u65b0\u578b\u7814\u7a76\u3002\u6587\u7ae0\u7ed3\u6784\u7075\u6d3b\uff0c\u53ef\u4ece\u4efb\u610f\u90e8\u5206\u9605\u8bfb\uff0c\u65e2\u4f5c\u4e3a\u6280\u672f\u5b9e\u73b0\u7684\u6279\u5224\u6027\u56de\u987e\uff0c\u4e5f\u4e3a\u8bbe\u8ba1\u6216\u6269\u5c55\u4eba\u7c7b-AI\u5171\u751f\u5173\u7cfb\u63d0\u4f9b\u4e86\u524d\u77bb\u6027\u7684\u53c2\u8003\u3002", "motivation": "\u5c3d\u7ba1\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u7684\u534f\u4f5c\u5728\u6280\u672f\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u5c31\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u6301\u7eed\u7684\u6280\u672f\u9e3f\u6c9f\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5f00\u653e\u6027\u3001\u590d\u6742\u4efb\u52a1\u65f6\u3002\u76ee\u524d\u7684\u7814\u7a76\u9886\u57df\u7f3a\u4e4f\u4e00\u4e2a\u80fd\u591f\u7edf\u4e00\u6574\u5408\u5404\u7c7b\u7814\u7a76\u7684\u7406\u8bba\u6846\u67b6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6982\u5ff5\u67b6\u6784\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5206\u5c42\u63a2\u7d22-\u5229\u7528\u7f51\u7edc\uff08Hierarchical Exploration-Exploitation Net\uff09\u7684\u65b0\u9896\u6982\u5ff5\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u7cfb\u7edf\u5730\u8fde\u63a5\u4e86\u591a\u4ee3\u7406\u534f\u8c03\u3001\u77e5\u8bc6\u7ba1\u7406\u3001\u63a7\u5236\u53cd\u9988\u56de\u8def\u548c\u9ad8\u5c42\u63a7\u5236\u673a\u5236\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5c06\u73b0\u6709\u7684\u7814\u7a76\u6210\u679c\uff08\u5305\u62ec\u7b26\u53f7AI\u6280\u672f\u3001\u57fa\u4e8e\u8fde\u63a5\u4e3b\u4e49\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u4ee5\u53ca\u6df7\u5408\u7ec4\u7ec7\u5b9e\u8df5\uff09\u6620\u5c04\u5230\u8fd9\u4e00\u6846\u67b6\u4e2d\uff0c\u6587\u7ae0\u4e3a\u91cd\u65b0\u5ba1\u89c6\u4f20\u7edf\u65b9\u6cd5\u548c\u5f00\u53d1\u65b0\u65b9\u6cd5\u63d0\u4f9b\u4e86\u8def\u5f84\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u4fc3\u8fdb\u4e86\u5bf9\u73b0\u6709\u6280\u672f\u7684\u6df1\u5165\u7406\u89e3\uff0c\u8fd8\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\uff0c\u7279\u522b\u662f\u5982\u4f55\u878d\u5408\u5b9a\u6027\u548c\u5b9a\u91cf\u8303\u5f0f\u6765\u63a8\u52a8\u4eba\u7c7b\u8ba4\u77e5\u4e0eAI\u80fd\u529b\u7684\u5171\u540c\u8fdb\u5316\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6982\u5ff5\u6846\u67b6\u4e3a\u6df1\u5316\u4eba\u7c7b\u8ba4\u77e5\u4e0eAI\u80fd\u529b\u7684\u5171\u8fdb\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u8d77\u70b9\u3002\u901a\u8fc7\u6574\u5408\u591a\u4ee3\u7406\u7cfb\u7edf\u3001\u77e5\u8bc6\u7ba1\u7406\u548c\u63a7\u5236\u673a\u5236\u7b49\u65b9\u9762\u7684\u6280\u672f\u7ec6\u8282\uff0c\u6587\u7ae0\u4e3a\u8bbe\u8ba1\u6216\u6269\u5c55\u4eba\u7c7b-AI\u5171\u751f\u5173\u7cfb\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2410.15466", "keyword": "LLM reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing.", "keywords": ["LLM reasoning"], "AI": {"tldr": "\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\uff0c\u901a\u8fc7\u91cd\u590d\u91c7\u6837\u6269\u5c55\u63a8\u7406\u8ba1\u7b97\u53ef\u4ee5\u6301\u7eed\u589e\u52a0\u8986\u76d6\u7387\uff08\u89e3\u51b3\u95ee\u9898\u7684\u6bd4\u4f8b\uff09\u3002\u672c\u6587\u7814\u7a76\u4e86\u8fd9\u79cd\u63d0\u5347\u662f\u5426\u90e8\u5206\u5f52\u56e0\u4e8e\u6807\u51c6\u8bc4\u4f30\u57fa\u51c6\u4e2d\u7684\u7b54\u6848\u5206\u5e03\u504f\u5411\u5c11\u6570\u5e38\u89c1\u7b54\u6848\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u5b9a\u4e49\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u6570\u5b66\u63a8\u7406\u548c\u4e8b\u5b9e\u77e5\u8bc6\u4e24\u4e2a\u9886\u57df\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u4e8e\u67d0\u4e9bLLM\uff0c\u8be5\u57fa\u7ebf\u4f18\u4e8e\u91cd\u590d\u91c7\u6837\uff1b\u800c\u5bf9\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5176\u8868\u73b0\u4e0e\u4e00\u79cd\u6df7\u5408\u7b56\u7565\u76f8\u5f53\uff08\u8be5\u7b56\u7565\u4ec5\u4f7f\u752810\u6b21\u6a21\u578b\u91c7\u6837\u5e76\u7ed3\u5408\u679a\u4e3e\u731c\u6d4b\u5269\u4f59\u7684\u7b54\u6848\uff09\u3002\u6b64\u57fa\u7ebf\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u8861\u91cf\u91cd\u590d\u91c7\u6837\u5728\u63d0\u793a\u65e0\u5173\u731c\u6d4b\u4e4b\u5916\u5bf9\u8986\u76d6\u7387\u7684\u6539\u8fdb\u3002", "motivation": "\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u91cd\u590d\u91c7\u6837\u8fdb\u884c\u63a8\u7406\u8ba1\u7b97\u65f6\uff0c\u968f\u7740\u6837\u672c\u6570\u91cf\u589e\u52a0\uff0c\u95ee\u9898\u89e3\u51b3\u8986\u76d6\u7387\u4e5f\u76f8\u5e94\u63d0\u9ad8\u3002\u7136\u800c\uff0c\u7814\u7a76\u8005\u6000\u7591\u8fd9\u4e00\u73b0\u8c61\u53ef\u80fd\u90e8\u5206\u5f52\u56e0\u4e8e\u8bc4\u4f30\u57fa\u51c6\u4e2d\u7684\u7b54\u6848\u5206\u5e03\u504f\u5411\u5c11\u6570\u5e38\u89c1\u7b54\u6848\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u4e00\u4e2a\u57fa\u7ebf\u6765\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\u3002", "method": "\u4f5c\u8005\u5b9a\u4e49\u4e86\u4e00\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6839\u636e\u8bad\u7ec3\u96c6\u4e2d\u7b54\u6848\u7684\u51fa\u73b0\u9891\u7387\u679a\u4e3e\u7b54\u6848\u3002\u7136\u540e\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u4e8b\u5b9e\u77e5\u8bc6\u4e24\u4e2a\u9886\u57df\u5bf9\u591a\u4e2aLLM\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u57fa\u7ebf\u65b9\u6cd5\u3001\u91cd\u590d\u91c7\u6837\u4ee5\u53ca\u4e00\u79cd\u6df7\u5408\u7b56\u7565\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5bf9\u4e8e\u67d0\u4e9bLLM\uff0c\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\u4f18\u4e8e\u91cd\u590d\u91c7\u6837\uff1b\u800c\u5bf9\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5176\u8986\u76d6\u7387\u4e0e\u6df7\u5408\u7b56\u7565\u76f8\u5f53\uff0c\u540e\u8005\u4ec5\u4f7f\u7528\u5c11\u91cf\u6a21\u578b\u91c7\u6837\u5e76\u7ed3\u5408\u679a\u4e3e\u731c\u6d4b\u5176\u4f59\u7b54\u6848\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7814\u7a76\u8005\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u91cd\u590d\u91c7\u6837\u5728\u63d0\u793a\u65e0\u5173\u731c\u6d4b\u4e4b\u5916\u5bf9\u8986\u76d6\u7387\u7684\u5b9e\u9645\u6539\u8fdb\u6548\u679c\u3002\u8fd9\u4e3a\u7406\u89e3LLM\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2505.17017", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.17017", "abs": "https://arxiv.org/abs/2505.17017", "authors": ["Chengzhuo Tong", "Ziyu Guo", "Renrui Zhang", "Wenyu Shan", "Xinyu Wei", "Zhenghao Xing", "Hongsheng Li", "Pheng-Ann Heng"], "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT", "summary": "Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b97\u6cd5\u5728\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662fGroup Relative Policy Optimization (GRPO)\u548cDirect Preference Optimization (DPO)\u4e24\u79cd\u7b97\u6cd5\u7684\u6027\u80fd\u53ca\u5956\u52b1\u6a21\u578b\u7684\u5f71\u54cd\u3002\u7814\u7a76\u63ed\u793a\u4e86\u4e0d\u540c\u7b97\u6cd5\u7684\u4f18\u52bf\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u79cd\u6269\u5c55\u7b56\u7565\u4ee5\u63d0\u5347\u5176\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u6709\u6548\u7684RL\u7b97\u6cd5\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5df2\u5728\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u4e2d\uff0c\u5982\u4f55\u786e\u4fdd\u6587\u672c-\u56fe\u50cf\u4e00\u81f4\u6027\u3001\u63d0\u9ad8\u56fe\u50cf\u7f8e\u5b66\u8d28\u91cf\u4ee5\u53ca\u8bbe\u8ba1\u590d\u6742\u7684\u5956\u52b1\u6a21\u578b\u4ecd\u5b58\u5728\u72ec\u7279\u6311\u6218\u3002\u76ee\u524d\u7684\u7814\u7a76\u5bf9\u8fd9\u4e9b\u9886\u57df\u7279\u5b9a\u7684\u6311\u6218\u53ca\u4e0d\u540cRL\u7b56\u7565\u7684\u7279\u6027\u5206\u6790\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u63a2\u8ba8GRPO\u548cDPO\u7b49\u7b97\u6cd5\u5728\u8be5\u9886\u57df\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76\u8005\u5bf9GRPO\u548cDPO\u7b97\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u5305\u62ec\u5b83\u4eec\u5728\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u9886\u57df\u5185\u6027\u80fd\u548c\u9886\u57df\u5916\u6cdb\u5316\u80fd\u529b\u3002\u540c\u65f6\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u5956\u52b1\u6a21\u578b\u5bf9\u5176\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u5e76\u7cfb\u7edf\u5730\u63a2\u7d22\u4e86\u4e09\u79cd\u6d41\u884c\u7684\u6269\u5c55\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u8fd9\u4e24\u79cd\u7b97\u6cd5\u5728\u9886\u57df\u5185\u5916\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cGRPO\u548cDPO\u7b97\u6cd5\u5404\u6709\u4f18\u52bf\uff0c\u4e14\u5177\u6709\u66f4\u5f3a\u5185\u5728\u6cdb\u5316\u80fd\u529b\u7684\u5956\u52b1\u6a21\u578b\u53ef\u4ee5\u589e\u5f3a\u6240\u5e94\u7528RL\u7b97\u6cd5\u7684\u6cdb\u5316\u6f5c\u529b\u3002\u6b64\u5916\uff0c\u4e09\u79cd\u6269\u5c55\u7b56\u7565\u80fd\u591f\u6709\u6548\u63d0\u5347\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u9ad8\u6548\u7684RL\u7b97\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u6df1\u5165\u5206\u6790\u4e86GRPO\u548cDPO\u7b97\u6cd5\u5728\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8868\u73b0\u53ca\u5176\u4e0e\u5956\u52b1\u6a21\u578b\u7684\u5173\u7cfb\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9009\u62e9\u5408\u9002\u7684\u5956\u52b1\u6a21\u578b\u548c\u6269\u5c55\u7b56\u7565\u5bf9\u4e8e\u63d0\u5347RL\u7b97\u6cd5\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u672a\u6765\u5728\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u4e2d\u5b9e\u73b0\u9c81\u68d2CoT\u63a8\u7406\u7684RL\u7b97\u6cd5\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.16637", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.16637", "abs": "https://arxiv.org/abs/2505.16637", "authors": ["Wenjie Yang", "Mao Zheng", "Mingyang Song", "Zheng Li"], "title": "SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities in machine translation (MT). However, most advanced MT-specific\nLLMs heavily rely on external supervision signals during training, such as\nhuman-annotated reference data or trained reward models (RMs), which are often\nexpensive to obtain and challenging to scale. To overcome this limitation, we\npropose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for\nMT that is reference-free, fully online, and relies solely on self-judging\nrewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as\nthe backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,\ne.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like\nQwen2.5-32B-Instruct in English $\\leftrightarrow$ Chinese translation tasks\nfrom WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR\nwith external supervision from COMET, our strongest model, SSR-X-Zero-7B,\nachieves state-of-the-art performance in English $\\leftrightarrow$ Chinese\ntranslation, surpassing all existing open-source models under 72B parameters\nand even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.\nOur analysis highlights the effectiveness of the self-rewarding mechanism\ncompared to the external LLM-as-a-judge approach in MT and demonstrates its\ncomplementary benefits when combined with trained RMs. Our findings provide\nvaluable insight into the potential of self-improving RL methods. We have\npublicly released our code, data and models.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u53c2\u8003\u7684\u7b80\u5355\u81ea\u6211\u5956\u52b1\uff08SSR\uff09\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u3002\u8be5\u6846\u67b6\u5229\u7528\u81ea\u5224\u65ad\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u51cf\u5c11\u5bf9\u5916\u90e8\u76d1\u7763\u4fe1\u53f7\u4f9d\u8d56\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86\u7ffb\u8bd1\u6027\u80fd\u3002\u4f7f\u752813K\u5355\u8bed\u6570\u636e\u548cQwen-2.5-7B\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0cSSR-Zero-7B\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u4e13\u7528\u4e8e\u673a\u5668\u7ffb\u8bd1\u7684\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5176\u4ed6\u66f4\u5927\u89c4\u6a21\u7684\u901a\u7528\u6a21\u578b\u3002\u8fdb\u4e00\u6b65\u7ed3\u5408\u5916\u90e8COMET\u76d1\u7763\uff0cSSR-X-Zero-7B\u6a21\u578b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u82f1\u8bed\u4e0e\u4e2d\u6587\u4e92\u8bd1\u6548\u679c\uff0c\u8d85\u8d8a\u4e86\u5305\u62ecGPT-4o\u548cGemini 1.5 Pro\u5728\u5185\u7684\u95ed\u6e90\u6a21\u578b\u3002\u7814\u7a76\u8868\u660e\uff0c\u81ea\u6211\u5956\u52b1\u673a\u5236\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u6709\u6548\u6027\u4f18\u4e8e\u5916\u90e8LLM-as-a-judge\u65b9\u6cd5\uff0c\u5e76\u4e14\u4e0e\u8bad\u7ec3\u597d\u7684\u5956\u52b1\u6a21\u578b\u76f8\u7ed3\u5408\u65f6\u5177\u6709\u4e92\u8865\u4f18\u52bf\u3002\u6b64\u7814\u7a76\u63ed\u793a\u4e86\u81ea\u6211\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\u4e86\u4ee3\u7801\u3001\u6570\u636e\u548c\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u5148\u8fdb\u7684\u673a\u5668\u7ffb\u8bd1\u4e13\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9ad8\u5ea6\u4f9d\u8d56\u6602\u8d35\u4e14\u96be\u4ee5\u6269\u5c55\u7684\u5916\u90e8\u76d1\u7763\u4fe1\u53f7\uff0c\u4f8b\u5982\u4eba\u5de5\u6807\u6ce8\u7684\u53c2\u8003\u6570\u636e\u6216\u8bad\u7ec3\u597d\u7684\u5956\u52b1\u6a21\u578b\u3002\u4e3a\u4e86\u964d\u4f4e\u8fd9\u79cd\u4f9d\u8d56\u5e76\u63d0\u5347\u6548\u7387\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e0\u9700\u53c2\u8003\u3001\u5b8c\u5168\u5728\u7ebf\u4e14\u4ec5\u4f9d\u8d56\u81ea\u6211\u5224\u65ad\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u2014\u2014Simple Self-Rewarding (SSR) RL\u3002", "method": "SSR\u6846\u67b6\u901a\u8fc7\u81ea\u5224\u65ad\u5956\u52b1\u4fe1\u53f7\u5bf9\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u53c2\u8003\u6570\u636e\u6216\u5916\u90e8\u76d1\u7763\u3002\u5177\u4f53\u800c\u8a00\uff0c\u4f7f\u752813K\u5355\u8bed\u6570\u636e\u548cQwen-2.5-7B\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aSSR-Zero-7B\u7684\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u5f15\u5165\u6765\u81eaCOMET\u7684\u5916\u90e8\u76d1\u7763\u589e\u5f3a\u4e86SSR\u6846\u67b6\uff0c\u5f62\u6210\u4e86\u66f4\u5f3a\u7684SSR-X-Zero-7B\u6a21\u578b\u3002", "result": "SSR-Zero-7B\u5728WMT23\u3001WMT24\u548cFlores200\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u673a\u5668\u7ffb\u8bd1\u4e13\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982TowerInstruct-13B\u548cGemmaX-28-9B\uff09\uff0c\u4ee5\u53ca\u66f4\u5927\u89c4\u6a21\u7684\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982Qwen2.5-32B-Instruct\uff09\u3002\u800c\u589e\u5f3a\u7248SSR-X-Zero-7B\u6a21\u578b\u66f4\u662f\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u82f1\u8bed\u4e0e\u4e2d\u6587\u4e92\u8bd1\u6548\u679c\uff0c\u4e0d\u4ec5\u8d85\u8fc7\u4e86\u6240\u6709\u53c2\u6570\u91cf\u5c0f\u4e8e72B\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u8fd8\u8d85\u8d8a\u4e86\u95ed\u6e90\u6a21\u578b\u5982GPT-4o\u548cGemini 1.5 Pro\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u81ea\u6211\u5956\u52b1\u673a\u5236\u5728\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u6bd4\u5916\u90e8LLM-as-a-judge\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u5e76\u4e14\u5f53\u4e0e\u8bad\u7ec3\u597d\u7684\u5956\u52b1\u6a21\u578b\u7ed3\u5408\u65f6\u8868\u73b0\u51fa\u4e92\u8865\u4f18\u52bf\u3002\u8fd9\u4e3a\u672a\u6765\u81ea\u6211\u6539\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u516c\u5f00\u4e86\u4ee3\u7801\u3001\u6570\u636e\u548c\u6a21\u578b\uff0c\u4fc3\u8fdb\u4e86\u76f8\u5173\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u53d1\u5c55\u3002"}}
{"id": "2505.16421", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.16421", "abs": "https://arxiv.org/abs/2505.16421", "authors": ["Zhepei Wei", "Wenlin Yao", "Yao Liu", "Weizhi Zhang", "Qin Lu", "Liang Qiu", "Changlong Yu", "Puyang Xu", "Chao Zhang", "Bing Yin", "Hyokun Yun", "Lihong Li"], "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "While reinforcement learning (RL) has demonstrated remarkable success in\nenhancing large language models (LLMs), it has primarily focused on single-turn\ntasks such as solving math problems. Training effective web agents for\nmulti-turn interactions remains challenging due to the complexity of\nlong-horizon decision-making across dynamic web interfaces. In this work, we\npresent WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework\nfor training web agents. It learns directly from online interactions with web\nenvironments by asynchronously generating diverse trajectories, entirely guided\nby binary rewards depending on task success. Experiments on the WebArena-Lite\nbenchmark demonstrate the effectiveness of WebAgent-R1, boosting the task\nsuccess rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to\n44.8%, significantly outperforming existing state-of-the-art methods and strong\nproprietary models such as OpenAI o3. In-depth analyses reveal the\neffectiveness of the thinking-based prompting strategy and test-time scaling\nthrough increased interactions for web tasks. We further investigate different\nRL initialization policies by introducing two variants, namely WebAgent-R1-Zero\nand WebAgent-R1-CoT, which highlight the importance of the warm-up training\nstage (i.e., behavior cloning) and provide insights on incorporating long\nchain-of-thought (CoT) reasoning in web agents.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWebAgent-R1\u7684\u7aef\u5230\u7aef\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u7f51\u7edc\u4ee3\u7406\u3002\u901a\u8fc7\u5728\u7ebf\u4e0e\u7f51\u7edc\u73af\u5883\u4ea4\u4e92\u751f\u6210\u591a\u6837\u8f68\u8ff9\uff0c\u5e76\u4ec5\u7531\u4efb\u52a1\u6210\u529f\u4e0e\u5426\u7684\u4e8c\u5143\u5956\u52b1\u5f15\u5bfc\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86Qwen-2.5-3B\u548cLlama-3.1-8B\u5728WebArena-Lite\u57fa\u51c6\u4e0a\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u8d85\u8fc7\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u548c\u5f3a\u5927\u7684\u4e13\u6709\u6a21\u578b\u5982OpenAI o3\u3002\u6df1\u5165\u5206\u6790\u63ed\u793a\u4e86\u57fa\u4e8e\u601d\u8003\u7684\u63d0\u793a\u7b56\u7565\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u6709\u6548\u6027\uff0c\u8fd8\u63a2\u8ba8\u4e86\u4e0d\u540c\u7684\u5f3a\u5316\u5b66\u4e60\u521d\u59cb\u5316\u7b56\u7565\u53ca\u5176\u53d8\u4f53WebAgent-R1-Zero\u548cWebAgent-R1-CoT\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u5c31\uff0c\u4f46\u4e3b\u8981\u96c6\u4e2d\u5728\u5355\u8f6e\u4efb\u52a1\u4e0a\uff0c\u4f8b\u5982\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u3002\u5bf9\u4e8e\u9700\u8981\u957f\u65f6\u95f4\u51b3\u7b56\u7684\u52a8\u6001\u7f51\u7edc\u754c\u9762\u591a\u8f6e\u4ea4\u4e92\u8bad\u7ec3\u6709\u6548\u7684\u7f51\u7edc\u4ee3\u7406\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u7aef\u5230\u7aef\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "WebAgent-R1\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u76f4\u63a5\u4ece\u4e0e\u7f51\u7edc\u73af\u5883\u7684\u5728\u7ebf\u4ea4\u4e92\u4e2d\u5b66\u4e60\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5f02\u6b65\u751f\u6210\u591a\u6837\u8f68\u8ff9\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5b8c\u5168\u7531\u4efb\u52a1\u6210\u529f\u4e0e\u5426\u7684\u4e8c\u5143\u5956\u52b1\u5f15\u5bfc\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e24\u4e2a\u53d8\u4f53\uff1aWebAgent-R1-Zero\u548cWebAgent-R1-CoT\uff0c\u4ee5\u7814\u7a76\u4e0d\u540c\u7684\u5f3a\u5316\u5b66\u4e60\u521d\u59cb\u5316\u7b56\u7565\u3002\u524d\u8005\u5f3a\u8c03\u884c\u4e3a\u514b\u9686\uff08warm-up training stage\uff09\u7684\u91cd\u8981\u6027\uff0c\u540e\u8005\u63d0\u4f9b\u4e86\u5c06\u957f\u94fe\u63a8\u7406\uff08CoT\uff09\u878d\u5165\u7f51\u7edc\u4ee3\u7406\u7684\u89c1\u89e3\u3002", "result": "\u5728WebArena-Lite\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWebAgent-R1\u5927\u5e45\u63d0\u5347\u4e86Qwen-2.5-3B\u7684\u4efb\u52a1\u6210\u529f\u7387\u4ece6.1%\u81f333.9%\uff0cLlama-3.1-8B\u7684\u4efb\u52a1\u6210\u529f\u7387\u4ece8.5%\u81f344.8%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u548c\u5f3a\u5927\u7684\u4e13\u6709\u6a21\u578b\u5982OpenAI o3\u3002\u6df1\u5165\u5206\u6790\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u57fa\u4e8e\u601d\u8003\u7684\u63d0\u793a\u7b56\u7565\u548c\u901a\u8fc7\u589e\u52a0\u4ea4\u4e92\u8fdb\u884c\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u6709\u6548\u6027\u3002", "conclusion": "WebAgent-R1\u4f5c\u4e3a\u4e00\u79cd\u7b80\u5355\u7684\u7aef\u5230\u7aef\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7f51\u7edc\u4ee3\u7406\u5728\u590d\u6742\u3001\u52a8\u6001\u7f51\u7edc\u754c\u9762\u4e2d\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002\u5176\u901a\u8fc7\u5728\u7ebf\u4ea4\u4e92\u5b66\u4e60\u548c\u4e8c\u5143\u5956\u52b1\u5f15\u5bfc\u7684\u65b9\u5f0f\uff0c\u4e0d\u4ec5\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u800c\u4e14\u63ed\u793a\u4e86\u884c\u4e3a\u514b\u9686\u548c\u957f\u94fe\u63a8\u7406\u5728\u7f51\u7edc\u4ee3\u7406\u4e2d\u7684\u91cd\u8981\u6027\u3002\u8fd9\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u542f\u793a\u548c\u65b9\u5411\u3002"}}
{"id": "2505.16410", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.16410", "abs": "https://arxiv.org/abs/2505.16410", "authors": ["Guanting Dong", "Yifei Chen", "Xiaoxi Li", "Jiajie Jin", "Hongjin Qian", "Yutao Zhu", "Hangyu Mao", "Guorui Zhou", "Zhicheng Dou", "Ji-Rong Wen"], "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Working in progress", "summary": "Recently, large language models (LLMs) have shown remarkable reasoning\ncapabilities via large-scale reinforcement learning (RL). However, leveraging\nthe RL algorithm to empower effective multi-tool collaborative reasoning in\nLLMs remains an open challenge. In this paper, we introduce Tool-Star, an\nRL-based framework designed to empower LLMs to autonomously invoke multiple\nexternal tools during stepwise reasoning. Tool-Star integrates six types of\ntools and incorporates systematic designs in both data synthesis and training.\nTo address the scarcity of tool-use data, we propose a general tool-integrated\nreasoning data synthesis pipeline, which combines tool-integrated prompting\nwith hint-based sampling to automatically and scalably generate tool-use\ntrajectories. A subsequent quality normalization and difficulty-aware\nclassification process filters out low-quality samples and organizes the\ndataset from easy to hard. Furthermore, we propose a two-stage training\nframework to enhance multi-tool collaborative reasoning by: (1) cold-start\nfine-tuning, which guides LLMs to explore reasoning patterns via\ntool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with\nhierarchical reward design, which reinforces reward understanding and promotes\neffective tool collaboration. Experimental analyses on over 10 challenging\nreasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.\nThe code is available at https://github.com/dongguanting/Tool-Star.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "Tool-Star\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\u4e2d\u81ea\u4e3b\u8c03\u7528\u591a\u4e2a\u5916\u90e8\u5de5\u5177\u7684\u80fd\u529b\u3002\u5b83\u901a\u8fc7\u5de5\u5177\u96c6\u6210\u63a8\u7406\u6570\u636e\u5408\u6210\u7ba1\u9053\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u6765\u89e3\u51b3\u5de5\u5177\u4f7f\u7528\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u591a\u5de5\u5177\u534f\u4f5c\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u5177\u6709\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5982\u4f55\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6709\u6548\u5b9e\u73b0LLMs\u4e2d\u7684\u591a\u5de5\u5177\u534f\u540c\u63a8\u7406\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u5de5\u5177\u4f7f\u7528\u6570\u636e\u548c\u4fc3\u8fdb\u591a\u5de5\u5177\u534f\u4f5c\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "Tool-Star\u96c6\u6210\u4e86\u516d\u79cd\u7c7b\u578b\u7684\u5de5\u5177\uff0c\u5e76\u5728\u6570\u636e\u5408\u6210\u548c\u8bad\u7ec3\u4e2d\u91c7\u7528\u4e86\u7cfb\u7edf\u5316\u8bbe\u8ba1\uff1a\n1. \u63d0\u51fa\u4e86\u4e00\u822c\u5de5\u5177\u96c6\u6210\u63a8\u7406\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u7ed3\u5408\u5de5\u5177\u96c6\u6210\u63d0\u793a\u4e0e\u57fa\u4e8e\u63d0\u793a\u7684\u91c7\u6837\u6280\u672f\uff0c\u81ea\u52a8\u751f\u6210\u5de5\u5177\u4f7f\u7528\u8f68\u8ff9\u3002\n2. \u91c7\u7528\u8d28\u91cf\u5f52\u4e00\u5316\u548c\u96be\u5ea6\u611f\u77e5\u5206\u7c7b\u8fc7\u7a0b\u5bf9\u6837\u672c\u8fdb\u884c\u8fc7\u6ee4\u548c\u7ec4\u7ec7\u3002\n3. \u8bbe\u8ba1\u4e86\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u51b7\u542f\u52a8\u5fae\u8c03\u5f15\u5bfcLLMs\u63a2\u7d22\u63a8\u7406\u6a21\u5f0f\uff1b\u591a\u5de5\u5177\u81ea\u6211\u6279\u8bc4RL\u7b97\u6cd5\u901a\u8fc7\u5206\u5c42\u5956\u52b1\u8bbe\u8ba1\u52a0\u5f3a\u5956\u52b1\u7406\u89e3\u548c\u5de5\u5177\u534f\u4f5c\u3002", "result": "\u5b9e\u9a8c\u5206\u6790\u663e\u793a\uff0cTool-Star\u5728\u8d85\u8fc710\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e86\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u63d0\u5347LLMs\u591a\u5de5\u5177\u534f\u4f5c\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "Tool-Star\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u5730\u589e\u5f3a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u5de5\u5177\u534f\u4f5c\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002\u5176\u63d0\u51fa\u7684\u5de5\u5177\u96c6\u6210\u63a8\u7406\u6570\u636e\u5408\u6210\u7ba1\u9053\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u4e3a\u89e3\u51b3\u5de5\u5177\u4f7f\u7528\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4fc3\u8fdb\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\u534f\u4f5c\u3002\u9879\u76ee\u4ee3\u7801\u5df2\u5f00\u6e90\uff0c\u53ef\u4f9b\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2505.16401", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.16401", "abs": "https://arxiv.org/abs/2505.16401", "authors": ["Xiaoqing Zhang", "Huabin Zheng", "Ang Lv", "Yuhan Liu", "Zirui Song", "Flood Sung", "Xiuying Chen", "Rui Yan"], "title": "Divide-Fuse-Conquer: Eliciting \"Aha Moments\" in Multi-Scenario Games", "categories": ["cs.LG"], "comment": "25 pages, 13 figures, and 8 tables", "summary": "Large language models (LLMs) have been observed to suddenly exhibit advanced\nreasoning abilities during reinforcement learning (RL), resembling an ``aha\nmoment'' triggered by simple outcome-based rewards. While RL has proven\neffective in eliciting such breakthroughs in tasks involving mathematics,\ncoding, and vision, it faces significant challenges in multi-scenario games.\nThe diversity of game rules, interaction modes, and environmental complexities\noften leads to policies that perform well in one scenario but fail to\ngeneralize to others. Simply combining multiple scenarios during training\nintroduces additional challenges, such as training instability and poor\nperformance. To overcome these challenges, we propose Divide-Fuse-Conquer, a\nframework designed to enhance generalization in multi-scenario RL. This\napproach starts by heuristically grouping games based on characteristics such\nas rules and difficulties. Specialized models are then trained for each group\nto excel at games in the group is what we refer to as the divide step. Next, we\nfuse model parameters from different groups as a new model, and continue\ntraining it for multiple groups, until the scenarios in all groups are\nconquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align\ntrained with the Divide-Fuse-Conquer strategy reaches a performance level\ncomparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can\ninspire future research on using reinforcement learning to improve the\ngeneralization of LLMs.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDivide-Fuse-Conquer\u7684\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u573a\u666f\u6e38\u620f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u8be5\u7b56\u7565\u8bad\u7ec3\u7684Qwen2.5-32B-Align\u6a21\u578b\u5728TextArena\u6e38\u620f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u53ef\u4e0eClaude3.5\u5ab2\u7f8e\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u89c6\u89c9\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\uff0c\u4f46\u5728\u591a\u573a\u666f\u6e38\u620f\u4e2d\uff0c\u7531\u4e8e\u89c4\u5219\u3001\u4ea4\u4e92\u6a21\u5f0f\u548c\u73af\u5883\u590d\u6742\u6027\u7684\u591a\u6837\u6027\uff0c\u6a21\u578b\u5f80\u5f80\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u573a\u666f\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5e76\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u65b0\u7684\u65b9\u6cd5\u3002", "method": "Divide-Fuse-Conquer\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u6b65\u9aa4\uff1a1) \u5c06\u6e38\u620f\u57fa\u4e8e\u89c4\u5219\u548c\u96be\u5ea6\u7b49\u7279\u5f81\u8fdb\u884c\u5206\u7ec4\uff08Divide\uff09\uff1b2) \u4e3a\u6bcf\u7ec4\u8bad\u7ec3\u4e13\u95e8\u7684\u6a21\u578b\u4ee5\u4f18\u5316\u5176\u6027\u80fd\uff1b3) \u878d\u5408\u4e0d\u540c\u7ec4\u6a21\u578b\u7684\u53c2\u6570\u5f62\u6210\u65b0\u6a21\u578b\uff0c\u5e76\u7ee7\u7eed\u8de8\u7ec4\u8bad\u7ec3\uff0c\u76f4\u5230\u6240\u6709\u573a\u666f\u90fd\u88ab\u6709\u6548\u8986\u76d6\uff08Fuse\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u91c7\u7528Divide-Fuse-Conquer\u7b56\u7565\u8bad\u7ec3\u7684Qwen2.5-32B-Align\u6a21\u578b\u572818\u4e2aTextArena\u6e38\u620f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u4e0eClaude3.5\u76f8\u5f53\u7684\u6c34\u5e73\uff0c\u53d6\u5f97\u4e867\u80dc4\u5e73\u7684\u6210\u7ee9\u3002", "conclusion": "Divide-Fuse-Conquer\u6846\u67b6\u4e3a\u63d0\u5347\u591a\u573a\u666f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2505.16368", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.16368", "abs": "https://arxiv.org/abs/2505.16368", "authors": ["Huanyu Liu", "Jia Li", "Hao Zhu", "Kechi Zhang", "Yihong Dong", "Ge Li"], "title": "SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "How to design reinforcement learning (RL) tasks that effectively unleash the\nreasoning capability of large language models (LLMs) remains an open question.\nExisting RL tasks (e.g., math, programming, and constructing reasoning tasks)\nsuffer from three key limitations: (1) Scalability. They rely heavily on human\nannotation or expensive LLM synthesis to generate sufficient training data. (2)\nVerifiability. LLMs' outputs are hard to verify automatically and reliably. (3)\nControllable Difficulty. Most tasks lack fine-grained difficulty control,\nmaking it hard to train LLMs to develop reasoning ability from easy to hard.\n  To address these limitations, we propose Saturn, a SAT-based RL framework\nthat uses Boolean Satisfiability (SAT) problems to train and evaluate LLM\nreasoning. Saturn enables scalable task construction, rule-based verification,\nand precise difficulty control. Saturn designs a curriculum learning pipeline\nthat continuously improves LLMs' reasoning capability by constructing SAT tasks\nof increasing difficulty and training LLMs from easy to hard. To ensure stable\ntraining, we design a principled mechanism to control difficulty transitions.\n  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying\ndifficulty. It supports the evaluation of how LLM reasoning changes with\nproblem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain\nSaturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT\nproblems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of\n+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B\nand Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,\nAIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in\nconstructing RL tasks, Saturn achieves further improvements of +8.8%. We\nrelease the source code, data, and models to support future research.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSAT\u95ee\u9898\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6Saturn\uff0c\u65e8\u5728\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u4efb\u52a1\u6784\u5efa\u3001\u89c4\u5219\u9a8c\u8bc1\u548c\u7cbe\u786e\u96be\u5ea6\u63a7\u5236\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728SAT\u95ee\u9898\u3001\u6570\u5b66\u548c\u7f16\u7a0b\u4efb\u52a1\u4e0a\u5747\u663e\u8457\u63d0\u5347\u4e86LLMs\u7684\u8868\u73b0\u3002", "motivation": "\u76ee\u524d\u7684\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a(1) \u53ef\u6269\u5c55\u6027\u5dee\uff0c\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6216\u6602\u8d35\u7684LLM\u5408\u6210\u6570\u636e\uff1b(2) \u9a8c\u8bc1\u56f0\u96be\uff0cLLM\u8f93\u51fa\u96be\u4ee5\u81ea\u52a8\u53ef\u9760\u5730\u9a8c\u8bc1\uff1b(3) \u96be\u5ea6\u63a7\u5236\u4e0d\u8db3\uff0c\u5927\u591a\u6570\u4efb\u52a1\u7f3a\u4e4f\u7cbe\u7ec6\u7684\u96be\u5ea6\u5206\u7ea7\uff0c\u5bfc\u81f4\u8bad\u7ec3\u8fc7\u7a0b\u96be\u4ee5\u5faa\u5e8f\u6e10\u8fdb\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u6709\u6548\u91ca\u653eLLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86Saturn\u6846\u67b6\uff0c\u5229\u7528\u5e03\u5c14\u53ef\u6ee1\u8db3\u6027\uff08SAT\uff09\u95ee\u9898\u6765\u8bad\u7ec3\u548c\u8bc4\u4f30LLMs\u7684\u63a8\u7406\u80fd\u529b\u3002\u5177\u4f53\u800c\u8a00\uff0cSaturn\u5177\u5907\u4ee5\u4e0b\u7279\u70b9\uff1a\n- **\u53ef\u6269\u5c55\u4efb\u52a1\u6784\u5efa**\uff1a\u901a\u8fc7\u81ea\u52a8\u751f\u6210SAT\u95ee\u9898\uff0c\u51cf\u5c11\u5bf9\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\u3002\n- **\u89c4\u5219\u9a8c\u8bc1**\uff1a\u57fa\u4e8e\u660e\u786e\u7684\u89c4\u5219\u81ea\u52a8\u9a8c\u8bc1\u6a21\u578b\u8f93\u51fa\uff0c\u786e\u4fdd\u53ef\u9760\u6027\u3002\n- **\u7cbe\u786e\u96be\u5ea6\u63a7\u5236**\uff1a\u8bbe\u8ba1\u4e86\u8bfe\u7a0b\u5b66\u4e60\u7ba1\u9053\uff0c\u9010\u6b65\u589e\u52a0\u4efb\u52a1\u96be\u5ea6\uff0c\u4ece\u7b80\u5355\u5230\u590d\u6742\u8bad\u7ec3LLMs\u3002\n\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b2,660\u4e2a\u4e0d\u540c\u96be\u5ea6\u7684SAT\u95ee\u9898\u7684\u6570\u636e\u96c6Saturn-2.6k\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u4e2a\u6539\u8fdb\u7248\u6a21\u578bSaturn-1.5B\u548cSaturn-7B\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a\n1. \u5728SAT\u95ee\u9898\u4e0a\uff0cSaturn-1.5B\u548cSaturn-7B\u5206\u522b\u5b9e\u73b0\u4e86\u5e73\u5747+14.0\u548c+28.1\u7684pass@3\u63d0\u5347\u3002\n2. \u5728\u6570\u5b66\u548c\u7f16\u7a0b\u4efb\u52a1\u4e2d\uff0cSaturn-1.5B\u548cSaturn-7B\u5206\u522b\u5728\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982AIME\u3001LiveCodeBench\uff09\u4e2d\u63d0\u9ad8\u4e86\u5e73\u5747\u5206\u6570+4.9\u548c+1.8\u3002\n3. \u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684RL\u4efb\u52a1\u6784\u5efa\u65b9\u6cd5\u76f8\u6bd4\uff0cSaturn\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86+8.8%\u7684\u6027\u80fd\u3002\n\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u4e86Saturn\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "Saturn\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709RL\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u9650\u5236\uff0c\u5305\u62ec\u53ef\u6269\u5c55\u6027\u3001\u9a8c\u8bc1\u53ef\u9760\u6027\u548c\u96be\u5ea6\u63a7\u5236\u3002\u5b83\u901a\u8fc7SAT\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u8bad\u7ec3\u548c\u8bc4\u4f30LLMs\u7684\u63a8\u7406\u80fd\u529b\u3002\u4f5c\u8005\u5f00\u6e90\u4e86\u4ee3\u7801\u3001\u6570\u636e\u548c\u6a21\u578b\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2505.16363", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.16363", "abs": "https://arxiv.org/abs/2505.16363", "authors": ["Huishuai Zhang", "Bohan Wang", "Luoxin Chen"], "title": "AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We introduce AdamS, a simple yet effective alternative to Adam for large\nlanguage model (LLM) pretraining and post-training. By leveraging a novel\ndenominator, i.e., the root of weighted sum of squares of the momentum and the\ncurrent gradient, AdamS eliminates the need for second-moment estimates. Hence,\nAdamS is efficient, matching the memory and compute footprint of SGD with\nmomentum while delivering superior optimization performance. Moreover, AdamS is\neasy to adopt: it can directly inherit hyperparameters of AdamW, and is\nentirely model-agnostic, integrating seamlessly into existing pipelines without\nmodifications to optimizer APIs or architectures. The motivation behind AdamS\nstems from the observed $(L_0, L_1)$ smoothness properties in transformer\nobjectives, where local smoothness is governed by gradient magnitudes that can\nbe further approximated by momentum magnitudes. We establish rigorous\ntheoretical convergence guarantees and provide practical guidelines for\nhyperparameter selection. Empirically, AdamS demonstrates strong performance in\nvarious tasks, including pre-training runs on GPT-2 and Llama2 (up to 13B\nparameters) and reinforcement learning in post-training regimes. With its\nefficiency, simplicity, and theoretical grounding, AdamS stands as a compelling\nalternative to existing optimizers.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdamS\u7684\u65b0\u4f18\u5316\u5668\uff0c\u5b83\u901a\u8fc7\u65b0\u9896\u7684\u5206\u6bcd\u8bbe\u8ba1\u53bb\u9664\u4e86\u5bf9\u4e8c\u9636\u77e9\u4f30\u8ba1\u7684\u9700\u6c42\uff0c\u4ece\u800c\u5728\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u3001\u7b80\u5355\u6027\u548c\u4f18\u8d8a\u7684\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u5668\u5982Adam\u9700\u8981\u4e8c\u9636\u77e9\u4f30\u8ba1\uff0c\u8fd9\u589e\u52a0\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u8d1f\u62c5\u3002\u800c\u89c2\u5bdf\u5230Transformer\u76ee\u6807\u51fd\u6570\u5177\u6709$(L_0, L_1)$\u5e73\u6ed1\u6027\u7279\u6027\uff0c\u5c40\u90e8\u5e73\u6ed1\u6027\u7531\u68af\u5ea6\u5927\u5c0f\u51b3\u5b9a\uff0c\u4e14\u53ef\u4ee5\u8fdb\u4e00\u6b65\u7528\u52a8\u91cf\u5927\u5c0f\u8fd1\u4f3c\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u65e0\u9700\u4e8c\u9636\u77e9\u4f30\u8ba1\u7684AdamS\u4f18\u5316\u5668\u3002", "method": "AdamS\u91c7\u7528\u4e86\u4e00\u4e2a\u65b0\u7684\u5206\u6bcd\uff0c\u5373\u52a8\u91cf\u548c\u5f53\u524d\u68af\u5ea6\u5e73\u65b9\u7684\u52a0\u6743\u548c\u7684\u5e73\u65b9\u6839\uff0c\u4ece\u800c\u907f\u514d\u4e86\u5bf9\u4e8c\u9636\u77e9\u7684\u4f30\u8ba1\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0e\u5e26\u6709\u52a8\u91cf\u7684SGD\u4e00\u6837\u9ad8\u6548\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u4f18\u4e8eAdam\u7684\u4f18\u5316\u6027\u80fd\u3002\u5e76\u4e14\u53ef\u4ee5\u76f4\u63a5\u7ee7\u627fAdamW\u7684\u8d85\u53c2\u6570\uff0c\u65e0\u9700\u4fee\u6539\u4f18\u5316\u5668API\u6216\u67b6\u6784\u3002", "result": "\u5728\u591a\u9879\u4efb\u52a1\u4e2d\uff0c\u5305\u62ecGPT-2\u548cLlama2\uff08\u9ad8\u8fbe13B\u53c2\u6570\uff09\u7684\u9884\u8bad\u7ec3\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u540e\u8bad\u7ec3\u573a\u666f\u4e0b\uff0cAdamS\u5747\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u3002", "conclusion": "AdamS\u4ee5\u5176\u9ad8\u6548\u6027\u3001\u7b80\u5355\u6027\u548c\u7406\u8bba\u652f\u6301\u6210\u4e3a\u73b0\u6709\u4f18\u5316\u5668\u7684\u4e00\u4e2a\u6709\u529b\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\u9636\u6bb5\u3002"}}
{"id": "2505.15793", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.15793", "abs": "https://arxiv.org/abs/2505.15793", "authors": ["Zhiwen Chen", "Bo Leng", "Zhuoren Li", "Hanming Deng", "Guizhe Jin", "Ran Yu", "Huanxi Wen"], "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations. Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM-Hinted RL\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002\u8be5\u8303\u5f0f\u4e2d\u7684HCRMP\u67b6\u6784\u5305\u62ec\u589e\u5f3a\u8bed\u4e49\u8868\u793a\u6a21\u5757\u3001\u4e0a\u4e0b\u6587\u7a33\u5b9a\u6027\u951a\u5b9a\u6a21\u5757\u548c\u8bed\u4e49\u7f13\u5b58\u6a21\u5757\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u4ea4\u901a\u5bc6\u5ea6\u4e0b\u5b9e\u73b0\u9ad8\u8fbe80.3%\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u5728\u5173\u952e\u5b89\u5168\u9a7e\u9a76\u6761\u4ef6\u4e0b\u964d\u4f4e11.4%\u7684\u78b0\u649e\u7387\u3002", "motivation": "\u5f53\u524d\u4ee5LLM\u4e3a\u4e3b\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8fc7\u4e8e\u4f9d\u8d56LLM\u8f93\u51fa\uff0c\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u95ee\u9898\uff0c\u8fd9\u76f4\u63a5\u5f71\u54cd\u4e86\u9a7e\u9a76\u7b56\u7565\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u9700\u8981\u4fdd\u6301LLM\u4e0eRL\u4e4b\u95f4\u7684\u76f8\u5bf9\u72ec\u7acb\u6027\u3002", "method": "\u63d0\u51fa\u4e86LLM-Hinted RL\u8303\u5f0f\u53caHCRMP\u67b6\u6784\uff1a1) \u589e\u5f3a\u8bed\u4e49\u8868\u793a\u6a21\u5757\u6269\u5c55\u72b6\u6001\u7a7a\u95f4\uff1b2) \u4e0a\u4e0b\u6587\u7a33\u5b9a\u6027\u951a\u5b9a\u6a21\u5757\u5229\u7528\u77e5\u8bc6\u5e93\u4fe1\u606f\u63d0\u9ad8\u591a\u6279\u8bc4\u6743\u91cd\u63d0\u793a\u7684\u53ef\u9760\u6027\uff1b3) \u8bed\u4e49\u7f13\u5b58\u6a21\u5757\u5c06LLM\u4f4e\u9891\u6307\u5bfc\u4e0eRL\u9ad8\u9891\u63a7\u5236\u65e0\u7f1d\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHCRMP\u5728\u591a\u79cd\u9a7e\u9a76\u6761\u4ef6\u4e0b\u4efb\u52a1\u6210\u529f\u7387\u8fbe\u523080.3%\uff0c\u5e76\u5728\u5173\u952e\u5b89\u5168\u9a7e\u9a76\u6761\u4ef6\u4e0b\u5c06\u78b0\u649e\u7387\u964d\u4f4e\u4e8611.4%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u9a7e\u9a76\u6027\u80fd\u3002", "conclusion": "LLM-Hinted RL\u8303\u5f0f\u80fd\u591f\u6709\u6548\u89e3\u51b3LLM\u5e7b\u89c9\u95ee\u9898\u5e76\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6027\u80fd\uff0c\u6240\u63d0\u51fa\u7684HCRMP\u67b6\u6784\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6574\u4f53\u9a7e\u9a76\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u548c\u9ad8\u5bc6\u5ea6\u4ea4\u901a\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.15311", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.15311", "abs": "https://arxiv.org/abs/2505.15311", "authors": ["Yurun Yuan", "Fan Chen", "Zeyu Jia", "Alexander Rakhlin", "Tengyang Xie"], "title": "Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Policy-based methods currently dominate reinforcement learning (RL) pipelines\nfor large language model (LLM) reasoning, leaving value-based approaches\nlargely unexplored. We revisit the classical paradigm of Bellman Residual\nMinimization and introduce Trajectory Bellman Residual Minimization (TBRM), an\nalgorithm that naturally adapts this idea to LLMs, yielding a simple yet\neffective off-policy algorithm that optimizes a single trajectory-level Bellman\nobjective using the model's own logits as $Q$-values. TBRM removes the need for\ncritics, importance-sampling ratios, or clipping, and operates with only one\nrollout per prompt. We prove convergence to the near-optimal KL-regularized\npolicy from arbitrary off-policy data via an improved\nchange-of-trajectory-measure analysis. Experiments on standard\nmathematical-reasoning benchmarks show that TBRM consistently outperforms\npolicy-based baselines, like PPO and GRPO, with comparable or lower\ncomputational and memory overhead. Our results indicate that value-based RL\nmight be a principled and efficient alternative for enhancing reasoning\ncapabilities in LLMs.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8f68\u8ff9\u8d1d\u5c14\u66fc\u6b8b\u5dee\u6700\u5c0f\u5316\uff08TBRM\uff09\u7684\u7b97\u6cd5\uff0c\u5c06\u7ecf\u5178\u7684\u8d1d\u5c14\u66fc\u6b8b\u5dee\u6700\u5c0f\u5316\u601d\u60f3\u5e94\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4ee5\u4f18\u5316\u5355\u4e00\u8f68\u8ff9\u7ea7\u522b\u7684\u8d1d\u5c14\u66fc\u76ee\u6807\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u6279\u8bc4\u8005\u3001\u91cd\u8981\u6027\u91c7\u6837\u6bd4\u7387\u6216\u526a\u8f91\uff0c\u5e76\u4e14\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u5982PPO\u548cGRPO\uff0c\u540c\u65f6\u5177\u6709\u76f8\u5f53\u6216\u66f4\u4f4e\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7ba1\u9053\u4e2d\uff0c\u57fa\u4e8e\u7b56\u7565\u7684\u65b9\u6cd5\u4e3b\u5bfc\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u9886\u57df\uff0c\u800c\u57fa\u4e8e\u4ef7\u503c\u7684\u65b9\u6cd5\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u91cd\u65b0\u5ba1\u89c6\u4e86\u7ecf\u5178\u7684\u8d1d\u5c14\u66fc\u6b8b\u5dee\u6700\u5c0f\u5316\u8303\u5f0f\uff0c\u5e76\u8bd5\u56fe\u5c06\u5176\u5e94\u7528\u4e8eLLM\u63a8\u7406\u3002", "method": "TBRM\u7b97\u6cd5\u901a\u8fc7\u4f7f\u7528\u6a21\u578b\u81ea\u8eab\u7684logits\u4f5c\u4e3a$Q$-\u503c\u6765\u4f18\u5316\u5355\u4e00\u8f68\u8ff9\u7ea7\u522b\u7684\u8d1d\u5c14\u66fc\u76ee\u6807\u3002\u5b83\u4e0d\u9700\u8981\u6279\u8bc4\u8005\u3001\u91cd\u8981\u6027\u91c7\u6837\u6bd4\u7387\u6216\u526a\u8f91\uff0c\u5e76\u4e14\u53ea\u9700\u8981\u6bcf\u4e2a\u63d0\u793a\u8fdb\u884c\u4e00\u6b21rollout\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u901a\u8fc7\u6539\u8fdb\u7684\u8f68\u8ff9\u5ea6\u91cf\u5206\u6790\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u4ece\u4efb\u610f\u79bb\u7ebf\u6570\u636e\u6536\u655b\u5230\u63a5\u8fd1\u6700\u4f18\u7684KL\u6b63\u5219\u5316\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6807\u51c6\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\uff0cTBRM\u4e00\u81f4\u5730\u4f18\u4e8e\u57fa\u4e8e\u7b56\u7565\u7684\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982PPO\u548cGRPO\uff09\uff0c\u5e76\u4e14\u5177\u6709\u76f8\u5f53\u6216\u66f4\u4f4e\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u3002\u8fd9\u8868\u660e\u57fa\u4e8e\u4ef7\u503c\u7684RL\u53ef\u80fd\u662f\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\u7684\u4e00\u79cd\u6709\u539f\u5219\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u57fa\u4e8e\u4ef7\u503c\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982TBRM\uff09\u4e3a\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u3001\u6709\u6548\u7684\u65b9\u5411\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u7b80\u5316\u4e86\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u8fd8\u5c55\u793a\u4e86\u4e0e\u73b0\u6709\u57fa\u4e8e\u7b56\u7565\u65b9\u6cd5\u76f8\u6bd4\u7684\u7ade\u4e89\u6027\u80fd\u548c\u6548\u7387\u4f18\u52bf\u3002"}}
{"id": "2505.15306", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.15306", "abs": "https://arxiv.org/abs/2505.15306", "authors": ["Yiwen Song", "Qianyue Hao", "Qingmin Liao", "Jian Yuan", "Yong Li"], "title": "Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Model ensemble is a useful approach in reinforcement learning (RL) for\ntraining effective agents. Despite wide success of RL, training effective\nagents remains difficult due to the multitude of factors requiring careful\ntuning, such as algorithm selection, hyperparameter settings, and even random\nseed choices, all of which can significantly influence an agent's performance.\nModel ensemble helps overcome this challenge by combining multiple weak agents\ninto a single, more powerful one, enhancing overall performance. However,\nexisting ensemble methods, such as majority voting and Boltzmann addition, are\ndesigned as fixed strategies and lack a semantic understanding of specific\ntasks, limiting their adaptability and effectiveness. To address this, we\npropose LLM-Ens, a novel approach that enhances RL model ensemble with\ntask-specific semantic understandings driven by large language models (LLMs).\nGiven a task, we first design an LLM to categorize states in this task into\ndistinct 'situations', incorporating high-level descriptions of the task\nconditions. Then, we statistically analyze the strengths and weaknesses of each\nindividual agent to be used in the ensemble in each situation. During the\ninference time, LLM-Ens dynamically identifies the changing task situation and\nswitches to the agent that performs best in the current situation, ensuring\ndynamic model selection in the evolving task condition. Our approach is\ndesigned to be compatible with agents trained with different random seeds,\nhyperparameter settings, and various RL algorithms. Extensive experiments on\nthe Atari benchmark show that LLM-Ens significantly improves the RL model\nensemble, surpassing well-known baselines by up to 20.9%. For reproducibility,\nour code is open-source at\nhttps://anonymous.4open.science/r/LLM4RLensemble-F7EE.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLLM-Ens\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6a21\u578b\u96c6\u6210\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u6839\u636e\u4efb\u52a1\u8bed\u4e49\u7406\u89e3\u5c06\u72b6\u6001\u5206\u7c7b\u4e3a\u4e0d\u540c\u7684\u60c5\u5883\uff0c\u5e76\u52a8\u6001\u9009\u62e9\u5728\u5f53\u524d\u60c5\u5883\u4e0b\u8868\u73b0\u6700\u4f73\u7684\u4ee3\u7406\u3002\u5b9e\u9a8c\u8868\u660e\uff0cLLM-Ens\u663e\u8457\u63d0\u5347\u4e86RL\u6a21\u578b\u96c6\u6210\u7684\u6548\u679c\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf20.9%\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u53d6\u5f97\u4e86\u5e7f\u6cdb\u7684\u6210\u529f\uff0c\u4f46\u8bad\u7ec3\u6709\u6548\u7684\u667a\u80fd\u4f53\u4ecd\u7136\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u4f8b\u5982\u7b97\u6cd5\u9009\u62e9\u3001\u8d85\u53c2\u6570\u8bbe\u7f6e\u548c\u968f\u673a\u79cd\u5b50\u9009\u62e9\u7b49\u90fd\u4f1a\u663e\u8457\u5f71\u54cd\u6027\u80fd\u3002\u73b0\u6709\u7684\u96c6\u6210\u65b9\u6cd5\uff08\u5982\u591a\u6570\u6295\u7968\u548cBoltzmann\u52a0\u6743\uff09\u7531\u4e8e\u7f3a\u4e4f\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u5176\u9002\u5e94\u6027\u548c\u6709\u6548\u6027\u53d7\u5230\u9650\u5236\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u96c6\u6210\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "LLM-Ens\u65b9\u6cd5\u9996\u5148\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u4efb\u52a1\u4e2d\u7684\u72b6\u6001\u8fdb\u884c\u5206\u7c7b\uff0c\u5c06\u5176\u5212\u5206\u4e3a\u4e0d\u540c\u7684'\u60c5\u5883'\uff0c\u5e76\u7ed3\u5408\u4efb\u52a1\u6761\u4ef6\u7684\u9ad8\u5c42\u63cf\u8ff0\u3002\u7136\u540e\u7edf\u8ba1\u5206\u6790\u6bcf\u4e2a\u72ec\u7acb\u4ee3\u7406\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u7684\u4f18\u52a3\u52bf\u3002\u5728\u63a8\u7406\u9636\u6bb5\uff0cLLM-Ens\u52a8\u6001\u8bc6\u522b\u4efb\u52a1\u60c5\u5883\u7684\u53d8\u5316\uff0c\u5e76\u5207\u6362\u5230\u5728\u5f53\u524d\u60c5\u5883\u4e0b\u8868\u73b0\u6700\u4f73\u7684\u4ee3\u7406\uff0c\u4ece\u800c\u5b9e\u73b0\u52a8\u6001\u6a21\u578b\u9009\u62e9\u3002\u8be5\u65b9\u6cd5\u517c\u5bb9\u4f7f\u7528\u4e0d\u540c\u968f\u673a\u79cd\u5b50\u3001\u8d85\u53c2\u6570\u8bbe\u7f6e\u548c\u591a\u79cdRL\u7b97\u6cd5\u8bad\u7ec3\u7684\u4ee3\u7406\u3002", "result": "\u5728Atari\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLLM-Ens\u663e\u8457\u63d0\u9ad8\u4e86RL\u6a21\u578b\u96c6\u6210\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u5df2\u77e5\u7684\u57fa\u7ebf\u65b9\u6cd5\u6700\u591a\u8fbe20.9%\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u7814\u7a76\uff0c\u4f5c\u8005\u5f00\u6e90\u4e86\u4ee3\u7801\u3002", "conclusion": "LLM-Ens\u901a\u8fc7\u5f15\u5165\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4efb\u52a1\u8bed\u4e49\u7406\u89e3\uff0c\u6210\u529f\u589e\u5f3a\u4e86RL\u6a21\u578b\u96c6\u6210\u7684\u6548\u679c\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u548c\u826f\u597d\u7684\u9002\u5e94\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u672a\u6765\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u96c6\u6210\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
