<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 3]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.GR](#cs.GR) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: 本文是一篇立场论文，批判性地综述了人类与AI智能体协作的广泛经验发展。文章强调了技术成就和持续存在的差距，并提出了一种新的概念架构（分层探索-利用网络），以系统地整合多智能体协调、知识管理、控制论反馈环路及高层控制机制等技术细节。该框架旨在为现有贡献提供一个统一的理论基础，促进传统方法的改进并启发融合定性和定量范式的新研究。


<details>
  <summary>Details</summary>
Motivation: 当前关于人类与AI智能体协作的研究虽然取得了显著的技术成就，但缺乏一个统一的理论框架来整合这些多样化的研究，尤其是在处理开放性、复杂任务时。这促使作者提出一种新的概念架构来解决这一问题。

Method: 作者提出了一种名为分层探索-利用网络（Hierarchical Exploration-Exploitation Net）的新概念架构，通过将现有的贡献映射到这个框架上，包括符号AI技术、基于连接主义的大语言模型智能体以及混合组织实践，从而实现对传统方法的修订，并激发新的研究方向。

Result: 该框架不仅有助于重新审视传统的研究方法，还能够激励新的工作，融合定性和定量的研究范式，推动人类认知与AI能力的共同进化。此外，文章结构灵活，可从任何部分开始阅读，既作为技术实施的批判性回顾，又作为设计或扩展人类-AI共生关系的前瞻性参考。

Conclusion: 本文提出的分层探索-利用网络框架为未来设计和扩展人类-AI协作提供了理论基础和实践指导，标志着向更深层次的人类认知与AI能力共进化迈进了一步。

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [2] [Artificial Expert Intelligence through PAC-reasoning](https://arxiv.org/abs/2412.02441)
*Shai Shalev-Shwartz,Amnon Shashua,Gal Beniamini,Yoav Levine,Or Sharir,Noam Wies,Ido Ben-Shaul,Tomer Nussbaum,Shir Granot Peled*

Main category: cs.AI

TL;DR: 人工专家智能（AEI）通过引入'大概率近似正确(PAC)推理'框架，结合领域专业知识与顶级人类专家的精确推理能力，旨在超越通用人工智能(AGI)和狭义AI的限制。此框架将复杂问题分解并控制推理精度，建立了一个具有误差边界保证的推理时学习基础。


<details>
  <summary>Details</summary>
Motivation: 当前的人工智能系统在预定义任务上表现出色，但在适应性和解决新问题时的精确性方面存在不足。为了克服这些局限性，需要一种新的推理方法，能够集成特定领域的专业知识，并具备类似顶尖人类专家的精确推理能力。

Method: AEI引入了'大概率近似正确(PAC)推理'的框架，该框架提供了可靠的理论保证，用于可靠地分解复杂问题，并提供了一种实际机制来控制推理精度。受人类思维中系统1（直觉思考）和系统2（反思推理）的启发，AEI提出了一种新的推理类型——系统3（精确推理），其灵感来源于科学研究的严谨性。

Result: 通过PAC推理框架，AEI实现了具有误差边界保证的推理时间学习，提高了AI系统在解决复杂问题时的适应性和精确性。

Conclusion: AEI为构建更强大、更灵活的人工智能系统奠定了基础，尤其是在需要精确推理和适应性的应用场景中。它通过整合领域专业知识和科学严谨的推理方法，开创了新一代AI系统的可能性。

Abstract: Artificial Expert Intelligence (AEI) seeks to transcend the limitations of
both Artificial General Intelligence (AGI) and narrow AI by integrating
domain-specific expertise with critical, precise reasoning capabilities akin to
those of top human experts. Existing AI systems often excel at predefined tasks
but struggle with adaptability and precision in novel problem-solving. To
overcome this, AEI introduces a framework for ``Probably Approximately Correct
(PAC) Reasoning". This paradigm provides robust theoretical guarantees for
reliably decomposing complex problems, with a practical mechanism for
controlling reasoning precision. In reference to the division of human thought
into System 1 for intuitive thinking and System 2 for reflective
reasoning~\citep{tversky1974judgment}, we refer to this new type of reasoning
as System 3 for precise reasoning, inspired by the rigor of the scientific
method. AEI thus establishes a foundation for error-bounded, inference-time
learning.

</details>


### [3] [LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation](https://arxiv.org/abs/2505.20671)
*Heng Tan,Hua Yan,Yu Yang*

Main category: cs.AI

TL;DR: 本研究提出了一种由大语言模型（LLM）引导的强化学习（RL）策略调制框架，无需额外模型训练或人工干预即可改善RL训练。通过利用LLM识别关键状态、提供动作建议和分配隐式奖励来指导策略改进，该方法在标准RL基准测试中优于现有最先进基线。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在多个领域取得了显著成功，但在复杂任务中训练有效的策略仍然具有挑战性。现有的缓解训练瓶颈的方法要么依赖昂贵且不确定的自动化策略改进，要么需要难以扩展到大规模环境的人工反馈。因此，研究者寻求一种既不需要额外模型训练也不需要人工干预的新方法来提升RL训练效果。

Method: 研究者设计了一个由大语言模型（LLM）引导的策略调制框架。首先，LLM被用来从次优代理的轨迹中识别出关键状态。然后，基于这些关键状态，LLM提供动作建议并分配隐式奖励以指导策略的改进。整个过程无需额外的模型训练或人工干预。

Result: 实验结果表明，该方法在多个标准RL基准上表现优异，超越了当前最先进的基线方法。这证明了基于LLM的解释在解决RL训练瓶颈方面的有效性。

Conclusion: 这项研究表明，使用大语言模型可以有效地改进强化学习中的策略训练，而无需额外的模型训练或人工干预。这种方法为未来RL研究提供了一个新的方向，特别是在处理复杂任务时，展示了LLM在强化学习领域的潜力。

Abstract: While reinforcement learning (RL) has achieved notable success in various
domains, training effective policies for complex tasks remains challenging.
Agents often converge to local optima and fail to maximize long-term rewards.
Existing approaches to mitigate training bottlenecks typically fall into two
categories: (i) Automated policy refinement, which identifies critical states
from past trajectories to guide policy updates, but suffers from costly and
uncertain model training; and (ii) Human-in-the-loop refinement, where human
feedback is used to correct agent behavior, but this does not scale well to
environments with large or continuous action spaces. In this work, we design a
large language model-guided policy modulation framework that leverages LLMs to
improve RL training without additional model training or human intervention. We
first prompt an LLM to identify critical states from a sub-optimal agent's
trajectories. Based on these states, the LLM then provides action suggestions
and assigns implicit rewards to guide policy refinement. Experiments across
standard RL benchmarks demonstrate that our method outperforms state-of-the-art
baselines, highlighting the effectiveness of LLM-based explanations in
addressing RL training bottlenecks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [MvDeCor: Multi-view Dense Correspondence Learning for Fine-grained 3D Segmentation](https://arxiv.org/abs/2208.08580)
*Gopal Sharma,Kangxue Yin,Subhransu Maji,Evangelos Kalogerakis,Or Litany,Sanja Fidler*

Main category: cs.CV

TL;DR: 提出了一种结合2D和3D信息的方法MvDeCor，用于细粒度3D形状分割任务，利用自监督技术和多视角渲染，学习到视图不变且几何一致的表示，实验表明该方法在有纹理和无纹理的数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 受到基于视图的表面表示比点云或体素占用等3D表示更能有效建模高分辨率表面细节和纹理这一观察的启发，旨在通过结合2D和3D的优势来改进细粒度3D形状分割任务。

Method: 给定一个3D形状，从多个视角对其进行渲染，并在对比学习框架内设置密集对应学习任务，从而学习到视图不变且几何一致的2D表示。

Result: 在RenderPeople（有纹理）和PartNet（无纹理）3D数据集上的实验表明，该方法在细粒度部件分割任务上优于现有最佳方法，尤其是在训练时只有稀疏视角可用或形状有纹理的情况下，改进更为显著。

Conclusion: 所提出的方法MvDeCor通过结合2D处理和3D几何推理的优势，在细粒度3D形状分割任务上取得了更好的泛化性能，特别是在标注数据有限的情况下。

Abstract: We propose to utilize self-supervised techniques in the 2D domain for
fine-grained 3D shape segmentation tasks. This is inspired by the observation
that view-based surface representations are more effective at modeling
high-resolution surface details and texture than their 3D counterparts based on
point clouds or voxel occupancy. Specifically, given a 3D shape, we render it
from multiple views, and set up a dense correspondence learning task within the
contrastive learning framework. As a result, the learned 2D representations are
view-invariant and geometrically consistent, leading to better generalization
when trained on a limited number of labeled shapes compared to alternatives
that utilize self-supervision in 2D or 3D alone. Experiments on textured
(RenderPeople) and untextured (PartNet) 3D datasets show that our method
outperforms state-of-the-art alternatives in fine-grained part segmentation.
The improvements over baselines are greater when only a sparse set of views is
available for training or when shapes are textured, indicating that MvDeCor
benefits from both 2D processing and 3D geometric reasoning.

</details>


### [5] [Mix3D: Out-of-Context Data Augmentation for 3D Scenes](https://arxiv.org/abs/2110.02210)
*Alexey Nekrasov,Jonas Schult,Or Litany,Bastian Leibe,Francis Engelmann*

Main category: cs.CV

TL;DR: 提出了一种名为Mix3D的数据增强技术，通过组合两个增强场景生成新的训练样本，从而平衡全局场景上下文和局部几何结构。该方法可以显著提升模型在室内（ScanNet、S3DIS）和室外数据集（SemanticKITTI）上的性能。MinkowskiNet在使用Mix3D后，在ScanNet测试基准上达到了78.1 mIoU的性能。代码已公开。


<details>
  <summary>Details</summary>
Motivation: 当前的研究主要集中在具有大容量和感受野的模型上，这些模型能够充分捕捉输入3D场景的全局上下文。然而，强上下文先验可能会导致误判，例如将过马路的行人误认为是汽车。因此，需要一种方法来平衡全局场景上下文和局部几何结构，以超越训练集中上下文先验的限制。

Method: 提出了一种“混合”技术，通过结合两个增强场景创建新的训练样本。这种方法将对象实例隐式地放置到新颖的、超出上下文的环境中，从而使模型难以仅依赖于场景上下文，并促使模型从局部结构中推断语义。

Result: 实验表明，在室内（ScanNet、S3DIS）和室外数据集（SemanticKITTI）上，使用Mix3D训练的模型性能得到了显著提升。特别是，MinkowskiNet在使用Mix3D后，在ScanNet测试基准上超过了所有先前的最先进方法，达到了78.1 mIoU的性能。

Conclusion: Mix3D是一种有效的数据增强技术，可以通过简单的集成显著提升现有3D场景分割模型的性能。它强调了全局上下文和局部结构的重要性，并证明了混合场景对模型泛化能力的积极影响。代码已在https://nekrasov.dev/mix3d/公开。

Abstract: We present Mix3D, a data augmentation technique for segmenting large-scale 3D
scenes. Since scene context helps reasoning about object semantics, current
works focus on models with large capacity and receptive fields that can fully
capture the global context of an input 3D scene. However, strong contextual
priors can have detrimental implications like mistaking a pedestrian crossing
the street for a car. In this work, we focus on the importance of balancing
global scene context and local geometry, with the goal of generalizing beyond
the contextual priors in the training set. In particular, we propose a "mixing"
technique which creates new training samples by combining two augmented scenes.
By doing so, object instances are implicitly placed into novel out-of-context
environments and therefore making it harder for models to rely on scene context
alone, and instead infer semantics from local structure as well. We perform
detailed analysis to understand the importance of global context, local
structures and the effect of mixing scenes. In experiments, we show that models
trained with Mix3D profit from a significant performance boost on indoor
(ScanNet, S3DIS) and outdoor datasets (SemanticKITTI). Mix3D can be trivially
used with any existing method, e.g., trained with Mix3D, MinkowskiNet
outperforms all prior state-of-the-art methods by a significant margin on the
ScanNet test benchmark 78.1 mIoU. Code is available at:
https://nekrasov.dev/mix3d/

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [Skywork Open Reasoner 1 Technical Report](https://arxiv.org/abs/2505.22312)
*Jujie He,Jiacai Liu,Chris Yuhao Liu,Rui Yan,Chaojie Wang,Peng Cheng,Xiaoyu Zhang,Fuxiang Zhang,Jiacheng Xu,Wei Shen,Siyuan Li,Liang Zeng,Tianwen Wei,Cheng Cheng,Bo An,Yang Liu,Yahui Zhou*

Main category: cs.LG

TL;DR: 文章介绍了Skywork-OR1，一种针对长链思维模型的有效且可扩展的强化学习实现方法。通过改进DeepSeek-R1-Distill系列模型，Skywork-OR1在多个基准测试中显著提升了大语言模型的推理能力，并开源了模型权重、训练代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 研究者希望通过强化学习进一步提升大型语言模型（LLMs）的推理能力，特别是针对长链思维任务的性能优化。基于DeepSeek-R1的成功经验，他们希望探索更有效的RL方法以提高模型在特定任务上的表现。

Method: Skywork-OR1建立在DeepSeek-R1-Distill模型系列的基础上，采用强化学习技术对模型进行微调。研究团队进行了全面的消融实验以验证训练流程中核心组件的有效性，并深入分析了熵坍缩现象及其对模型性能的影响。

Result: 对于32B参数模型，平均准确率从57.8%提升至72.8%（+15.0%），而7B参数模型则从43.6%提升到57.5%（+13.9%）。Skywork-OR1-32B在AIME24和AIME25基准上超过了DeepSeek-R1和Qwen3-32B，同时在LiveCodeBench上取得了相当的结果。Skywork-OR1-7B及其变体也展现了与同规模模型竞争的推理能力。

Conclusion: Skywork-OR1证明了强化学习在提升大型语言模型推理能力方面的有效性，尤其是在长链思维任务中。研究团队通过开源模型权重、训练代码和数据集，支持社区进一步的研究和发展。此外，缓解过早的熵坍缩被证实是提高测试性能的关键因素。

Abstract: The success of DeepSeek-R1 underscores the significant role of reinforcement
learning (RL) in enhancing the reasoning capabilities of large language models
(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL
implementation for long Chain-of-Thought (CoT) models. Building on the
DeepSeek-R1-Distill model series, our RL approach achieves notable performance
gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench
from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)
for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and
Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable
results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models
demonstrate competitive reasoning capabilities among models of similar size. We
perform comprehensive ablation studies on the core components of our training
pipeline to validate their effectiveness. Additionally, we thoroughly
investigate the phenomenon of entropy collapse, identify key factors affecting
entropy dynamics, and demonstrate that mitigating premature entropy collapse is
critical for improved test performance. To support community research, we fully
open-source our model weights, training code, and training datasets.

</details>


### [7] [Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding](https://arxiv.org/abs/2505.21908)
*Hanyin Wang,Zhenbang Wu,Gururaj Kolar,Hariprasad Korsapati,Brian Bartlett,Bryan Hull,Jimeng Sun*

Main category: cs.LG

TL;DR: 本研究介绍了一种名为DRG-Sapphire的模型，它通过大规模强化学习（RL）技术实现了从临床记录中自动化进行诊断相关组（DRG）编码的任务。该模型基于Qwen2.5-7B，并使用基于规则奖励的群体相对策略优化（GRPO）方法进行训练。DRG-Sapphire在MIMIC-IV基准测试中取得了最先进的准确率，并且能够生成医生验证的DRG编码推理，从而显著提高了可解释性。此外，研究还揭示了将强化学习应用于知识密集型、OOD任务时面临的广泛挑战。


<details>
  <summary>Details</summary>
Motivation: 由于DRG编码对于医院的报销和运营至关重要，但目前依赖于劳动密集型的手动分配方式，因此需要开发一种自动化的解决方案来解决这一问题。然而，大型语言模型（LLMs）在处理DRG编码任务时面临困难，主要原因是预训练语料库很少包含私密的临床或账单数据，导致任务具有分布外（OOD）特性。

Method: 研究人员开发了DRG-Sapphire模型，基于Qwen2.5-7B并通过大规模强化学习进行训练。模型采用了群体相对策略优化（GRPO）算法，结合基于规则的奖励机制，以应对特定领域的挑战。此外，研究引入了一系列强化学习增强技术，专门用于解决之前数学任务中未见过的领域特定问题。

Result: DRG-Sapphire模型在MIMIC-IV基准测试上表现优异，达到了最先进水平的准确率。同时，该模型能够生成医生认可的DRG编码推理过程，增强了结果的可解释性。此外，研究发现强化学习性能与监督微调（SFT）样本数量的对数呈近似线性关系，表明基础模型中编码的领域知识对强化学习效果有根本性的限制。

Conclusion: 这项研究表明，在类似DRG编码这样的OOD任务中，强大的强化学习性能需要在强化学习之前注入足够的领域知识。因此，对于此类任务，扩展监督微调可能比单独扩展强化学习更有效且计算效率更高。这为未来在知识密集型、OOD任务中应用强化学习提供了重要启示。

Abstract: Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement
and operations but require labor-intensive assignment. Large Language Models
(LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of
the task: pretraining corpora rarely contain private clinical or billing data.
We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL)
for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained
with Group Relative Policy Optimization (GRPO) using rule-based rewards,
DRG-Sapphire introduces a series of RL enhancements to address domain-specific
challenges not seen in previous mathematical tasks. Our model achieves
state-of-the-art accuracy on the MIMIC-IV benchmark and generates
physician-validated reasoning for DRG assignments, significantly enhancing
explainability. Our study further sheds light on broader challenges of applying
RL to knowledge-intensive, OOD tasks. We observe that RL performance scales
approximately linearly with the logarithm of the number of supervised
fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally
constrained by the domain knowledge encoded in the base model. For OOD tasks
like DRG coding, strong RL performance requires sufficient knowledge infusion
prior to RL. Consequently, scaling SFT may be more effective and
computationally efficient than scaling RL alone for such tasks.

</details>


### [8] [TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction](https://arxiv.org/abs/2505.21807)
*Tommy Xu,Zhitian Zhang,Xiangyu Sun,Lauren Kelly Zung,Hossein Hajimirsadeghi,Greg Mori*

Main category: cs.LG

TL;DR: 本研究提出了一种基于强化学习的推理型大语言模型（LLMs）方法，用于对表格数据进行更准确且可解释的预测。通过引入定制奖励函数，该模型不仅提高了预测精度，还生成了人类可理解的推理依据。实验结果表明，在金融基准数据集上，该模型表现优于大多数现有LLMs。


<details>
  <summary>Details</summary>
Motivation: 尽管梯度提升机和一些深度学习模型在表格数据上表现出色，但它们缺乏可解释性；而虽然大语言模型（LLMs）能生成类似人类的推理和解释，但在表格数据预测任务上的性能较差。因此，需要一种结合高预测精度与良好可解释性的新方法。

Method: 研究者提出了一种利用强化学习训练推理型大语言模型的方法，针对表格数据预测任务设计了自定义奖励函数，引导模型同时追求高预测精度和可解释性。

Result: 实验结果表明，所提出的模型在金融基准数据集上取得了有希望的性能，超越了大多数现有的大语言模型。

Conclusion: 本文提出的方法成功地将推理型大语言模型应用于表格数据预测任务，既提升了预测精度又增强了可解释性，为未来的研究提供了新的方向。

Abstract: Predictive modeling on tabular data is the cornerstone of many real-world
applications. Although gradient boosting machines and some recent deep models
achieve strong performance on tabular data, they often lack interpretability.
On the other hand, large language models (LLMs) have demonstrated powerful
capabilities to generate human-like reasoning and explanations, but remain
under-performed for tabular data prediction. In this paper, we propose a new
approach that leverages reasoning-based LLMs, trained using reinforcement
learning, to perform more accurate and explainable predictions on tabular data.
Our method introduces custom reward functions that guide the model not only
toward high prediction accuracy but also toward human-understandable reasons
for its predictions. Experimental results show that our model achieves
promising performance on financial benchmark datasets, outperforming most
existing LLMs.

</details>


### [9] [Reinforcing General Reasoning without Verifiers](https://arxiv.org/abs/2505.21493)
*Xiangxin Zhou,Zichen Liu,Anya Sims,Haonan Wang,Tianyu Pang,Chongxuan Li,Liang Wang,Min Lin,Chao Du*

Main category: cs.LG

TL;DR: 本文提出了一种名为VeriFree的验证器自由方法，该方法通过强化学习直接最大化生成参考答案的概率，无需依赖额外的验证模型。这种方法在MMLU-Pro、GPQA、SuperGPQA和数学相关基准测试中表现优异，甚至超越了基于验证器的方法。此外，VeriFree减少了计算需求并提供了多角度的理论见解。


<details>
  <summary>Details</summary>
Motivation: 当前使用DeepSeek-R1-Zero风格的强化学习训练大语言模型（LLMs）在代码和数学推理方面取得了显著进展，但其局限在于仅适用于可以通过规则验证答案的任务，无法自然扩展到如化学、医疗、工程等实际领域。现有的解决方法依赖于额外的LLM作为验证模型，但存在诸如对强验证模型的依赖、奖励欺骗的风险以及训练时维护验证模型内存的实际负担等问题。因此，需要一种新的方法来克服这些限制并扩展到更广泛的推理领域。

Method: 研究者提出了一种称为VeriFree的验证器自由方法，该方法绕过了答案验证步骤，转而通过强化学习直接最大化生成参考答案的概率。这种方法将策略和隐式验证器集成到一个统一模型中进行训练，并且从变分优化的角度进行了理论分析。

Result: 实验结果表明，VeriFree在MMLU-Pro、GPQA、SuperGPQA和数学相关基准测试中的表现与基于验证器的方法相当，甚至在某些情况下超越了它们。此外，VeriFree还展示了显著的实际优势，例如减少计算需求和避免依赖强大的验证模型。

Conclusion: VeriFree提供了一种有效且高效的替代方案，用于扩展DeepSeek-R1-Zero风格的训练方法至一般推理领域。它不仅在性能上可以匹敌甚至超越传统的基于验证器的方法，而且降低了计算成本并简化了实现过程。从多个角度来看，VeriFree为未来的研究提供了有价值的启示。

Abstract: The recent paradigm shift towards training large language models (LLMs) using
DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has
led to impressive advancements in code and mathematical reasoning. However,
this methodology is limited to tasks where rule-based answer verification is
possible and does not naturally extend to real-world domains such as chemistry,
healthcare, engineering, law, biology, business, and economics. Current
practical workarounds use an additional LLM as a model-based verifier; however,
this introduces issues such as reliance on a strong verifier LLM,
susceptibility to reward hacking, and the practical burden of maintaining the
verifier model in memory during training. To address this and extend
DeepSeek-R1-Zero-style training to general reasoning domains, we propose a
verifier-free method (VeriFree) that bypasses answer verification and instead
uses RL to directly maximize the probability of generating the reference
answer. We compare VeriFree with verifier-based methods and demonstrate that,
in addition to its significant practical benefits and reduced compute
requirements, VeriFree matches and even surpasses verifier-based methods on
extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related
benchmarks. Moreover, we provide insights into this method from multiple
perspectives: as an elegant integration of training both the policy and
implicit verifier in a unified model, and as a variational optimization
approach. Code is available at https://github.com/sail-sg/VeriFree.

</details>


### [10] [Can Large Reasoning Models Self-Train?](https://arxiv.org/abs/2505.21444)
*Sheikh Shafayat,Fahim Tajwar,Ruslan Salakhutdinov,Jeff Schneider,Andrea Zanette*

Main category: cs.LG

TL;DR: 本文提出了一种在线自训练强化学习算法，利用模型的自一致性推断正确性信号，无需真实监督即可训练。该算法在数学推理任务上表现优异，接近使用标准答案训练的强化学习方法，但存在奖励欺骗的风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）性能提升需要减少对人工监督的依赖，而现有的强化学习方法受限于人工设计的验证器。为解决这一问题，作者探索了自训练方法，即模型通过自身的判断提供监督信号。

Method: 作者提出了一个基于在线自训练的强化学习算法，利用模型的自一致性来推断正确性信号，并在没有任何真实监督的情况下进行训练。此方法应用于复杂的数学推理任务中。

Result: 实验表明，该算法在数学推理任务上的表现迅速达到与使用标准答案训练的强化学习方法相当的水平。然而，研究也揭示了该算法的一个固有限制：自动生成的代理奖励可能与正确性相关，但也可能导致奖励欺骗现象，即模型倾向于生成自信但错误的输出。

Conclusion: 研究表明，自监督改进可以在不依赖外部标签的情况下实现显著的性能提升，但也暴露了其根本挑战，如奖励欺骗问题。这为未来的研究提供了方向，尤其是在如何避免奖励欺骗和提高模型的可靠性方面。

Abstract: Scaling the performance of large language models (LLMs) increasingly depends
on methods that reduce reliance on human supervision. Reinforcement learning
from automated verification offers an alternative, but it incurs scalability
limitations due to dependency upon human-designed verifiers. Self-training,
where the model's own judgment provides the supervisory signal, presents a
compelling direction. We propose an online self-training reinforcement learning
algorithm that leverages the model's self-consistency to infer correctness
signals and train without any ground-truth supervision. We apply the algorithm
to challenging mathematical reasoning tasks and show that it quickly reaches
performance levels rivaling reinforcement-learning methods trained explicitly
on gold-standard answers. Additionally, we analyze inherent limitations of the
algorithm, highlighting how the self-generated proxy reward initially
correlated with correctness can incentivize reward hacking, where confidently
incorrect outputs are favored. Our results illustrate how self-supervised
improvement can achieve significant performance gains without external labels,
while also revealing its fundamental challenges.

</details>


### [11] [Accelerating RL for LLM Reasoning with Optimal Advantage Regression](https://arxiv.org/abs/2505.20686)
*Kianté Brantley,Mingyu Chen,Zhaolin Gao,Jason D. Lee,Wen Sun,Wenhao Zhan,Xuezhou Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为$A$*-PO的新型两阶段策略优化框架，通过离线采样和在线更新来高效训练大型语言模型（LLMs）进行推理任务，减少了计算时间和内存使用。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的策略优化方法在强化学习中存在高计算开销和内存消耗的问题，主要源于每次提示需要多次生成以及依赖于当前策略的批评网络或优势估计。

Method: 第一阶段利用参考策略的离线采样来估计最优值函数$V$*，从而避免昂贵的在线价值估计；第二阶段则通过每个提示仅一次生成的最小二乘回归损失来进行在线策略更新。此外，理论上证明了可以在无需复杂探索策略的情况下优化KL正则化的RL目标。

Result: 在广泛的数学推理基准测试中，$A$*-PO的表现具有竞争力，并且与PPO、GRPO和REBEL相比，将训练时间减少了高达2倍，峰值内存使用减少了超过30%。

Conclusion: $A$*-PO为大型语言模型的推理任务提供了一种高效的训练方法，显著降低了计算资源需求，同时保持了优秀的性能。

Abstract: Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning
large language models (LLMs) to improve complex reasoning abilities. However,
state-of-the-art policy optimization methods often suffer from high
computational overhead and memory consumption, primarily due to the need for
multiple generations per prompt and the reliance on critic networks or
advantage estimates of the current policy. In this paper, we propose $A$*-PO, a
novel two-stage policy optimization framework that directly approximates the
optimal advantage function and enables efficient training of LLMs for reasoning
tasks. In the first stage, we leverage offline sampling from a reference policy
to estimate the optimal value function $V$*, eliminating the need for costly
online value estimation. In the second stage, we perform on-policy updates
using a simple least-squares regression loss with only a single generation per
prompt. Theoretically, we establish performance guarantees and prove that the
KL-regularized RL objective can be optimized without requiring complex
exploration strategies. Empirically, $A$*-PO achieves competitive performance
across a wide range of mathematical reasoning benchmarks, while reducing
training time by up to 2$\times$ and peak memory usage by over 30% compared to
PPO, GRPO, and REBEL. Implementation of $A$*-PO can be found at
https://github.com/ZhaolinGao/A-PO.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 在大规模语言模型（LLMs）中，通过重复采样来扩展推理计算，可以随着样本数量的增加持续提高覆盖率（解决问题的比例）。研究发现，这种提升部分归因于标准评估基准中的答案分布偏向于少量常见答案。通过定义一个基于训练集中答案频率的基线方法，实验表明在某些LLMs上该基线优于重复采样，而在其他模型上与混合策略表现相当。此基线有助于更准确地衡量重复采样在提示无关猜测之外的实际改进效果。


<details>
  <summary>Details</summary>
Motivation: 研究者观察到，在大语言模型中，通过重复采样进行推理计算时，随着样本数的增加，问题解决的覆盖率也会增加。为了探究这一现象背后的原因，他们怀疑这可能与标准评估基准中的答案分布有关，这些分布往往偏向于少数常见的答案。因此，需要设计一种方法来验证这一假设，并更好地理解重复采样的实际贡献。

Method: 研究者提出了一种基线方法，该方法根据训练集中答案的出现频率对答案进行枚举。随后，他们在两个领域（数学推理和事实知识）进行了实验，比较了这种方法与重复模型采样以及一种混合策略的表现。混合策略结合了少量模型采样和基于枚举的猜测。

Result: 实验结果表明，在某些大语言模型中，提出的基线方法在性能上优于重复采样；而在其他模型中，其表现与混合策略相当。这说明重复采样的优势并非完全来自模型本身的推理能力，而是部分归因于答案分布的特点。

Conclusion: 通过引入基于训练集答案分布的基线方法，研究者能够更准确地评估重复采样在大语言模型中的实际效果。这一方法揭示了重复采样在提示无关猜测之外的改进程度，并为未来评估和优化大语言模型提供了新的视角。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [13] [A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains](https://arxiv.org/abs/2402.00559)
*Alon Jacovi,Yonatan Bitton,Bernd Bohnet,Jonathan Herzig,Or Honovich,Michael Tseng,Michael Collins,Roee Aharoni,Mor Geva*

Main category: cs.CL

TL;DR: 本文介绍了REVEAL数据集，一个用于评估复杂推理链验证方法的基准数据集。该数据集提供了详细的标注信息，包括推理步骤的相关性、证据关联性和逻辑正确性。研究发现当前的自动验证器在验证推理链时存在困难，特别是在逻辑正确性和矛盾检测方面。


<details>
  <summary>Details</summary>
Motivation: 复杂推理任务中，更准确的推理链通常可以提高下游任务的性能。然而，缺乏细粒度的步骤级数据集来全面评估这些验证方法，阻碍了这一领域的发展。

Method: 作者提出了REVEAL（Reasoning Verification Evaluation）数据集，包含开放域问答设置中语言模型答案的每个推理步骤的相关性、证据关联性和逻辑正确性的综合标签。此数据集涵盖了多种数据集和最先进的语言模型。

Result: 通过对REVEAL数据集的评估，发现自动验证器在验证推理链时表现不佳，尤其是在验证逻辑正确性和检测矛盾方面。

Conclusion: REVEAL数据集为复杂推理链验证方法提供了重要的基准资源，揭示了当前自动验证器的不足，并为进一步的研究和发展指明了方向。

Abstract: Prompting language models to provide step-by-step answers (e.g.,
"Chain-of-Thought") is the prominent approach for complex reasoning tasks,
where more accurate reasoning chains typically improve downstream task
performance. Recent literature discusses automatic methods to verify reasoning
to evaluate and improve their correctness. However, no fine-grained step-level
datasets are available to enable thorough evaluation of such verification
methods, hindering progress in this direction. We introduce REVEAL: Reasoning
Verification Evaluation, a dataset to benchmark automatic verifiers of complex
Chain-of-Thought reasoning in open-domain question-answering settings. REVEAL
includes comprehensive labels for the relevance, attribution to evidence
passages, and logical correctness of each reasoning step in a language model's
answer, across a variety of datasets and state-of-the-art language models.
Evaluation on REVEAL shows that verifiers struggle at verifying reasoning
chains - in particular, verifying logical correctness and detecting
contradictions. Available at https://reveal-dataset.github.io/ .

</details>


### [14] [Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start](https://arxiv.org/abs/2505.22334)
*Lai Wei,Yuting Li,Kaipeng Zheng,Chen Wang,Yue Wang,Linghe Kong,Lichao Sun,Weiran Huang*

Main category: cs.CL

TL;DR: 近期大语言模型（LLMs）在链式思维推理方面取得了显著进展，强化学习（RL）在其中起到了关键作用。然而，模型中出现的自我修正模式并不一定与更好的推理性能相关。本文提出了一种两阶段方法来增强多模态推理能力：首先通过监督微调（SFT）引入结构化的链式思维推理模式作为冷启动，然后使用基于GRPO的强化学习进一步优化这些能力。实验表明，这种方法在多个具有挑战性的多模态推理基准上始终优于仅使用SFT或仅使用RL的方法。最终模型在3B和7B规模的开源多模态LLMs中达到了最先进的性能水平，并且其3B模型的性能可与一些7B模型相媲美。


<details>
  <summary>Details</summary>
Motivation: 尽管'顿悟时刻'模式通常被认为是强化学习训练后出现的特性，但研究者发现这些模式在多模态大语言模型（MLLMs）中于强化学习训练前就已经存在，但它们不一定与更好的推理性能相关联。因此，有必要探索一种系统性方法，以充分利用这些模式并提高模型的推理能力。

Method: 本文提出了一种两阶段方法：
1. **监督微调（SFT）**：用作冷启动，向模型引入结构化的链式思维推理模式。
2. **基于GRPO的强化学习（RL）**：进一步优化由SFT生成的推理模式，以提高模型的推理能力。
此方法旨在结合SFT和RL的优点，从而更有效地提升多模态推理性能。

Result: 实验结果表明，该两阶段方法在多个具有挑战性的多模态推理基准上显著优于仅使用SFT或仅使用RL的方法。具体来说：
- 在MathVista和We-Math等数据集上，7B模型的性能从66.3%提升到73.4%，从62.9%提升到70.4%。
- 3B模型的性能可与一些7B模型相媲美。
这表明该方法不仅适用于大规模模型，还能有效提升较小规模模型的性能。

Conclusion: 本文通过引入一种两阶段方法（SFT+RL），成功提升了多模态大语言模型的推理能力，并在多个基准测试中达到了最先进的性能水平。此外，该方法为构建先进的多模态推理模型提供了实际指导。代码已公开，可供研究者进一步探索和改进。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
impressive chain-of-thought reasoning capabilities, with reinforcement learning
(RL) playing a crucial role in this progress. While "aha moment"
patterns--where models exhibit self-correction through reflection--are often
attributed to emergent properties from RL, we first demonstrate that these
patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not
necessarily correlate with improved reasoning performance. Building on these
insights, we present a comprehensive study on enhancing multimodal reasoning
through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start
with structured chain-of-thought reasoning patterns, followed by (2)
reinforcement learning via GRPO to further refine these capabilities. Our
extensive experiments show that this combined approach consistently outperforms
both SFT-only and RL-only methods across challenging multimodal reasoning
benchmarks. The resulting models achieve state-of-the-art performance among
open-source MLLMs at both 3B and 7B scales, with our 7B model showing
substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on
MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving
performance competitive with several 7B models. Overall, this work provides
practical guidance for building advanced multimodal reasoning models. Our code
is available at https://github.com/waltonfuture/RL-with-Cold-Start.

</details>


### [15] [Thinker: Learning to Think Fast and Slow](https://arxiv.org/abs/2505.21097)
*Stephen Chung,Wenyu Du,Jie Fu*

Main category: cs.CL

TL;DR: 最近的研究表明，通过在数学和编程等领域将强化学习（RL）应用于问答（QA）任务，可以提升大语言模型（LLM）的推理能力。受到心理学中的双重处理理论的启发，本文提出了一种包含四个阶段的简单修改方法：快速思考、验证、缓慢思考和总结。实验结果表明，该方法显著提高了Qwen2.5-1.5B和DeepSeek-R1-Qwen-1.5B的平均准确率，并且在快速思考模式下，Qwen2.5-1.5B仅使用少于1000个token就能达到26.8%的准确率，展示了显著的推理效率提升。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在长上下文长度的情况下能够表现出搜索行为，但这种行为通常不精确且缺乏信心，导致冗长、重复的回答，并暴露出直觉和验证方面的不足。为了解决这些问题，作者从心理学的双重处理理论中获得灵感，设计了一种新的问答任务框架。

Method: 该方法包括四个阶段：快速思考（Fast Thinking），要求LLM在严格的token预算内回答问题；验证（Verification），模型评估其初始响应；缓慢思考（Slow Thinking），模型通过更多深思熟虑来改进初始响应；总结（Summarization），将上一阶段的改进提炼成精确的步骤。

Result: 实验结果表明，对于Qwen2.5-1.5B，平均准确率从24.9%提高到27.9%，而DeepSeek-R1-Qwen-1.5B则从45.9%提高到49.8%。值得注意的是，在快速思考模式下，Qwen2.5-1.5B仅使用不到1000个token就能达到26.8%的准确率，显示出显著的推理效率提升。

Conclusion: 这些发现表明，直觉和深思熟虑的推理是不同的、互补的系统，可以通过有针对性的训练来获益。

Abstract: Recent studies show that the reasoning capabilities of Large Language Models
(LLMs) can be improved by applying Reinforcement Learning (RL) to
question-answering (QA) tasks in areas such as math and coding. With a long
context length, LLMs may learn to perform search, as indicated by the
self-correction behavior observed in DeepSeek R1. However, this search behavior
is often imprecise and lacks confidence, resulting in long, redundant responses
and highlighting deficiencies in intuition and verification. Inspired by the
Dual Process Theory in psychology, we introduce a simple modification to the QA
task that includes four stages: Fast Thinking, where the LLM must answer within
a strict token budget; Verification, where the model evaluates its initial
response; Slow Thinking, where it refines the initial response with more
deliberation; and Summarization, where it distills the refinement from the
previous stage into precise steps. Our proposed task improves average accuracy
from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for
DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone
achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial
inference efficiency gains. These findings suggest that intuition and
deliberative reasoning are distinct, complementary systems benefiting from
targeted training.

</details>


### [16] [SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution](https://arxiv.org/abs/2505.20732)
*Hanlin Wang,Chak Tou Leong,Jiashuo Wang,Jian Wang,Wenjie Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为Stepwise Progress Attribution (SPA) 的通用奖励重新分配框架，通过分解最终奖励为每步的贡献，有效解决强化学习中延迟奖励的问题。实验表明，SPA在多个代理基准测试中显著优于现有方法，提升了成功率和接地准确性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在训练大型语言模型代理处理复杂任务方面具有巨大潜力，但延迟奖励问题限制了其效果。反馈信号通常仅在任务完成后提供，导致早期动作无法获得充分指导，影响代理训练效果。

Method: 作者提出了Stepwise Progress Attribution (SPA)，一种将最终奖励分解为每步贡献的框架。通过训练一个进度估计器，累积轨迹中的每步贡献以匹配任务完成情况。在策略优化过程中，结合每步估计贡献与环境中的动作接地信号，形成细粒度的中间奖励，用于更有效的代理训练。

Result: 在Webshop、ALFWorld和VirtualHome等常见代理基准测试中，SPA在成功率上平均提高了2.5%，接地准确性上平均提高了1.9%。进一步分析表明，该方法提供了更有效的中间奖励，显著改进了强化学习训练过程。

Conclusion: Stepwise Progress Attribution (SPA) 作为一种通用奖励重新分配框架，成功解决了强化学习中延迟奖励的挑战，显著提升了代理训练的效果。此方法在多个基准测试中表现出色，为未来研究提供了新的方向。

Abstract: Reinforcement learning (RL) holds significant promise for training LLM agents
to handle complex, goal-oriented tasks that require multi-step interactions
with external environments. However, a critical challenge when applying RL to
these agentic tasks arises from delayed rewards: feedback signals are typically
available only after the entire task is completed. This makes it non-trivial to
assign delayed rewards to earlier actions, providing insufficient guidance
regarding environmental constraints and hindering agent training. In this work,
we draw on the insight that the ultimate completion of a task emerges from the
cumulative progress an agent makes across individual steps. We propose Stepwise
Progress Attribution (SPA), a general reward redistribution framework that
decomposes the final reward into stepwise contributions, each reflecting its
incremental progress toward overall task completion. To achieve this, we train
a progress estimator that accumulates stepwise contributions over a trajectory
to match the task completion. During policy optimization, we combine the
estimated per-step contribution with a grounding signal for actions executed in
the environment as the fine-grained, intermediate reward for effective agent
training. Extensive experiments on common agent benchmarks (including Webshop,
ALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the
state-of-the-art method in both success rate (+2.5\% on average) and grounding
accuracy (+1.9\% on average). Further analyses demonstrate that our method
remarkably provides more effective intermediate rewards for RL training. Our
code is available at https://github.com/WangHanLinHenry/SPA-RL-Agent.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [17] [Casper DPM: Cascaded Perceptual Dynamic Projection Mapping onto Hands](https://arxiv.org/abs/2409.04397)
*Yotam Erel,Or Kozlovsky-Mordenfeld,Daisuke Iwai,Kosuke Sato,Amit H. Bermano*

Main category: cs.GR

TL;DR: 本文提出了一种技术，可以动态地将3D内容投影到人手上，并且感知运动到光子的延迟较短。通过结合慢速的3D手部姿态粗略估计和高速2D校正步骤，改善了投影与手的对齐情况，增加了投影面积并减少了感知延迟。该方法支持任意纹理或效果的应用，并通过用户研究证明了其在减少延迟伪影、提高任务速度和易用性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 准确快速计算人手的姿态和形状是一项具有挑战性的任务，因为手是关节连接且可变形的。现有的直接从3D姿态估计渲染帧并投影的方法存在明显的延迟问题，影响用户体验和任务执行效率。因此，需要一种改进的方法来解决这些问题。

Method: 该方法首先进行慢速的3D手部姿态粗略估计，然后通过高速2D校正步骤进一步优化投影的对齐情况。这些校正步骤不仅提高了投影与手部的对齐精度，还增加了投影面积并减少了感知延迟。由于该方法基于完整的3D手部重建，因此可以应用任意纹理或合理性能的效果。

Result: 通过两项用户研究，结果表明使用该方法时，用户对延迟伪影的敏感度降低，并且能够更快、更轻松地完成相关任务，相较于直接从3D姿态估计渲染帧并投影的简单方法有显著改进。此外，文章展示了几个新颖的使用案例和应用场景。

Conclusion: 本文提出的技术成功解决了手部姿态估计中延迟和对齐问题，提升了用户体验和任务执行效率。通过结合3D粗略估计和2D校正步骤，实现了更精确的投影对齐和更大的投影面积。该技术为未来的手部交互和投影应用提供了新的可能性。

Abstract: We present a technique for dynamically projecting 3D content onto human hands
with short perceived motion-to-photon latency. Computing the pose and shape of
human hands accurately and quickly is a challenging task due to their
articulated and deformable nature. We combine a slower 3D coarse estimation of
the hand pose with high speed 2D correction steps which improve the alignment
of the projection to the hands, increase the projected surface area, and reduce
perceived latency. Since our approach leverages a full 3D reconstruction of the
hands, any arbitrary texture or reasonably performant effect can be applied,
which was not possible before. We conducted two user studies to assess the
benefits of using our method. The results show subjects are less sensitive to
latency artifacts and perform faster and with more ease a given associated task
over the naive approach of directly projecting rendered frames from the 3D pose
estimation. We demonstrate several novel use cases and applications.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [18] [Accelerating ab initio melting property calculations with machine learning: Application to the high entropy alloy TaVCrW](https://arxiv.org/abs/2408.08654)
*Li-Fang Zhu,Fritz Koermann,Qing Chen,Malin Selleby,Joerg Neugebauer,and Blazej Grabowski*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出了一种高效基于DFT的方法，结合特别设计的机器学习势能，大幅降低计算熔化性质的成本，并将其应用于高熵合金TaVCrW的熔化特性研究。


<details>
  <summary>Details</summary>
Motivation: 实验测量高熔点材料的熔化性质极具挑战性，传统基于密度泛函理论（DFT）的自由能方法虽然准确但计算成本高昂，难以实现高通量计算。因此需要一种更高效的理论预测方法。

Method: 通过结合特定设计的机器学习势能与DFT方法，利用机器学习势能重现初态相空间，从而用更高效的自由能微扰计算替代昂贵的热力学积分计算。此方法可节省80%的计算资源。

Result: 该方法成功应用于高熵合金TaVCrW，计算得到其熔点、熔化熵、熔化焓、熔点处体积变化以及固液两相的热容，结果与calphad外推值合理一致。

Conclusion: 所提出的方法显著降低了计算熔化性质的资源消耗，为高通量计算和新型高温材料设计提供了有力工具。

Abstract: Melting properties are critical for designing novel materials, especially for
discovering high-performance, high-melting refractory materials. Experimental
measurements of these properties are extremely challenging due to their high
melting temperatures. Complementary theoretical predictions are, therefore,
indispensable. The conventional free energy approach using density functional
theory (DFT) has been a gold standard for such purposes because of its high
accuracy. However,it generally involves expensive thermodynamic integration
using ab initio molecular dynamic simulations. The high computational cost
makes high-throughput calculations infeasible. Here, we propose a highly
efficient DFT-based method aided by a specially designed machine learning
potential. As the machine learning potential can closely reproduce the ab
initio phase space, even for multi-component alloys, the costly thermodynamic
integration can be fully substituted with more efficient free energy
perturbation calculations. The method achieves overall savings of computational
resources by 80% compared to current alternatives. We apply the method to the
high-entropy alloy TaVCrW and calculate its melting properties, including
melting temperature, entropy and enthalpy of fusion, and volume change at the
melting point. Additionally, the heat capacities of solid and liquid TaVCrW are
calculated. The results agree reasonably with the calphad extrapolated values.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [19] [Prices and preferences in the electric vehicle market](https://arxiv.org/abs/2403.00458)
*Chung Yi See,Vasco Rato Santos,Lucas Woodley,Megan Yeo,Daniel Palmer,Shuheng Zhang,and Ashley Nunes*

Main category: econ.EM

TL;DR: 尽管电动汽车比汽油车污染少，但较高的采购价格阻碍了其普及。现有讨论主要强调电池成本是造成价格差异的主要原因，并认为电池成本下降是广泛采用的关键条件。本文通过分析2011年至2023年电动汽车属性和市场条件数据，发现电动汽车价格主要受标准配置的便利设施、附加功能和经销商安装的配件影响，而与电池容量的关系较弱。此外，电动汽车续航里程与价格呈负相关，表明续航焦虑可能没有想象中那么重要。然而，电池容量与价格正相关，因为更多的容量通常意味着更高的马力。总体而言，高采购价格反映了消费者对功能丰富且动力强劲的车辆的偏好。这种偏好导致了燃油经济性较低的车辆出现，从而减少了生命周期排放效益至少3.26%，具体取决于电池化学成分和电网的碳强度。这些发现对全球脱碳努力具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 目前关于电动汽车高价问题的讨论主要集中于电池成本，但这种观点是否全面值得质疑。因此，本文旨在深入研究影响电动汽车价格的其他因素，以重新评估高价背后的原因及消费者偏好对环境效益的影响。

Method: 收集并分析2011年至2023年间有关电动汽车属性和市场条件的数据，包括标准配置、附加功能、电池容量、续航里程等因素。通过统计分析方法，探讨这些因素与电动汽车价格之间的关系。

Result: 研究结果表明：(1) 电动汽车价格主要受标准配置的便利设施、附加功能和经销商安装的配件影响；(2) 续航里程与价格呈负相关；(3) 电池容量与价格正相关，因其通常带来更高的马力；(4) 消费者偏好导致燃油经济性降低，减少了至少3.26%的生命周期排放效益。

Conclusion: 电动汽车高价并非单纯由电池成本决定，而是更多地反映了消费者对功能丰富且动力强劲车辆的偏好。这种偏好在一定程度上削弱了电动汽车的环境效益，因此在推动电气化作为脱碳路径时，需要更加关注消费者选择对整体减排目标的影响。

Abstract: Although electric vehicles are less polluting than gasoline powered vehicles,
adoption is challenged by higher procurement prices. Existing discourse
emphasizes EV battery costs as being principally responsible for this price
differential and widespread adoption is routinely conditioned upon battery
costs declining. We scrutinize such reasoning by sourcing data on EV attributes
and market conditions between 2011 and 2023. Our findings are fourfold. First,
EV prices are influenced principally by the number of amenities, additional
features, and dealer-installed accessories sold as standard on an EV, and to a
lesser extent, by EV horsepower. Second, EV range is negatively correlated with
EV price implying that range anxiety concerns may be less consequential than
existing discourse suggests. Third, battery capacity is positively correlated
with EV price, due to more capacity being synonymous with the delivery of more
horsepower. Collectively, this suggests that higher procurement prices for EVs
reflects consumer preference for vehicles that are feature dense and more
powerful. Fourth and finally, accommodating these preferences have produced
vehicles with lower fuel economy, a shift that reduces envisioned lifecycle
emissions benefits by at least 3.26 percent, subject to the battery pack
chemistry leveraged and the carbon intensity of the electrical grid. These
findings warrant attention as decarbonization efforts increasingly emphasize
electrification as a pathway for complying with domestic and international
climate agreements.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [20] [Model-Based AI planning and Execution Systems for Robotics](https://arxiv.org/abs/2505.04493)
*Or Wertheim,Ronen I. Brafman*

Main category: cs.RO

TL;DR: 本文综述了基于模型的机器人任务级控制系统的多样设计选择、现有系统解决的问题及提出的解决方案，并对未来发展方向提出建议。


<details>
  <summary>Details</summary>
Motivation: 尽管现代机器人学中早已提出基于模型的规划与执行系统这一理念，但直到ROSPlan系统的出现，才真正将通用推理架构与现代机器人平台相结合。因此，需要探讨当前基于模型的机器人任务级控制系统的设计选择和问题解决方案，以指导未来的发展方向。

Method: 作者对现有的基于模型的机器人任务级控制系统进行了全面分析，比较了不同系统的设计选择和解决问题的方法，总结了它们的优点和不足之处。

Result: 通过对比分析，文章明确了现有系统在灵活性、通用性等方面的表现，并指出了目前尚未解决的关键问题。

Conclusion: 为了推动基于模型的机器人任务级控制系统的发展，未来研究应关注提高系统的适应性、可扩展性和与实际应用的结合程度，同时探索更高效的规划算法和执行机制。

Abstract: Model-based planning and execution systems offer a principled approach to
building flexible autonomous robots that can perform diverse tasks by
automatically combining a host of basic skills. This idea is almost as old as
modern robotics. Yet, while diverse general-purpose reasoning architectures
have been proposed since, general-purpose systems that are integrated with
modern robotic platforms have emerged only recently, starting with the
influential ROSPlan system. Since then, a growing number of model-based systems
for robot task-level control have emerged. In this paper, we consider the
diverse design choices and issues existing systems attempt to address, the
different solutions proposed so far, and suggest avenues for future
development.

</details>


### [21] [Hardware Design and Learning-Based Software Architecture of Musculoskeletal Wheeled Robot Musashi-W for Real-World Applications](https://arxiv.org/abs/2403.11729)
*Kento Kawaharazuka,Akihiro Miki,Masahiro Bando,Temma Suzuki,Yoshimoto Ribayashi,Yasunori Toshimitsu,Yuya Nagamatsu,Kei Okada,and Masayuki Inaba*

Main category: cs.RO

TL;DR: 通过结合轮式底盘和肌肉骨骼上肢，开发了名为Musashi-W的机器人，并通过静态和动态身体图式学习、反射控制和视觉识别构建了其软件系统。该研究展示了Musashi-W在清洁任务、搬运重物以及通过动态布料操作设置桌子等任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管已开发出多种肌肉骨骼类人机器人，但它们尚未能应用于现实世界任务中，其中一个主要原因是柔性身体的双足行走困难。因此，研究者试图通过设计一种新的机器人结构来克服这一限制。

Method: 研究者开发了一种名为Musashi-W的肌肉骨骼轮式机器人，它结合了轮式基座和肌肉骨骼上肢以适应现实世界应用。此外，还构建了一个包含静态和动态身体图式学习、反射控制和视觉识别的软件系统。

Result: 实验结果表明，Musashi-W的硬件和软件能够充分发挥肌肉骨骼上肢的优势，成功完成了由人类教导的清洁任务、考虑肌肉增加的重物搬运以及通过可变刚度的动态布料操作设置桌子的任务。

Conclusion: Musashi-W的成功开发及其在多种任务中的表现证明了肌肉骨骼轮式机器人的潜力，为未来将其应用于更广泛的现实世界场景奠定了基础。

Abstract: Various musculoskeletal humanoids have been developed so far. While these
humanoids have the advantage of their flexible and redundant bodies that mimic
the human body, they are still far from being applied to real-world tasks. One
of the reasons for this is the difficulty of bipedal walking in a flexible
body. Thus, we developed a musculoskeletal wheeled robot, Musashi-W, by
combining a wheeled base and musculoskeletal upper limbs for real-world
applications. Also, we constructed its software system by combining static and
dynamic body schema learning, reflex control, and visual recognition. We show
that the hardware and software of Musashi-W can make the most of the advantages
of the musculoskeletal upper limbs, through several tasks of cleaning by human
teaching, carrying a heavy object considering muscle addition, and setting a
table through dynamic cloth manipulation with variable stiffness.

</details>
