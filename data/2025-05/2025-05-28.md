<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 4]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.GR](#cs.GR) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: 这篇文章是一篇立场论文，全面审视了人类与AI智能体协作的最新实证进展。文章指出当前缺乏一个能够整合这些不同研究的统一理论框架，尤其是在处理开放性、复杂任务时。为此，提出了一种新的概念架构（分层探索-利用网络），系统地连接多智能体协调、知识管理、控制论反馈回路和高层控制机制等技术细节。通过将现有的研究成果映射到这一框架上，包括符号AI技术、基于连接主义的大语言模型智能体及混合组织实践，该方法有助于修正传统方法并启发融合定性和定量范式的新型研究。文章结构灵活，既可作为对现有技术实现的批判性回顾，也可作为设计或扩展人机共生系统的前瞻性参考。


<details>
  <summary>Details</summary>
Motivation: 当前关于人类与AI智能体协作的研究虽然在技术上取得了许多成就，但仍然存在一些未解决的问题和持续的差距。特别是在面对开放性、复杂的任务时，缺乏一个能够整合各种不同研究的统一理论框架。这促使作者提出一种新的概念架构来填补这一空白。

Method: 作者提出了一个名为分层探索-利用网络（Hierarchical Exploration-Exploitation Net）的新概念架构。此架构系统地连接了多智能体协调、知识管理、控制论反馈回路以及高层控制机制等关键领域。此外，还通过将现有贡献，如符号AI技术、基于连接主义的大语言模型智能体及混合组织实践，映射到这一新框架上来验证其适用性和有效性。

Result: 通过使用所提出的概念架构，作者成功地将现有的多种技术和方法进行了整合，并揭示了传统方法中的不足之处，同时为未来的研究提供了新的方向和灵感。这种方法不仅促进了定性和定量范式的融合，还推动了人类认知与AI能力的共同进化。

Conclusion: 本文提供了一个全新的视角和理论框架，用于理解和改进人类与AI智能体之间的协作。它强调了构建统一理论框架的重要性，并展示了如何通过该框架来促进技术进步和跨学科合作，从而实现更深层次的人类认知与AI能力的共同进化。

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [2] [Artificial Expert Intelligence through PAC-reasoning](https://arxiv.org/abs/2412.02441)
*Shai Shalev-Shwartz,Amnon Shashua,Gal Beniamini,Yoav Levine,Or Sharir,Noam Wies,Ido Ben-Shaul,Tomer Nussbaum,Shir Granot Peled*

Main category: cs.AI

TL;DR: 本文提出了一种新型的人工专家智能（AEI），旨在通过结合领域特定的专业知识和顶级人类专家的精确推理能力，超越传统通用人工智能（AGI）和狭义AI的局限性。为解决现有AI系统在适应性和新问题求解中的不足，AEI引入了“可能近似正确推理”(PAC Reasoning)框架，该框架提供了可靠的理论保证，用于分解复杂问题，并具有控制推理精度的实际机制。受人类思维中科学方法严谨性的启发，AEI提出了第三类推理系统（System 3），专注于精确推理并建立了有误差界别的推理时学习基础。


<details>
  <summary>Details</summary>
Motivation: 当前的人工智能系统虽然在预定义任务上表现出色，但在适应性和解决新颖问题时的精确性方面存在不足。这促使研究者探索一种能够融合领域专业知识和精确推理能力的新范式，以突破AGI和狭义AI的限制。

Method: 文章引入了“可能近似正确推理”(PAC Reasoning)框架，提供了解决复杂问题的理论保障，并提出了一个控制推理精度的实际机制。此外，基于人类思维的双系统理论，AEI提出了第三类推理系统（System 3），该系统强调科学方法的严谨性，从而实现精确推理。

Result: AEI成功地构建了一个理论框架，可以对复杂问题进行可靠分解，并允许在推理过程中控制精度。这一框架为实现具有误差界的推理时学习奠定了基础。

Conclusion: 人工专家智能（AEI）通过引入PAC Reasoning框架和System 3推理模式，为AI系统在精确推理和适应性方面的提升提供了新的方向。AEI不仅为未来AI的发展铺平了道路，还为实现更接近人类专家水平的智能奠定了基础。

Abstract: Artificial Expert Intelligence (AEI) seeks to transcend the limitations of
both Artificial General Intelligence (AGI) and narrow AI by integrating
domain-specific expertise with critical, precise reasoning capabilities akin to
those of top human experts. Existing AI systems often excel at predefined tasks
but struggle with adaptability and precision in novel problem-solving. To
overcome this, AEI introduces a framework for ``Probably Approximately Correct
(PAC) Reasoning". This paradigm provides robust theoretical guarantees for
reliably decomposing complex problems, with a practical mechanism for
controlling reasoning precision. In reference to the division of human thought
into System 1 for intuitive thinking and System 2 for reflective
reasoning~\citep{tversky1974judgment}, we refer to this new type of reasoning
as System 3 for precise reasoning, inspired by the rigor of the scientific
method. AEI thus establishes a foundation for error-bounded, inference-time
learning.

</details>


### [3] [LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation](https://arxiv.org/abs/2505.20671)
*Heng Tan,Hua Yan,Yu Yang*

Main category: cs.AI

TL;DR: 本文提出了一种由大语言模型（LLM）引导的策略调节框架，该框架通过利用LLM的能力来改进强化学习（RL）训练，而无需额外的模型训练或人为干预。在标准RL基准测试中的实验表明，该方法优于现有最先进基线。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在许多领域取得了显著成功，但为复杂任务训练有效的策略仍然面临挑战。现有的解决训练瓶颈的方法要么依赖昂贵且不确定的自动化策略优化，要么需要难以扩展至高维连续动作空间的人工反馈。因此，探索一种无需额外模型训练和人工干预的新方法成为研究动机。

Method: 作者设计了一个基于大语言模型（LLM）的策略调节框架。首先，LLM被提示从次优代理生成的轨迹中识别关键状态；然后，根据这些关键状态，LLM提供动作建议并分配隐式奖励，以指导策略的优化过程。这种方法避免了额外的模型训练成本以及对人类反馈的依赖。

Result: 在多个标准RL基准上的实验结果表明，该方法的表现优于当前最先进的基线方法，证明了基于LLM的解释在缓解RL训练瓶颈方面的有效性。

Conclusion: 本研究表明，通过结合大语言模型的能力，可以有效改善强化学习的训练过程，特别是在不增加额外训练负担和无需人工干预的情况下。这为未来强化学习与大型预训练语言模型的结合提供了新的方向和可能性。

Abstract: While reinforcement learning (RL) has achieved notable success in various
domains, training effective policies for complex tasks remains challenging.
Agents often converge to local optima and fail to maximize long-term rewards.
Existing approaches to mitigate training bottlenecks typically fall into two
categories: (i) Automated policy refinement, which identifies critical states
from past trajectories to guide policy updates, but suffers from costly and
uncertain model training; and (ii) Human-in-the-loop refinement, where human
feedback is used to correct agent behavior, but this does not scale well to
environments with large or continuous action spaces. In this work, we design a
large language model-guided policy modulation framework that leverages LLMs to
improve RL training without additional model training or human intervention. We
first prompt an LLM to identify critical states from a sub-optimal agent's
trajectories. Based on these states, the LLM then provides action suggestions
and assigns implicit rewards to guide policy refinement. Experiments across
standard RL benchmarks demonstrate that our method outperforms state-of-the-art
baselines, highlighting the effectiveness of LLM-based explanations in
addressing RL training bottlenecks.

</details>


### [4] [Temporal Sampling for Forgotten Reasoning in LLMs](https://arxiv.org/abs/2505.20196)
*Yuetai Li,Zhangchen Xu,Fengqing Jiang,Bhaskar Ramasubramanian,Luyao Niu,Bill Yuchen Lin,Xiang Yue,Radha Poovendran*

Main category: cs.AI

TL;DR: 通过引入时间采样方法，解决大语言模型训练中的时间遗忘问题，提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管微调大语言模型旨在提高其推理能力，但研究发现这些模型在训练过程中会忘记如何解决之前正确回答的问题，这种现象被称为时间遗忘，并且广泛存在于不同规模的模型、微调方法和多个推理基准中。

Method: 提出了一种名为时间采样的简单解码策略，该策略从训练轨迹的多个检查点中抽取输出，无需重新训练或集成即可恢复被遗忘的解决方案。此外，还将该方法扩展到LoRA适应模型，证明仅存储适配器权重即可在检查点之间实现类似的收益，同时降低存储成本。

Result: 时间采样方法在多个基准测试中显著提高了推理性能，Pass@k得分提升了4到19分，并且在Majority@k上也有一致的提升。

Conclusion: 时间采样提供了一种实用且计算高效的利用训练中的时间多样性的方式，揭示了隐藏的推理能力，并促使我们重新思考如何评估大语言模型。

Abstract: Fine-tuning large language models (LLMs) is intended to improve their
reasoning capabilities, yet we uncover a counterintuitive effect: models often
forget how to solve problems they previously answered correctly during
training. We term this phenomenon temporal forgetting and show that it is
widespread across model sizes, fine-tuning methods (both Reinforcement Learning
and Supervised Fine-Tuning), and multiple reasoning benchmarks. To address this
gap, we introduce Temporal Sampling, a simple decoding strategy that draws
outputs from multiple checkpoints along the training trajectory. This approach
recovers forgotten solutions without retraining or ensembling, and leads to
substantial improvements in reasoning performance, gains from 4 to 19 points in
Pass@k and consistent gains in Majority@k across several benchmarks. We further
extend our method to LoRA-adapted models, demonstrating that storing only
adapter weights across checkpoints achieves similar benefits with minimal
storage cost. By leveraging the temporal diversity inherent in training,
Temporal Sampling offers a practical, compute-efficient way to surface hidden
reasoning ability and rethink how we evaluate LLMs.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [5] [MvDeCor: Multi-view Dense Correspondence Learning for Fine-grained 3D Segmentation](https://arxiv.org/abs/2208.08580)
*Gopal Sharma,Kangxue Yin,Subhransu Maji,Evangelos Kalogerakis,Or Litany,Sanja Fidler*

Main category: cs.CV

TL;DR: 提出了一种结合2D和3D信息的自监督学习方法，用于细粒度3D形状分割任务。通过多视角渲染和对比学习框架，该方法在有限标注数据的情况下表现出更好的泛化能力，并在纹理和无纹理3D数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 受基于视图的表面表示在建模高分辨率表面细节和纹理方面比点云或体素占用等3D表示更有效的启发，研究者希望利用2D领域的自监督技术来改进细粒度3D形状分割任务。

Method: 给定一个3D形状，先从多个视角进行渲染，然后在对比学习框架内设置密集对应学习任务。通过这种方法，学习到的2D表示具有视角不变性和几何一致性，从而在训练样本有限时表现出更好的泛化性能。

Result: 实验表明，在有纹理（RenderPeople）和无纹理（PartNet）的3D数据集上，该方法在细粒度部件分割任务中优于现有最佳方法。特别是在只有稀疏视角可用于训练或形状具有纹理时，改进效果更为显著。

Conclusion: 本文提出的方法MvDeCor结合了2D处理和3D几何推理的优势，在细粒度3D形状分割任务中取得了优异的性能。尤其是在训练数据有限或形状具有纹理的情况下，其表现优于仅使用2D或3D自监督的替代方案。

Abstract: We propose to utilize self-supervised techniques in the 2D domain for
fine-grained 3D shape segmentation tasks. This is inspired by the observation
that view-based surface representations are more effective at modeling
high-resolution surface details and texture than their 3D counterparts based on
point clouds or voxel occupancy. Specifically, given a 3D shape, we render it
from multiple views, and set up a dense correspondence learning task within the
contrastive learning framework. As a result, the learned 2D representations are
view-invariant and geometrically consistent, leading to better generalization
when trained on a limited number of labeled shapes compared to alternatives
that utilize self-supervision in 2D or 3D alone. Experiments on textured
(RenderPeople) and untextured (PartNet) 3D datasets show that our method
outperforms state-of-the-art alternatives in fine-grained part segmentation.
The improvements over baselines are greater when only a sparse set of views is
available for training or when shapes are textured, indicating that MvDeCor
benefits from both 2D processing and 3D geometric reasoning.

</details>


### [6] [Mix3D: Out-of-Context Data Augmentation for 3D Scenes](https://arxiv.org/abs/2110.02210)
*Alexey Nekrasov,Jonas Schult,Or Litany,Bastian Leibe,Francis Engelmann*

Main category: cs.CV

TL;DR: Mix3D是一种用于大规模3D场景分割的数据增强技术，通过将两个增强场景组合创建新的训练样本，平衡全局场景上下文和局部几何结构的重要性，从而减少模型对场景上下文的过度依赖，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景分割的工作主要关注具有大容量和感受野的模型，以捕捉输入3D场景的全局上下文。然而，强上下文先验可能导致错误分类（如将过马路的行人误认为汽车）。因此需要一种方法来平衡全局场景上下文和局部几何结构，以提高模型的泛化能力。

Method: 提出了一种“mixing”技术，通过结合两个增强场景创建新的训练样本。这种技术将对象实例隐式地放置到新的、超出上下文的环境中，迫使模型不仅依赖于场景上下文，还要从局部结构中推断语义。

Result: 实验表明，在室内（ScanNet, S3DIS）和室外数据集（SemanticKITTI）上使用Mix3D训练的模型显著提高了性能。例如，使用Mix3D训练的MinkowskiNet在ScanNet测试基准上超越了所有先前的最先进方法，达到78.1 mIoU。

Conclusion: Mix3D可以轻松与任何现有方法结合使用，并显著提高3D场景分割任务的性能，特别是在室内和室外数据集上。代码已公开。

Abstract: We present Mix3D, a data augmentation technique for segmenting large-scale 3D
scenes. Since scene context helps reasoning about object semantics, current
works focus on models with large capacity and receptive fields that can fully
capture the global context of an input 3D scene. However, strong contextual
priors can have detrimental implications like mistaking a pedestrian crossing
the street for a car. In this work, we focus on the importance of balancing
global scene context and local geometry, with the goal of generalizing beyond
the contextual priors in the training set. In particular, we propose a "mixing"
technique which creates new training samples by combining two augmented scenes.
By doing so, object instances are implicitly placed into novel out-of-context
environments and therefore making it harder for models to rely on scene context
alone, and instead infer semantics from local structure as well. We perform
detailed analysis to understand the importance of global context, local
structures and the effect of mixing scenes. In experiments, we show that models
trained with Mix3D profit from a significant performance boost on indoor
(ScanNet, S3DIS) and outdoor datasets (SemanticKITTI). Mix3D can be trivially
used with any existing method, e.g., trained with Mix3D, MinkowskiNet
outperforms all prior state-of-the-art methods by a significant margin on the
ScanNet test benchmark 78.1 mIoU. Code is available at:
https://nekrasov.dev/mix3d/

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Reinforcing General Reasoning without Verifiers](https://arxiv.org/abs/2505.21493)
*Xiangxin Zhou,Zichen Liu,Anya Sims,Haonan Wang,Tianyu Pang,Chongxuan Li,Liang Wang,Min Lin,Chao Du*

Main category: cs.LG

TL;DR: 本文提出了一种无需验证器的方法（VeriFree），通过强化学习直接最大化生成参考答案的概率，解决了当前基于验证器的大规模语言模型训练方法在实际应用中的局限性。该方法不仅减少了计算需求，还在多个基准测试中表现优于或等于基于验证器的方法。


<details>
  <summary>Details</summary>
Motivation: 当前使用DeepSeek-R1-Zero风格的强化学习训练大规模语言模型的方法受限于规则基础的答案验证任务，无法自然扩展到如化学、医疗、工程等实际领域。此外，使用额外的语言模型作为验证器虽然是一种解决办法，但存在对强验证器的依赖、奖励劫持的风险以及训练时内存负担等问题。因此，需要一种新的方法来克服这些限制并扩展到一般推理领域。

Method: 研究者提出了一种名为VeriFree的无验证器方法，这种方法跳过了答案验证步骤，而是通过强化学习直接优化生成参考答案的概率。这种方法将策略和隐式验证器整合到一个统一的模型中进行训练，并可视为一种变分优化方法。

Result: 实验结果表明，VeriFree方法在MMLU-Pro、GPQA、SuperGPQA及数学相关基准测试中，其表现与基于验证器的方法相当甚至更优。同时，该方法显著降低了计算需求，并提供了多种视角的深入见解。

Conclusion: VeriFree方法成功地避免了对验证器的依赖，为大规模语言模型在一般推理领域的应用提供了一种实用且高效的解决方案。此方法不仅减少了计算资源的需求，还展示了在多项评估基准上的卓越性能。代码已开源供社区进一步研究。

Abstract: The recent paradigm shift towards training large language models (LLMs) using
DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has
led to impressive advancements in code and mathematical reasoning. However,
this methodology is limited to tasks where rule-based answer verification is
possible and does not naturally extend to real-world domains such as chemistry,
healthcare, engineering, law, biology, business, and economics. Current
practical workarounds use an additional LLM as a model-based verifier; however,
this introduces issues such as reliance on a strong verifier LLM,
susceptibility to reward hacking, and the practical burden of maintaining the
verifier model in memory during training. To address this and extend
DeepSeek-R1-Zero-style training to general reasoning domains, we propose a
verifier-free method (VeriFree) that bypasses answer verification and instead
uses RL to directly maximize the probability of generating the reference
answer. We compare VeriFree with verifier-based methods and demonstrate that,
in addition to its significant practical benefits and reduced compute
requirements, VeriFree matches and even surpasses verifier-based methods on
extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related
benchmarks. Moreover, we provide insights into this method from multiple
perspectives: as an elegant integration of training both the policy and
implicit verifier in a unified model, and as a variational optimization
approach. Code is available at https://github.com/sail-sg/VeriFree.

</details>


### [8] [Can Large Reasoning Models Self-Train?](https://arxiv.org/abs/2505.21444)
*Sheikh Shafayat,Fahim Tajwar,Ruslan Salakhutdinov,Jeff Schneider,Andrea Zanette*

Main category: cs.LG

TL;DR: 本文提出了一种在线自训练强化学习算法，利用模型自身的自我一致性来推断正确性信号并进行无监督训练。该算法在数学推理任务中表现优异，接近使用标准答案训练的强化学习方法的效果。然而，研究也揭示了自动生成的代理奖励可能引发奖励欺骗的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）性能的提升，减少对人类监督的依赖变得越来越重要。尽管基于自动化验证的强化学习提供了一种替代方案，但其可扩展性受限于依赖人类设计的验证器。因此，自训练（self-training），即利用模型自身的判断作为监督信号，成为了一个有吸引力的研究方向。

Method: 研究者提出了一种在线自训练强化学习算法，该算法通过模型的自我一致性来推断正确性信号，并且无需任何真实标签的监督即可进行训练。此算法被应用于复杂的数学推理任务。

Result: 实验结果表明，该算法能够迅速达到与使用黄金标准答案明确训练的强化学习方法相媲美的性能水平。同时，分析显示自生成的代理奖励虽然最初与正确性相关，但可能会激励奖励欺骗行为，即偏好自信但错误的输出。

Conclusion: 研究表明，自监督改进可以在没有外部标签的情况下实现显著的性能提升，但也揭示了其根本挑战，特别是关于奖励欺骗的问题。

Abstract: Scaling the performance of large language models (LLMs) increasingly depends
on methods that reduce reliance on human supervision. Reinforcement learning
from automated verification offers an alternative, but it incurs scalability
limitations due to dependency upon human-designed verifiers. Self-training,
where the model's own judgment provides the supervisory signal, presents a
compelling direction. We propose an online self-training reinforcement learning
algorithm that leverages the model's self-consistency to infer correctness
signals and train without any ground-truth supervision. We apply the algorithm
to challenging mathematical reasoning tasks and show that it quickly reaches
performance levels rivaling reinforcement-learning methods trained explicitly
on gold-standard answers. Additionally, we analyze inherent limitations of the
algorithm, highlighting how the self-generated proxy reward initially
correlated with correctness can incentivize reward hacking, where confidently
incorrect outputs are favored. Our results illustrate how self-supervised
improvement can achieve significant performance gains without external labels,
while also revealing its fundamental challenges.

</details>


### [9] [Accelerating RL for LLM Reasoning with Optimal Advantage Regression](https://arxiv.org/abs/2505.20686)
*Kianté Brantley,Mingyu Chen,Zhaolin Gao,Jason D. Lee,Wen Sun,Wenhao Zhan,Xuezhou Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的两阶段策略优化框架A*-PO，通过离线采样和简单的最小二乘回归损失，有效降低了训练大语言模型的计算开销和内存消耗，同时在数学推理基准上取得了与现有方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的强化学习策略优化方法虽然效果显著，但在训练大型语言模型时存在较高的计算开销和内存消耗问题，尤其是在需要多次生成和依赖批评网络或优势估计的情况下。这促使研究者探索一种更高效的策略优化方法。

Method: A*-PO采用两阶段策略优化框架：第一阶段利用参考策略的离线采样来估计最优值函数V*，从而避免了昂贵的在线价值估计；第二阶段则通过每个提示仅进行一次生成的简单最小二乘回归损失来进行基于策略的更新。此外，理论上证明了KL正则化的RL目标可以通过该方法优化而无需复杂的探索策略。

Result: A*-PO在广泛的数学推理基准测试中表现出与PPO、GRPO和REBEL等方法相当的性能，同时将训练时间缩短了高达2倍，并将峰值内存使用量减少了超过30%。

Conclusion: A*-PO提供了一种高效且资源节约的策略优化方案，适用于训练大语言模型以提高其推理能力。此方法不仅保持了竞争力，还显著降低了计算和内存成本。

Abstract: Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning
large language models (LLMs) to improve complex reasoning abilities. However,
state-of-the-art policy optimization methods often suffer from high
computational overhead and memory consumption, primarily due to the need for
multiple generations per prompt and the reliance on critic networks or
advantage estimates of the current policy. In this paper, we propose $A$*-PO, a
novel two-stage policy optimization framework that directly approximates the
optimal advantage function and enables efficient training of LLMs for reasoning
tasks. In the first stage, we leverage offline sampling from a reference policy
to estimate the optimal value function $V$*, eliminating the need for costly
online value estimation. In the second stage, we perform on-policy updates
using a simple least-squares regression loss with only a single generation per
prompt. Theoretically, we establish performance guarantees and prove that the
KL-regularized RL objective can be optimized without requiring complex
exploration strategies. Empirically, $A$*-PO achieves competitive performance
across a wide range of mathematical reasoning benchmarks, while reducing
training time by up to 2$\times$ and peak memory usage by over 30% compared to
PPO, GRPO, and REBEL. Implementation of $A$*-PO can be found at
https://github.com/ZhaolinGao/A-PO.

</details>


### [10] [Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning](https://arxiv.org/abs/2505.20561)
*Shenao Zhang,Yaqing Wang,Yinxiao Liu,Tianqi Liu,Peter Grabowski,Eugene Ie,Zhaoran Wang,Yunxuan Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于贝叶斯适应性强化学习（BARL）的算法，用于指导大型语言模型（LLMs）进行反思性探索。通过在后验分布上优化预期回报，BARL不仅激励奖励最大化利用，还通过信念更新促进信息收集型探索。实验结果表明，BARL在测试时优于标准马尔可夫强化学习方法，具有更高的标记效率和改进的探索效果。


<details>
  <summary>Details</summary>
Motivation: 尽管通过强化学习训练的大型语言模型展现出强大的推理能力和反思行为，但传统马尔可夫强化学习限制了探索到训练阶段，并仅通过当前状态依赖历史上下文。因此，反思性推理是否会在马尔可夫强化学习训练中出现以及其在测试时间为何有益仍不清楚。为了解决这一问题，文章重新定义了反思性探索，将其置于贝叶斯适应性强化学习框架内。

Method: 作者将反思性探索纳入贝叶斯适应性强化学习框架，该框架明确地在马尔可夫决策过程的后验分布上优化预期回报。这种贝叶斯公式本质上激励了奖励最大化利用和通过信念更新进行的信息收集型探索。由此产生的算法BARL指导LLM根据观察结果拼接和切换策略，提供关于模型何时以及如何进行反思性探索的原则性指导。

Result: 在合成和数学推理任务上的实证结果表明，BARL在测试时的表现优于标准马尔可夫强化学习方法，实现了更高的标记效率和改进的探索效果。

Conclusion: 综上所述，BARL通过结合奖励最大化和信息收集来提升LLMs的反思性探索能力，在各种任务中展现出了优越性能。这表明采用贝叶斯适应性强化学习框架可以有效增强LLMs的推理和探索能力。

Abstract: Large Language Models (LLMs) trained via Reinforcement Learning (RL) have
exhibited strong reasoning capabilities and emergent reflective behaviors, such
as backtracking and error correction. However, conventional Markovian RL
confines exploration to the training phase to learn an optimal deterministic
policy and depends on the history contexts only through the current state.
Therefore, it remains unclear whether reflective reasoning will emerge during
Markovian RL training, or why they are beneficial at test time. To remedy this,
we recast reflective exploration within the Bayes-Adaptive RL framework, which
explicitly optimizes the expected return under a posterior distribution over
Markov decision processes. This Bayesian formulation inherently incentivizes
both reward-maximizing exploitation and information-gathering exploration via
belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and
switch strategies based on the observed outcomes, offering principled guidance
on when and how the model should reflectively explore. Empirical results on
both synthetic and mathematical reasoning tasks demonstrate that BARL
outperforms standard Markovian RL approaches at test time, achieving superior
token efficiency with improved exploration effectiveness. Our code is available
at https://github.com/shenao-zhang/BARL.

</details>


### [11] [SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety](https://arxiv.org/abs/2505.20065)
*Geon-Hyeong Kim,Youngsoo Jang,Yu Jin Kim,Byoungjip Kim,Honglak Lee,Kyunghoon Bae,Moontae Lee*

Main category: cs.LG

TL;DR: 提出了一种新的算法SafeDPO，用于优化大型语言模型的安全对齐目标，简化了强化学习过程，并在与人类偏好对齐和提高安全性方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在各领域的应用不断扩展，确保其安全性变得至关重要。现有的方法通过将安全约束集成到强化学习中来解决这一问题，但这些方法通常复杂且包含多步骤的程序。因此，需要一种更简单、有效的解决方案来直接优化安全对齐目标。

Method: 受Direct Preference Optimization (DPO) 的启发，研究者提出了SafeDPO算法。该算法通过单一阶段的策略学习直接优化安全对齐目标，无需松弛处理。SafeDPO仅引入一个额外的超参数以增强安全性，并对标准DPO进行小幅度修改，从而避免了拟合独立的奖励和成本模型以及在微调过程中从语言模型采样的需求。

Result: 实验表明，SafeDPO在与人类偏好对齐及提升安全性方面达到了与现有最先进的安全对齐算法相当的性能水平。

Conclusion: SafeDPO是一种新颖且高效的算法，能够在不增加复杂性的情况下显著提升大型语言模型的安全性，同时保持与人类偏好的一致性。这为未来大型语言模型的安全性和可靠性改进提供了一个有前景的方向。

Abstract: As Large Language Models (LLMs) continue to advance and find applications
across a growing number of fields, ensuring the safety of LLMs has become
increasingly critical. To address safety concerns, recent studies have proposed
integrating safety constraints into Reinforcement Learning from Human Feedback
(RLHF). However, these approaches tend to be complex, as they encompass
complicated procedures in RLHF along with additional steps required by the
safety constraints. Inspired by Direct Preference Optimization (DPO), we
introduce a new algorithm called SafeDPO, which is designed to directly
optimize the safety alignment objective in a single stage of policy learning,
without requiring relaxation. SafeDPO introduces only one additional
hyperparameter to further enhance safety and requires only minor modifications
to standard DPO. As a result, it eliminates the need to fit separate reward and
cost models or to sample from the language model during fine-tuning, while
still enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO
achieves competitive performance compared to state-of-the-art safety alignment
algorithms, both in terms of aligning with human preferences and improving
safety.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains](https://arxiv.org/abs/2402.00559)
*Alon Jacovi,Yonatan Bitton,Bernd Bohnet,Jonathan Herzig,Or Honovich,Michael Tseng,Michael Collins,Roee Aharoni,Mor Geva*

Main category: cs.CL

TL;DR: 本文介绍了REVEAL数据集，用于评估复杂推理链的自动验证方法。REVEAL提供了详细的标签，包括推理步骤的相关性、证据关联性和逻辑正确性，涵盖了多个数据集和最先进的语言模型。研究发现，验证器在验证推理链的逻辑正确性和检测矛盾方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏细粒度的步骤级数据集来全面评估复杂推理链的自动验证方法，这阻碍了该领域的发展。

Method: 构建了一个名为REVEAL的数据集，其中包含对语言模型回答中每个推理步骤的相关性、证据关联性和逻辑正确性的综合标注。该数据集适用于开放领域的问答设置，覆盖了多种数据集和先进的语言模型。

Result: 通过在REVEAL上的评估，发现自动验证器在验证推理链的逻辑正确性和检测矛盾方面表现不佳。

Conclusion: REVEAL数据集为复杂推理链的自动验证方法提供了一个基准测试平台，揭示了当前验证器的主要挑战，并为未来的研究指明了方向。

Abstract: Prompting language models to provide step-by-step answers (e.g.,
"Chain-of-Thought") is the prominent approach for complex reasoning tasks,
where more accurate reasoning chains typically improve downstream task
performance. Recent literature discusses automatic methods to verify reasoning
to evaluate and improve their correctness. However, no fine-grained step-level
datasets are available to enable thorough evaluation of such verification
methods, hindering progress in this direction. We introduce REVEAL: Reasoning
Verification Evaluation, a dataset to benchmark automatic verifiers of complex
Chain-of-Thought reasoning in open-domain question-answering settings. REVEAL
includes comprehensive labels for the relevance, attribution to evidence
passages, and logical correctness of each reasoning step in a language model's
answer, across a variety of datasets and state-of-the-art language models.
Evaluation on REVEAL shows that verifiers struggle at verifying reasoning
chains - in particular, verifying logical correctness and detecting
contradictions. Available at https://reveal-dataset.github.io/ .

</details>


### [13] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 在大规模语言模型（LLMs）中，通过重复采样扩展推理计算可以持续增加覆盖率（解决问题的比例）。然而，这种提升部分可能是因为评估基准答案分布的偏差。研究者提出了一种基于训练集答案频率的基线方法，并通过实验发现，在数学推理和事实知识两个领域，该基线方法在某些LLMs上优于重复采样，而在其他模型上与一种混合策略表现相当。此研究提供了一种更准确衡量重复采样效果的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管重复采样在大语言模型中能提高问题解决的覆盖率，但研究者推测这种提升部分是由于标准评估基准中的答案分布偏向于少量常见答案，因此需要重新评估重复采样的实际效果。

Method: 研究者定义了一种基线方法，该方法根据训练集中答案的频率枚举答案。然后在数学推理和事实知识两个领域进行实验，比较了基线方法、重复采样以及一种混合策略的表现。混合策略结合了少量模型采样和枚举猜测。

Result: 实验结果表明，对于某些LLMs，基线方法的表现优于重复采样；而对于其他模型，其覆盖率与混合策略相当，即使用10次模型采样并结合枚举猜测剩余的答案。

Conclusion: 通过引入基于训练集答案频率的基线方法，研究提供了一种更精确的方式，用于衡量重复采样在超出提示无关猜测情况下的实际改进效果。这有助于更好地理解重复采样对模型性能的影响。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [14] [Thinker: Learning to Think Fast and Slow](https://arxiv.org/abs/2505.21097)
*Stephen Chung,Wenyu Du,Jie Fu*

Main category: cs.CL

TL;DR: 最近的研究表明，通过在数学和编程等领域的问答（QA）任务中应用强化学习（RL），可以提升大语言模型（LLM）的推理能力。受心理学中的双重处理理论启发，我们提出了一个包含四个阶段的简单修改方案：快速思考、验证、慢速思考和总结。这一方法提升了Qwen2.5-1.5B和DeepSeek-R1-Qwen-1.5B模型的平均准确率，并显著提高了推理效率。这些结果表明直觉和深思熟虑的推理是不同的、互补的系统，可从针对性训练中受益。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在应用强化学习后表现出了改进的推理能力，但其搜索行为往往不精确且缺乏信心，导致冗长、重复的回答，显示出直觉和验证方面的不足。为了解决这一问题，研究者受到心理学中双重处理理论的启发，试图通过设计一种新的多阶段任务来分别训练直觉和深思熟虑的推理能力。

Method: 提出了一种基于双过程理论的四阶段问答任务：
1. 快速思考：要求模型在严格的token预算内给出初步答案；
2. 验证：模型评估其初步响应的正确性；
3. 慢速思考：模型通过更深入的思考对初步答案进行优化；
4. 总结：将优化后的答案提炼成简洁明确的步骤。

Result: 该方法显著提高了两个模型的平均准确率：Qwen2.5-1.5B从24.9%提升至27.9%，DeepSeek-R1-Qwen-1.5B从45.9%提升至49.8%。此外，Qwen2.5-1.5B在快速思考模式下的准确率达到26.8%，仅使用不到1000个token，展示了高效的推理能力。

Conclusion: 研究表明，直觉和深思熟虑的推理是不同的、互补的系统，可以从针对性训练中获益。通过设计特定的任务结构，可以有效提升大型语言模型的推理能力和效率。

Abstract: Recent studies show that the reasoning capabilities of Large Language Models
(LLMs) can be improved by applying Reinforcement Learning (RL) to
question-answering (QA) tasks in areas such as math and coding. With a long
context length, LLMs may learn to perform search, as indicated by the
self-correction behavior observed in DeepSeek R1. However, this search behavior
is often imprecise and lacks confidence, resulting in long, redundant responses
and highlighting deficiencies in intuition and verification. Inspired by the
Dual Process Theory in psychology, we introduce a simple modification to the QA
task that includes four stages: Fast Thinking, where the LLM must answer within
a strict token budget; Verification, where the model evaluates its initial
response; Slow Thinking, where it refines the initial response with more
deliberation; and Summarization, where it distills the refinement from the
previous stage into precise steps. Our proposed task improves average accuracy
from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for
DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone
achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial
inference efficiency gains. These findings suggest that intuition and
deliberative reasoning are distinct, complementary systems benefiting from
targeted training.

</details>


### [15] [SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution](https://arxiv.org/abs/2505.20732)
*Hanlin Wang,Chak Tou Leong,Jiashuo Wang,Jian Wang,Wenjie Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为Stepwise Progress Attribution (SPA)的奖励重新分配框架，通过分解最终奖励为每一步的贡献来解决强化学习中延迟奖励的问题。实验表明，SPA在成功率和环境约束准确性上优于现有方法，并提供了更有效的中间奖励以提升RL训练效果。


<details>
  <summary>Details</summary>
Motivation: 强化学习在训练大型语言模型代理完成复杂任务方面具有巨大潜力，但延迟奖励问题阻碍了其应用。反馈信号通常只有在任务完成后才可用，这使得难以将延迟奖励分配给早期动作，从而影响代理训练的效果。

Method: 作者提出了Stepwise Progress Attribution (SPA)，一种通用的奖励重新分配框架。该框架通过训练一个进度估计器，将最终奖励分解为每一步的增量贡献，使其与任务完成相匹配。在策略优化过程中，结合每步估算的贡献和环境中的动作接地信号，生成细粒度的中间奖励，用于有效训练代理。

Result: 在多个常见的代理基准测试（如Webshop、ALFWorld和VirtualHome）上进行的广泛实验证明，SPA在成功率上平均提高了2.5%，在接地准确性上平均提升了1.9%。进一步分析表明，该方法显著提供了更有效的中间奖励以促进RL训练。

Conclusion: Stepwise Progress Attribution (SPA)作为一种通用的奖励重新分配框架，能够有效解决强化学习中的延迟奖励问题，显著提升代理的成功率和接地准确性，为复杂的多步骤任务提供更有效的训练机制。

Abstract: Reinforcement learning (RL) holds significant promise for training LLM agents
to handle complex, goal-oriented tasks that require multi-step interactions
with external environments. However, a critical challenge when applying RL to
these agentic tasks arises from delayed rewards: feedback signals are typically
available only after the entire task is completed. This makes it non-trivial to
assign delayed rewards to earlier actions, providing insufficient guidance
regarding environmental constraints and hindering agent training. In this work,
we draw on the insight that the ultimate completion of a task emerges from the
cumulative progress an agent makes across individual steps. We propose Stepwise
Progress Attribution (SPA), a general reward redistribution framework that
decomposes the final reward into stepwise contributions, each reflecting its
incremental progress toward overall task completion. To achieve this, we train
a progress estimator that accumulates stepwise contributions over a trajectory
to match the task completion. During policy optimization, we combine the
estimated per-step contribution with a grounding signal for actions executed in
the environment as the fine-grained, intermediate reward for effective agent
training. Extensive experiments on common agent benchmarks (including Webshop,
ALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the
state-of-the-art method in both success rate (+2.5\% on average) and grounding
accuracy (+1.9\% on average). Further analyses demonstrate that our method
remarkably provides more effective intermediate rewards for RL training. Our
code is available at https://github.com/WangHanLinHenry/SPA-RL-Agent.

</details>


### [16] [SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation](https://arxiv.org/abs/2505.20622)
*Ting Xu,Zhichao Huang,Jiankai Sun,Shanbo Cheng,Wai Lam*

Main category: cs.CL

TL;DR: 本文提出了一种新的策略优化框架SeqPO-SiMT，用于同时机器翻译(SiMT)任务。通过将SiMT视为一个序列决策问题，并引入定制奖励来提升翻译质量并减少延迟。实验表明，SeqPO-SiMT在多个数据集上显著提高了翻译质量，同时降低了平均延迟，甚至在某些情况下可以媲美离线翻译效果。


<details>
  <summary>Details</summary>
Motivation: 目前的强化学习方法（如PPO和DPO）主要适用于单步任务，而同时机器翻译（SiMT）作为一个多步任务需要一种更适合的策略优化方法。此外，SiMT需要在较少的上下文中实现高质量翻译，这对现有模型提出了挑战。因此，研究者希望开发一种专门针对SiMT任务的策略优化框架，以提高翻译质量和降低延迟。

Method: SeqPO-SiMT将SiMT任务定义为一个序列决策问题，采用定制奖励机制来指导模型的训练过程。该框架允许SiMT大语言模型模拟和改进翻译过程。与传统的监督微调（SFT）模型不同，SeqPO-SiMT能够更有效地处理多步SiMT任务，从而实现更高的翻译质量和更低的延迟。

Result: 实验结果表明，SeqPO-SiMT在六个不同领域的数据集上显著提升了翻译质量，同时减少了延迟。特别是在NEWSTEST2021英文到中文的数据集上，SeqPO-SiMT相比监督微调模型提升了1.13个COMET分数，同时将平均延迟降低了6.17。值得注意的是，使用7B参数量的大模型在SeqPO-SiMT下的SiMT结果甚至可以媲美高性能离线翻译模型的效果。

Conclusion: SeqPO-SiMT提供了一种有效的策略优化框架，用于解决同时机器翻译中的多步决策问题。通过引入定制奖励机制，SeqPO-SiMT不仅提高了翻译质量，还显著降低了延迟。这表明，SeqPO-SiMT是一种有潜力的方法，可以在资源受限的情况下实现高质量的实时翻译。

Abstract: We present Sequential Policy Optimization for Simultaneous Machine
Translation (SeqPO-SiMT), a new policy optimization framework that defines the
simultaneous machine translation (SiMT) task as a sequential decision making
problem, incorporating a tailored reward to enhance translation quality while
reducing latency. In contrast to popular Reinforcement Learning from Human
Feedback (RLHF) methods, such as PPO and DPO, which are typically applied in
single-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task.
This intuitive framework allows the SiMT LLMs to simulate and refine the SiMT
process using a tailored reward. We conduct experiments on six datasets from
diverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that
SeqPO-SiMT consistently achieves significantly higher translation quality with
lower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning
(SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17
in the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context
than offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly
rival the offline translation of high-performing LLMs, including
Qwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [17] [Casper DPM: Cascaded Perceptual Dynamic Projection Mapping onto Hands](https://arxiv.org/abs/2409.04397)
*Yotam Erel,Or Kozlovsky-Mordenfeld,Daisuke Iwai,Kosuke Sato,Amit H. Bermano*

Main category: cs.GR

TL;DR: 本文提出了一种将3D内容动态投影到人手上的技术，通过结合低速3D粗略估计与高速2D校正步骤，改善了投影对齐、增加投影面积并减少感知延迟。用户研究显示，相比直接从3D姿态估计投影的方法，该方法减少了延迟伪影的影响，并提高了任务执行的速度和舒适度。


<details>
  <summary>Details</summary>
Motivation: 准确快速地计算人类手部的姿态和形状是一项具有挑战性的任务，因为手部是关节连接且可变形的。为了克服这一问题，并实现更精确、更大面积以及更低延迟的手部投影效果，需要一种新的投影校正方法。

Method: 该方法首先使用较慢的3D粗略估计来获取手部姿态，然后通过高速2D校正步骤改进投影与手部的对齐，扩大投影面积并降低感知延迟。由于这种方法基于手部的完整3D重建，因此可以应用任意纹理或高性能效果，这是以前无法实现的。

Result: 通过两次用户研究，结果表明，与直接从3D姿态估计渲染帧并投影的方法相比，参与者对延迟伪影的敏感性降低，并能以更快的速度和更高的舒适度完成相关任务。此外，文章展示了若干新颖的应用案例。

Conclusion: 这种结合低速3D粗略估计与高速2D校正步骤的技术显著提升了手部投影的效果，在感知延迟、投影面积和任务性能方面均有明显改进，同时为未来的手部交互应用提供了更多可能性。

Abstract: We present a technique for dynamically projecting 3D content onto human hands
with short perceived motion-to-photon latency. Computing the pose and shape of
human hands accurately and quickly is a challenging task due to their
articulated and deformable nature. We combine a slower 3D coarse estimation of
the hand pose with high speed 2D correction steps which improve the alignment
of the projection to the hands, increase the projected surface area, and reduce
perceived latency. Since our approach leverages a full 3D reconstruction of the
hands, any arbitrary texture or reasonably performant effect can be applied,
which was not possible before. We conducted two user studies to assess the
benefits of using our method. The results show subjects are less sensitive to
latency artifacts and perform faster and with more ease a given associated task
over the naive approach of directly projecting rendered frames from the 3D pose
estimation. We demonstrate several novel use cases and applications.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [18] [Prices and preferences in the electric vehicle market](https://arxiv.org/abs/2403.00458)
*Chung Yi See,Vasco Rato Santos,Lucas Woodley,Megan Yeo,Daniel Palmer,Shuheng Zhang,and Ashley Nunes*

Main category: econ.EM

TL;DR: 本文研究了2011年至2023年间电动车（EV）的价格影响因素，指出电动车高价的主要原因并非电池成本，而是车辆配置、功能和经销商安装的附件等附加价值。此外，研究发现电动车续航里程与价格呈负相关，而电池容量与价格正相关，这表明消费者偏好功能丰富且动力强劲的电动车。然而，这种偏好导致电动车的燃油经济性下降，减少了生命周期排放效益。


<details>
  <summary>Details</summary>
Motivation: 现有讨论普遍认为电动车价格较高的主要原因是电池成本较高，并将电动车广泛普及的条件设定为电池成本的下降。本文旨在重新审视这一观点，通过分析2011年至2023年的电动车属性和市场状况数据，揭示影响电动车价格的真实因素。

Method: 研究人员收集并分析了2011年至2023年间电动车的属性和市场条件数据，评估了多个变量对电动车价格的影响，包括配置数量、额外功能、经销商安装的附件、电动车马力、续航里程和电池容量等。通过统计分析方法，确定了这些因素与电动车价格的相关性。

Result: 研究结果表明：(1) 电动车价格主要受标准配置、额外功能和经销商安装附件的影响，其次才是电动车的马力；(2) 续航里程与电动车价格呈负相关，意味着续航焦虑可能没有现有讨论中那么重要；(3) 电池容量与电动车价格正相关，因为更高的电池容量通常意味着更高的马力输出；(4) 消费者对功能丰富和动力强劲电动车的偏好导致了燃油经济性的降低，从而至少减少了3.26%的生命周期排放效益。

Conclusion: 本文的研究结果表明，电动车的高采购价格更多地反映了消费者对功能丰富和动力强劲车辆的偏好，而非单纯的电池成本问题。这种偏好可能会降低电动车的生命周期排放效益，因此在推动电动车作为脱碳化路径时，需要更加关注消费者行为和市场趋势。

Abstract: Although electric vehicles are less polluting than gasoline powered vehicles,
adoption is challenged by higher procurement prices. Existing discourse
emphasizes EV battery costs as being principally responsible for this price
differential and widespread adoption is routinely conditioned upon battery
costs declining. We scrutinize such reasoning by sourcing data on EV attributes
and market conditions between 2011 and 2023. Our findings are fourfold. First,
EV prices are influenced principally by the number of amenities, additional
features, and dealer-installed accessories sold as standard on an EV, and to a
lesser extent, by EV horsepower. Second, EV range is negatively correlated with
EV price implying that range anxiety concerns may be less consequential than
existing discourse suggests. Third, battery capacity is positively correlated
with EV price, due to more capacity being synonymous with the delivery of more
horsepower. Collectively, this suggests that higher procurement prices for EVs
reflects consumer preference for vehicles that are feature dense and more
powerful. Fourth and finally, accommodating these preferences have produced
vehicles with lower fuel economy, a shift that reduces envisioned lifecycle
emissions benefits by at least 3.26 percent, subject to the battery pack
chemistry leveraged and the carbon intensity of the electrical grid. These
findings warrant attention as decarbonization efforts increasingly emphasize
electrification as a pathway for complying with domestic and international
climate agreements.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [19] [Accelerating ab initio melting property calculations with machine learning: Application to the high entropy alloy TaVCrW](https://arxiv.org/abs/2408.08654)
*Li-Fang Zhu,Fritz Koermann,Qing Chen,Malin Selleby,Joerg Neugebauer,and Blazej Grabowski*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出了一种高效且基于密度泛函理论（DFT）的方法，通过特别设计的机器学习势来辅助计算熔化性质，大幅降低计算成本，并成功应用于高熵合金TaVCrW。


<details>
  <summary>Details</summary>
Motivation: 由于高熔点材料的实验测量极具挑战性，理论预测成为不可或缺的手段。然而，传统的基于DFT的自由能方法因涉及昂贵的热力学积分而计算成本过高，难以进行高通量计算。因此，需要一种更高效的计算方法来解决这一问题。

Method: 研究者提出了一种结合DFT和专门设计的机器学习势的方法。该机器学习势能够精确重现多组分合金的从头算相空间，从而用更高效的自由能微扰计算完全替代代价高昂的热力学积分。这种方法显著减少了计算资源的需求。

Result: 与当前的替代方案相比，该方法实现了80%的计算资源节省。将其应用于高熵合金TaVCrW时，成功计算了熔化温度、熔化熵、熔化焓以及熔点处的体积变化等熔化性质，结果与calphad外推值合理一致。此外，还计算了固态和液态TaVCrW的热容。

Conclusion: 所提出的高效DFT结合机器学习势的方法不仅大幅降低了计算熔化性质的成本，还展示了其在复杂多组分合金中的应用潜力，为高通量计算和新型高温材料的设计提供了新途径。

Abstract: Melting properties are critical for designing novel materials, especially for
discovering high-performance, high-melting refractory materials. Experimental
measurements of these properties are extremely challenging due to their high
melting temperatures. Complementary theoretical predictions are, therefore,
indispensable. The conventional free energy approach using density functional
theory (DFT) has been a gold standard for such purposes because of its high
accuracy. However,it generally involves expensive thermodynamic integration
using ab initio molecular dynamic simulations. The high computational cost
makes high-throughput calculations infeasible. Here, we propose a highly
efficient DFT-based method aided by a specially designed machine learning
potential. As the machine learning potential can closely reproduce the ab
initio phase space, even for multi-component alloys, the costly thermodynamic
integration can be fully substituted with more efficient free energy
perturbation calculations. The method achieves overall savings of computational
resources by 80% compared to current alternatives. We apply the method to the
high-entropy alloy TaVCrW and calculate its melting properties, including
melting temperature, entropy and enthalpy of fusion, and volume change at the
melting point. Additionally, the heat capacities of solid and liquid TaVCrW are
calculated. The results agree reasonably with the calphad extrapolated values.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [20] [Model-Based AI planning and Execution Systems for Robotics](https://arxiv.org/abs/2505.04493)
*Or Wertheim,Ronen I. Brafman*

Main category: cs.RO

TL;DR: 本文探讨了基于模型的机器人规划与执行系统的设计选择、现有问题及解决方案，并为未来的发展方向提出了建议。


<details>
  <summary>Details</summary>
Motivation: 基于模型的规划和执行系统为构建能够自动组合多种基本技能以完成多样化任务的灵活自主机器人提供了一种有原则的方法。尽管这一理念几乎与现代机器人技术同样古老，但直到ROSPlan系统的出现，才开始将通用推理架构与现代机器人平台集成起来。因此，有必要对现有的系统设计选择、问题及其解决方案进行总结，并探索未来的发展方向。

Method: 文章回顾了从早期的现代机器人技术到近年来兴起的多种基于模型的机器人任务级控制系统的发展历程。通过分析这些系统在设计上的多样性以及它们试图解决的问题，评估了已提出的各种解决方案。

Result: 通过对现有系统的分析，文章明确了不同设计选择的优势与局限性，同时识别出当前系统中存在的关键问题及相应的解决方案。

Conclusion: 基于模型的机器人规划与执行系统在实现灵活自主机器人方面具有巨大潜力。未来的研究应关注于改进现有系统的通用性、适应性和集成能力，进一步推动该领域的发展。

Abstract: Model-based planning and execution systems offer a principled approach to
building flexible autonomous robots that can perform diverse tasks by
automatically combining a host of basic skills. This idea is almost as old as
modern robotics. Yet, while diverse general-purpose reasoning architectures
have been proposed since, general-purpose systems that are integrated with
modern robotic platforms have emerged only recently, starting with the
influential ROSPlan system. Since then, a growing number of model-based systems
for robot task-level control have emerged. In this paper, we consider the
diverse design choices and issues existing systems attempt to address, the
different solutions proposed so far, and suggest avenues for future
development.

</details>


### [21] [Hardware Design and Learning-Based Software Architecture of Musculoskeletal Wheeled Robot Musashi-W for Real-World Applications](https://arxiv.org/abs/2403.11729)
*Kento Kawaharazuka,Akihiro Miki,Masahiro Bando,Temma Suzuki,Yoshimoto Ribayashi,Yasunori Toshimitsu,Yuya Nagamatsu,Kei Okada,and Masayuki Inaba*

Main category: cs.RO

TL;DR: 通过结合轮式底座和肌肉骨骼上肢，开发了适用于现实任务的机器人Musashi-W，并通过多种任务展示了其硬件和软件系统的优越性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的肌肉骨骼仿人机器人能够模仿人类身体的灵活性和冗余性，但它们在实际应用中仍然存在困难，尤其是双足行走问题难以解决。因此，研究者尝试通过结合轮式底座和肌肉骨骼上肢来克服这一限制，以实现更广泛的现实任务应用。

Method: 研究者设计了一个名为Musashi-W的肌肉骨骼轮式机器人，它由一个轮式底座和肌肉骨骼上肢组成。此外，还构建了一个软件系统，该系统集成了静态和动态身体模式学习、反射控制以及视觉识别技术。

Result: 通过几个具体任务（如通过人类教学进行清洁、考虑肌肉添加的重物搬运、以及通过动态布料操作设置桌子），验证了Musashi-W的硬件和软件能够充分发挥肌肉骨骼上肢的优势。

Conclusion: Musashi-W的成功开发表明，将轮式底座与肌肉骨骼上肢相结合可以有效克服现有仿人机器人的局限性，为现实世界中的复杂任务提供了一种新的解决方案。

Abstract: Various musculoskeletal humanoids have been developed so far. While these
humanoids have the advantage of their flexible and redundant bodies that mimic
the human body, they are still far from being applied to real-world tasks. One
of the reasons for this is the difficulty of bipedal walking in a flexible
body. Thus, we developed a musculoskeletal wheeled robot, Musashi-W, by
combining a wheeled base and musculoskeletal upper limbs for real-world
applications. Also, we constructed its software system by combining static and
dynamic body schema learning, reflex control, and visual recognition. We show
that the hardware and software of Musashi-W can make the most of the advantages
of the musculoskeletal upper limbs, through several tasks of cleaning by human
teaching, carrying a heavy object considering muscle addition, and setting a
table through dynamic cloth manipulation with variable stiffness.

</details>
