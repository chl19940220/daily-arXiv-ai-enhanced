<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.CR](#cs.CR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: 本文综述了人类与AI代理协作的最新经验发展，提出了一种新的概念架构（Hierarchical Exploration-Exploitation Net），以系统地整合多代理协调、知识管理、控制机制等技术细节，并为未来设计或扩展人类-AI共生关系提供了参考。


<details>
  <summary>Details</summary>
Motivation: 当前在处理开放性复杂任务时，缺乏一个统一的理论框架来整合各种研究和技术成果。

Method: 通过将现有的贡献（包括符号AI技术、连接主义LLM代理和混合组织实践）映射到提出的框架上，系统地分析和整合多代理协调、知识管理和控制机制等技术细节。

Result: 该方法有助于修正传统方法，并启发融合定性和定量范式的新工作，促进人类认知与AI能力的共同进化。

Conclusion: 这一新框架为设计或扩展人类-AI共生关系提供了理论基础，并推动了两者更深层次的共同进化。

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [2] [Automated Meta Prompt Engineering for Alignment with the Theory of Mind](https://arxiv.org/abs/2505.09024)
*Aaron Baughman,Rahul Agarwal,Eduardo Morales,Gozde Akay*

Main category: cs.AI

TL;DR: 文章介绍了一种元提示（meta-prompting）方法，该方法通过联合生成流畅文本和优化人类心理预期与大语言模型（LLM）神经处理之间的相似性来解决复杂任务。实验表明，在美国网球公开赛2024等实际生产系统中，LLM作为判断者（LLMaaJ）能够通过情境学习教导其他LLM生成内容，并在平均4.38次迭代后达到53.8%的人类评审预期对齐率。通过结合希尔伯特向量空间中的内容特质几何解释，LLMaaJ可以优化人类的理论思维（ToM），从而提高内容质量并扩展网球赛事报道的覆盖范围。此方法已推广到体育和娱乐领域的其他现场活动。


<details>
  <summary>Details</summary>
Motivation: 为了使大语言模型生成的内容更符合人类的心理预期，研究者提出了一种新的方法，即通过元提示和代理强化学习技术，让一个LLM（作为判断者LLMaaJ）指导另一个LLM生成内容，同时考虑有意和无意生成的文本特性。目的是解决理论思维（ToM）对齐问题，预测并包含人类编辑意见于文本生成过程中。

Method: 使用代理强化学习技术，其中LLMaaJ通过情境学习指导另一个LLM生成内容；用户在美国网球公开赛2024期间修改AI生成的长篇文本以测量人类心理信念；利用希尔伯特向量空间中的几何解释来优化内容特质（如事实性、新颖性、重复性和相关性）。

Result: 在实验和实际生产系统的结果分析中，人类内容评审者的期望与AI生成内容的对齐率达到53.8%，平均迭代次数为4.38次。这种方法提高了内容质量，扩展了网球动作的报道覆盖范围。

Conclusion: 通过元提示和代理强化学习，LLMaaJ可以在生成内容时有效预测并包含人类编辑意见，从而优化人类的理论思维对齐，提高生成内容的质量。此方法不仅适用于美国网球公开赛2024，还可推广到体育和娱乐领域的其他现场活动。

Abstract: We introduce a method of meta-prompting that jointly produces fluent text for
complex tasks while optimizing the similarity of neural states between a
human's mental expectation and a Large Language Model's (LLM) neural
processing. A technique of agentic reinforcement learning is applied, in which
an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,
how to produce content by interpreting the intended and unintended generated
text traits. To measure human mental beliefs around content production, users
modify long form AI-generated text articles before publication at the US Open
2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)
alignment problem by anticipating and including human edits within the creation
of text from an LLM. Throughout experimentation and by interpreting the results
of a live production system, the expectations of human content reviewers had
100% of alignment with AI 53.8% of the time with an average iteration count of
4.38. The geometric interpretation of content traits such as factualness,
novelty, repetitiveness, and relevancy over a Hilbert vector space combines
spatial volume (all trait importance) with vertices alignment (individual trait
relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an
increase in content quality by extending the coverage of tennis action. Our
work that was deployed at the US Open 2024 has been used across other live
events within sports and entertainment.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs](https://arxiv.org/abs/2505.10425)
*Jingyao Wang,Wenwen Qiang,Zeen Song,Changwen Zheng,Hui Xiong*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）在复杂任务中表现出色，但现有方法未能平衡推理效果和计算效率。本文提出了一种名为Learning to Think (L2T)的信息理论强化微调框架，旨在使LLMs以更少的token实现最优推理。L2T通过快速估计奖励、优化每个环节的贡献并惩罚过度更新，提高了推理的效果和效率。实验结果表明，L2T在不同基准测试和基础模型上均展现出优势。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽然擅长复杂任务，但在推理过程中存在计算效率低下的问题，常常产生不必要的长推理链，浪费了token资源。因此，需要一种新的方法来平衡推理效果与计算效率。

Method: 提出了Learning to Think (L2T)，一个信息理论强化微调框架。该框架将每次查询-响应交互视为多阶段的分层会话，并引入了一个普遍的密集过程奖励，用于量化每阶段参数的信息增益。此外，还提出了一种基于PAC-Bayes边界和Fisher信息矩阵的快速奖励估计方法。通过立即奖励每阶段的贡献并惩罚过多更新，L2T利用强化学习优化模型，最大化每阶段的使用并实现有效更新。

Result: 理论分析表明，所提出的奖励估计方法显著降低了计算复杂度，同时保持高估计准确性。实验证明，L2T在各种推理基准和基础模型上都显示出优势，提升了推理的效果和效率。

Conclusion: L2T作为一种信息理论强化微调框架，能够有效提升大型语言模型在推理任务中的效果和效率，减少token消耗并降低计算复杂度。

Abstract: Large language models (LLMs) excel at complex tasks thanks to advances in
reasoning abilities. However, existing methods overlook the trade-off between
reasoning effectiveness and computational efficiency, often encouraging
unnecessarily long reasoning chains and wasting tokens. To address this, we
propose Learning to Think (L2T), an information-theoretic reinforcement
fine-tuning framework for LLMs to make the models achieve optimal reasoning
with fewer tokens. Specifically, L2T treats each query-response interaction as
a hierarchical session of multiple episodes and proposes a universal dense
process reward, i.e., quantifies the episode-wise information gain in
parameters, requiring no extra annotations or task-specific evaluators. We
propose a method to quickly estimate this reward based on PAC-Bayes bounds and
the Fisher information matrix. Theoretical analyses show that it significantly
reduces computational complexity with high estimation accuracy. By immediately
rewarding each episode's contribution and penalizing excessive updates, L2T
optimizes the model via reinforcement learning to maximize the use of each
episode and achieve effective updates. Empirical results on various reasoning
benchmarks and base models demonstrate the advantage of L2T across different
tasks, boosting both reasoning effectiveness and efficiency.

</details>


### [4] [ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts](https://arxiv.org/abs/2505.10010)
*Jing-Cheng Pang,Kaiyuan Li,Yidi Wang,Si-Hang Yang,Shengyi Jiang,Yang Yu*

Main category: cs.LG

TL;DR: 文章介绍了一个名为ImagineBench的全新基准测试平台，用于评估结合真实与大型语言模型（LLM）生成的经验的离线强化学习算法。该基准包括多种环境数据集、任务类型和语言指令，通过评估现有算法发现性能有待提升，并指出了未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: 强化学习面临依赖大量真实交互数据的问题，而LLMs可以通过生成合成经验来缓解这一问题。然而，缺乏标准化的基准测试阻碍了这一领域的发展。因此，需要一个综合性的基准来评估利用真实和想象经验的离线RL算法。

Method: 构建了一个名为ImagineBench的基准测试平台，包含：1）真实和LLM生成的数据集；2）涵盖运动、机器人操作和导航等多样环境；3）不同复杂度的自然语言任务指令。使用此基准系统性地评估了最先进的离线RL算法。

Result: 单纯应用现有的离线RL算法在未见任务上表现不佳，在困难任务上的成功率为35.44%，远低于仅基于真实数据训练的方法64.37%的成功率。这表明需要改进算法以更好地利用LLM生成的经验。

Conclusion: 文章提出了ImagineBench作为首个评估结合真实和想象经验的离线RL算法的基准，并揭示了现有算法的不足之处，同时指出未来研究的机会，如更好地利用想象经验、快速在线适应和持续学习等。

Abstract: A central challenge in reinforcement learning (RL) is its dependence on
extensive real-world interaction data to learn task-specific policies. While
recent work demonstrates that large language models (LLMs) can mitigate this
limitation by generating synthetic experience (noted as imaginary rollouts) for
mastering novel tasks, progress in this emerging field is hindered due to the
lack of a standard benchmark. To bridge this gap, we introduce ImagineBench,
the first comprehensive benchmark for evaluating offline RL algorithms that
leverage both real rollouts and LLM-imaginary rollouts. The key features of
ImagineBench include: (1) datasets comprising environment-collected and
LLM-imaginary rollouts; (2) diverse domains of environments covering
locomotion, robotic manipulation, and navigation tasks; and (3) natural
language task instructions with varying complexity levels to facilitate
language-conditioned policy learning. Through systematic evaluation of
state-of-the-art offline RL algorithms, we observe that simply applying
existing offline RL algorithms leads to suboptimal performance on unseen tasks,
achieving 35.44% success rate in hard tasks in contrast to 64.37% of method
training on real rollouts for hard tasks. This result highlights the need for
algorithm advancements to better leverage LLM-imaginary rollouts. Additionally,
we identify key opportunities for future research: including better utilization
of imaginary rollouts, fast online adaptation and continual learning, and
extension to multi-modal tasks. Our code is publicly available at
https://github.com/LAMDA-RL/ImagineBench.

</details>


### [5] [Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback](https://arxiv.org/abs/2505.09925)
*Yutao Yang,Jie Zhou,Junsong Li,Qianjun Pan,Bihao Zhan,Qin Chen,Xipeng Qiu,Liang He*

Main category: cs.LG

TL;DR: 本文提出了一种交互式持续学习框架RiCL，通过利用大型语言模型（LLMs）从动态反馈中有效学习新技能，解决了传统持续学习中的两大限制：静态数据集和清洁标签假设。RiCL包含三个关键组件：时间一致性感知净化器、交互感知直接偏好优化策略以及抗噪声对比学习模块。实验表明，RiCL在处理含噪数据时显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法存在两个主要问题：1) 使用静态数据集而非实时人类标注数据；2) 假设标签清洁，未考虑现实世界中普遍存在的噪声反馈。为了解决这些问题，提出了RiCL框架。

Method: RiCL框架包含以下三个关键组件：1) 时间一致性感知净化器，用于自动区分数据流中的干净样本与噪声样本；2) 交互感知直接偏好优化策略，通过协调AI生成反馈与人类提供反馈来对齐模型行为与人类意图；3) 抗噪声对比学习模块，利用数据内在关系捕获鲁棒表示，避免依赖不可靠标签。

Result: 在FewRel和TACRED两个基准数据集上的广泛实验证明，RiCL方法在处理含有现实噪声模式的数据时，显著优于现有的最先进的在线持续学习和噪声标签学习方法组合。

Conclusion: RiCL框架成功解决了传统持续学习中的两大限制，并在处理含噪数据方面表现出色，为交互式持续学习提供了新的解决方案。

Abstract: This paper introduces an interactive continual learning paradigm where AI
models dynamically learn new skills from real-time human feedback while
retaining prior knowledge. This paradigm distinctively addresses two major
limitations of traditional continual learning: (1) dynamic model updates using
streaming, real-time human-annotated data, rather than static datasets with
fixed labels, and (2) the assumption of clean labels, by explicitly handling
the noisy feedback common in real-world interactions. To tackle these problems,
we propose RiCL, a Reinforced interactive Continual Learning framework
leveraging Large Language Models (LLMs) to learn new skills effectively from
dynamic feedback. RiCL incorporates three key components: a temporal
consistency-aware purifier to automatically discern clean from noisy samples in
data streams; an interaction-aware direct preference optimization strategy to
align model behavior with human intent by reconciling AI-generated and
human-provided feedback; and a noise-resistant contrastive learning module that
captures robust representations by exploiting inherent data relationships, thus
avoiding reliance on potentially unreliable labels. Extensive experiments on
two benchmark datasets (FewRel and TACRED), contaminated with realistic noise
patterns, demonstrate that our RiCL approach substantially outperforms existing
combinations of state-of-the-art online continual learning and noisy-label
learning methods.

</details>


### [6] [Adversarial Attack on Large Language Models using Exponentiated Gradient Descent](https://arxiv.org/abs/2505.09820)
*Sajib Biswas,Mao Nishino,Samuel Jacob Chacko,Xiuwen Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于指数梯度下降和Bregman投影方法的内在优化技术，能够有效破解多个大型语言模型（LLMs），并证明了其收敛性。实验表明，该技术在五个开源LLM上相较于其他三种最先进的破解技术具有更高的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管许多LLMs通过人类反馈的强化学习等技术进行了对齐，但它们仍然容易受到越狱攻击。现有的对抗攻击方法要么在离散空间中搜索可能越狱目标模型的token，要么尝试优化由模型词汇表中的token表示的连续空间。然而，这些方法分别存在效率低下或效果不佳的问题。

Method: 作者开发了一种内在优化技术，使用指数梯度下降与Bregman投影方法，确保优化的一热编码始终位于概率单纯形内。此方法结合了离散空间和连续空间的优点，并且作者还证明了该技术的收敛性。

Result: 在四个公开数据集上的实验结果表明，该技术在破解五个开源LLM时，相较于其他三种最先进的越狱技术，具有更高的成功率和更高效的性能。

Conclusion: 提出的内在优化技术有效地利用了空间的约束和结构，在破解LLMs方面表现出了优越的性能，同时源代码已公开，可供进一步研究和应用。

Abstract: As Large Language Models (LLMs) are widely used, understanding them
systematically is key to improving their safety and realizing their full
potential. Although many models are aligned using techniques such as
reinforcement learning from human feedback (RLHF), they are still vulnerable to
jailbreaking attacks. Some of the existing adversarial attack methods search
for discrete tokens that may jailbreak a target model while others try to
optimize the continuous space represented by the tokens of the model's
vocabulary. While techniques based on the discrete space may prove to be
inefficient, optimization of continuous token embeddings requires projections
to produce discrete tokens, which might render them ineffective. To fully
utilize the constraints and the structures of the space, we develop an
intrinsic optimization technique using exponentiated gradient descent with the
Bregman projection method to ensure that the optimized one-hot encoding always
stays within the probability simplex. We prove the convergence of the technique
and implement an efficient algorithm that is effective in jailbreaking several
widely used LLMs. We demonstrate the efficacy of the proposed technique using
five open-source LLMs on four openly available datasets. The results show that
the technique achieves a higher success rate with great efficiency compared to
three other state-of-the-art jailbreaking techniques. The source code for our
implementation is available at:
https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack

</details>


### [7] [Self Rewarding Self Improving](https://arxiv.org/abs/2505.08827)
*Toby Simonds,Kevin Lopez,Akira Yoshiyama,Dominique Garmier*

Main category: cs.LG

TL;DR: 通过自我评判机制，大型语言模型可以在没有参考答案的情况下实现自我改进。实验表明，在 Countdown 数字游戏和 MIT Integration Bee 积分竞赛问题上，模型能够提供可靠的奖励信号以支持强化学习。结合合成问题生成技术，形成了完整的自我改进循环，显著提升了性能（如Qwen 2.5 7B相比基线提高了8%，并在积分任务上超越了GPT-4o）。此方法可能引发从人类引导训练向自我导向学习的范式转变。


<details>
  <summary>Details</summary>
Motivation: 当前许多强化学习环境受限于难以创建程序化奖励机制，特别是在训练数据稀缺或评估要求复杂的领域。研究旨在探索一种无需真实答案参考的自我改进方法，利用生成与验证解决方案之间的固有不对称性。

Method: 在 Countdown 数字游戏和 MIT Integration Bee 积分竞赛问题上进行实验，通过实施自我评判机制，模型可以生成练习问题、解决问题并评估自身表现。同时结合合成问题生成技术形成完整的自我改进循环。

Result: 实现了显著的性能提升，Qwen 2.5 7B相比基线提高了8%，并在积分任务上超越了GPT-4o的表现。证明了LLM评判者能提供有效的奖励信号用于模型训练。

Conclusion: 该研究表明，大型语言模型可以通过自我评判和强化学习实现自我改进，并可能推动从人类引导训练向自我导向学习的范式转变，从而加速在数据稀缺或复杂评估需求领域的进步。

Abstract: We demonstrate that large language models can effectively self-improve
through self-judging without requiring reference solutions, leveraging the
inherent asymmetry between generating and verifying solutions. Our experiments
on Countdown puzzles and MIT Integration Bee problems show that models can
provide reliable reward signals without ground truth answers, enabling
reinforcement learning in domains previously not possible. By implementing
self-judging, we achieve significant performance gains maintaining alignment
with formal verification. When combined with synthetic question generation, we
establish a complete self-improvement loop where models generate practice
problems, solve them, and evaluate their own performance-achieving an 8%
improvement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on
integration tasks. Our findings demonstrate that LLM judges can provide
effective reward signals for training models, unlocking many reinforcement
learning environments previously limited by the difficulty of creating
programmatic rewards. This suggests a potential paradigm shift toward AI
systems that continuously improve through self-directed learning rather than
human-guided training, potentially accelerating progress in domains with scarce
training data or complex evaluation requirements.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [8] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 在大规模语言模型（LLMs）中，通过重复采样来扩展推理计算，随着样本数量的增加，覆盖率（解决问题的比例）持续提高。实验表明，这种改进部分是由于标准评估基准的答案分布偏向于少量常见答案。研究提出了一种基于训练集答案频率的基线方法，在数学推理和事实知识两个领域中，该基线方法对于某些LLMs优于重复采样，而对于其他LLMs与混合策略表现相当。此基线有助于更准确地测量重复采样在提示无关猜测之外对覆盖率的提升。


<details>
  <summary>Details</summary>
Motivation: 探索重复采样在大语言模型中的效果是否受到评估基准答案分布的影响，并尝试通过一种基于训练集答案频率的基线方法来更准确地衡量重复采样的实际作用。

Method: 定义一个基线方法，该方法根据训练集中答案的频率枚举答案；在数学推理和事实知识两个领域进行实验，比较基线方法、重复采样以及混合策略的表现。

Result: 基线方法在某些LLMs上优于重复采样，而在其他LLMs上与混合策略表现相当；揭示了重复采样效果的部分原因在于评估基准答案分布的偏差。

Conclusion: 重复采样在大语言模型中的效果部分归因于评估基准答案分布的偏差，使用基于训练集答案频率的基线方法可以更准确地评估重复采样的实际贡献。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [9] [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320)
*Chenxi Whitehouse,Tianlu Wang,Ping Yu,Xian Li,Jason Weston,Ilia Kulikov,Swarnadeep Saha*

Main category: cs.CL

TL;DR: 本文介绍了一种名为J1的强化学习方法，用于训练以大型语言模型（LLM）作为评判者的模型。该方法通过可验证的奖励机制促进思考并减少评判偏差，且在相同规模训练下优于其他现有模型。


<details>
  <summary>Details</summary>
Motivation: AI的进步受限于评估质量，而强大的LLM-as-a-Judge模型被证明是核心解决方案。为了提升判断能力，需要找到最佳的训练方法来增强链式思维推理能力。

Method: 引入J1方法，将可验证和不可验证的提示转换为带有可验证奖励的判断任务，激励模型进行思考并减轻判断偏差。

Result: J1方法在8B或70B规模训练时优于其他现有模型，包括从DeepSeek-R1蒸馏出的模型。即使训练较小的模型，J1也在某些基准上超越o1-mini和R1。

Conclusion: 通过学习概述评估标准、与自动生成的参考答案比较以及重新评估模型响应的正确性，J1模型能够做出更好的判断。

Abstract: The progress of AI is bottlenecked by the quality of evaluation, and powerful
LLM-as-a-Judge models have proved to be a core solution. Improved judgment
ability is enabled by stronger chain-of-thought reasoning, motivating the need
to find the best recipes for training such models to think. In this work we
introduce J1, a reinforcement learning approach to training such models. Our
method converts both verifiable and non-verifiable prompts to judgment tasks
with verifiable rewards that incentivize thinking and mitigate judgment bias.
In particular, our approach outperforms all other existing 8B or 70B models
when trained at those sizes, including models distilled from DeepSeek-R1. J1
also outperforms o1-mini, and even R1 on some benchmarks, despite training a
smaller model. We provide analysis and ablations comparing Pairwise-J1 vs
Pointwise-J1 models, offline vs online training recipes, reward strategies,
seed prompts, and variations in thought length and content. We find that our
models make better judgments by learning to outline evaluation criteria,
comparing against self-generated reference answers, and re-evaluating the
correctness of model responses.

</details>


### [10] [Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning](https://arxiv.org/abs/2505.10182)
*Yoichi Ishibashi,Taro Yano,Masafumi Oyamada*

Main category: cs.CL

TL;DR: 通过对比实验表明，使用合成数据进行持续预训练（Reasoning CPT）能够显著提高大模型在多个领域的推理能力，尤其是在难题上的表现。此外，模型还能根据问题难度调整推理深度。


<details>
  <summary>Details</summary>
Motivation: 当前通过监督微调和强化学习提升大语言模型推理能力的方法主要局限于特定领域（如数学和编程），且对训练数据的广度和可扩展性有约束。而持续预训练（CPT）无需任务特定信号，但如何有效生成适用于推理的训练数据及其对多领域的影响尚待探索。

Method: 采用基于合成数据的推理持续预训练（Reasoning CPT），利用STEM和法律语料库中的隐藏思维过程来重建文本背后的思考过程，并将其应用于Gemma2-9B模型。然后在MMLU基准上与标准CPT进行比较。

Result: Reasoning CPT在所有评估领域中均表现出性能提升，特别是在难题上的改进更为显著（最高可达8分）。同时，一个领域的推理技能可以有效迁移到其他领域，模型还学会了根据问题难度调整推理深度。

Conclusion: 合成数据驱动的Reasoning CPT是一种有效的策略，能够提升大模型跨领域的推理能力，特别是在复杂问题上的表现。

Abstract: Large Language Models (LLMs) have demonstrated significant improvements in
reasoning capabilities through supervised fine-tuning and reinforcement
learning. However, when training reasoning models, these approaches are
primarily applicable to specific domains such as mathematics and programming,
which imposes fundamental constraints on the breadth and scalability of
training data. In contrast, continual pretraining (CPT) offers the advantage of
not requiring task-specific signals. Nevertheless, how to effectively
synthesize training data for reasoning and how such data affect a wide range of
domains remain largely unexplored. This study provides a detailed evaluation of
Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden
thought processes underlying texts, based on the premise that texts are the
result of the author's thinking process. Specifically, we apply Reasoning CPT
to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and
Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis
reveals that Reasoning CPT consistently improves performance across all
evaluated domains. Notably, reasoning skills acquired in one domain transfer
effectively to others; the performance gap with conventional methods widens as
problem difficulty increases, with gains of up to 8 points on the most
challenging problems. Furthermore, models trained with hidden thoughts learn to
adjust the depth of their reasoning according to problem difficulty.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [11] [Improved Algorithms for Differentially Private Language Model Alignment](https://arxiv.org/abs/2505.08849)
*Keyu Chen,Hao Tang,Qinglin Liu,Yizhao Xu*

Main category: cs.CR

TL;DR: 本文提出新的隐私保护算法用于语言模型与人类偏好的对齐，并在不同隐私预算下进行了有效性分析，其中DP-AdamW算法结合DPO技术在适度隐私预算条件下表现最佳，提升了15%的对齐质量。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型与人类偏好对齐过程中涉及敏感用户数据，存在隐私问题，而以往结合差分隐私的方法性能有限，因此需要更高效的隐私保护对齐方法。

Method: 提出了适用于直接偏好优化（DPO）和基于人类反馈的强化学习（RLHF）两种对齐技术的新型隐私保护算法，并通过系统实验验证其在大规模语言模型中的表现。

Result: 新提出的DP-AdamW算法结合DPO技术，在适度隐私预算（ε=2-5）下超越现有方法，提升了高达15%的对齐质量，同时研究了隐私保证、对齐效果和计算需求之间的权衡。

Conclusion: 所提出的隐私保护算法为语言模型对齐提供了更高的性能和隐私保障，并给出了优化隐私-效用权衡的实用指导。

Abstract: Language model alignment is crucial for ensuring that large language models
(LLMs) align with human preferences, yet it often involves sensitive user data,
raising significant privacy concerns. While prior work has integrated
differential privacy (DP) with alignment techniques, their performance remains
limited. In this paper, we propose novel algorithms for privacy-preserving
alignment and rigorously analyze their effectiveness across varying privacy
budgets and models. Our framework can be deployed on two celebrated alignment
techniques, namely direct preference optimization (DPO) and reinforcement
learning from human feedback (RLHF). Through systematic experiments on
large-scale language models, we demonstrate that our approach achieves
state-of-the-art performance. Notably, one of our algorithms, DP-AdamW,
combined with DPO, surpasses existing methods, improving alignment quality by
up to 15% under moderate privacy budgets ({\epsilon}=2-5). We further
investigate the interplay between privacy guarantees, alignment efficacy, and
computational demands, providing practical guidelines for optimizing these
trade-offs.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [12] [Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting](https://arxiv.org/abs/2505.09395)
*Chen-Yu Liu,Kuan-Cheng Chen,Yi-Chien Chen,Samuel Yen-Chi Chen,Wei-Hao Huang,Wei-Jia Huang,Yen-Jui Chang*

Main category: quant-ph

TL;DR: 文章提出了一种新的方法QPA（Quantum Parameter Adaptation），结合Attention-based Multi-ConvGRU模型，首次将量子机器学习应用于大规模台风轨迹预测中，显著减少了可训练参数数量，同时保持了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的台风轨迹预测方法计算需求高，深度学习模型需要大量资源。为了降低计算成本并提高效率，研究者引入了量子机器学习方法。

Method: 利用Quantum-Train (QT)框架成功开发出Quantum Parameter Adaptation (QPA)，该方法仅在训练期间使用量子神经网络生成可训练参数，推断时无需量子硬件，并将其与Attention-based Multi-ConvGRU模型集成。

Result: QPA大幅减少了可训练参数的数量，同时保持了预测性能，提供了一种可扩展且节能的气候建模方法。

Conclusion: QPA为高效学习台风预报模型提供了参数高效的训练方式，推动了混合量子经典学习在高性能量化预测中的应用和可持续发展。

Abstract: Typhoon trajectory forecasting is essential for disaster preparedness but
remains computationally demanding due to the complexity of atmospheric dynamics
and the resource requirements of deep learning models. Quantum-Train (QT), a
hybrid quantum-classical framework that leverages quantum neural networks
(QNNs) to generate trainable parameters exclusively during training,
eliminating the need for quantum hardware at inference time. Building on QT's
success across multiple domains, including image classification, reinforcement
learning, flood prediction, and large language model (LLM) fine-tuning, we
introduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting
model learning. Integrated with an Attention-based Multi-ConvGRU model, QPA
enables parameter-efficient training while maintaining predictive accuracy.
This work represents the first application of quantum machine learning (QML) to
large-scale typhoon trajectory prediction, offering a scalable and
energy-efficient approach to climate modeling. Our results demonstrate that QPA
significantly reduces the number of trainable parameters while preserving
performance, making high-performance forecasting more accessible and
sustainable through hybrid quantum-classical learning.

</details>
