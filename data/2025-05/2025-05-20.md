<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: 本文是一篇立场论文，全面回顾了人类与AI代理协作领域的最新实证进展。文章提出了一个新的概念框架——分层探索-利用网络（Hierarchical Exploration-Exploitation Net），旨在系统整合多代理协调、知识管理、控制论反馈循环和高级控制机制等技术细节。该框架不仅能够修订传统方法，还为融合定性和定量范式的新研究提供了灵感。文章结构灵活，可从任何部分开始阅读，既是对现有技术实现的批判性综述，也为未来设计或扩展人机共生系统提供了前瞻性的参考。


<details>
  <summary>Details</summary>
Motivation: 当前关于人类与AI代理协作的研究虽然取得了显著的技术成就，但仍然存在许多未解决的问题。特别是，在处理开放性、复杂任务时，缺乏一个统一的理论框架来整合各种不同的研究方向和技术成果。因此，需要提出一种新的概念架构，以促进这一领域的发展并推动人机协同进化。

Method: 作者通过分析现有的研究成果，包括符号AI技术、基于连接主义的大语言模型代理以及混合型组织实践，构建了一个名为分层探索-利用网络（Hierarchical Exploration-Exploitation Net）的新框架。该框架系统地连接了多代理协调、知识管理、控制论反馈回路和高层次控制机制等多个方面，并将这些元素映射到现有贡献中，从而揭示了传统方法的局限性并启发了新方向。

Result: 提出的框架不仅有助于重新审视传统的遗留方法，还能够激励研究者开发融合定性与定量范式的创新工作。此外，该框架为设计或扩展人类与AI代理之间的共生关系提供了理论支持，进一步推动了人机认知能力的共同进化。

Conclusion: 本文总结了人类与AI代理协作领域的现状，指出了其技术成就及存在的不足，并提出了一种新的概念框架。该框架可以作为未来研究的基础，帮助实现更深层次的人类认知与AI能力的协同进化。

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [2] [VeriReason: Reinforcement Learning with Testbench Feedback for Reasoning-Enhanced Verilog Generation](https://arxiv.org/abs/2505.11849)
*Yiting Wang,Guoheng Sun,Wanghao Ye,Gang Qu,Ang Li*

Main category: cs.AI

TL;DR: VeriReason是一个结合监督微调和强化学习的框架，专门用于RTL代码生成。通过整合测试平台评估、结构启发式方法以及自我检查能力，它在VerilogEval基准测试中实现了83.1%的功能正确性，并在首次尝试功能正确性方面比基线方法提高了2.8倍。这是首个将显式推理能力与强化学习结合用于Verilog生成的系统，开创了自动化RTL综合的新标准。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的方法在RTL代码生成中面临诸多挑战，包括训练数据稀缺、规范与代码对齐差、缺乏验证机制以及难以平衡泛化与专业化等问题。为了克服这些限制，提高RTL生成的质量和效率，需要一个更先进的解决方案。

Method: VeriReason框架采用监督微调与Guided Reward Proximal Optimization (GRPO)强化学习相结合的方法。利用精心策划的训练样本和反馈驱动的奖励模型，将测试平台评估与结构启发式方法结合起来，同时嵌入自我检查能力以实现自主错误修正。

Result: 在VerilogEval基准测试中，VeriReason达到了83.1%的功能正确性，在首次尝试功能正确性上比基线方法高出2.8倍，并且展现出对未见过设计的强大泛化能力。

Conclusion: VeriReason是首个成功将显式推理能力与强化学习结合用于Verilog生成的系统，为自动化RTL综合树立了新的标杆。

Abstract: Automating Register Transfer Level (RTL) code generation using Large Language
Models (LLMs) offers substantial promise for streamlining digital circuit
design and reducing human effort. However, current LLM-based approaches face
significant challenges with training data scarcity, poor specification-code
alignment, lack of verification mechanisms, and balancing generalization with
specialization. Inspired by DeepSeek-R1, we introduce VeriReason, a framework
integrating supervised fine-tuning with Guided Reward Proximal Optimization
(GRPO) reinforcement learning for RTL generation. Using curated training
examples and a feedback-driven reward model, VeriReason combines testbench
evaluations with structural heuristics while embedding self-checking
capabilities for autonomous error correction. On the VerilogEval Benchmark,
VeriReason delivers significant improvements: achieving 83.1% functional
correctness on the VerilogEval Machine benchmark, substantially outperforming
both comparable-sized models and much larger commercial systems like GPT-4
Turbo. Additionally, our approach demonstrates up to a 2.8X increase in
first-attempt functional correctness compared to baseline methods and exhibits
robust generalization to unseen designs. To our knowledge, VeriReason
represents the first system to successfully integrate explicit reasoning
capabilities with reinforcement learning for Verilog generation, establishing a
new state-of-the-art for automated RTL synthesis. The models and datasets are
available at: https://huggingface.co/collections/AI4EDA-CASE Code is Available
at: https://github.com/NellyW8/VeriReason

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Optimizing Anytime Reasoning via Budget Relative Policy Optimization](https://arxiv.org/abs/2505.13438)
*Penghui Qi,Zichen Liu,Tianyu Pang,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: 提升推理能力对于大语言模型至关重要。现有方法通常使用强化学习来最大化推理过程结束时获得的可验证奖励，但这些方法仅优化最终性能，导致训练和部署效率低下。本文提出了一种新框架AnytimeReasoner，旨在优化随时推理性能，提高token效率和推理灵活性。通过截断完整思考过程并引入可验证密集奖励，改进了强化学习中的信用分配。此外，还引入了一种新的方差减少技术Budget Relative Policy Optimization (BRPO)，以增强学习过程的稳健性和效率。实验结果表明，该方法在数学推理任务中显著优于GRPO，提高了训练和token效率。


<details>
  <summary>Details</summary>
Motivation: 现有的方法虽然能够通过强化学习优化大语言模型的推理能力，但它们主要关注在固定的大token预算下的最终性能，忽略了推理过程中不同阶段的优化需求，这限制了模型在实际应用中的效率和灵活性。因此，需要一种新的方法来解决这一问题，同时提升模型在不同token预算约束下的表现。

Method: 本文提出了AnytimeReasoner框架，通过从先验分布中采样token预算并截断完整的思考过程，使模型能够在每个截断点生成最优答案摘要以供验证。此过程引入了可验证的密集奖励，有助于强化学习中的有效信用分配。接着，作者采用分离的方式优化思考策略和摘要策略，以最大化累积奖励。此外，为了进一步提高学习过程的稳健性和效率，文章还引入了一种新的方差减少技术——Budget Relative Policy Optimization (BRPO)。

Result: 在数学推理任务的实验中，AnytimeReasoner方法在各种先验分布和不同思考预算下均显著优于GRPO，不仅提升了模型的训练效率，还大幅提高了token使用效率。

Conclusion: 本文提出的AnytimeReasoner框架成功优化了大语言模型在不同token预算下的推理性能，提高了token效率和推理灵活性。通过引入可验证密集奖励和新的方差减少技术BRPO，显著增强了强化学习过程的稳健性和效率。实验结果证明，该方法在数学推理任务中表现出色，为未来的研究提供了新的方向。

Abstract: Scaling test-time compute is crucial for enhancing the reasoning capabilities
of large language models (LLMs). Existing approaches typically employ
reinforcement learning (RL) to maximize a verifiable reward obtained at the end
of reasoning traces. However, such methods optimize only the final performance
under a large and fixed token budget, which hinders efficiency in both training
and deployment. In this work, we present a novel framework, AnytimeReasoner, to
optimize anytime reasoning performance, which aims to improve token efficiency
and the flexibility of reasoning under varying token budget constraints. To
achieve this, we truncate the complete thinking process to fit within sampled
token budgets from a prior distribution, compelling the model to summarize the
optimal answer for each truncated thinking for verification. This introduces
verifiable dense rewards into the reasoning process, facilitating more
effective credit assignment in RL optimization. We then optimize the thinking
and summary policies in a decoupled manner to maximize the cumulative reward.
Additionally, we introduce a novel variance reduction technique, Budget
Relative Policy Optimization (BRPO), to enhance the robustness and efficiency
of the learning process when reinforcing the thinking policy. Empirical results
in mathematical reasoning tasks demonstrate that our method consistently
outperforms GRPO across all thinking budgets under various prior distributions,
enhancing both training and token efficiency.

</details>


### [4] [Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs](https://arxiv.org/abs/2505.13026)
*Jack Chen,Fazhong Liu,Naruto Liu,Yuhan Luo,Erqu Qin,Harry Zheng,Tian Dong,Haojin Zhu,Yan Meng,Xiao Wang*

Main category: cs.LG

TL;DR: 提出了一种新的混合训练框架SASR，它动态平衡了监督微调(SFT)和强化学习(RL)，解决了单独使用这两种方法时的挑战。通过梯度范数和分布散度的自适应调整算法，SASR在不同任务上表现出更好的泛化能力，并优于SFT、RL和静态混合训练方法。


<details>
  <summary>Details</summary>
Motivation: 当前的大规模语言模型主要依赖监督微调(SFT)和强化学习(RL)来提升推理能力，但分别面临过拟合和模式崩溃的问题。现有的静态混合训练方案在不同任务上的泛化能力较差且对数据质量依赖较高。因此，需要一种能够动态平衡SFT和RL的训练方法以解决上述问题。

Method: SASR首先利用SFT进行初始热启动，以建立基本的推理技能；然后采用基于梯度范数和原始分布散度的自适应动态调整算法，将SFT与在线RL方法GRPO无缝结合。通过监控模型训练状态并按顺序调整训练过程，确保了不同训练方案之间的平滑过渡，同时保持核心推理能力并探索不同的路径。

Result: 实验结果表明，SASR在数学推理和逻辑问题解决方面超越了单独使用SFT、RL以及静态混合训练方法的表现，展现出更好的泛化能力和稳定性。

Conclusion: SASR作为一种步进式自适应混合训练框架，成功地统一了SFT和RL，并通过动态平衡两种方法克服了它们各自的局限性。该框架为大规模语言模型的推理能力训练提供了更优的解决方案，并展示了在不同任务上的广泛适用性和优越性能。

Abstract: Large language models (LLMs) excel at mathematical reasoning and logical
problem-solving. The current popular training paradigms primarily use
supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the
models' reasoning abilities. However, when using SFT or RL alone, there are
respective challenges: SFT may suffer from overfitting, while RL is prone to
mode collapse. The state-of-the-art methods have proposed hybrid training
schemes. However, static switching faces challenges such as poor generalization
across different tasks and high dependence on data quality. In response to
these challenges, inspired by the curriculum learning-quiz mechanism in human
reasoning cultivation, We propose SASR, a step-wise adaptive hybrid training
framework that theoretically unifies SFT and RL and dynamically balances the
two throughout optimization. SASR uses SFT for initial warm-up to establish
basic reasoning skills, and then uses an adaptive dynamic adjustment algorithm
based on gradient norm and divergence relative to the original distribution to
seamlessly integrate SFT with the online RL method GRPO. By monitoring the
training status of LLMs and adjusting the training process in sequence, SASR
ensures a smooth transition between training schemes, maintaining core
reasoning abilities while exploring different paths. Experimental results
demonstrate that SASR outperforms SFT, RL, and static hybrid training methods.

</details>


### [5] [DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management](https://arxiv.org/abs/2505.12951)
*Xuerui Su,Liya Guo,Yue Wang,Yi Zhu,Zhiming Ma,Zun Wang,Yuting Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习算法DGRO，用于提升大型语言模型的推理能力。通过解耦传统的正则化系数和优化奖励方差，该方法在探索与利用之间实现了更好的平衡，并在逻辑推理数据集上取得了96.9%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 当前大多数推理方法依赖手工设计的基于规则的奖励函数，但这些函数在理论和经验上的影响尚未得到充分研究，同时RL算法中的探索与利用权衡涉及复杂的考量。因此，需要一种更灵活且高效的RL算法来改进LLM的推理能力。

Method: 文章提出了Decoupled Group Reward Optimization (DGRO)算法，将传统的正则化系数分解为两个独立的超参数，分别控制策略梯度项和采样策略的距离。此外，还通过理论分析和实验验证了奖励方差对收敛速度和最终性能的影响，并进行了详细的消融研究以评估其表现和优化动态。

Result: DGRO在Logic数据集上达到了96.9%的平均准确率，展现了强大的泛化能力，并在数学基准测试中表现出色。

Conclusion: DGRO算法通过解耦策略梯度项和采样策略距离，以及优化奖励方差，显著提升了LLM的推理能力，并在多个基准测试中展现出优越的性能和良好的泛化性。这为未来LLM的推理能力优化提供了新的方向。

Abstract: Inference scaling further accelerates Large Language Models (LLMs) toward
Artificial General Intelligence (AGI), with large-scale Reinforcement Learning
(RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning
approaches usually rely on handcrafted rule-based reward functions. However,
the tarde-offs of exploration and exploitation in RL algorithms involves
multiple complex considerations, and the theoretical and empirical impacts of
manually designed reward functions remain insufficiently explored. In this
paper, we propose Decoupled Group Reward Optimization (DGRO), a general RL
algorithm for LLM reasoning. On the one hand, DGRO decouples the traditional
regularization coefficient into two independent hyperparameters: one scales the
policy gradient term, and the other regulates the distance from the sampling
policy. This decoupling not only enables precise control over balancing
exploration and exploitation, but also can be seamlessly extended to Online
Policy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward
Optimization. On the other hand, we observe that reward variance significantly
affects both convergence speed and final model performance. We conduct both
theoretical analysis and extensive empirical validation to assess DGRO,
including a detailed ablation study that investigates its performance and
optimization dynamics. Experimental results show that DGRO achieves
state-of-the-art performance on the Logic dataset with an average accuracy of
96.9\%, and demonstrates strong generalization across mathematical benchmarks.

</details>


### [6] [Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning](https://arxiv.org/abs/2505.12432)
*Zirun Guo,Minjie Hong,Tao Jin*

Main category: cs.LG

TL;DR: 本文提出了一种名为Observe-R1的新框架，通过强化学习（RL）逐步提升多模态大语言模型（MLLMs）的推理能力。该框架受到人类学习进程的启发，构建了一个名为NeuraLadder的数据集，并引入了多模态格式约束和奖励机制，以增强模型的视觉能力和推理质量。实验表明，Observe-R1在推理和通用基准测试中表现出色，且其策略在消融研究中得到了验证。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）在提高大语言模型（LLMs）的推理能力方面显示出潜力，但将RL适应于多模态数据和格式的具体挑战尚未得到充分探索。因此，需要一种新的框架来解决这些问题并提升多模态大语言模型（MLLMs）的推理能力。

Method: Observe-R1框架采用了渐进式学习范式，基于数据样本的难度和复杂性组织和采样NeuraLadder数据集。同时，引入多模态格式约束以促进对图像的仔细观察，以及奖励系统和动态加权机制，优先处理不确定性和中等难度的问题。这些方法共同作用，确保训练过程更有效，并生成更清晰、简洁的推理链。

Result: 实验结果表明，Observe-R1在Qwen2.5-VL-3B和Qwen2.5-VL-7B模型上显著优于一系列更大的推理模型，在推理和通用基准测试中表现优异，推理链更清晰、简洁。消融研究表明，所提出的策略是有效的，突显了方法的鲁棒性和泛化能力。

Conclusion: Observe-R1框架成功提升了多模态大语言模型的推理能力，证明了渐进式学习范式和多模态格式约束的有效性。未来的工作可以进一步扩展这种方法的应用范围，以解决更多复杂的多模态任务。数据集和代码已公开发布，为后续研究提供了宝贵的资源。

Abstract: Reinforcement Learning (RL) has shown promise in improving the reasoning
abilities of Large Language Models (LLMs). However, the specific challenges of
adapting RL to multimodal data and formats remain relatively unexplored. In
this work, we present Observe-R1, a novel framework aimed at enhancing the
reasoning capabilities of multimodal large language models (MLLMs). We draw
inspirations from human learning progression--from simple to complex and easy
to difficult, and propose a gradual learning paradigm for MLLMs. To this end,
we construct the NeuraLadder dataset, which is organized and sampled according
to the difficulty and complexity of data samples for RL training. To tackle
multimodal tasks, we introduce a multimodal format constraint that encourages
careful observation of images, resulting in enhanced visual abilities and
clearer and more structured responses. Additionally, we implement a bonus
reward system that favors concise, correct answers within a length constraint,
alongside a dynamic weighting mechanism that prioritizes uncertain and
medium-difficulty problems, ensuring that more informative samples have a
greater impact on training. Our experiments with the Qwen2.5-VL-3B and
Qwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that
Observe-R1 outperforms a series of larger reasoning models on both reasoning
and general benchmarks, achieving superior clarity and conciseness in reasoning
chains. Ablation studies validate the effectiveness of our strategies,
highlighting the robustness and generalization of our approach. The dataset and
code will be released at https://github.com/zrguo/Observe-R1.

</details>


### [7] [Graph-Reward-SQL: Execution-Free Reinforcement Learning for Text-to-SQL via Graph Matching and Stepwise Reward](https://arxiv.org/abs/2505.12380)
*Han Weng,Boyi Liu,Yuanfeng Song,Dun Zeng,Yingxiang Yang,Yi Zhan,Longjie Cui,Xiaoming Yin,Yang Sun*

Main category: cs.LG

TL;DR: 本文提出了一种新的文本到SQL的强化学习微调框架Graph-Reward-SQL，使用GMNScore结果奖励模型和StepRTM逐步奖励模型，有效减少了推理时间和GPU内存使用，同时提高了SQL查询的功能正确性和结构清晰度。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在文本到SQL任务中，要么因频繁数据库调用导致执行延迟高，要么因大语言模型带来显著的GPU内存开销，影响了效率和可扩展性。

Method: 提出了Graph-Reward-SQL框架，利用SQL图表示提供精确奖励信号并减少推理时间和GPU内存使用；引入StepRTM逐步奖励模型对公共表表达式子查询进行中间监督，以提高SQL查询的功能正确性和结构清晰度。

Result: 通过在Spider和BIRD等标准基准上的广泛比较和消融实验，证明了所提出的方法始终优于现有的奖励模型。

Conclusion: Graph-Reward-SQL框架及其组件StepRTM在提升文本到SQL任务性能的同时，显著降低了计算资源需求，为未来的研究提供了新的方向。

Abstract: Reinforcement learning (RL) has been widely adopted to enhance the
performance of large language models (LLMs) on Text-to-SQL tasks. However,
existing methods often rely on execution-based or LLM-based Bradley-Terry
reward models. The former suffers from high execution latency caused by
repeated database calls, whereas the latter imposes substantial GPU memory
overhead, both of which significantly hinder the efficiency and scalability of
RL pipelines. To this end, we propose a novel Text-to-SQL RL fine-tuning
framework named Graph-Reward-SQL, which employs the GMNScore outcome reward
model. We leverage SQL graph representations to provide accurate reward signals
while significantly reducing inference time and GPU memory usage. Building on
this foundation, we further introduce StepRTM, a stepwise reward model that
provides intermediate supervision over Common Table Expression (CTE)
subqueries. This encourages both functional correctness and structural clarity
of SQL. Extensive comparative and ablation experiments on standard benchmarks,
including Spider and BIRD, demonstrate that our method consistently outperforms
existing reward models.

</details>


### [8] [Self-Destructive Language Model](https://arxiv.org/abs/2505.12186)
*Yuhui Wang,Rongyi Zhu,Ting Wang*

Main category: cs.LG

TL;DR: 介绍了一种新的防御方法SEAM，通过使大型语言模型在有害数据微调时性能显著下降来增强对齐安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的防御措施虽然试图强化大型语言模型的对齐，但未能解决模型在有害数据上固有的可训练性问题，导致对更强攻击的脆弱性。

Method: 引入SEAM，一种新的对齐增强防御方法，通过特定损失函数和对抗梯度上升技术，使模型在合法任务上保持能力，而在有害数据微调时性能下降。同时开发了高效的无Hessian梯度估计方法以实现实际训练。

Result: 广泛评估表明，SEAM创建了一个对攻击者无利可图的局面：在低强度攻击下达到最先进的鲁棒性，在高强度攻击下导致灾难性的性能崩溃。

Conclusion: SEAM为大型语言模型提供了一种有效的防御机制，使其对错位尝试具有内在的弹性，从而增强了对齐安全性。

Abstract: Harmful fine-tuning attacks pose a major threat to the security of large
language models (LLMs), allowing adversaries to compromise safety guardrails
with minimal harmful data. While existing defenses attempt to reinforce LLM
alignment, they fail to address models' inherent "trainability" on harmful
data, leaving them vulnerable to stronger attacks with increased learning rates
or larger harmful datasets. To overcome this critical limitation, we introduce
SEAM, a novel alignment-enhancing defense that transforms LLMs into
self-destructive models with intrinsic resilience to misalignment attempts.
Specifically, these models retain their capabilities for legitimate tasks while
exhibiting substantial performance degradation when fine-tuned on harmful data.
The protection is achieved through a novel loss function that couples the
optimization trajectories of benign and harmful data, enhanced with adversarial
gradient ascent to amplify the self-destructive effect. To enable practical
training, we develop an efficient Hessian-free gradient estimate with
theoretical error bounds. Extensive evaluation across LLMs and datasets
demonstrates that SEAM creates a no-win situation for adversaries: the
self-destructive models achieve state-of-the-art robustness against
low-intensity attacks and undergo catastrophic performance collapse under
high-intensity attacks, rendering them effectively unusable. (warning: this
paper contains potentially harmful content generated by LLMs.)

</details>


### [9] [AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning](https://arxiv.org/abs/2505.11896)
*Chenwei Lou,Zewei Sun,Xinnian Liang,Meng Qu,Wei Shen,Wenqi Wang,Yuntao Li,Qingping Yang,Shuangzhi Wu*

Main category: cs.LG

TL;DR: AdaCoT是一种新的框架，它通过强化学习方法使大语言模型能够自适应地决定何时使用Chain-of-Thought（CoT）提示，从而在保证复杂任务性能的同时减少了不必要的计算成本。


<details>
  <summary>Details</summary>
Motivation: 尽管Chain-of-Thought（CoT）提示显著增强了大语言模型的推理能力，但其为所有查询生成详细的推理步骤，导致了较高的计算成本和效率低下，特别是在处理简单输入时。因此需要一种方法来平衡模型性能与CoT调用的成本。

Method: AdaCoT将自适应推理问题建模为帕累托优化问题，目标是平衡模型性能和CoT调用的成本。采用基于强化学习的方法，特别是近端策略优化（PPO），通过调整惩罚系数动态控制CoT触发决策边界，让模型根据隐式查询复杂度判断是否需要CoT。同时提出了选择性损失屏蔽（SLM）技术以防止多阶段RL训练期间决策边界的崩溃，确保自适应触发的稳健性和稳定性。

Result: 实验结果表明，AdaCoT成功地在帕累托前沿进行导航，大幅降低了不需要复杂推理的查询中的CoT使用率。例如，在生产流量测试集中，AdaCoT将CoT触发率降低到3.18%，并减少了69.06%的平均响应标记，同时在复杂任务上保持高性能。

Conclusion: AdaCoT提供了一种有效的解决方案，能够在减少不必要CoT使用的同时，维持甚至提升大语言模型在复杂任务上的表现，实现了性能和计算成本之间的良好权衡。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities but
often face challenges with tasks requiring sophisticated reasoning. While
Chain-of-Thought (CoT) prompting significantly enhances reasoning, it
indiscriminately generates lengthy reasoning steps for all queries, leading to
substantial computational costs and inefficiency, especially for simpler
inputs. To address this critical issue, we introduce AdaCoT (Adaptive
Chain-of-Thought), a novel framework enabling LLMs to adaptively decide when to
invoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem
that seeks to balance model performance with the costs associated with CoT
invocation (both frequency and computational overhead). We propose a
reinforcement learning (RL) based method, specifically utilizing Proximal
Policy Optimization (PPO), to dynamically control the CoT triggering decision
boundary by adjusting penalty coefficients, thereby allowing the model to
determine CoT necessity based on implicit query complexity. A key technical
contribution is Selective Loss Masking (SLM), designed to counteract decision
boundary collapse during multi-stage RL training, ensuring robust and stable
adaptive triggering. Experimental results demonstrate that AdaCoT successfully
navigates the Pareto frontier, achieving substantial reductions in CoT usage
for queries not requiring elaborate reasoning. For instance, on our production
traffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18\% and
decreased average response tokens by 69.06%, while maintaining high performance
on complex tasks.

</details>


### [10] [J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge](https://arxiv.org/abs/2505.11875)
*Chi-Min Chan,Chunpu Xu,Jiaming Ji,Zhen Ye,Pengcheng Wen,Chunyang Jiang,Yaodong Yang,Wei Xue,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: 本文提出了一种名为J1-7B的模型，通过强化学习和可验证奖励进行训练，并在推理时应用简单的测试时间扩展策略（STTS），以提升性能和解释性。实验表明，J1-7B超越了之前的LLM-as-a-Judge模型4.8%，并在STTS下表现出5.1%更强的扩展趋势。研究发现，显著的扩展趋势主要在强化学习阶段形成。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究的重点正在从强调模型训练转向提高评估质量。传统的基于奖励模型的评估方法虽然有效，但缺乏解释性，而大规模推理模型可以通过更多的token进行深入思考和答案优化，进一步提升性能和解释性。因此，引入更可扩展和可解释的监督方法成为必要。

Method: 首先，在拒绝采样收集的反思增强数据集上对模型进行监督微调；然后使用带有可验证奖励的强化学习进行训练。在推理阶段，采用简单测试时间扩展（STTS）策略以进一步提升性能。

Result: 实验结果表明，J1-7B模型超越了之前最先进的LLM-as-a-Judge模型4.8%，并且在STTS下表现出5.1%更强的扩展趋势。此外，研究发现现有的LLM-as-a-Judge本身不具有这种扩展趋势，仅在反思增强数据集上微调的模型也表现出较弱的扩展行为，而显著的扩展趋势主要在强化学习阶段出现。

Conclusion: 本文提出的J1-7B模型通过结合强化学习和测试时间扩展策略，显著提升了评估质量和性能。研究还揭示了扩展趋势主要在强化学习阶段形成，为未来进一步改进评估方法提供了重要启示。

Abstract: The current focus of AI research is shifting from emphasizing model training
towards enhancing evaluation quality, a transition that is crucial for driving
further advancements in AI systems. Traditional evaluation methods typically
rely on reward models assigning scalar preference scores to outputs. Although
effective, such approaches lack interpretability, leaving users often uncertain
about why a reward model rates a particular response as high or low. The advent
of LLM-as-a-Judge provides a more scalable and interpretable method of
supervision, offering insights into the decision-making process. Moreover, with
the emergence of large reasoning models, which consume more tokens for deeper
thinking and answer refinement, scaling test-time computation in the
LLM-as-a-Judge paradigm presents an avenue for further boosting performance and
providing more interpretability through reasoning traces. In this paper, we
introduce $\textbf{J1-7B}$, which is first supervised fine-tuned on
reflection-enhanced datasets collected via rejection-sampling and subsequently
trained using Reinforcement Learning (RL) with verifiable rewards. At inference
time, we apply Simple Test-Time Scaling (STTS) strategies for additional
performance improvement. Experimental results demonstrate that $\textbf{J1-7B}$
surpasses the previous state-of-the-art LLM-as-a-Judge by $ \textbf{4.8}$\% and
exhibits a $ \textbf{5.1}$\% stronger scaling trend under STTS. Additionally,
we present three key findings: (1) Existing LLM-as-a-Judge does not inherently
exhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced
datasets continues to demonstrate similarly weak scaling behavior. (3)
Significant scaling trend emerges primarily during the RL phase, suggesting
that effective STTS capability is acquired predominantly through RL training.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 在大规模语言模型（LLMs）中，通过重复采样扩展推理计算会随着样本数量的增加而持续提高覆盖率（解决问题的比例）。研究发现这种改进部分归因于评估基准答案分布的偏差，并提出了一种基于训练集答案频率的基线方法，能够更准确地衡量重复采样的实际效果。


<details>
  <summary>Details</summary>
Motivation: 重复采样在大语言模型中的推理任务中能显著提升问题解决的覆盖率。然而，研究者推测这一提升可能部分源于标准评估基准中答案分布的偏倚（即答案集中于一小部分常见答案）。为了验证这一假设并更准确地衡量重复采样的效果，需要设计一种基线方法来模拟这种答案分布特性。

Method: 研究者定义了一个基线方法，该方法根据训练集中答案的频率枚举答案。通过在数学推理和事实知识两个领域进行实验，比较了此基线方法与重复模型采样的表现。此外，还引入了一种混合策略，该策略仅使用10次模型采样，剩余的答案则通过枚举猜测完成。

Result: 实验结果显示，在某些大语言模型中，基于训练集答案频率的基线方法优于重复模型采样；而在其他模型中，其表现与混合策略相当，即用少量模型采样结合枚举猜测能达到相似的覆盖率。

Conclusion: 该研究提出的方法提供了一种更精确的方式，用于测量在特定设置下重复采样相对于无提示猜测所能带来的额外覆盖提升。这有助于更好地理解重复采样的实际作用以及评估基准答案分布对结果的影响。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [12] [Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs](https://arxiv.org/abs/2505.12929)
*Zhihe Yang,Xufang Luo,Zilong Wang,Dongqi Han,Zhiyuan He,Dongsheng Li,Yunjian Xu*

Main category: cs.CL

TL;DR: 本文研究了强化学习训练中低概率token对模型更新的不成比例影响问题，并提出了两种新方法：Advantage Reweighting和Low-Probability Token Isolation (Lopti)，以减弱低概率token的梯度并强调高概率token的参数更新。实验表明，这两种方法显著提高了使用GRPO训练的大语言模型在逻辑推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）在提升大语言模型（LLMs）推理能力方面取得了显著进展，但低概率token由于其较大的梯度幅度对模型更新产生过大的影响，这一问题尚未得到充分探索。这种影响压制了高概率token的梯度，而高概率token对于LLMs的性能至关重要。

Method: 作者提出了两种方法来解决上述问题：1. Advantage Reweighting，重新调整优势函数以减少低概率token的影响；2. Low-Probability Token Isolation (Lopti)，隔离低概率token以避免它们干扰模型更新。这两种方法旨在平衡不同概率token之间的梯度更新。

Result: 实验结果表明，提出的方法显著提高了基于GRPO训练的大语言模型的性能，在K&K逻辑谜题推理任务中实现了高达46.2%的性能提升。

Conclusion: 通过引入Advantage Reweighting和Lopti，可以有效减轻低概率token对强化学习训练过程中的负面影响，同时提高高概率token的学习效率，从而增强LLMs的推理能力。这些方法为更高效的RL训练提供了一种新的思路。

Abstract: Reinforcement learning (RL) has become a cornerstone for enhancing the
reasoning capabilities of large language models (LLMs), with recent innovations
such as Group Relative Policy Optimization (GRPO) demonstrating exceptional
effectiveness. In this study, we identify a critical yet underexplored issue in
RL training: low-probability tokens disproportionately influence model updates
due to their large gradient magnitudes. This dominance hinders the effective
learning of high-probability tokens, whose gradients are essential for LLMs'
performance but are substantially suppressed. To mitigate this interference, we
propose two novel methods: Advantage Reweighting and Low-Probability Token
Isolation (Lopti), both of which effectively attenuate gradients from
low-probability tokens while emphasizing parameter updates driven by
high-probability tokens. Our approaches promote balanced updates across tokens
with varying probabilities, thereby enhancing the efficiency of RL training.
Experimental results demonstrate that they substantially improve the
performance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K
Logic Puzzle reasoning tasks. Our implementation is available at
https://github.com/zhyang2226/AR-Lopti.

</details>
