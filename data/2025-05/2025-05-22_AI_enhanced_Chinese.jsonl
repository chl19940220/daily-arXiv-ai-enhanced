{"id": "2505.00018", "keyword": "LLM Agent", "pdf": "https://arxiv.org/pdf/2505.00018", "abs": "https://arxiv.org/abs/2505.00018", "authors": ["Ju Wu", "Calvin K. L. Or"], "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability.", "keywords": ["LLM Agent"], "AI": {"tldr": "\u8fd9\u7bc7\u6587\u7ae0\u662f\u4e00\u7bc7\u7acb\u573a\u8bba\u6587\uff0c\u5168\u9762\u56de\u987e\u4e86\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u534f\u4f5c\u7684\u6700\u65b0\u7ecf\u9a8c\u6027\u8fdb\u5c55\uff0c\u5f3a\u8c03\u5176\u6280\u672f\u6210\u5c31\u548c\u6301\u7eed\u5b58\u5728\u7684\u5dee\u8ddd\u3002\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u5ff5\u67b6\u6784\u2014\u2014\u5206\u5c42\u63a2\u7d22-\u5229\u7528\u7f51\u7edc\uff08Hierarchical Exploration-Exploitation Net\uff09\uff0c\u65e8\u5728\u7cfb\u7edf\u5730\u6574\u5408\u591a\u4ee3\u7406\u534f\u8c03\u3001\u77e5\u8bc6\u7ba1\u7406\u3001\u63a7\u5236\u8bba\u53cd\u9988\u5faa\u73af\u548c\u9ad8\u5c42\u63a7\u5236\u673a\u5236\u7684\u6280\u672f\u7ec6\u8282\u3002\u8be5\u6846\u67b6\u4e0d\u4ec5\u6709\u52a9\u4e8e\u91cd\u65b0\u8bc4\u4f30\u4f20\u7edf\u65b9\u6cd5\uff0c\u8fd8\u4e3a\u878d\u5408\u5b9a\u6027\u548c\u5b9a\u91cf\u8303\u5f0f\u7684\u65b0\u5de5\u4f5c\u63d0\u4f9b\u4e86\u7075\u611f\u3002\u6b64\u5916\uff0c\u6587\u7ae0\u7ed3\u6784\u7075\u6d3b\uff0c\u53ef\u4ece\u4efb\u4f55\u90e8\u5206\u5f00\u59cb\u9605\u8bfb\uff0c\u65e2\u662f\u5bf9\u73b0\u6709\u6280\u672f\u5b9e\u73b0\u7684\u6279\u5224\u6027\u7efc\u8ff0\uff0c\u4e5f\u4e3a\u8bbe\u8ba1\u6216\u6269\u5c55\u4eba\u7c7b-AI\u5171\u751f\u5173\u7cfb\u63d0\u4f9b\u4e86\u524d\u77bb\u6027\u53c2\u8003\u3002\u8fd9\u4e9b\u89c1\u89e3\u5171\u540c\u63a8\u52a8\u4e86\u4eba\u7c7b\u8ba4\u77e5\u4e0eAI\u80fd\u529b\u7684\u6df1\u5ea6\u534f\u540c\u8fdb\u5316\u3002", "motivation": "\u5c3d\u7ba1\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u7684\u534f\u4f5c\u5728\u8bb8\u591a\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6280\u672f\u8fdb\u6b65\uff0c\u4f46\u76ee\u524d\u4ecd\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u8fde\u8d2f\u5730\u6574\u5408\u8fd9\u4e9b\u591a\u6837\u5316\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5f00\u653e\u6027\u590d\u6742\u4efb\u52a1\u65f6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7efc\u5408\u5404\u79cd\u6280\u672f\u548c\u65b9\u6cd5\u7684\u65b0\u67b6\u6784\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u6df1\u5c42\u6b21\u7684\u4eba\u7c7b\u4e0eAI\u534f\u540c\u8fdb\u5316\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5206\u5c42\u63a2\u7d22-\u5229\u7528\u7f51\u7edc\uff08Hierarchical Exploration-Exploitation Net\uff09\u7684\u6982\u5ff5\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5c06\u73b0\u6709\u7684\u8d21\u732e\uff08\u5305\u62ec\u7b26\u53f7AI\u6280\u672f\u3001\u8fde\u63a5\u4e3b\u4e49\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u4ee5\u53ca\u6df7\u5408\u7ec4\u7ec7\u5b9e\u8df5\uff09\u6620\u5c04\u5230\u4e00\u4e2a\u591a\u5c42\u9762\u7684\u6846\u67b6\u4e2d\u3002\u901a\u8fc7\u8fd9\u4e00\u6846\u67b6\uff0c\u53ef\u4ee5\u7cfb\u7edf\u5730\u8fde\u63a5\u591a\u4ee3\u7406\u534f\u8c03\u3001\u77e5\u8bc6\u7ba1\u7406\u3001\u63a7\u5236\u8bba\u53cd\u9988\u5faa\u73af\u548c\u9ad8\u5c42\u63a7\u5236\u673a\u5236\u7b49\u5173\u952e\u8981\u7d20\u3002", "result": "\u8be5\u8bba\u6587\u901a\u8fc7\u63d0\u51fa\u7684\u65b0\u6846\u67b6\uff0c\u6210\u529f\u5730\u4e3a\u73b0\u6709\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u91cd\u65b0\u5ba1\u89c6\u7684\u89c6\u89d2\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u5e94\u7528\u6307\u660e\u4e86\u65b9\u5411\u3002\u5b83\u4fc3\u8fdb\u4e86\u5bf9\u4f20\u7edf\u65b9\u6cd5\u7684\u6539\u8fdb\uff0c\u5e76\u542f\u53d1\u4e86\u7ed3\u5408\u5b9a\u6027\u548c\u5b9a\u91cf\u8303\u5f0f\u7684\u521b\u65b0\u7814\u7a76\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u5f53\u524d\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u534f\u4f5c\u9886\u57df\u7684\u6280\u672f\u6210\u5c31\u4e0e\u4e0d\u8db3\u4e4b\u5904\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u6982\u5ff5\u67b6\u6784\uff0c\u4ee5\u89e3\u51b3\u7f3a\u4e4f\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u7684\u95ee\u9898\u3002\u8fd9\u4e0d\u4ec5\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u4e5f\u63a8\u52a8\u4e86\u4eba\u7c7b\u8ba4\u77e5\u4e0eAI\u80fd\u529b\u7684\u534f\u540c\u8fdb\u5316\uff0c\u4ece\u800c\u5411\u66f4\u6df1\u5c42\u6b21\u7684\u4eba\u673a\u534f\u4f5c\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2410.15466", "keyword": "LLM reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing.", "keywords": ["LLM reasoning"], "AI": {"tldr": "\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\uff0c\u901a\u8fc7\u91cd\u590d\u91c7\u6837\u6765\u6269\u5c55\u63a8\u7406\u8ba1\u7b97\u53ef\u4ee5\u6301\u7eed\u589e\u52a0\u8986\u76d6\u7387\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u63d0\u5347\u90e8\u5206\u53ef\u80fd\u662f\u56e0\u4e3a\u6807\u51c6\u8bc4\u4f30\u57fa\u51c6\u4e2d\u7684\u7b54\u6848\u5206\u5e03\u504f\u5411\u4e8e\u5c11\u91cf\u5e38\u89c1\u7b54\u6848\u3002\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u5b83\u5728\u67d0\u4e9bLLM\u4e0a\u4f18\u4e8e\u91cd\u590d\u91c7\u6837\uff0c\u800c\u5728\u5176\u4ed6\u6a21\u578b\u4e0a\u4e0e\u6df7\u5408\u7b56\u7565\u8868\u73b0\u76f8\u5f53\u3002\u6b64\u57fa\u7ebf\u6709\u52a9\u4e8e\u66f4\u7cbe\u786e\u5730\u6d4b\u91cf\u91cd\u590d\u91c7\u6837\u5728\u975e\u63d0\u793a\u76f8\u5173\u731c\u6d4b\u4e4b\u5916\u7684\u5b9e\u9645\u6539\u8fdb\u3002", "motivation": "\u5c3d\u7ba1\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u901a\u8fc7\u91cd\u590d\u91c7\u6837\u53ef\u4ee5\u63d0\u9ad8\u95ee\u9898\u89e3\u51b3\u7684\u8986\u76d6\u7387\uff0c\u4f46\u7814\u7a76\u8005\u63a8\u6d4b\u8fd9\u4e00\u63d0\u5347\u53ef\u80fd\u90e8\u5206\u5f52\u56e0\u4e8e\u6807\u51c6\u8bc4\u4f30\u57fa\u51c6\u4e2d\u7b54\u6848\u5206\u5e03\u7684\u504f\u5dee\uff0c\u5373\u7b54\u6848\u96c6\u4e2d\u4e8e\u4e00\u5c0f\u90e8\u5206\u5e38\u89c1\u7b54\u6848\u3002\u56e0\u6b64\uff0c\u4ed6\u4eec\u5e0c\u671b\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\u5e76\u63a2\u7d22\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u8005\u5b9a\u4e49\u4e86\u4e00\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6839\u636e\u8bad\u7ec3\u96c6\u4e2d\u7b54\u6848\u7684\u51fa\u73b0\u9891\u7387\u679a\u4e3e\u7b54\u6848\u3002\u968f\u540e\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u4e8b\u5b9e\u77e5\u8bc6\u4e24\u4e2a\u9886\u57df\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u8fd9\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3001\u91cd\u590d\u6a21\u578b\u91c7\u6837\u4ee5\u53ca\u4e00\u79cd\u6df7\u5408\u7b56\u7565\u7684\u8868\u73b0\u3002\u6df7\u5408\u7b56\u7565\u7ed3\u5408\u4e86\u5c11\u91cf\u6a21\u578b\u91c7\u6837\u548c\u57fa\u4e8e\u679a\u4e3e\u7684\u731c\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u67d0\u4e9bLLM\u4e0a\uff0c\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\u4f18\u4e8e\u91cd\u590d\u91c7\u6837\uff1b\u800c\u5728\u5176\u4ed6\u6a21\u578b\u4e0a\uff0c\u5176\u8868\u73b0\u4e0e\u6df7\u5408\u7b56\u7565\u76f8\u5f53\uff0c\u540e\u8005\u4ec5\u4f7f\u7528\u5c11\u91cf\u6a21\u578b\u91c7\u6837\u5e76\u7ed3\u5408\u679a\u4e3e\u731c\u6d4b\u5b8c\u6210\u5176\u4f59\u5c1d\u8bd5\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7814\u7a76\u8005\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u91cd\u590d\u91c7\u6837\u5728\u975e\u63d0\u793a\u76f8\u5173\u731c\u6d4b\u4e4b\u5916\u5bf9\u8986\u76d6\u7387\u7684\u5b9e\u9645\u6539\u8fdb\u3002\u8fd9\u4e3a\u672a\u6765\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u53c2\u8003\u3002"}}
{"id": "2505.15793", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.15793", "abs": "https://arxiv.org/abs/2505.15793", "authors": ["Zhiwen Chen", "Bo Leng", "Zhuoren Li", "Hanming Deng", "Guizhe Jin", "Ran Yu", "Huanxi Wen"], "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations.Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7ed3\u5408\u53ef\u4ee5\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u4ee5LLM\u4e3a\u4e3b\u5bfc\u7684\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u4e8eLLM\u8f93\u51fa\uff0c\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u8f85\u52a9\u7684RL\u8303\u5f0f\uff0c\u901a\u8fc7\u8bed\u4e49\u63d0\u793a\u589e\u5f3a\u72b6\u6001\u8868\u793a\u548c\u4f18\u5316\u7b56\u7565\uff0c\u540c\u65f6RL\u4ee3\u7406\u901a\u8fc7\u7b56\u7565\u5b66\u4e60\u62b5\u6d88\u6f5c\u5728\u7684\u9519\u8bef\u8bed\u4e49\u6307\u793a\u3002\u57fa\u4e8e\u6b64\u8303\u5f0f\uff0c\u8bbe\u8ba1\u4e86HCRMP\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5305\u62ec\u589e\u5f3a\u8bed\u4e49\u8868\u793a\u6a21\u5757\u3001\u4e0a\u4e0b\u6587\u7a33\u5b9a\u6027\u951a\u5b9a\u6a21\u5757\u548c\u8bed\u4e49\u7f13\u5b58\u6a21\u5757\u3002\u5b9e\u9a8c\u8868\u660e\uff0cHCRMP\u5728\u4e0d\u540c\u4ea4\u901a\u5bc6\u5ea6\u4e0b\u4efb\u52a1\u6210\u529f\u7387\u9ad8\u8fbe80.3%\uff0c\u5e76\u5728\u5173\u952e\u5b89\u5168\u9a7e\u9a76\u6761\u4ef6\u4e0b\u663e\u8457\u964d\u4f4e\u4e8611.4%\u7684\u78b0\u649e\u7387\u3002", "motivation": "\u5f53\u524dLLM\u4e3b\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u8868\u73b0\u51fa\u4e86\u5e7b\u89c9\u95ee\u9898\uff0c\u5373LLM\u8f93\u51fa\u53ef\u80fd\u5305\u542b\u4e0d\u51c6\u786e\u6216\u9519\u8bef\u7684\u4fe1\u606f\u3002\u8fd9\u4e9b\u5e7b\u89c9\u76f4\u63a5\u5a01\u80c1\u5230\u9a7e\u9a76\u7b56\u7565\u7684\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u5f97LLM\u548cRL\u4e4b\u95f4\u7684\u5173\u7cfb\u66f4\u52a0\u72ec\u7acb\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86LLM-Hinted RL\u8303\u5f0f\uff0c\u5176\u4e2dLLM\u7528\u4e8e\u751f\u6210\u8bed\u4e49\u63d0\u793a\u4ee5\u589e\u5f3a\u72b6\u6001\u8868\u793a\u548c\u4f18\u5316\u7b56\u7565\uff0c\u800cRL\u4ee3\u7406\u5219\u901a\u8fc7\u7b56\u7565\u5b66\u4e60\u6765\u62b5\u6d88\u53ef\u80fd\u7684\u9519\u8bef\u8bed\u4e49\u63d0\u793a\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8bbe\u8ba1\u4e86HCRMP\u67b6\u6784\uff0c\u5305\u62ec\uff1a\n1. \u589e\u5f3a\u8bed\u4e49\u8868\u793a\u6a21\u5757\uff1a\u6269\u5c55\u72b6\u6001\u7a7a\u95f4\u3002\n2. \u4e0a\u4e0b\u6587\u7a33\u5b9a\u6027\u951a\u5b9a\u6a21\u5757\uff1a\u5229\u7528\u77e5\u8bc6\u5e93\u4fe1\u606f\u589e\u5f3a\u591a\u6279\u8bc4\u6743\u91cd\u63d0\u793a\u7684\u53ef\u9760\u6027\u3002\n3. \u8bed\u4e49\u7f13\u5b58\u6a21\u5757\uff1a\u5c06LLM\u4f4e\u9891\u6307\u5bfc\u4e0eRL\u9ad8\u9891\u63a7\u5236\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728CARLA\u4eff\u771f\u73af\u5883\u4e2d\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHCRMP\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6574\u4f53\u9a7e\u9a76\u6027\u80fd\uff1a\n- \u5728\u4e0d\u540c\u4ea4\u901a\u5bc6\u5ea6\u4e0b\uff0c\u4efb\u52a1\u6210\u529f\u7387\u8fbe\u523080.3%\u3002\n- \u5728\u5173\u952e\u5b89\u5168\u9a7e\u9a76\u6761\u4ef6\u4e0b\uff0c\u78b0\u649e\u7387\u964d\u4f4e11.4%\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u9a7e\u9a76\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM-Hinted RL\u8303\u5f0f\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86HCRMP\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5e7b\u89c9\u95ee\u9898\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u6027\u80fd\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHCRMP\u4e0d\u4ec5\u5728\u591a\u79cd\u9a7e\u9a76\u6761\u4ef6\u4e0b\u5177\u6709\u9ad8\u6210\u529f\u7387\uff0c\u8fd8\u5728\u5173\u952e\u5b89\u5168\u9a7e\u9a76\u6761\u4ef6\u4e0b\u663e\u8457\u964d\u4f4e\u4e86\u78b0\u649e\u7387\uff0c\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u9a7e\u9a76\u6027\u80fd\u3002"}}
{"id": "2505.15311", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.15311", "abs": "https://arxiv.org/abs/2505.15311", "authors": ["Yurun Yuan", "Fan Chen", "Zeyu Jia", "Alexander Rakhlin", "Tengyang Xie"], "title": "Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Policy-based methods currently dominate reinforcement learning (RL) pipelines\nfor large language model (LLM) reasoning, leaving value-based approaches\nlargely unexplored. We revisit the classical paradigm of Bellman Residual\nMinimization and introduce Trajectory Bellman Residual Minimization (TBRM), an\nalgorithm that naturally adapts this idea to LLMs, yielding a simple yet\neffective off-policy algorithm that optimizes a single trajectory-level Bellman\nobjective using the model's own logits as $Q$-values. TBRM removes the need for\ncritics, importance-sampling ratios, or clipping, and operates with only one\nrollout per prompt. We prove convergence to the near-optimal KL-regularized\npolicy from arbitrary off-policy data via an improved\nchange-of-trajectory-measure analysis. Experiments on standard\nmathematical-reasoning benchmarks show that TBRM consistently outperforms\npolicy-based baselines, like PPO and GRPO, with comparable or lower\ncomputational and memory overhead. Our results indicate that value-based RL\nmight be a principled and efficient alternative for enhancing reasoning\ncapabilities in LLMs.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u8f68\u8ff9\u7684\u8d1d\u5c14\u66fc\u6b8b\u5dee\u6700\u5c0f\u5316\u7b97\u6cd5\uff08TBRM\uff09\uff0c\u5c06\u7ecf\u5178\u7684\u8d1d\u5c14\u66fc\u6b8b\u5dee\u6700\u5c0f\u5316\u65b9\u6cd5\u5e94\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u6279\u8bc4\u5bb6\u3001\u91cd\u8981\u6027\u91c7\u6837\u6216\u526a\u8f91\u7684\u7b80\u5355\u6709\u6548\u7684\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTBRM\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u57fa\u4e8e\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u5982PPO\u548cGRPO\uff0c\u5e76\u5177\u6709\u76f8\u5f53\u6216\u66f4\u4f4e\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7ba1\u9053\u4e3b\u8981\u4f9d\u8d56\u4e8e\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u800c\u57fa\u4e8e\u4ef7\u503c\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u91cd\u65b0\u5ba1\u89c6\u4e86\u7ecf\u5178\u7684\u8d1d\u5c14\u66fc\u6b8b\u5dee\u6700\u5c0f\u5316\u65b9\u6cd5\uff0c\u5e0c\u671b\u5c06\u5176\u9002\u914d\u5230LLMs\u4e2d\uff0c\u4ee5\u63d0\u4f9b\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u539f\u5219\u6027\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTrajectory Bellman Residual Minimization (TBRM) \u7684\u65b0\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5229\u7528\u6a21\u578b\u81ea\u8eab\u7684logits\u4f5c\u4e3a$Q$-values\uff0c\u901a\u8fc7\u4f18\u5316\u5355\u6761\u8f68\u8ff9\u7ea7\u522b\u7684\u8d1d\u5c14\u66fc\u76ee\u6807\u51fd\u6570\u6765\u5b9e\u73b0\u5f3a\u5316\u5b66\u4e60\u3002TBRM\u4e0d\u9700\u8981\u6279\u8bc4\u5bb6\u3001\u91cd\u8981\u6027\u91c7\u6837\u6bd4\u7387\u6216\u526a\u8f91\u64cd\u4f5c\uff0c\u5e76\u4e14\u6bcf\u6b21\u63d0\u793a\u53ea\u9700\u8981\u4e00\u6b21rollout\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u901a\u8fc7\u6539\u8fdb\u7684\u8f68\u8ff9\u5ea6\u91cf\u5206\u6790\u8bc1\u660e\u4e86\u4ece\u4efb\u610f\u79bb\u7b56\u7565\u6570\u636e\u6536\u655b\u5230\u63a5\u8fd1\u6700\u4f18\u7684KL\u6b63\u5219\u5316\u7b56\u7565\u7684\u53ef\u80fd\u6027\u3002", "result": "\u5728\u6807\u51c6\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTBRM\u4e00\u81f4\u5730\u4f18\u4e8e\u57fa\u4e8e\u7b56\u7565\u7684\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982PPO\u548cGRPO\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u751a\u81f3\u66f4\u4f4e\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u3002\u8fd9\u8868\u660e\u57fa\u4e8e\u4ef7\u503c\u7684RL\u53ef\u80fd\u662f\u589e\u5f3aLLMs\u63a8\u7406\u80fd\u529b\u7684\u4e00\u79cd\u6709\u539f\u5219\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u57fa\u4e8e\u4ef7\u503c\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662fTBRM\u7b97\u6cd5\u5728\u63d0\u9ad8\u63a8\u7406\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u4e8e\u7b56\u7565\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u57fa\u4e8e\u4ef7\u503c\u7684RL\u53ef\u80fd\u662f\u4e00\u79cd\u66f4\u9ad8\u6548\u548c\u539f\u5219\u6027\u7684\u9009\u62e9\uff0c\u4e3a\u672a\u6765\u8fdb\u4e00\u6b65\u63a2\u7d22\u8fd9\u4e00\u9886\u57df\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.15306", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.15306", "abs": "https://arxiv.org/abs/2505.15306", "authors": ["Yiwen Song", "Qianyue Hao", "Qingmin Liao", "Jian Yuan", "Yong Li"], "title": "Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Model ensemble is a useful approach in reinforcement learning (RL) for\ntraining effective agents. Despite wide success of RL, training effective\nagents remains difficult due to the multitude of factors requiring careful\ntuning, such as algorithm selection, hyperparameter settings, and even random\nseed choices, all of which can significantly influence an agent's performance.\nModel ensemble helps overcome this challenge by combining multiple weak agents\ninto a single, more powerful one, enhancing overall performance. However,\nexisting ensemble methods, such as majority voting and Boltzmann addition, are\ndesigned as fixed strategies and lack a semantic understanding of specific\ntasks, limiting their adaptability and effectiveness. To address this, we\npropose LLM-Ens, a novel approach that enhances RL model ensemble with\ntask-specific semantic understandings driven by large language models (LLMs).\nGiven a task, we first design an LLM to categorize states in this task into\ndistinct 'situations', incorporating high-level descriptions of the task\nconditions. Then, we statistically analyze the strengths and weaknesses of each\nindividual agent to be used in the ensemble in each situation. During the\ninference time, LLM-Ens dynamically identifies the changing task situation and\nswitches to the agent that performs best in the current situation, ensuring\ndynamic model selection in the evolving task condition. Our approach is\ndesigned to be compatible with agents trained with different random seeds,\nhyperparameter settings, and various RL algorithms. Extensive experiments on\nthe Atari benchmark show that LLM-Ens significantly improves the RL model\nensemble, surpassing well-known baselines by up to 20.9%. For reproducibility,\nour code is open-source at\nhttps://anonymous.4open.science/r/LLM4RLensemble-F7EE.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLLM-Ens\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6a21\u578b\u96c6\u6210\u7684\u6548\u679c\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bed\u4e49\u7406\u89e3\u4efb\u52a1\u6761\u4ef6\u5e76\u52a8\u6001\u9009\u62e9\u6700\u4f73\u4ee3\u7406\uff0c\u5728Atari\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf20.9%\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u53d6\u5f97\u4e86\u5e7f\u6cdb\u7684\u6210\u529f\uff0c\u4f46\u8bad\u7ec3\u6709\u6548\u7684\u667a\u80fd\u4f53\u4ecd\u7136\u56f0\u96be\u91cd\u91cd\u3002\u7531\u4e8e\u7b97\u6cd5\u9009\u62e9\u3001\u8d85\u53c2\u6570\u8bbe\u7f6e\u548c\u968f\u673a\u79cd\u5b50\u7b49\u591a\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u667a\u80fd\u4f53\u7684\u6027\u80fd\u6ce2\u52a8\u8f83\u5927\u3002\u867d\u7136\u6a21\u578b\u96c6\u6210\u65b9\u6cd5\u53ef\u4ee5\u7ed3\u5408\u591a\u4e2a\u5f31\u667a\u80fd\u4f53\u4ee5\u63d0\u5347\u6574\u4f53\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u7684\u96c6\u6210\u7b56\u7565\uff08\u5982\u591a\u6570\u6295\u7968\u548cBoltzmann\u52a0\u6743\uff09\u7f3a\u4e4f\u5bf9\u5177\u4f53\u4efb\u52a1\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u9650\u5236\u4e86\u5176\u9002\u5e94\u6027\u548c\u6709\u6548\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u66f4\u6709\u6548\u7684\u96c6\u6210\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86LLM-Ens\u65b9\u6cd5\uff0c\u5177\u4f53\u6b65\u9aa4\u5982\u4e0b\uff1a1) \u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5c06\u4efb\u52a1\u4e2d\u7684\u72b6\u6001\u5206\u7c7b\u4e3a\u4e0d\u540c\u7684\u201c\u60c5\u5883\u201d\uff0c\u5e76\u7ed3\u5408\u4efb\u52a1\u6761\u4ef6\u7684\u9ad8\u7ea7\u63cf\u8ff0\uff1b2) \u7edf\u8ba1\u5206\u6790\u6bcf\u4e2a\u72ec\u7acb\u667a\u80fd\u4f53\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u7684\u4f18\u52a3\u52bf\uff1b3) \u5728\u63a8\u7406\u9636\u6bb5\uff0c\u6839\u636e\u5f53\u524d\u4efb\u52a1\u60c5\u5883\u52a8\u6001\u5207\u6362\u5230\u6700\u9002\u5408\u7684\u667a\u80fd\u4f53\uff0c\u786e\u4fdd\u52a8\u6001\u6a21\u578b\u9009\u62e9\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u517c\u5bb9\u4f7f\u7528\u4e0d\u540c\u968f\u673a\u79cd\u5b50\u3001\u8d85\u53c2\u6570\u8bbe\u7f6e\u548c\u5404\u79cdRL\u7b97\u6cd5\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u3002", "result": "\u5728Atari\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLLM-Ens\u663e\u8457\u63d0\u9ad8\u4e86RL\u6a21\u578b\u96c6\u6210\u7684\u6027\u80fd\uff0c\u76f8\u8f83\u4e8e\u77e5\u540d\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e86\u9ad8\u8fbe20.9%\u3002\u8fd9\u4e00\u7ed3\u679c\u8bc1\u660e\u4e86LLM-Ens\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "LLM-Ens\u662f\u4e00\u79cd\u65b0\u9896\u4e14\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u96c6\u6210\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u5f15\u5165\u4efb\u52a1\u7279\u5b9a\u7684\u8bed\u4e49\u7406\u89e3\u52a8\u6001\u9009\u62e9\u5408\u9002\u7684\u667a\u80fd\u4f53\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u517c\u5bb9\u591a\u79cdRL\u667a\u80fd\u4f53\uff0c\u8fd8\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff0c\u4fbf\u4e8e\u540e\u7eed\u7814\u7a76\u548c\u590d\u73b0\u3002"}}
{"id": "2505.15293", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.15293", "abs": "https://arxiv.org/abs/2505.15293", "authors": ["Qianyue Hao", "Yiwen Song", "Qingmin Liao", "Jian Yuan", "Yong Li"], "title": "LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Policy exploration is critical in reinforcement learning (RL), where existing\napproaches include greedy, Gaussian process, etc. However, these approaches\nutilize preset stochastic processes and are indiscriminately applied in all\nkinds of RL tasks without considering task-specific features that influence\npolicy exploration. Moreover, during RL training, the evolution of such\nstochastic processes is rigid, which typically only incorporates a decay in the\nvariance, failing to adjust flexibly according to the agent's real-time\nlearning status. Inspired by the analyzing and reasoning capability of large\nlanguage models (LLMs), we design LLM-Explorer to adaptively generate\ntask-specific exploration strategies with LLMs, enhancing the policy\nexploration in RL. In our design, we sample the learning trajectory of the\nagent during the RL training in a given task and prompt the LLM to analyze the\nagent's current policy learning status and then generate a probability\ndistribution for future policy exploration. Updating the probability\ndistribution periodically, we derive a stochastic process specialized for the\nparticular task and dynamically adjusted to adapt to the learning process. Our\ndesign is a plug-in module compatible with various widely applied RL\nalgorithms, including the DQN series, DDPG, TD3, and any possible variants\ndeveloped based on them. Through extensive experiments on the Atari and MuJoCo\nbenchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy\nexploration, achieving an average performance improvement up to 37.27%. Our\ncode is open-source at https://anonymous.4open.science/r/LLM-Explorer-19BE for\nreproducibility.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLLM-Explorer\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u7279\u5b9a\u4efb\u52a1\u7684\u63a2\u7d22\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u63a2\u7d22\u3002\u901a\u8fc7\u5728Atari\u548cMuJoCo\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6027\u80fd\u5e73\u5747\u63d0\u5347\u4e8637.27%\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u63a2\u7d22\u65b9\u6cd5\u4f7f\u7528\u9884\u8bbe\u7684\u968f\u673a\u8fc7\u7a0b\uff0c\u4e0d\u8003\u8651\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\uff0c\u5e76\u4e14\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8c03\u6574\u4e0d\u591f\u7075\u6d3b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6839\u636e\u4efb\u52a1\u7279\u70b9\u548c\u5b9e\u65f6\u5b66\u4e60\u72b6\u6001\u81ea\u9002\u5e94\u8c03\u6574\u7b56\u7565\u63a2\u7d22\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u63d2\u4ef6\u6a21\u5757LLM-Explorer\uff0c\u5b83\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u91c7\u6837\u667a\u80fd\u4f53\u7684\u5b66\u4e60\u8f68\u8ff9\uff0c\u5e76\u63d0\u793aLLM\u5206\u6790\u5f53\u524d\u7b56\u7565\u5b66\u4e60\u72b6\u6001\uff0c\u751f\u6210\u672a\u6765\u7684\u6982\u7387\u5206\u5e03\u7528\u4e8e\u7b56\u7565\u63a2\u7d22\u3002\u6b64\u6982\u7387\u5206\u5e03\u4f1a\u5468\u671f\u6027\u66f4\u65b0\uff0c\u4ece\u800c\u4e3a\u7279\u5b9a\u4efb\u52a1\u63a8\u5bfc\u51fa\u4e13\u95e8\u7684\u968f\u673a\u8fc7\u7a0b\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u4ee5\u9002\u5e94\u5b66\u4e60\u8fc7\u7a0b\u3002\u8be5\u6a21\u5757\u517c\u5bb9\u591a\u79cdRL\u7b97\u6cd5\uff0c\u5982DQN\u7cfb\u5217\u3001DDPG\u3001TD3\u7b49\u3002", "result": "\u5728Atari\u548cMuJoCo\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLLM-Explorer\u663e\u8457\u589e\u5f3a\u4e86RL\u7b56\u7565\u63a2\u7d22\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe37.27%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "LLM-Explorer\u662f\u4e00\u79cd\u6709\u6548\u7684\u63d2\u4ef6\u6a21\u5757\uff0c\u53ef\u4ee5\u589e\u5f3a\u5404\u79cdRL\u7b97\u6cd5\u7684\u7b56\u7565\u63a2\u7d22\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u4efb\u52a1\u573a\u666f\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u7814\u7a76\u3002"}}
{"id": "2505.15201", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.15201", "abs": "https://arxiv.org/abs/2505.15201", "authors": ["Christian Walder", "Deep Karkhanis"], "title": "Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts\nfor each problem and reward them independently. This optimizes for pass@1\nperformance and prioritizes the strength of isolated samples at the expense of\nthe diversity and collective utility of sets of samples. This under-utilizes\nthe sampling capacity, limiting exploration and eventual improvement on harder\nexamples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a\ntransformation on the final rewards which leads to direct optimization of\npass@k performance, thus optimizing for sets of samples that maximize reward\nwhen considered jointly. Our contribution is to derive novel low variance\nunbiased estimators for pass@k and its gradient, in both the binary and\ncontinuous reward settings. We show optimization with our estimators reduces to\nstandard RL with rewards that have been jointly transformed by a stable and\nefficient transformation function.\n  While previous efforts are restricted to k=n, ours is the first to enable\nrobust optimization of pass@k for any arbitrary k <= n. Moreover, instead of\ntrading off pass@1 performance for pass@k gains, our method allows annealing k\nduring training, optimizing both metrics and often achieving strong pass@1\nnumbers alongside significant pass@k gains.\n  We validate our reward transformations on toy experiments, which reveal the\nvariance reducing properties of our formulations. We also include real-world\nexamples using the open-source LLM, GEMMA-2. We find that our transformation\neffectively optimizes for the target k. Furthermore, higher k values enable\nsolving more and harder problems, while annealing k boosts both the pass@1 and\npass@k . Crucially, for challenging task sets where conventional pass@1\noptimization stalls, our pass@k approach unblocks learning, likely due to\nbetter exploration by prioritizing joint utility over the utility of individual\nsamples.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5Pass-at-k Policy Optimization (PKPO)\uff0c\u901a\u8fc7\u4f18\u5316\u6837\u672c\u96c6\u5408\u7684\u8054\u5408\u6548\u7528\uff0c\u63d0\u5347\u63a2\u7d22\u80fd\u529b\u548c\u89e3\u51b3\u66f4\u96be\u95ee\u9898\u7684\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u4e86\u5bf9\u4efb\u610fk<=n\u7684\u7a33\u5065pass@k\u4f18\u5316\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8c03\u6574k\u503c\u4ee5\u540c\u65f6\u4f18\u5316pass@1\u548cpass@k\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cPKPO\u80fd\u591f\u6709\u6548\u51cf\u5c11\u65b9\u5dee\u3001\u63d0\u9ad8\u89e3\u51b3\u96be\u9898\u7684\u80fd\u529b\uff0c\u5e76\u5728\u6311\u6218\u6027\u4efb\u52a1\u96c6\u4e2d\u89e3\u9501\u5b66\u4e60\u8fc7\u7a0b\u3002", "motivation": "\u4f20\u7edf\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u4f18\u5316\u65f6\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u6837\u672c\u7684\u5f3a\u5ea6\uff08\u5373pass@1\u6027\u80fd\uff09\uff0c\u800c\u5ffd\u7565\u4e86\u6837\u672c\u96c6\u5408\u7684\u591a\u6837\u6027\u548c\u8054\u5408\u6548\u7528\uff0c\u8fd9\u9650\u5236\u4e86\u91c7\u6837\u80fd\u529b\u7684\u5145\u5206\u5229\u7528\uff0c\u4ece\u800c\u5f71\u54cd\u63a2\u7d22\u6548\u7387\u548c\u5728\u66f4\u96be\u95ee\u9898\u4e0a\u7684\u6539\u8fdb\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u76f4\u63a5\u4f18\u5316\u6837\u672c\u96c6\u5408\u8054\u5408\u6548\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86Pass-at-k Policy Optimization (PKPO) \u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6700\u7ec8\u5956\u52b1\u8f6c\u5316\u4e3a\u4f18\u5316pass@k\u6027\u80fd\u7684\u65b9\u5f0f\uff0c\u76f4\u63a5\u4f18\u5316\u6837\u672c\u96c6\u5408\u7684\u8054\u5408\u6548\u7528\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63a8\u5bfc\u4e86\u9002\u7528\u4e8e\u4e8c\u5143\u548c\u8fde\u7eed\u5956\u52b1\u8bbe\u7f6e\u4e0b\u7684\u4f4e\u65b9\u5dee\u65e0\u504f\u4f30\u8ba1\u5668\uff0c\u8fd9\u4e9b\u4f30\u8ba1\u5668\u53ef\u4ee5\u5c06\u4f18\u5316\u95ee\u9898\u7b80\u5316\u4e3a\u6807\u51c6\u7684\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u4f46\u4f7f\u7528\u7ecf\u8fc7\u7a33\u5b9a\u9ad8\u6548\u8f6c\u6362\u51fd\u6570\u5904\u7406\u540e\u7684\u5956\u52b1\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u652f\u6301\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5bf9k\u503c\u8fdb\u884c\u9000\u706b\u64cd\u4f5c\uff0c\u4ece\u800c\u540c\u65f6\u4f18\u5316pass@1\u548cpass@k\u6027\u80fd\u3002", "result": "\u5728\u73a9\u5177\u5b9e\u9a8c\u4e2d\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u5956\u52b1\u8f6c\u6362\u65b9\u6cd5\u5177\u6709\u51cf\u5c11\u65b9\u5dee\u7684\u7279\u6027\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0c\u4f7f\u7528\u5f00\u6e90LLM GEMMA-2\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u66f4\u9ad8\u7684k\u503c\u53ef\u4ee5\u5e2e\u52a9\u89e3\u51b3\u66f4\u591a\u548c\u66f4\u96be\u7684\u95ee\u9898\uff0c\u800ck\u503c\u7684\u9000\u706b\u64cd\u4f5c\u5219\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86pass@1\u548cpass@k\u6027\u80fd\u3002\u7279\u522b\u662f\u5728\u4f20\u7edfpass@1\u4f18\u5316\u96be\u4ee5\u53d6\u5f97\u8fdb\u5c55\u7684\u6311\u6218\u6027\u4efb\u52a1\u96c6\u4e2d\uff0cPKPO\u901a\u8fc7\u4f18\u5148\u8003\u8651\u6837\u672c\u96c6\u5408\u7684\u8054\u5408\u6548\u7528\u800c\u975e\u5355\u4e2a\u6837\u672c\u7684\u6548\u7528\uff0c\u663e\u8457\u6539\u5584\u4e86\u63a2\u7d22\u80fd\u529b\u5e76\u89e3\u9501\u4e86\u5b66\u4e60\u8fc7\u7a0b\u3002", "conclusion": "PKPO\u662f\u4e00\u79cd\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u4f18\u5316\u6837\u672c\u96c6\u5408\u7684\u8054\u5408\u6548\u7528\u6765\u63d0\u5347\u63a2\u7d22\u80fd\u529b\u548c\u89e3\u51b3\u66f4\u96be\u95ee\u9898\u7684\u80fd\u529b\u3002\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0cPKPO\u4e0d\u4ec5\u80fd\u591f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7075\u6d3b\u8c03\u6574k\u503c\u4ee5\u5e73\u8861\u4e0d\u540c\u6027\u80fd\u6307\u6807\uff0c\u8fd8\u80fd\u5728\u6311\u6218\u6027\u4efb\u52a1\u96c6\u4e2d\u663e\u8457\u6539\u5584\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2505.15134", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.15134", "abs": "https://arxiv.org/abs/2505.15134", "authors": ["Shivam Agarwal", "Zimin Zhang", "Lifan Yuan", "Jiawei Han", "Hao Peng"], "title": "The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Entropy minimization (EM) trains the model to concentrate even more\nprobability mass on its most confident outputs. We show that this simple\nobjective alone, without any labeled data, can substantially improve large\nlanguage models' (LLMs) performance on challenging math, physics, and coding\ntasks. We explore three approaches: (1) EM-FT minimizes token-level entropy\nsimilarly to instruction finetuning, but on unlabeled outputs drawn from the\nmodel; (2) EM-RL: reinforcement learning with negative entropy as the only\nreward to maximize; (3) EM-INF: inference-time logit adjustment to reduce\nentropy without any training data or parameter updates. On Qwen-7B, EM-RL,\nwithout any labeled data, achieves comparable or better performance than strong\nRL baselines such as GRPO and RLOO that are trained on 60K labeled examples.\nFurthermore, EM-INF enables Qwen-32B to match or exceed the performance of\nproprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the\nchallenging SciCode benchmark, while being 3x more efficient than\nself-consistency and sequential refinement. Our findings reveal that many\npretrained LLMs possess previously underappreciated reasoning capabilities that\ncan be effectively elicited through entropy minimization alone, without any\nlabeled data or even any parameter updates.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u71b5\u6700\u5c0f\u5316\uff08EM\uff09\u5728\u65e0\u6807\u7b7e\u6570\u636e\u4e0b\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u7f16\u7a0b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u901a\u8fc7\u4e09\u79cd\u65b9\u6cd5\uff1aEM-FT\uff08\u57fa\u4e8e\u4ee4\u724c\u7ea7\u71b5\u6700\u5c0f\u5316\u7684\u5fae\u8c03\uff09\u3001EM-RL\uff08\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u8d1f\u71b5\u4f5c\u4e3a\u552f\u4e00\u5956\u52b1\uff09\u548cEM-INF\uff08\u63a8\u7406\u65f6\u8c03\u6574logit\u4ee5\u51cf\u5c11\u71b5\uff09\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728Qwen-7B\u4e0a\uff0cEM-RL\u65e0\u9700\u4efb\u4f55\u6807\u8bb0\u6570\u636e\u5373\u53ef\u8fbe\u5230\u6216\u8d85\u8fc7\u4f7f\u752860K\u6807\u8bb0\u6837\u672c\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\uff08\u5982GRPO\u548cRLOO\uff09\u3002\u6b64\u5916\uff0cEM-INF\u4f7fQwen-32B\u5728SciCode\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5339\u914d\u6216\u8d85\u8d8aGPT-4o\u7b49\u4e13\u6709\u6a21\u578b\uff0c\u4e14\u6548\u7387\u66f4\u9ad8\u3002\u7814\u7a76\u8868\u660e\uff0c\u8bb8\u591a\u9884\u8bad\u7ec3LLMs\u5177\u5907\u672a\u88ab\u5145\u5206\u8ba4\u8bc6\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4ec5\u901a\u8fc7\u71b5\u6700\u5c0f\u5316\u5373\u53ef\u6709\u6548\u6fc0\u53d1\u8fd9\u4e9b\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u7f16\u7a0b\u7b49\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4ecd\u6709\u5f85\u63d0\u9ad8\u3002\u540c\u65f6\uff0c\u5f53\u524d\u5927\u591a\u6570\u6539\u8fdb\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u8fd9\u589e\u52a0\u4e86\u6210\u672c\u548c\u9650\u5236\u4e86\u5e94\u7528\u573a\u666f\u3002\u56e0\u6b64\uff0c\u63a2\u7d22\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u63d0\u5347LLMs\u6027\u80fd\u7684\u65b9\u6cd5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u71b5\u6700\u5c0f\u5316\u4f5c\u4e3a\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6280\u672f\uff0c\u53ef\u80fd\u6210\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u5173\u952e\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u71b5\u6700\u5c0f\u5316\u65b9\u6cd5\uff1a\n1. EM-FT\uff1a\u7c7b\u4f3c\u4e8e\u6307\u4ee4\u5fae\u8c03\uff0c\u4f46\u4f7f\u7528\u4ece\u6a21\u578b\u751f\u6210\u7684\u65e0\u6807\u7b7e\u8f93\u51fa\u8fdb\u884c\u4ee4\u724c\u7ea7\u71b5\u6700\u5c0f\u5316\u3002\n2. EM-RL\uff1a\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u8d1f\u71b5\u4f5c\u4e3a\u552f\u4e00\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u4f18\u5316\u3002\n3. EM-INF\uff1a\u5728\u63a8\u7406\u9636\u6bb5\u901a\u8fc7\u8c03\u6574logit\u6765\u51cf\u5c11\u71b5\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u6216\u53c2\u6570\u66f4\u65b0\u3002\n\u901a\u8fc7\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u6a21\u578b\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u96c6\u4e2d\u66f4\u591a\u6982\u7387\u8d28\u91cf\u5230\u6700\u81ea\u4fe1\u7684\u8f93\u51fa\u4e0a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEM-RL\u5728Qwen-7B\u4e0a\u65e0\u9700\u4efb\u4f55\u6807\u6ce8\u6570\u636e\u5373\u53ef\u8fbe\u5230\u6216\u8d85\u8fc7\u4f7f\u752860K\u6807\u6ce8\u6837\u672c\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\uff08\u5982GRPO\u548cRLOO\uff09\u3002\u6b64\u5916\uff0cEM-INF\u663e\u8457\u63d0\u5347\u4e86Qwen-32B\u5728SciCode\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u5176\u80fd\u591f\u5339\u914d\u6216\u8d85\u8d8aGPT-4o\u3001Claude 3 Opus\u548cGemini 1.5 Pro\u7b49\u4e13\u6709\u6a21\u578b\uff0c\u540c\u65f6\u6bd4\u81ea\u4e00\u81f4\u6027\uff08self-consistency\uff09\u548c\u987a\u5e8f\u4f18\u5316\uff08sequential refinement\uff09\u65b9\u6cd5\u66f4\u9ad8\u6548\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u71b5\u6700\u5c0f\u5316\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u65e0\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347LLMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u8bb8\u591a\u9884\u8bad\u7ec3LLMs\u5177\u5907\u672a\u88ab\u5145\u5206\u8ba4\u8bc6\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4ec5\u901a\u8fc7\u71b5\u6700\u5c0f\u5316\u5373\u53ef\u6709\u6548\u6fc0\u53d1\u8fd9\u4e9b\u80fd\u529b\u3002\u6b64\u65b9\u6cd5\u4e3a\u672a\u6765\u8fdb\u4e00\u6b65\u63a2\u7d22\u548c\u4f18\u5316LLMs\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.15074", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.15074", "abs": "https://arxiv.org/abs/2505.15074", "authors": ["Yuhang Zhou", "Jing Zhu", "Shengyi Qian", "Zhuokai Zhao", "Xiyao Wang", "Xiaoyu Liu", "Ming Li", "Paiheng Xu", "Wei Ai", "Furong Huang"], "title": "DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages, 3 figures", "summary": "Large Language Models (LLMs) are increasingly aligned with human preferences\nthrough Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods,\nGroup Relative Policy Optimization (GRPO) has gained attention for its\nsimplicity and strong performance, notably eliminating the need for a learned\nvalue function. However, GRPO implicitly assumes a balanced domain distribution\nand uniform semantic alignment across groups - assumptions that rarely hold in\nreal-world datasets. When applied to multi-domain, imbalanced data, GRPO\ndisproportionately optimizes for dominant domains, neglecting underrepresented\nones and resulting in poor generalization and fairness. We propose\nDomain-Informed Self-Consistency Policy Optimization (DISCO), a principled\nextension to GRPO that addresses inter-group imbalance with two key\ninnovations. Domain-aware reward scaling counteracts frequency bias by\nreweighting optimization based on domain prevalence. Difficulty-aware reward\nscaling leverages prompt-level self-consistency to identify and prioritize\nuncertain prompts that offer greater learning value. Together, these strategies\npromote more equitable and effective policy learning across domains. Extensive\nexperiments across multiple LLMs and skewed training distributions show that\nDISCO improves generalization, outperforms existing GRPO variants by 5% on\nQwen3 models, and sets new state-of-the-art results on multi-domain alignment\nbenchmarks.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDISCO\u7684\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e86GRPO\u5728\u591a\u57df\u4e0d\u5e73\u8861\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u5c3d\u7ba1GRPO\u65b9\u6cd5\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\u4e14\u7b80\u5355\u6613\u7528\uff0c\u4f46\u5b83\u5047\u8bbe\u9886\u57df\u5206\u5e03\u5e73\u8861\u4e14\u7ec4\u95f4\u8bed\u4e49\u5bf9\u9f50\u5747\u5300\uff0c\u8fd9\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e2d\u5f88\u5c11\u6210\u7acb\u3002\u56e0\u6b64\uff0c\u5728\u591a\u57df\u3001\u4e0d\u5e73\u8861\u7684\u6570\u636e\u4e0a\u5e94\u7528\u65f6\uff0cGRPO\u4f1a\u8fc7\u5ea6\u4f18\u5316\u4e3b\u5bfc\u9886\u57df\u800c\u5ffd\u89c6\u8f83\u5c11\u4ee3\u8868\u7684\u9886\u57df\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u548c\u516c\u5e73\u6027\u8f83\u5dee\u3002", "method": "DISCO\u662fGRPO\u7684\u4e00\u4e2a\u6269\u5c55\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u9886\u57df\u611f\u77e5\u5956\u52b1\u7f29\u653e\u901a\u8fc7\u6839\u636e\u9886\u57df\u9891\u7387\u91cd\u65b0\u52a0\u6743\u4f18\u5316\u6765\u5bf9\u6297\u9891\u7387\u504f\u5dee\uff1b2) \u56f0\u96be\u5ea6\u611f\u77e5\u5956\u52b1\u7f29\u653e\u5229\u7528\u63d0\u793a\u7ea7\u522b\u7684\u81ea\u6211\u4e00\u81f4\u6027\u8bc6\u522b\u548c\u4f18\u5148\u5904\u7406\u63d0\u4f9b\u66f4\u5927\u5b66\u4e60\u4ef7\u503c\u7684\u4e0d\u786e\u5b9a\u63d0\u793a\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDISCO\u63d0\u9ad8\u4e86\u591a\u4e2aLLM\u5728\u504f\u659c\u8bad\u7ec3\u5206\u5e03\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728Qwen3\u6a21\u578b\u4e0a\u6bd4\u73b0\u6709GRPO\u53d8\u4f53\u9ad8\u51fa5%\uff0c\u5e76\u5728\u591a\u57df\u5bf9\u9f50\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f73\u7ed3\u679c\u3002", "conclusion": "DISCO\u901a\u8fc7\u89e3\u51b3\u7ec4\u95f4\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4fc3\u8fdb\u4e86\u8de8\u9886\u57df\u7684\u66f4\u516c\u5e73\u548c\u6709\u6548\u7684\u7b56\u7565\u5b66\u4e60\uff0c\u4e3a\u591a\u57df\u5bf9\u9f50\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.15034", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.15034", "abs": "https://arxiv.org/abs/2505.15034", "authors": ["Kaiwen Zha", "Zhengqi Gao", "Maohao Shen", "Zhang-Wei Hong", "Duane S. Boning", "Dina Katabi"], "title": "RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Tech report. The first two authors contributed equally", "summary": "Reinforcement learning (RL) has recently emerged as a compelling approach for\nenhancing the reasoning capabilities of large language models (LLMs), where an\nLLM generator serves as a policy guided by a verifier (reward model). However,\ncurrent RL post-training methods for LLMs typically use verifiers that are\nfixed (rule-based or frozen pretrained) or trained discriminatively via\nsupervised fine-tuning (SFT). Such designs are susceptible to reward hacking\nand generalize poorly beyond their training distributions. To overcome these\nlimitations, we propose Tango, a novel framework that uses RL to concurrently\ntrain both an LLM generator and a verifier in an interleaved manner. A central\ninnovation of Tango is its generative, process-level LLM verifier, which is\ntrained via RL and co-evolves with the generator. Importantly, the verifier is\ntrained solely based on outcome-level verification correctness rewards without\nrequiring explicit process-level annotations. This generative RL-trained\nverifier exhibits improved robustness and superior generalization compared to\ndeterministic or SFT-trained verifiers, fostering effective mutual\nreinforcement with the generator. Extensive experiments demonstrate that both\ncomponents of Tango achieve state-of-the-art results among 7B/8B-scale models:\nthe generator attains best-in-class performance across five competition-level\nmath benchmarks and four challenging out-of-domain reasoning tasks, while the\nverifier leads on the ProcessBench dataset. Remarkably, both components exhibit\nparticularly substantial improvements on the most difficult mathematical\nreasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6Tango\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u540c\u65f6\u8bad\u7ec3\u751f\u6210\u5668\u548c\u9a8c\u8bc1\u5668\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u5956\u52b1\u6b3a\u9a97\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTango\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u5f53\u524d\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u56fa\u5b9a\u7684\u6216\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u8bad\u7ec3\u7684\u9a8c\u8bc1\u5668\uff0c\u8fd9\u4e9b\u8bbe\u8ba1\u5bb9\u6613\u53d7\u5230\u5956\u52b1\u6b3a\u9a97\u7684\u5f71\u54cd\uff0c\u5e76\u4e14\u5728\u8bad\u7ec3\u5206\u5e03\u4e4b\u5916\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002", "method": "Tango\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4ee5\u4ea4\u9519\u7684\u65b9\u5f0f\u540c\u65f6\u8bad\u7ec3LLM\u751f\u6210\u5668\u548c\u9a8c\u8bc1\u5668\u3002\u9a8c\u8bc1\u5668\u662f\u751f\u6210\u6027\u7684\u3001\u8fc7\u7a0b\u7ea7\u522b\u7684LLM\u9a8c\u8bc1\u5668\uff0c\u4ec5\u57fa\u4e8e\u7ed3\u679c\u7ea7\u522b\u7684\u9a8c\u8bc1\u6b63\u786e\u6027\u5956\u52b1\u8fdb\u884c\u8bad\u7ec3\uff0c\u800c\u4e0d\u9700\u8981\u660e\u786e\u7684\u8fc7\u7a0b\u7ea7\u522b\u6ce8\u91ca\u3002\u751f\u6210\u5668\u548c\u9a8c\u8bc1\u5668\u5171\u540c\u8fdb\u5316\uff0c\u4fc3\u8fdb\u4e86\u6709\u6548\u7684\u76f8\u4e92\u589e\u5f3a\u3002", "result": "Tango\u7684\u751f\u6210\u5668\u5728\u4e94\u4e2a\u7ade\u8d5b\u7ea7\u522b\u7684\u6570\u5b66\u57fa\u51c6\u548c\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u9886\u57df\u5916\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6700\u4f73\u6027\u80fd\uff1b\u9a8c\u8bc1\u5668\u5728ProcessBench\u6570\u636e\u96c6\u4e0a\u9886\u5148\u3002\u7279\u522b\u662f\u5728\u6700\u96be\u7684\u6570\u5b66\u63a8\u7406\u95ee\u9898\u4e0a\uff0c\u4e24\u4e2a\u7ec4\u4ef6\u90fd\u8868\u73b0\u51fa\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "Tango\u6846\u67b6\u901a\u8fc7\u540c\u65f6\u8bad\u7ec3\u751f\u6210\u5668\u548c\u9a8c\u8bc1\u5668\uff0c\u63d0\u9ad8\u4e86\u9a8c\u8bc1\u5668\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u751f\u6210\u5668\u548c\u9a8c\u8bc1\u5668\u4e4b\u95f4\u7684\u6709\u6548\u76f8\u4e92\u589e\u5f3a\uff0c\u57287B/8B\u89c4\u6a21\u7684\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
{"id": "2505.14970", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.14970", "abs": "https://arxiv.org/abs/2505.14970", "authors": ["Xiaoyin Chen", "Jiarui Lu", "Minsu Kim", "Dinghuai Zhang", "Jian Tang", "Alexandre Pich\u00e9", "Nicolas Gontier", "Yoshua Bengio", "Ehsan Kamalloo"], "title": "Self-Evolving Curriculum for LLM Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has proven effective for fine-tuning large\nlanguage models (LLMs), significantly enhancing their reasoning abilities in\ndomains such as mathematics and code generation. A crucial factor influencing\nRL fine-tuning success is the training curriculum: the order in which training\nproblems are presented. While random curricula serve as common baselines, they\nremain suboptimal; manually designed curricula often rely heavily on\nheuristics, and online filtering methods can be computationally prohibitive. To\naddress these limitations, we propose Self-Evolving Curriculum (SEC), an\nautomatic curriculum learning method that learns a curriculum policy\nconcurrently with the RL fine-tuning process. Our approach formulates\ncurriculum selection as a non-stationary Multi-Armed Bandit problem, treating\neach problem category (e.g., difficulty level or problem type) as an individual\narm. We leverage the absolute advantage from policy gradient methods as a proxy\nmeasure for immediate learning gain. At each training step, the curriculum\npolicy selects categories to maximize this reward signal and is updated using\nthe TD(0) method. Across three distinct reasoning domains: planning, inductive\nreasoning, and mathematics, our experiments demonstrate that SEC significantly\nimproves models' reasoning capabilities, enabling better generalization to\nharder, out-of-distribution test problems. Additionally, our approach achieves\nbetter skill balance when fine-tuning simultaneously on multiple reasoning\ndomains. These findings highlight SEC as a promising strategy for RL\nfine-tuning of LLMs.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSelf-Evolving Curriculum\uff08SEC\uff09\u7684\u81ea\u52a8\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fc7\u7a0b\u4e2d\u4f18\u5316\u8bad\u7ec3\u8bfe\u7a0b\u9009\u62e9\u3002SEC\u901a\u8fc7\u5c06\u8bfe\u7a0b\u9009\u62e9\u89c6\u4e3a\u975e\u5e73\u7a33\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7684\u7edd\u5bf9\u4f18\u52bf\u4f5c\u4e3a\u5373\u65f6\u5b66\u4e60\u6536\u76ca\u7684\u4ee3\u7406\u6307\u6807\uff0c\u4ece\u800c\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u8bfe\u7a0b\u987a\u5e8f\u4ee5\u6700\u5927\u5316\u5956\u52b1\u4fe1\u53f7\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSEC\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u89c4\u5212\u3001\u5f52\u7eb3\u63a8\u7406\u548c\u6570\u5b66\u7b49\u4e0d\u540c\u63a8\u7406\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6280\u80fd\u5e73\u8861\u3002", "motivation": "\u5c3d\u7ba1\u968f\u673a\u8bfe\u7a0b\u548c\u624b\u52a8\u8bbe\u8ba1\u7684\u8bfe\u7a0b\u662f\u5e38\u89c1\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f46\u5b83\u4eec\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u6216\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u7684\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u4f18\u7684\u81ea\u52a8\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u6765\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u8bad\u7ec3\u8bfe\u7a0b\u987a\u5e8f\u3002", "method": "SEC\u5c06\u8bfe\u7a0b\u9009\u62e9\u5efa\u6a21\u4e3a\u4e00\u4e2a\u975e\u5e73\u7a33\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u6bcf\u4e2a\u95ee\u9898\u7c7b\u522b\uff08\u5982\u96be\u5ea6\u7ea7\u522b\u6216\u95ee\u9898\u7c7b\u578b\uff09\u88ab\u89c6\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684\u624b\u81c2\u3002\u5229\u7528\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7684\u7edd\u5bf9\u4f18\u52bf\u4f5c\u4e3a\u5373\u65f6\u5b66\u4e60\u6536\u76ca\u7684\u4ee3\u7406\u6307\u6807\uff0c\u5728\u6bcf\u4e00\u6b65\u8bad\u7ec3\u4e2d\uff0c\u8bfe\u7a0b\u7b56\u7565\u9009\u62e9\u7c7b\u522b\u4ee5\u6700\u5927\u5316\u5956\u52b1\u4fe1\u53f7\uff0c\u5e76\u4f7f\u7528TD(0)\u65b9\u6cd5\u8fdb\u884c\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSEC\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u63a8\u7406\u9886\u57df\uff08\u5305\u62ec\u89c4\u5212\u3001\u5f52\u7eb3\u63a8\u7406\u548c\u6570\u5b66\uff09\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u597d\u5730\u6cdb\u5316\u5230\u66f4\u96be\u7684\u3001\u5206\u5e03\u5916\u6d4b\u8bd5\u95ee\u9898\u4e0a\u3002\u6b64\u5916\uff0cSEC\u5728\u540c\u65f6\u5fae\u8c03\u591a\u4e2a\u63a8\u7406\u9886\u57df\u65f6\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6280\u80fd\u5e73\u8861\u3002", "conclusion": "SEC\u4f5c\u4e3a\u4e00\u79cd\u81ea\u52a8\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03LLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u9014\u7684\u7b56\u7565\uff0c\u80fd\u591f\u5728\u63d0\u9ad8\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u597d\u7684\u6280\u80fd\u5e73\u8861\u3002"}}
{"id": "2505.14625", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.14625", "abs": "https://arxiv.org/abs/2505.14625", "authors": ["Zhangchen Xu", "Yuetai Li", "Fengqing Jiang", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Radha Poovendran"], "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u9a8c\u8bc1\u5668\u9519\u8bef\u62d2\u7edd\u6a21\u578b\u6b63\u786e\u8f93\u51fa\uff08\u5047\u9634\u6027\uff09\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684LLM\u9a8c\u8bc1\u5668tinyV\u6765\u7f13\u89e3\u8be5\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u7684\u8868\u73b0\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u5956\u52b1\u4fe1\u53f7\u7684\u53ef\u9760\u6027\uff0c\u4f46\u5f53\u524d\u7684\u9a8c\u8bc1\u5668\u5b58\u5728\u5927\u91cf\u5047\u9634\u6027\u95ee\u9898\uff0c\u5373\u9519\u8bef\u5730\u62d2\u7edd\u4e86\u6a21\u578b\u7684\u6b63\u786e\u8f93\u51fa\u3002\u8fd9\u79cd\u73b0\u8c61\u5728Big-Math-RL-Verified\u6570\u636e\u96c6\u4e0a\u5c24\u4e3a\u660e\u663e\uff0c\u8d85\u8fc738%\u7684\u6a21\u578b\u751f\u6210\u54cd\u5e94\u53d7\u5230\u5f71\u54cd\u3002\u5047\u9634\u6027\u95ee\u9898\u5265\u593a\u4e86\u6a21\u578b\u7684\u6709\u7528\u68af\u5ea6\u4fe1\u53f7\uff0c\u4e25\u91cd\u5f71\u54cd\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6548\u679c\u548c\u6536\u655b\u901f\u5ea6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u51cf\u5c11\u5047\u9634\u6027\uff0c\u63d0\u9ad8\u9a8c\u8bc1\u5668\u7684\u51c6\u786e\u6027\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86tinyV\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u8f7b\u91cf\u7ea7LLM\u7684\u9a8c\u8bc1\u5668\uff0c\u65e8\u5728\u589e\u5f3a\u73b0\u6709\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u3002tinyV\u80fd\u591f\u52a8\u6001\u8bc6\u522b\u6f5c\u5728\u7684\u5047\u9634\u6027\uff0c\u5e76\u6062\u590d\u6709\u6548\u7684\u54cd\u5e94\u4ee5\u751f\u6210\u66f4\u51c6\u786e\u7684\u5956\u52b1\u4f30\u8ba1\u3002\u901a\u8fc7\u5c06tinyV\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u53ef\u4ee5\u6539\u5584\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528tinyV\u53ef\u4ee5\u5c06\u901a\u8fc7\u7387\u63d0\u5347\u9ad8\u8fbe10%\uff0c\u5e76\u52a0\u901f\u6a21\u578b\u7684\u6536\u655b\u901f\u5ea6\u3002\u8fd9\u8868\u660etinyV\u5728\u7f13\u89e3\u5047\u9634\u6027\u95ee\u9898\u65b9\u9762\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u7684\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63ed\u793a\u4e86\u9a8c\u8bc1\u5668\u5047\u9634\u6027\u95ee\u9898\u5bf9\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u91cd\u5927\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u548c\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u5176\u5371\u5bb3\u3002tinyV\u4f5c\u4e3a\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u5047\u9634\u6027\uff0c\u63d0\u9ad8\u5956\u52b1\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u8fdb\u800c\u6539\u5584\u5f3a\u5316\u5b66\u4e60\u7684\u8868\u73b0\u3002\u8fd9\u4e00\u7814\u7a76\u6210\u679c\u5f3a\u8c03\u4e86\u89e3\u51b3\u9a8c\u8bc1\u5668\u5047\u9634\u6027\u95ee\u9898\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
