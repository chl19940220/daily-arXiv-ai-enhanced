<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: 本文是一篇立场论文，全面回顾了人类与AI代理协作的最新经验进展，强调其技术成就和持续存在的差距。文章提出了一个新颖的概念架构——分层探索开发网络（Hierarchical Exploration-Exploitation Net），旨在系统地整合多代理协调、知识管理、控制论反馈循环及高层控制机制的技术细节。该框架有助于修订传统方法，并激励融合定性和定量范式的新研究。文章结构灵活，可从任何章节阅读，既是对现有技术实现的批判性评论，也为设计或扩展人机共生关系提供了前瞻性的参考。


<details>
  <summary>Details</summary>
Motivation: 当前关于人类与AI代理协作的研究虽然在技术上取得了显著成就，但缺乏一个统一的理论框架来连贯地整合这些不同的研究，尤其是在处理开放性复杂任务时。这促使作者提出一个能够综合各类技术细节的新概念架构，以推动更深层次的人类认知与AI能力协同进化。

Method: 作者提出了一种名为分层探索开发网络（Hierarchical Exploration-Exploitation Net）的新颖概念架构。该架构通过将现有的贡献，包括符号AI技术、连接主义的LLM驱动型代理以及混合组织实践，映射到这个框架中，从而系统地链接多代理协调、知识管理、控制论反馈循环和高层控制机制等技术细节。这种方法允许对遗留方法进行重新评估，并为结合定性和定量范式的新工作提供灵感。

Result: 该论文不仅梳理了当前人类与AI代理协作领域的技术成就和不足，还通过提出的概念架构为未来研究指明了方向。它促进了对传统方法的反思，并为融合不同范式的研究提供了基础。此外，文章灵活的结构使其成为理解现有技术和设计未来人机共生关系的重要参考。

Conclusion: 本文总结认为，尽管人类与AI代理协作领域已经取得重要进展，但仍然需要一个统一的理论框架来支持更复杂的任务。所提出的分层探索开发网络为这一目标提供了一个潜在解决方案，同时为深入探讨人类认知与AI能力的协同进化奠定了基础。

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO](https://arxiv.org/abs/2505.17017)
*Chengzhuo Tong,Ziyu Guo,Renrui Zhang,Wenyu Shan,Xinyu Wei,Zhenghao Xing,Hongsheng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 本研究通过对比GRPO和DPO算法在自回归图像生成中的表现，揭示了它们各自的优势，并探讨了奖励模型对强化学习算法泛化能力的影响。此外，还系统分析了三种扩展策略以提升算法的领域内和领域外性能，为未来开发更有效的强化学习算法提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 尽管RL在LLM的CoT推理中取得了显著进展，但在自回归图像生成领域，关于不同RL算法及其挑战的研究仍然不足。为了弥补这一差距，需要深入分析特定领域的挑战以及不同RL策略的特点。

Method: 研究选择了GRPO和DPO两种RL算法，评估其在自回归图像生成任务中的领域内性能和领域外泛化能力，并分析了不同奖励模型对其能力的影响。同时，系统性地探索了三种扩展策略以提高算法性能。

Result: 研究表明，GRPO和DPO具有不同的优势，且具备更强内在泛化能力的奖励模型可以增强所应用的RL算法的泛化潜力。此外，针对每种范式，研究得出了高效扩展性能的独特见解。

Conclusion: 该研究首次全面探讨了GRPO和DPO算法在自回归图像生成中的应用，强调了奖励模型的重要性，并为未来的RL算法开发提供了有价值的参考和新思路。

Abstract: Recent advancements underscore the significant role of Reinforcement Learning
(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large
language models (LLMs). Two prominent RL algorithms, Direct Preference
Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central
to these developments, showcasing different pros and cons. Autoregressive image
generation, also interpretable as a sequential CoT reasoning process, presents
unique challenges distinct from LLM-based CoT reasoning. These encompass
ensuring text-image consistency, improving image aesthetic quality, and
designing sophisticated reward models, rather than relying on simpler
rule-based rewards. While recent efforts have extended RL to this domain, these
explorations typically lack an in-depth analysis of the domain-specific
challenges and the characteristics of different RL strategies. To bridge this
gap, we provide the first comprehensive investigation of the GRPO and DPO
algorithms in autoregressive image generation, evaluating their in-domain
performance and out-of-domain generalization, while scrutinizing the impact of
different reward models on their respective capabilities. Our findings reveal
that GRPO and DPO exhibit distinct advantages, and crucially, that reward
models possessing stronger intrinsic generalization capabilities potentially
enhance the generalization potential of the applied RL algorithms. Furthermore,
we systematically explore three prevalent scaling strategies to enhance both
their in-domain and out-of-domain proficiency, deriving unique insights into
efficiently scaling performance for each paradigm. We hope our study paves a
new path for inspiring future work on developing more effective RL algorithms
to achieve robust CoT reasoning in the realm of autoregressive image
generation. Code is released at
https://github.com/ZiyuGuo99/Image-Generation-CoT

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Divide-Fuse-Conquer: Eliciting "Aha Moments" in Multi-Scenario Games](https://arxiv.org/abs/2505.16401)
*Xiaoqing Zhang,Huabin Zheng,Ang Lv,Yuhan Liu,Zirui Song,Flood Sung,Xiuying Chen,Rui Yan*

Main category: cs.LG

TL;DR: 本文提出了一种名为Divide-Fuse-Conquer的框架，用于增强多场景强化学习中的泛化能力。该方法通过基于游戏特性的启发式分组、训练专用模型以及融合模型参数来实现更好的泛化性能。实验表明，在18个TextArena游戏中，使用此策略训练的Qwen2.5-32B-Align达到了与Claude3.5相当的性能水平。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在数学、编码和视觉任务中取得了显著成果，但在多场景游戏中面临重大挑战，例如规则多样性、交互模式复杂性和环境复杂性等问题，导致模型在一个场景中表现良好但无法泛化到其他场景。此外，简单地将多个场景组合在一起进行训练会引入诸如训练不稳定性等额外挑战。因此，需要一种新的方法来克服这些困难并提升多场景强化学习的泛化能力。

Method: 研究者提出了Divide-Fuse-Conquer框架，该方法首先根据游戏规则和难度等特性启发式地对游戏进行分组；然后为每个组训练专门的模型以使其在特定组的游戏上表现出色（即“divide”步骤）；接下来，将不同组的模型参数融合成一个新的模型，并继续对其进行多组训练，直到所有组的场景都被攻克。

Result: 实验结果表明，在18个TextArena游戏中，采用Divide-Fuse-Conquer策略训练的Qwen2.5-32B-Align模型达到了与Claude3.5相当的性能水平，实现了7场胜利和4场平局。这证明了所提方法在提升多场景强化学习泛化能力方面的有效性。

Conclusion: Divide-Fuse-Conquer框架为解决多场景强化学习中的泛化问题提供了一种有效的方法。通过启发式分组、专门模型训练和模型参数融合，该方法成功提升了大型语言模型在多场景游戏中的泛化能力。这一研究成果有望为未来利用强化学习改进大型语言模型泛化能力的研究提供启示。

Abstract: Large language models (LLMs) have been observed to suddenly exhibit advanced
reasoning abilities during reinforcement learning (RL), resembling an ``aha
moment'' triggered by simple outcome-based rewards. While RL has proven
effective in eliciting such breakthroughs in tasks involving mathematics,
coding, and vision, it faces significant challenges in multi-scenario games.
The diversity of game rules, interaction modes, and environmental complexities
often leads to policies that perform well in one scenario but fail to
generalize to others. Simply combining multiple scenarios during training
introduces additional challenges, such as training instability and poor
performance. To overcome these challenges, we propose Divide-Fuse-Conquer, a
framework designed to enhance generalization in multi-scenario RL. This
approach starts by heuristically grouping games based on characteristics such
as rules and difficulties. Specialized models are then trained for each group
to excel at games in the group is what we refer to as the divide step. Next, we
fuse model parameters from different groups as a new model, and continue
training it for multiple groups, until the scenarios in all groups are
conquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align
trained with the Divide-Fuse-Conquer strategy reaches a performance level
comparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can
inspire future research on using reinforcement learning to improve the
generalization of LLMs.

</details>


### [4] [SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning](https://arxiv.org/abs/2505.16368)
*Huanyu Liu,Jia Li,Hao Zhu,Kechi Zhang,Yihong Dong,Ge Li*

Main category: cs.LG

TL;DR: 本文针对现有强化学习任务在可扩展性、可验证性和难度控制上的局限性，提出了一种基于SAT问题的强化学习框架Saturn。该框架通过课程学习方式训练大语言模型（LLMs）的推理能力，解决了上述问题并显著提升了模型性能。实验结果显示，Saturn在SAT问题、数学和编程任务上均表现优异，且优于当前最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 目前，如何设计有效的强化学习（RL）任务以充分释放大语言模型（LLMs）的推理能力仍是一个开放问题。现有的RL任务（如数学、编程和构建推理任务）存在以下三个主要限制：
1. **可扩展性**：这些任务依赖于大量的人工标注或昂贵的LLM合成来生成足够的训练数据。
2. **可验证性**：LLMs的输出难以自动且可靠地验证。
3. **可控的难度**：大多数任务缺乏精细的难度控制，使得很难训练LLMs从简单到复杂逐步发展推理能力。
为了解决这些问题，研究者提出了一种新的基于SAT（布尔可满足性）问题的强化学习框架——Saturn。

Method: Saturn采用了一种基于SAT（布尔可满足性）问题的强化学习框架，用于训练和评估大语言模型（LLMs）的推理能力。具体方法包括：
1. **可扩展的任务构建**：利用SAT问题生成训练数据，减少了对人工标注或昂贵LLM合成的需求。
2. **基于规则的验证**：通过逻辑规则自动验证模型输出，确保了验证的可靠性和自动化。
3. **精确的难度控制**：设计了一种机制，能够精细调整任务难度，支持从简单到复杂的课程学习过程。
4. **课程学习管道**：通过构造难度递增的SAT任务，逐步提高LLMs的推理能力，并通过原理性的机制控制难度转换以确保稳定训练。
此外，Saturn还引入了一个包含2,660个不同难度的SAT问题的数据集（Saturn-2.6k），用于评估模型推理能力的变化。

Result: 实验结果表明，Saturn框架在多个方面取得了显著成果：
1. 在SAT问题上，Saturn-1.5B和Saturn-7B分别实现了平均pass@3的+14.0和+28.1的改进。
2. 在数学和编程任务上，Saturn-1.5B和Saturn-7B在基准测试中（如AIME、LiveCodeBench）分别提高了平均分数+4.9和+1.8。
3. 与当前最先进的方法相比，Saturn在构建RL任务时进一步提升了+8.8%的性能。
这证明了Saturn框架的有效性和优越性。

Conclusion: 我们提出了Saturn，一个基于SAT的强化学习框架，它通过布尔可满足性（SAT）问题训练和评估大语言模型的推理能力。该框架解决了现有任务在可扩展性、可验证性和难度控制方面的限制，并引入了一个课程学习管道以逐步提高模型的推理能力。实验结果表明，在SAT问题上，Saturn-1.5B和Saturn-7B分别实现了+14.0和+28.1的平均pass@3改进；在数学和编程任务上也表现出显著提升。此外，与当前最先进的方法相比，Saturn进一步提升了+8.8%的性能。我们公开了源代码、数据集和模型以支持未来的研究。

Abstract: How to design reinforcement learning (RL) tasks that effectively unleash the
reasoning capability of large language models (LLMs) remains an open question.
Existing RL tasks (e.g., math, programming, and constructing reasoning tasks)
suffer from three key limitations: (1) Scalability. They rely heavily on human
annotation or expensive LLM synthesis to generate sufficient training data. (2)
Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3)
Controllable Difficulty. Most tasks lack fine-grained difficulty control,
making it hard to train LLMs to develop reasoning ability from easy to hard.
  To address these limitations, we propose Saturn, a SAT-based RL framework
that uses Boolean Satisfiability (SAT) problems to train and evaluate LLM
reasoning. Saturn enables scalable task construction, rule-based verification,
and precise difficulty control. Saturn designs a curriculum learning pipeline
that continuously improves LLMs' reasoning capability by constructing SAT tasks
of increasing difficulty and training LLMs from easy to hard. To ensure stable
training, we design a principled mechanism to control difficulty transitions.
  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying
difficulty. It supports the evaluation of how LLM reasoning changes with
problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain
Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT
problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of
+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B
and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,
AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in
constructing RL tasks, Saturn achieves further improvements of +8.8%. We
release the source code, data, and models to support future research.

</details>


### [5] [AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training](https://arxiv.org/abs/2505.16363)
*Huishuai Zhang,Bohan Wang,Luoxin Chen*

Main category: cs.LG

TL;DR: 提出了一种新的优化器AdamS，用于大语言模型的预训练和后训练，具有高效性、简单性和理论支持。


<details>
  <summary>Details</summary>
Motivation: 在Transformer目标函数中观察到的(L0, L1)平滑性特性启发了AdamS的设计，其中局部平滑性由梯度大小控制，而梯度大小可以进一步由动量大小近似。因此，通过利用动量和当前梯度平方的加权和的平方根作为分母，AdamS消除了对二阶矩估计的需求。

Method: AdamS是一种新型优化器，其核心思想是使用动量和当前梯度平方的加权和的平方根作为分母，从而避免了对二阶矩的估计。这使得AdamS在内存和计算占用方面与带有动量的SGD相当，同时提供了更优的优化性能。此外，AdamS可以直接继承AdamW的超参数，并且完全与模型无关，能够无缝集成到现有流程中，无需修改优化器API或架构。

Result: 实验结果表明，AdamS在各种任务中表现出色，包括GPT-2和Llama2（多达13B参数）的预训练以及后训练中的强化学习任务。这些结果验证了AdamS的有效性和优越性。

Conclusion: AdamS作为一种高效的、简单的并且有理论依据的优化器，为现有的优化器提供了一个有吸引力的替代方案，适用于大语言模型的预训练和后训练阶段。

Abstract: We introduce AdamS, a simple yet effective alternative to Adam for large
language model (LLM) pretraining and post-training. By leveraging a novel
denominator, i.e., the root of weighted sum of squares of the momentum and the
current gradient, AdamS eliminates the need for second-moment estimates. Hence,
AdamS is efficient, matching the memory and compute footprint of SGD with
momentum while delivering superior optimization performance. Moreover, AdamS is
easy to adopt: it can directly inherit hyperparameters of AdamW, and is
entirely model-agnostic, integrating seamlessly into existing pipelines without
modifications to optimizer APIs or architectures. The motivation behind AdamS
stems from the observed $(L_0, L_1)$ smoothness properties in transformer
objectives, where local smoothness is governed by gradient magnitudes that can
be further approximated by momentum magnitudes. We establish rigorous
theoretical convergence guarantees and provide practical guidelines for
hyperparameter selection. Empirically, AdamS demonstrates strong performance in
various tasks, including pre-training runs on GPT-2 and Llama2 (up to 13B
parameters) and reinforcement learning in post-training regimes. With its
efficiency, simplicity, and theoretical grounding, AdamS stands as a compelling
alternative to existing optimizers.

</details>


### [6] [Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning](https://arxiv.org/abs/2505.15311)
*Yurun Yuan,Fan Chen,Zeyu Jia,Alexander Rakhlin,Tengyang Xie*

Main category: cs.LG

TL;DR: 在大型语言模型（LLM）推理中，策略优化方法目前占主导地位，而基于价值的方法尚未得到充分探索。本文重新审视了经典的贝尔曼残差最小化（Bellman Residual Minimization）范式，并提出了轨迹贝尔曼残差最小化（TBRM）算法，该算法将这一思想自然地应用于LLMs，提供了一种简单而有效的离线策略算法，通过使用模型自身的logits作为Q值来优化单个轨迹级别的贝尔曼目标。TBRM无需批评者、重要性采样比率或剪辑，并且每个提示只需一次模拟即可运行。我们通过改进的轨迹测度变化分析证明了从任意离线数据收敛到接近最优的KL正则化策略。实验表明，在标准数学推理基准上，与PPO和GRPO等基于策略的基线相比，TBRM以相当或更低的计算和内存开销实现了更优性能。我们的结果表明，基于价值的强化学习可能是提高LLMs推理能力的一个有原则且高效的替代方案。


<details>
  <summary>Details</summary>
Motivation: 当前在大型语言模型（LLM）推理领域，策略优化方法占据主导地位，而基于价值的方法尚未得到充分研究。因此，探索一种基于价值的强化学习方法，以提高LLMs的推理能力显得尤为重要。

Method: 本文提出的轨迹贝尔曼残差最小化（TBRM）算法，通过使用模型自身的logits作为Q值来优化单个轨迹级别的贝尔曼目标。该算法无需批评者、重要性采样比率或剪辑，并且每个提示只需一次模拟即可运行。此外，通过改进的轨迹测度变化分析证明了从任意离线数据收敛到接近最优的KL正则化策略。

Result: 实验结果表明，在标准数学推理基准上，与PPO和GRPO等基于策略的基线相比，TBRM以相当或更低的计算和内存开销实现了更优性能。这表明基于价值的强化学习可能是提高LLMs推理能力的一个有原则且高效的替代方案。

Conclusion: 本文提出了一种新的基于价值的强化学习算法——轨迹贝尔曼残差最小化（TBRM），该算法为LLMs提供了一个简单而有效的离线策略算法。通过理论分析和实验证明，TBRM不仅具有良好的收敛性，而且在计算和内存开销方面也表现出色，为提高LLMs的推理能力提供了新的思路。

Abstract: Policy-based methods currently dominate reinforcement learning (RL) pipelines
for large language model (LLM) reasoning, leaving value-based approaches
largely unexplored. We revisit the classical paradigm of Bellman Residual
Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an
algorithm that naturally adapts this idea to LLMs, yielding a simple yet
effective off-policy algorithm that optimizes a single trajectory-level Bellman
objective using the model's own logits as $Q$-values. TBRM removes the need for
critics, importance-sampling ratios, or clipping, and operates with only one
rollout per prompt. We prove convergence to the near-optimal KL-regularized
policy from arbitrary off-policy data via an improved
change-of-trajectory-measure analysis. Experiments on standard
mathematical-reasoning benchmarks show that TBRM consistently outperforms
policy-based baselines, like PPO and GRPO, with comparable or lower
computational and memory overhead. Our results indicate that value-based RL
might be a principled and efficient alternative for enhancing reasoning
capabilities in LLMs.

</details>


### [7] [Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One](https://arxiv.org/abs/2505.15306)
*Yiwen Song,Qianyue Hao,Qingmin Liao,Jian Yuan,Yong Li*

Main category: cs.LG

TL;DR: 本论文提出了一种名为LLM-Ens的新方法，利用大语言模型（LLMs）增强强化学习中的模型集成技术。通过任务特定的语义理解，LLM-Ens能够动态识别任务情境并切换到在当前情境下表现最佳的代理，从而显著提升集成模型的整体性能。实验表明，该方法在Atari基准测试中超越了现有基线方法20.9%。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习取得了广泛的成功，但训练有效的智能体仍然面临许多挑战，例如算法选择、超参数设置和随机种子的影响等。现有的模型集成方法如多数投票和Boltzmann加法虽然有效，但它们作为固定策略缺乏对具体任务的语义理解，限制了其适应性和效果。因此，需要一种更灵活且能根据任务特点调整的方法来改进模型集成。

Method: LLM-Ens方法首先设计一个大语言模型，用于将任务中的状态分类为不同的'情境'，并结合任务条件的高级描述。然后统计分析每个个体代理在每种情境下的优劣势。在推理阶段，LLM-Ens能够动态识别任务情境的变化，并切换到在当前情境下表现最佳的代理。此外，该方法兼容使用不同随机种子、超参数设置和各种RL算法训练的代理。

Result: 广泛的实验结果表明，LLM-Ens在Atari基准测试中显著提升了强化学习模型集成的效果，相比知名基线方法最多提高了20.9%。这一结果验证了LLM-Ens的有效性和优越性。

Conclusion: LLM-Ens通过引入任务特定的语义理解和动态模型选择机制，成功克服了传统模型集成方法的局限性，显著提升了强化学习模型集成的性能。此方法适用于多种RL算法训练的代理，并在Atari基准测试中展现了卓越的效果。代码已开源以促进可重复研究。

Abstract: Model ensemble is a useful approach in reinforcement learning (RL) for
training effective agents. Despite wide success of RL, training effective
agents remains difficult due to the multitude of factors requiring careful
tuning, such as algorithm selection, hyperparameter settings, and even random
seed choices, all of which can significantly influence an agent's performance.
Model ensemble helps overcome this challenge by combining multiple weak agents
into a single, more powerful one, enhancing overall performance. However,
existing ensemble methods, such as majority voting and Boltzmann addition, are
designed as fixed strategies and lack a semantic understanding of specific
tasks, limiting their adaptability and effectiveness. To address this, we
propose LLM-Ens, a novel approach that enhances RL model ensemble with
task-specific semantic understandings driven by large language models (LLMs).
Given a task, we first design an LLM to categorize states in this task into
distinct 'situations', incorporating high-level descriptions of the task
conditions. Then, we statistically analyze the strengths and weaknesses of each
individual agent to be used in the ensemble in each situation. During the
inference time, LLM-Ens dynamically identifies the changing task situation and
switches to the agent that performs best in the current situation, ensuring
dynamic model selection in the evolving task condition. Our approach is
designed to be compatible with agents trained with different random seeds,
hyperparameter settings, and various RL algorithms. Extensive experiments on
the Atari benchmark show that LLM-Ens significantly improves the RL model
ensemble, surpassing well-known baselines by up to 20.9%. For reproducibility,
our code is open-source at
https://anonymous.4open.science/r/LLM4RLensemble-F7EE.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [8] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 在大规模语言模型（LLMs）中，通过重复采样扩展推理计算可以持续增加覆盖率。然而，这种改进部分可能归因于评估基准答案分布的偏差。研究通过定义一个基于训练集答案频率的基线方法，发现该方法在某些LLMs上优于重复采样，而在其他LLMs上与混合策略表现相当。这为更准确地测量重复采样在提示无关猜测之外的改进提供了基础。


<details>
  <summary>Details</summary>
Motivation: 重复采样在大语言模型中的推理计算能够提高问题解决的覆盖率，但研究者怀疑这一提升部分是由于标准评估基准的 answers 分布偏向于一小部分常见答案，因此需要验证这一猜想并探索更有效的覆盖测量方法。

Method: 研究者定义了一个基线方法，该方法根据训练集中答案的出现频率枚举答案。通过在数学推理和事实知识两个领域进行实验，比较了该基线方法、重复模型采样以及混合策略的表现。混合策略结合了少量模型样本和枚举猜测。

Result: 实验结果表明，在某些LLMs上，基于训练集答案频率的基线方法比重复采样表现更好；而在其他LLMs上，其覆盖率与使用少量模型样本和枚举猜测的混合策略相当。

Conclusion: 通过提出基于训练集答案频率的基线方法，研究揭示了重复采样在大语言模型推理中的实际效果，并提供了一种更精确的测量方式，以评估重复采样在提示无关猜测之外对覆盖率的提升程度。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [9] [SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation](https://arxiv.org/abs/2505.16637)
*Wenjie Yang,Mao Zheng,Mingyang Song,Zheng Li*

Main category: cs.CL

TL;DR: 提出了一种无需参考的简单自我奖励强化学习（SSR）框架，用于机器翻译。该方法在英文与中文互译任务中超越了现有模型，并通过结合外部监督信号进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前先进的机器翻译大语言模型依赖昂贵且难以扩展的外部监督信号，例如人工标注数据或训练好的奖励模型（RMs）。为解决这一问题，研究者提出了一种无需参考、完全在线且仅依赖自我判断奖励的强化学习框架。

Method: 使用13K单语数据和Qwen-2.5-7B作为基础模型，开发了SSR-Zero-7B模型。此外，通过引入COMET提供的外部监督信号，构建了更强的SSR-X-Zero-7B模型。

Result: SSR-Zero-7B在WMT23、WMT24和Flores200基准上的英中互译任务中表现优于现有的专用机器翻译大模型和更大的通用大模型。而SSR-X-Zero-7B则达到了最先进的性能，超过了所有参数量小于72B的开源模型以及一些闭源模型如GPT-4o和Gemini 1.5 Pro。

Conclusion: 研究表明，自我奖励机制在机器翻译中的效果优于外部大模型作为评判者的传统方法，并且当与训练好的奖励模型结合时具有互补优势。这为自改进强化学习方法的潜力提供了重要见解。研究团队已公开发布代码、数据和模型。

Abstract: Large language models (LLMs) have recently demonstrated remarkable
capabilities in machine translation (MT). However, most advanced MT-specific
LLMs heavily rely on external supervision signals during training, such as
human-annotated reference data or trained reward models (RMs), which are often
expensive to obtain and challenging to scale. To overcome this limitation, we
propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for
MT that is reference-free, fully online, and relies solely on self-judging
rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as
the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,
e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like
Qwen2.5-32B-Instruct in English $\leftrightarrow$ Chinese translation tasks
from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR
with external supervision from COMET, our strongest model, SSR-X-Zero-7B,
achieves state-of-the-art performance in English $\leftrightarrow$ Chinese
translation, surpassing all existing open-source models under 72B parameters
and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.
Our analysis highlights the effectiveness of the self-rewarding mechanism
compared to the external LLM-as-a-judge approach in MT and demonstrates its
complementary benefits when combined with trained RMs. Our findings provide
valuable insight into the potential of self-improving RL methods. We have
publicly released our code, data and models.

</details>


### [10] [WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.16421)
*Zhepei Wei,Wenlin Yao,Yao Liu,Weizhi Zhang,Qin Lu,Liang Qiu,Changlong Yu,Puyang Xu,Chao Zhang,Bing Yin,Hyokun Yun,Lihong Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为WebAgent-R1的多轮强化学习框架，用于训练网络代理。该框架通过在线与网络环境交互并生成多样化的轨迹来学习，并完全由任务成功与否决定的二元奖励引导。实验表明，WebAgent-R1显著提高了Qwen-2.5-3B和Llama-3.1-8B在WebArena-Lite基准上的任务成功率，且优于现有的最先进方法和强大的专有模型（如OpenAI o3）。深入分析揭示了基于思考的提示策略和测试时扩展的有效性。此外，研究还探讨了不同的RL初始化策略，引入了两个变体：WebAgent-R1-Zero和WebAgent-R1-CoT，强调了预热训练阶段的重要性以及在网络代理中结合长链推理的价值。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）在增强大型语言模型（LLMs）方面取得了显著的成功，但其主要集中在单轮任务上，例如解决数学问题。对于涉及动态网络界面的多轮交互任务，训练有效的网络代理仍具有挑战性，因为需要在长时间范围内进行复杂的决策。

Method: WebAgent-R1是一种端到端的多轮强化学习框架，直接从与网络环境的在线交互中学习。它通过异步生成多样化的轨迹来进行学习，整个过程完全由任务成功与否决定的二元奖励指导。此外，研究引入了两个变体：WebAgent-R1-Zero和WebAgent-R1-CoT，以探讨不同的RL初始化策略，强调了行为克隆（warm-up training stage）的重要性，并提供了关于如何在网络代理中结合长链推理的见解。

Result: 实验结果表明，WebAgent-R1显著提高了Qwen-2.5-3B的任务成功率，从6.1%提升至33.9%，并使Llama-3.1-8B的任务成功率从8.5%提升至44.8%，远超现有最先进的方法和强大的专有模型。此外，深入分析显示基于思考的提示策略和测试时扩展的有效性，进一步验证了框架的优越性能。

Conclusion: WebAgent-R1作为一个简单而有效的多轮强化学习框架，显著提升了网络代理在复杂多轮交互任务中的表现。通过在线交互和二元奖励机制的学习，以及对不同初始化策略的研究，为未来开发更高效的网络代理提供了重要启示。

Abstract: While reinforcement learning (RL) has demonstrated remarkable success in
enhancing large language models (LLMs), it has primarily focused on single-turn
tasks such as solving math problems. Training effective web agents for
multi-turn interactions remains challenging due to the complexity of
long-horizon decision-making across dynamic web interfaces. In this work, we
present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework
for training web agents. It learns directly from online interactions with web
environments by asynchronously generating diverse trajectories, entirely guided
by binary rewards depending on task success. Experiments on the WebArena-Lite
benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task
success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to
44.8%, significantly outperforming existing state-of-the-art methods and strong
proprietary models such as OpenAI o3. In-depth analyses reveal the
effectiveness of the thinking-based prompting strategy and test-time scaling
through increased interactions for web tasks. We further investigate different
RL initialization policies by introducing two variants, namely WebAgent-R1-Zero
and WebAgent-R1-CoT, which highlight the importance of the warm-up training
stage (i.e., behavior cloning) and provide insights on incorporating long
chain-of-thought (CoT) reasoning in web agents.

</details>


### [11] [Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning](https://arxiv.org/abs/2505.16410)
*Guanting Dong,Yifei Chen,Xiaoxi Li,Jiajie Jin,Hongjin Qian,Yutao Zhu,Hangyu Mao,Guorui Zhou,Zhicheng Dou,Ji-Rong Wen*

Main category: cs.CL

TL;DR: Tool-Star是一个基于强化学习（RL）的框架，旨在使大语言模型（LLMs）能够自主调用多个外部工具进行逐步推理。它通过数据合成和训练中的系统设计，引入了通用的工具集成推理数据合成管道以及两阶段训练框架，有效提升了多工具协作推理的能力。实验结果表明，Tool-Star在超过10个具有挑战性的推理基准上表现出高效性和有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模强化学习使大语言模型展现出卓越的推理能力，但如何利用强化学习算法实现有效的多工具协作推理仍然是一个开放性挑战。为了解决这一问题并克服工具使用数据稀缺的问题，研究者们提出了Tool-Star框架。

Method: Tool-Star框架包含以下关键方法：1) 通用工具集成推理数据合成管道，结合工具集成提示与基于提示的采样技术自动生成工具使用轨迹；2) 质量归一化和难度感知分类过程，过滤低质量样本并将数据集从简单到复杂组织；3) 两阶段训练框架，包括冷启动微调（引导LLMs通过工具调用反馈探索推理模式）和多工具自我批评RL算法（通过分层奖励设计增强奖励理解和促进有效工具协作）。

Result: 实验分析表明，Tool-Star在超过10个具有挑战性的推理基准测试中显著提高了推理的有效性和效率。这证明了该框架在提升LLMs多工具协作推理能力方面的潜力。

Conclusion: Tool-Star提供了一种新颖且高效的解决方案来增强大语言模型的多工具协作推理能力。其提出的工具集成数据合成管道和两阶段训练框架为未来的研究奠定了基础，并展示了强化学习在推动LLMs更广泛应用方面的潜力。

Abstract: Recently, large language models (LLMs) have shown remarkable reasoning
capabilities via large-scale reinforcement learning (RL). However, leveraging
the RL algorithm to empower effective multi-tool collaborative reasoning in
LLMs remains an open challenge. In this paper, we introduce Tool-Star, an
RL-based framework designed to empower LLMs to autonomously invoke multiple
external tools during stepwise reasoning. Tool-Star integrates six types of
tools and incorporates systematic designs in both data synthesis and training.
To address the scarcity of tool-use data, we propose a general tool-integrated
reasoning data synthesis pipeline, which combines tool-integrated prompting
with hint-based sampling to automatically and scalably generate tool-use
trajectories. A subsequent quality normalization and difficulty-aware
classification process filters out low-quality samples and organizes the
dataset from easy to hard. Furthermore, we propose a two-stage training
framework to enhance multi-tool collaborative reasoning by: (1) cold-start
fine-tuning, which guides LLMs to explore reasoning patterns via
tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with
hierarchical reward design, which reinforces reward understanding and promotes
effective tool collaboration. Experimental analyses on over 10 challenging
reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.
The code is available at https://github.com/dongguanting/Tool-Star.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [12] [HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving](https://arxiv.org/abs/2505.15793)
*Zhiwen Chen,Bo Leng,Zhuoren Li,Hanming Deng,Guizhe Jin,Ran Yu,Huanxi Wen*

Main category: cs.RO

TL;DR: 将大型语言模型（LLM）与强化学习（RL）结合可以提升自动驾驶在复杂场景中的性能。本文提出了一种新的LLM-Hinted RL范式，通过语义提示进行状态增强和策略优化，并设计了HCRMP架构来实现这一目标。实验表明，HCRMP在不同交通密度下任务成功率高达80.3%，并在关键驾驶条件下显著降低碰撞率11.4%。


<details>
  <summary>Details</summary>
Motivation: 目前以LLM为主导的RL方法过度依赖LLM输出，容易产生幻觉问题。评估显示，在驾驶相关任务中，最先进的LLM非幻觉率仅为约57.95%。因此，LLM产生的幻觉可能直接影响驾驶策略的性能。为了解决这一问题，需要保持LLM和RL之间的相对独立性。

Method: 本文提出了一种新型的LLM-Hinted RL范式，其中LLM用于生成语义提示以增强状态表示和优化策略，而RL代理则通过策略学习来对抗潜在的错误语义指示。基于此范式，提出了HCRMP架构，其中包括：
- 增强语义表示模块（Augmented Semantic Representation Module），用于扩展状态空间。
- 上下文稳定性锚定模块（Contextual Stability Anchor Module），利用知识库信息提高多批评家权重提示的可靠性。
- 语义缓存模块（Semantic Cache Module），无缝整合LLM低频指导与RL高频控制。

Result: 实验在CARLA平台上进行了广泛的验证，结果表明：
- HCRMP在不同交通密度下的任务成功率达到80.3%。
- 在安全关键驾驶条件下，HCRMP显著降低了碰撞率，降幅达11.4%。
这证明了该方法在复杂驾驶场景中具有优异的性能。

Conclusion: 本文提出的LLM-Hinted RL范式及HCRMP架构有效地解决了LLM幻觉对驾驶策略的影响，同时保持了较高的驾驶性能。实验结果表明，HCRMP在各种驾驶条件下的表现优于现有方法，特别是在安全性方面有显著提升。

Abstract: Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can
enhance autonomous driving (AD) performance in complex scenarios. However,
current LLM-Dominated RL methods over-rely on LLM outputs, which are prone to
hallucinations. Evaluations show that state-of-the-art LLM indicates a
non-hallucination rate of only approximately 57.95% when assessed on essential
driving-related tasks. Thus, in these methods, hallucinations from the LLM can
directly jeopardize the performance of driving policies. This paper argues that
maintaining relative independence between the LLM and the RL is vital for
solving the hallucinations problem. Consequently, this paper is devoted to
propose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic
hints for state augmentation and policy optimization to assist RL agent in
motion planning, while the RL agent counteracts potential erroneous semantic
indications through policy learning to achieve excellent driving performance.
Based on this paradigm, we propose the HCRMP (LLM-Hinted Contextual
Reinforcement Learning Motion Planner) architecture, which is designed that
includes Augmented Semantic Representation Module to extend state space.
Contextual Stability Anchor Module enhances the reliability of multi-critic
weight hints by utilizing information from the knowledge base. Semantic Cache
Module is employed to seamlessly integrate LLM low-frequency guidance with RL
high-frequency control. Extensive experiments in CARLA validate HCRMP's strong
overall driving performance. HCRMP achieves a task success rate of up to 80.3%
under diverse driving conditions with different traffic densities. Under
safety-critical driving conditions, HCRMP significantly reduces the collision
rate by 11.4%, which effectively improves the driving performance in complex
scenarios.

</details>
