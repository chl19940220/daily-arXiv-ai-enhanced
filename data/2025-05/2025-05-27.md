<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.CL](#cs.CL) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: 本文是一篇立场论文，全面回顾了人类与AI代理协作的最新实证进展，强调了技术成就和持续存在的差距。提出了一种新的概念架构（分层探索-利用网络），系统地将多代理协调、知识管理、控制论反馈环路和高级控制机制等技术细节联系起来。该框架有助于重新审视传统方法，并启发融合定性和定量范式的新型研究。文章结构灵活，可从任意部分阅读，既是对现有技术实现的批判性综述，也是设计或扩展人机共生系统的前瞻性参考。


<details>
  <summary>Details</summary>
Motivation: 当前关于人类与AI代理协作的研究缺乏一个统一的理论框架，尤其是在处理开放性复杂任务时，不同研究之间的整合不足。这促使作者提出一种能够系统化整合这些技术细节的新概念架构。

Method: 作者提出了一个名为分层探索-利用网络（Hierarchical Exploration-Exploitation Net）的概念架构，将多代理协调、知识管理、控制论反馈环路和高级控制机制等技术细节进行系统性关联。通过将现有的研究成果映射到这一框架上，包括符号AI技术、连接主义的LLM基础代理以及混合组织实践，从而促进对传统方法的修订并激发新的研究方向。

Result: 该框架不仅帮助重新评估了传统的研究方法，还为融合定性和定量范式的新型研究提供了灵感。同时，文章的灵活结构使其既可以作为技术实现的批判性综述，也可以作为设计或扩展人机共生系统的参考。

Conclusion: 本文提出的分层探索-利用网络为深入探讨人类认知与AI能力的共同进化提供了一个重要的起点。它促进了未来研究在技术实现和理论框架上的进一步发展，推动了更深层次的人类与AI协作。

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety](https://arxiv.org/abs/2505.20065)
*Geon-Hyeong Kim,Youngsoo Jang,Yu Jin Kim,Byoungjip Kim,Honglak Lee,Kyunghoon Bae,Moontae Lee*

Main category: cs.LG

TL;DR: 随着大型语言模型（LLMs）在各个领域的应用不断扩展，确保其安全性变得至关重要。现有方法通过将安全约束集成到基于人类反馈的强化学习（RLHF）中来解决安全性问题，但这些方法通常较为复杂。受直接偏好优化（DPO）的启发，本文提出了一种名为SafeDPO的新算法，能够在策略学习的单一阶段直接优化安全性对齐目标，无需松弛操作。SafeDPO仅引入一个额外的超参数以增强安全性，并且只需要对标准DPO进行轻微修改。该算法无需拟合独立的奖励和成本模型，也无需在微调期间从语言模型中采样，同时仍然提高了LLMs的安全性。实验表明，SafeDPO在与人类偏好对齐以及提升安全性方面达到了与最先进的安全性对齐算法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前确保大型语言模型（LLMs）安全性的方法主要依赖于复杂的基于人类反馈的强化学习（RLHF），这些方法需要多步骤的复杂过程，包括拟合奖励和成本模型、从语言模型中采样等。因此，简化并提高安全性对齐的过程成为一个重要的研究方向。

Method: 本文提出的SafeDPO算法基于直接偏好优化（DPO），通过在单一策略学习阶段直接优化安全性对齐目标，避免了传统RLHF中的多步骤复杂过程。SafeDPO仅引入一个额外的超参数以进一步增强安全性，同时保持对标准DPO的轻微修改。这使得算法不需要拟合独立的奖励和成本模型，也不需要在微调过程中从语言模型中采样，从而显著简化了流程。

Result: 实验结果表明，SafeDPO在安全性对齐方面取得了与当前最先进的算法相当的性能。它不仅能够更好地与人类偏好对齐，还能有效提高LLMs的安全性，同时减少了计算复杂度和资源需求。

Conclusion: SafeDPO提供了一种简单而有效的解决方案，用于在大型语言模型中实现安全性对齐。通过直接优化安全性目标并减少复杂步骤，SafeDPO为未来的研究提供了一个更高效、更易于实现的框架，有助于推动LLMs在实际应用中的安全性保障。

Abstract: As Large Language Models (LLMs) continue to advance and find applications
across a growing number of fields, ensuring the safety of LLMs has become
increasingly critical. To address safety concerns, recent studies have proposed
integrating safety constraints into Reinforcement Learning from Human Feedback
(RLHF). However, these approaches tend to be complex, as they encompass
complicated procedures in RLHF along with additional steps required by the
safety constraints. Inspired by Direct Preference Optimization (DPO), we
introduce a new algorithm called SafeDPO, which is designed to directly
optimize the safety alignment objective in a single stage of policy learning,
without requiring relaxation. SafeDPO introduces only one additional
hyperparameter to further enhance safety and requires only minor modifications
to standard DPO. As a result, it eliminates the need to fit separate reward and
cost models or to sample from the language model during fine-tuning, while
still enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO
achieves competitive performance compared to state-of-the-art safety alignment
algorithms, both in terms of aligning with human preferences and improving
safety.

</details>


### [3] [An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning](https://arxiv.org/abs/2505.19954)
*Andrew Zamai,Nathanael Fijalkow,Boris Mansencal,Laurent Simon,Eloi Navet,Pierrick Coupe*

Main category: cs.LG

TL;DR: 本文提出了一种框架，将3D脑部MRI转换为文本报告，并利用大型语言模型（LLMs）协助诊断不同类型的痴呆症和正常老化。通过强化学习方法，该框架在保持诊断准确性的同时生成了基于神经影像发现的结构化诊断理由，从而提高了模型的透明性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法虽然在预测性能上表现出色，但由于其决策过程不透明，在临床应用中受到限制。因此，需要一种既能保持高诊断准确率又能提供透明决策依据的方法来增强临床实用性。

Method: 该研究设计了一个模块化管道，用于将3D T1加权脑部MRI转化为文本形式的放射学报告。接着，利用现代大型语言模型（LLMs）根据生成的报告进行不同类型痴呆症与正常老化的鉴别诊断。此外，采用强化学习技术激励LLMs生成结构化诊断理由，这些理由以神经影像结果为基础，无需监督式推理痕迹或从更大模型中蒸馏知识。

Result: 实验表明，该框架在诊断表现上与现有的深度学习方法相当，同时能够生成支持诊断结论的因果解释，从而增强了模型的可解释性。

Conclusion: 此框架成功地平衡了预测准确性和可解释性，为临床医生提供了既高效又透明的辅助诊断工具，有助于改善神经退行性痴呆的诊断效率和准确性。

Abstract: The differential diagnosis of neurodegenerative dementias is a challenging
clinical task, mainly because of the overlap in symptom presentation and the
similarity of patterns observed in structural neuroimaging. To improve
diagnostic efficiency and accuracy, deep learning-based methods such as
Convolutional Neural Networks and Vision Transformers have been proposed for
the automatic classification of brain MRIs. However, despite their strong
predictive performance, these models find limited clinical utility due to their
opaque decision making. In this work, we propose a framework that integrates
two core components to enhance diagnostic transparency. First, we introduce a
modular pipeline for converting 3D T1-weighted brain MRIs into textual
radiology reports. Second, we explore the potential of modern Large Language
Models (LLMs) to assist clinicians in the differential diagnosis between
Frontotemporal dementia subtypes, Alzheimer's disease, and normal aging based
on the generated reports. To bridge the gap between predictive accuracy and
explainability, we employ reinforcement learning to incentivize diagnostic
reasoning in LLMs. Without requiring supervised reasoning traces or
distillation from larger models, our approach enables the emergence of
structured diagnostic rationales grounded in neuroimaging findings. Unlike
post-hoc explainability methods that retrospectively justify model decisions,
our framework generates diagnostic rationales as part of the inference
process-producing causally grounded explanations that inform and guide the
model's decision-making process. In doing so, our framework matches the
diagnostic performance of existing deep learning methods while offering
rationales that support its diagnostic conclusions.

</details>


### [4] [What Can RL Bring to VLA Generalization? An Empirical Study](https://arxiv.org/abs/2505.19789)
*Jijia Liu,Feng Gao,Bingwen Wei,Xinlei Chen,Qingmin Liao,Yi Wu,Chao Yu,Yu Wang*

Main category: cs.LG

TL;DR: 本文研究了强化学习（RL）对视觉-语言动作（VLA）模型的泛化能力提升，并通过全面基准测试发现，使用PPO进行RL微调能显著增强语义理解和执行鲁棒性，同时保持与监督微调（SFT）相当的视觉鲁棒性。此外，作者提出了一种高效的PPO训练方法，证明其在改进VLA泛化方面的实际效用。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉-语言动作（VLA）模型在具身AI领域展现出了巨大的潜力，但主要依赖监督微调（SFT）的训练方式限制了其泛化能力，特别是在分布偏移下容易产生累积错误。虽然强化学习（RL）可以通过试错优化任务目标来克服这些限制，但目前尚缺乏对其在VLA模型中具体泛化优势的系统理解。

Method: 本研究引入了一个全面的基准测试框架，用于评估VLA模型的泛化性能，并系统地研究了不同维度（视觉、语义和执行）上强化学习微调的影响。通过广泛的实验比较了多种RL算法（如PPO、DPO和GRPO）与监督微调的效果。此外，还开发了一种简单高效的PPO训练方法以促进VLA模型的泛化能力。

Result: 实验结果表明，使用PPO进行RL微调可以显著提高VLA模型在语义理解和执行鲁棒性方面的泛化能力，同时保持与SFT相当的视觉鲁棒性。相较于其他LLM衍生方法（如DPO和GRPO），PPO被确认为更有效的RL算法。所提出的高效PPO训练方法也展现了其实用价值。

Conclusion: 强化学习（特别是PPO算法）能够有效改善VLA模型的泛化能力，在语义理解和执行鲁棒性方面表现尤为突出。本研究不仅填补了RL在VLA模型泛化优势方面的系统理解空白，还提供了一种实用的PPO训练方案，推动了VLA模型的实际应用和发展。

Abstract: Large Vision-Language Action (VLA) models have shown significant potential
for embodied AI. However, their predominant training via supervised fine-tuning
(SFT) limits generalization due to susceptibility to compounding errors under
distribution shifts. Reinforcement learning (RL) offers a path to overcome
these limitations by optimizing for task objectives via trial-and-error, yet a
systematic understanding of its specific generalization benefits for VLAs
compared to SFT is lacking. To address this, our study introduces a
comprehensive benchmark for evaluating VLA generalization and systematically
investigates the impact of RL fine-tuning across diverse visual, semantic, and
execution dimensions. Our extensive experiments reveal that RL fine-tuning,
particularly with PPO, significantly enhances generalization in semantic
understanding and execution robustness over SFT, while maintaining comparable
visual robustness. We identify PPO as a more effective RL algorithm for VLAs
than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for
efficient PPO training on VLAs, and demonstrate its practical utility for
improving VLA generalization. The project page is at https://rlvla.github.io

</details>


### [5] [Learning to Reason without External Rewards](https://arxiv.org/abs/2505.19590)
*Xuandong Zhao,Zhewei Kang,Aosong Feng,Sergey Levine,Dawn Song*

Main category: cs.LG

TL;DR: 提出了一种名为Intuitor的RLIF方法，使用模型自身的置信度（self-certainty）作为唯一奖励信号，无需外部奖励或标注数据。实验表明，在数学基准测试中，Intuitor的表现与GRPO相当，并在诸如代码生成等域外任务上表现出更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的通过可验证奖励进行强化学习（RLVR）的方法虽然有效，但依赖于昂贵且特定领域的监督。为了克服这一限制，研究探索了基于内在反馈的强化学习（RLIF），以使大型语言模型能够从内在信号中学习，而不需要外部奖励或标注数据。

Method: Intuitor方法利用模型自身的置信度（self-certainty）作为唯一的奖励信号，取代了GRPO中的外部奖励，从而实现了完全无监督的学习。

Result: 实验结果表明，Intuitor在数学基准测试中的表现与GRPO相当，并且在域外任务（如代码生成）上表现出更好的泛化能力，而不需要黄金解决方案或测试用例。

Conclusion: 研究表明，模型的内在信号可以驱动跨领域的有效学习，为没有可验证奖励的自主AI系统提供了一个可扩展的替代方案。

Abstract: Training large language models (LLMs) for complex reasoning via Reinforcement
Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on
costly, domain-specific supervision. We explore Reinforcement Learning from
Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic
signals without external rewards or labeled data. We propose Intuitor, an RLIF
method that uses a model's own confidence, termed self-certainty, as its sole
reward signal. Intuitor replaces external rewards in Group Relative Policy
Optimization (GRPO) with self-certainty scores, enabling fully unsupervised
learning. Experiments demonstrate that Intuitor matches GRPO's performance on
mathematical benchmarks while achieving superior generalization to
out-of-domain tasks like code generation, without requiring gold solutions or
test cases. Our findings show that intrinsic model signals can drive effective
learning across domains, offering a scalable alternative to RLVR for autonomous
AI systems where verifiable rewards are unavailable. Code is available at
https://github.com/sunblaze-ucb/Intuitor

</details>


### [6] [VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use](https://arxiv.org/abs/2505.19255)
*Mingyuan Wu,Jingcheng Yang,Jize Jiang,Meitang Li,Kaizhuo Yan,Hanchao Yu,Minjia Zhang,Chengxiang Zhai,Klara Nahrstedt*

Main category: cs.LG

TL;DR: 本文提出了一种名为VTool-R1的框架，这是首个能够训练视觉语言模型（VLMs）生成多模态思维链的方法。通过将基于Python的视觉编辑工具整合到强化学习微调（RFT）过程中，VTool-R1使VLMs能够学习何时以及如何生成有助于最终推理的视觉推理步骤。实验表明，这种方法通过教会VLMs‘用图像思考’并生成多模态思维链来提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前针对视觉语言模型（VLMs）的强化学习微调（RFT）方法大多局限于静态图像输入上的文本推理，未能实现真正的多模态推理响应。而测试时方法如Visual Sketchpad虽然包含视觉步骤，但缺乏有效的训练机制。因此需要一种新的框架来弥补这些不足，让VLMs能生成结合文本和中间视觉推理步骤的多模态思维链。

Method: VTool-R1框架将基于Python的视觉编辑工具融入到RFT过程中，使VLMs能够在生成推理步骤时，根据任务需求决定是否以及如何使用视觉工具进行推理。该方法以任务准确性为导向，通过结果导向奖励机制引导VLMs策略性地使用视觉工具进行推理，而无需依赖过程监督。

Result: 在结构化图表和表格的视觉问答任务中的实验表明，VTool-R1显著提升了VLMs的推理能力，使其能够通过生成多模态思维链来进行更有效的推理。

Conclusion: VTool-R1作为首个训练VLMs生成多模态思维链的框架，成功实现了文本与视觉推理步骤的结合，提高了VLMs在视觉问答任务中的表现，证明了通过强化学习引导VLMs进行多模态推理的有效性。

Abstract: Reinforcement Learning Finetuning (RFT) has significantly advanced the
reasoning capabilities of large language models (LLMs) by enabling long chains
of thought, self-correction, and effective tool use. While recent works attempt
to extend RFT to vision-language models (VLMs), these efforts largely produce
text-only reasoning conditioned on static image inputs, falling short of true
multimodal reasoning in the response. In contrast, test-time methods like
Visual Sketchpad incorporate visual steps but lack training mechanisms.
  We introduce VTool-R1, the first framework that trains VLMs to generate
multimodal chains of thought by interleaving text and intermediate visual
reasoning steps. VTool-R1 integrates Python-based visual editing tools into the
RFT process, enabling VLMs to learn when and how to generate visual reasoning
steps that benefit final reasoning. Trained with outcome-based rewards tied to
task accuracy, our approach elicits strategic visual tool use for reasoning
without relying on process-based supervision. Experiments on structured visual
question answering over charts and tables show that VTool-R1 enhances reasoning
performance by teaching VLMs to "think with images" and generate multimodal
chain of thoughts with tools.

</details>


### [7] [GhostPrompt: Jailbreaking Text-to-image Generative Models based on Dynamic Optimization](https://arxiv.org/abs/2505.18979)
*Zixuan Chen,Hao Lin,Ke Xu,Xinghao Jiang,Tanfeng Sun*

Main category: cs.LG

TL;DR: 本文提出了一种名为GhostPrompt的自动化越狱框架，该框架结合了动态提示优化和多模态反馈，能够有效规避现代文本和图像安全过滤器。通过动态优化和适应性安全指示注入，GhostPrompt显著提高了对ShieldLM-7B等模型的规避成功率，并能绕过未见过的过滤器如GPT-4.1，揭示了当前多模态防御中的系统性漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像生成模型的发展，意外生成不适宜内容（NSFW）的问题日益突出，因此需要集成文本和图像安全过滤器。然而，现有的规避方法对现代基于大型语言模型（LLM）的语义级检测过滤器无效，促使研究者开发更有效的规避技术以测试和改进这些过滤器的安全性。

Method: GhostPrompt框架包含两个关键部分：(i) 动态优化，利用文本安全过滤器反馈和CLIP相似度分数迭代引导LLM生成语义对齐的对抗性提示；(ii) 适应性安全指示注入，将良性视觉线索的注入建模为强化学习问题，以规避图像级别的过滤器。

Result: GhostPrompt显著提升了对ShieldLM-7B的规避率，从12.5%提高到99.0%，同时改善了CLIP分数并降低了时间成本。此外，它还能成功绕过未见过的过滤器，如GPT-4.1，并在实验中成功让DALLE 3生成NSFW图像。

Conclusion: GhostPrompt揭示了当前多模态防御中的系统性漏洞，为进一步研究AI安全和红队测试提供了支持。作者计划在受控访问协议下发布代码和对抗性提示，以促进相关领域的进一步研究。

Abstract: Text-to-image (T2I) generation models can inadvertently produce
not-safe-for-work (NSFW) content, prompting the integration of text and image
safety filters. Recent advances employ large language models (LLMs) for
semantic-level detection, rendering traditional token-level perturbation
attacks largely ineffective. However, our evaluation shows that existing
jailbreak methods are ineffective against these modern filters. We introduce
GhostPrompt, the first automated jailbreak framework that combines dynamic
prompt optimization with multimodal feedback. It consists of two key
components: (i) Dynamic Optimization, an iterative process that guides a large
language model (LLM) using feedback from text safety filters and CLIP
similarity scores to generate semantically aligned adversarial prompts; and
(ii) Adaptive Safety Indicator Injection, which formulates the injection of
benign visual cues as a reinforcement learning problem to bypass image-level
filters. GhostPrompt achieves state-of-the-art performance, increasing the
ShieldLM-7B bypass rate from 12.5\% (Sneakyprompt) to 99.0\%, improving CLIP
score from 0.2637 to 0.2762, and reducing the time cost by $4.2 \times$.
Moreover, it generalizes to unseen filters including GPT-4.1 and successfully
jailbreaks DALLE 3 to generate NSFW images in our evaluation, revealing
systemic vulnerabilities in current multimodal defenses. To support further
research on AI safety and red-teaming, we will release code and adversarial
prompts under a controlled-access protocol.

</details>


### [8] [On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization](https://arxiv.org/abs/2505.18830)
*Wenlong Deng,Yi Ren,Muchen Li,Danica J. Sutherland,Xiaoxiao Li,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 提出了一种新方法NTHR，用于解决强化学习中GRPO算法的Lazy Likelihood Displacement问题，通过实验验证其在数学推理基准上的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管Group Relative Policy Optimization (GRPO)算法被广泛应用于提升大语言模型的推理能力，但研究者发现了一个未被识别的现象——Lazy Likelihood Displacement（LLD），即在训练过程中正确响应的可能性仅略微增加甚至减少。此问题与Direct Preference Optimization (DPO)中的misalignment问题类似，均受到负梯度的影响。因此，需要一种新的方法来解决这一问题。

Method: 研究者提出了NTHR方法，该方法通过降低对导致LLD的token的惩罚权重来解决问题。NTHR利用了GRPO的基于组的结构，使用正确的响应作为锚点来识别有影响力的token，从而更精确地调整惩罚力度。

Result: 在数学推理基准上的实验表明，NTHR有效缓解了LLD问题，并在参数规模从0.5B到3B的模型上带来了一致的性能提升。

Conclusion: NTHR方法成功解决了GRPO算法中的LLD问题，提升了模型在数学推理任务中的表现，为未来的研究提供了新的思路。

Abstract: Reinforcement learning (RL) has become popular in enhancing the reasoning
capabilities of large language models (LLMs), with Group Relative Policy
Optimization (GRPO) emerging as a widely used algorithm in recent systems.
Despite GRPO's widespread adoption, we identify a previously unrecognized
phenomenon we term Lazy Likelihood Displacement (LLD), wherein the likelihood
of correct responses marginally increases or even decreases during training.
This behavior mirrors a recently discovered misalignment issue in Direct
Preference Optimization (DPO), attributed to the influence of negative
gradients. We provide a theoretical analysis of GRPO's learning dynamic,
identifying the source of LLD as the naive penalization of all tokens in
incorrect responses with the same strength. To address this, we develop a
method called NTHR, which downweights penalties on tokens contributing to the
LLD. Unlike prior DPO-based approaches, NTHR takes advantage of GRPO's
group-based structure, using correct responses as anchors to identify
influential tokens. Experiments on math reasoning benchmarks demonstrate that
NTHR effectively mitigates LLD, yielding consistent performance gains across
models ranging from 0.5B to 3B parameters.

</details>


### [9] [LLM-QFL: Distilling Large Language Model for Quantum Federated Learning](https://arxiv.org/abs/2505.18656)
*Dev Gurung,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 本研究将大型语言模型（LLMs）引入量子联邦学习（QFL），提出了一种联合微调方法，使LLM在QFL中进行知识蒸馏，从而提高效率和性能。通过减少通信轮次、优化选择客户端等策略，该方法显著提升了QFL的效率，并且具备理论严谨性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前量子联邦学习（QFL）需要解决通信成本高、收敛速度慢的问题，而大型语言模型（LLMs）具有强大的建模能力，将其引入QFL可以进一步提升其效率和性能。因此，本研究探索了LLM在QFL中的应用潜力。

Method: 我们提出了一种联合微调方法，允许每个客户端在本地对LLM进行适应性调整，同时保留隐私并减少不必要的全局更新。此外，微调后的LLM充当强化学习代理，通过优化优化器步骤、减少通信轮次以及智能选择客户端来改进QFL。为了增强可扩展性，还采用了PEFT方法（如LoRA和QLoRA）。

Result: 实验结果表明，该方法显著提高了QFL的效率，减少了通信成本并加快了收敛速度。同时，提供了可证明的理论保证以支持自适应联邦优化，并展示了在资源受限的量子设备上的可行性。

Conclusion: 这项工作开创性地结合了LLM和QFL，实现了实际效率提升、理论严谨性和可扩展性的统一，为未来的研究提供了新的方向。代码实现已经公开，可供进一步研究和应用。

Abstract: Inspired by the power of large language models (LLMs), our research adapts
them to quantum federated learning (QFL) to boost efficiency and performance.
We propose a federated fine-tuning method that distills an LLM within QFL,
allowing each client to locally adapt the model to its own data while
preserving privacy and reducing unnecessary global updates. The fine-tuned LLM
also acts as a reinforcement agent, optimizing QFL by adjusting optimizer
steps, cutting down communication rounds, and intelligently selecting clients.
Experiments show significant efficiency gains. We pioneer a synergy between LLM
and QFL, offering: i) practical efficiency: Reduced communication costs and
faster convergence. ii) theoretical rigor: Provable guarantees for adaptive
federated optimization. iii) scalability: PEFT methods (LoRA, QLoRA) enable
deployment on resource-constrained quantum devices. Code implementation is
available here 1.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [10] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 在大规模语言模型（LLMs）中，通过重复采样来扩展推理计算可以持续增加覆盖率（解决问题的比例）。然而，这种改进部分可能是由于标准评估基准中的答案分布偏向于少量常见答案。研究通过定义一个基于训练集答案频率的基线，发现这一基线在某些LLMs上优于重复采样，而在其他模型上与一种混合策略表现相当。这为更准确地衡量重复采样在提示无关猜测之外的覆盖率提升提供了方法。


<details>
  <summary>Details</summary>
Motivation: 当前研究观察到，在大规模语言模型中，随着采样次数增加，推理计算的覆盖率会持续提高。但研究者推测，这种提高可能部分归因于标准评估基准中答案分布的偏斜（即答案集中在少数常见答案上）。为了验证这一假设，并更准确地测量重复采样的实际效果，研究提出了一个基于训练集答案频率的基线进行对比实验。

Method: 研究者定义了一个基线方法，该方法根据训练集中答案的出现频率枚举答案。然后在两个领域（数学推理和事实知识）进行了实验，比较了这一基线与重复模型采样以及一种混合策略的表现。混合策略通过仅使用10次模型采样获取部分答案，其余答案则通过枚举猜测完成。

Result: 实验结果表明，对于某些LLMs，基于训练集答案频率的基线方法比重复模型采样表现更好；而对于其他模型，其覆盖范围与混合策略相当。这说明重复采样的效果可能受到答案分布特性的影响。

Conclusion: 本研究表明，重复采样对覆盖率的提升部分可归因于评估基准中答案分布的偏斜。通过引入基于训练集答案频率的基线，可以更准确地评估重复采样在提示无关猜测之外的实际贡献。这为未来研究改进LLMs的推理性能提供了新的视角。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [11] [MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning](https://arxiv.org/abs/2505.19714)
*Zhaopeng Feng,Yupu Liang,Shaosheng Cao,Jiayuan Su,Jiahan Ren,Zhe Xu,Yao Hu,Wenxuan Huang,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: 本文介绍了MT³，一种应用多任务强化学习（RL）到多模态大语言模型（MLLMs）中以实现端到端文本图像机器翻译（TIMT）的框架。通过引入XHSPost基准数据集和采用多混合奖励机制，该方法在MIT-10M基准上超越了现有模型，并展现出对未见语言对和数据集的强大泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前的文本图像机器翻译（TIMT）任务面临诸多挑战，包括准确的光学字符识别（OCR）、强大的视觉-文本推理以及高质量的翻译需求。尽管大规模强化学习在提升大语言模型（LLMs）和多模态大语言模型（MLLMs）推理能力方面取得了进展，但其在端到端TIMT中的应用尚未得到充分探索。因此，需要一种新的框架来解决这些问题并提高TIMT的整体性能。

Method: 提出了MT³框架，它采用多任务优化范式，专注于三个关键子技能：文本识别、上下文感知推理和翻译。该框架通过新颖的多混合奖励机制进行训练，该机制将基于规则的强化学习策略适应于TIMT的复杂性，提供跨任务的精细非二元反馈。此外，为了促进TIMT在真实跨文化和社交媒体环境中的评估，还引入了XHSPost作为首个社交媒体TIMT基准数据集。

Result: MT³-7B-Zero在最新的MIT-10M基准上实现了最先进的结果，显著超越了如Qwen2.5-VL-72B和InternVL2.5-78B等强大基线模型。该模型在多个指标上表现出色，并且对未见语言对和数据集显示出强大的泛化能力。深入分析表明，多任务协同效应、强化学习初始化、课程设计和奖励公式对推进MLLM驱动的TIMT具有重要作用。

Conclusion: MT³框架为端到端TIMT提供了一种创新的解决方案，通过多任务强化学习优化了多模态大语言模型的性能。实验结果证明了该方法的有效性和优越性，特别是在真实世界应用场景中。未来的研究可以进一步探索多任务强化学习在其他多模态任务中的潜力。

Abstract: Text Image Machine Translation (TIMT)-the task of translating textual content
embedded in images-is critical for applications in accessibility, cross-lingual
information access, and real-world document understanding. However, TIMT
remains a complex challenge due to the need for accurate optical character
recognition (OCR), robust visual-text reasoning, and high-quality translation,
often requiring cascading multi-stage pipelines. Recent advances in large-scale
Reinforcement Learning (RL) have improved reasoning in Large Language Models
(LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is
still underexplored. To bridge this gap, we introduce MT$^{3}$, the first
framework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts
a multi-task optimization paradigm targeting three key sub-skills: text
recognition, context-aware reasoning, and translation. It is trained using a
novel multi-mixed reward mechanism that adapts rule-based RL strategies to
TIMT's intricacies, offering fine-grained, non-binary feedback across tasks.
Furthermore, to facilitate the evaluation of TIMT in authentic cross-cultural
and real-world social media contexts, we introduced XHSPost, the first social
media TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on
the latest in-domain MIT-10M benchmark, outperforming strong baselines such as
Qwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics.
Additionally, the model shows strong generalization to out-of-distribution
language pairs and datasets. In-depth analyses reveal how multi-task synergy,
reinforcement learning initialization, curriculum design, and reward
formulation contribute to advancing MLLM-driven TIMT.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [12] [VLMLight: Traffic Signal Control via Vision-Language Meta-Control and Dual-Branch Reasoning](https://arxiv.org/abs/2505.19486)
*Maonan Wang,Yirong Chen,Aoyu Pang,Yuxin Cai,Chung Shue Chen,Yuheng Kan,Man-On Pun*

Main category: eess.SY

TL;DR: 本文提出了一种名为VLMLight的新型交通信号控制框架，它结合了视觉-语言元控制与双分支推理方法。通过图像基础的交通模拟器和大语言模型（LLM）作为安全优先的元控制器，该框架在保证常规条件下实时性能的同时，显著减少了紧急车辆的等待时间，并提供了一个可扩展、可解释且注重安全性的下一代交通信号控制解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的交通信号控制方法，无论是基于规则的启发式方法还是强化学习方法，在面对复杂、动态以及安全性至关重要的场景时，往往难以泛化。因此，需要一种新的方法来应对这些挑战，确保在不同场景下都能实现高效和安全的交通管理。

Method: VLMLight的核心是一个基于图像的交通模拟器，能够支持多视角视觉感知，从而让策略利用诸如车辆类型、运动和空间密度等丰富信息进行推理。其中，大语言模型（LLM）作为安全优先的元控制器，在常规交通情况下选择快速的强化学习策略，而在关键情况下则切换到结构化推理分支。在后者中，多个LLM代理协同工作以评估交通阶段、优先处理紧急车辆并验证规则合规性。

Result: 实验结果表明，VLMLight在减少紧急车辆等待时间方面比仅使用强化学习的系统提高了65%，同时在标准条件下保持了实时性能，性能下降不到1%。

Conclusion: VLMLight为下一代交通信号控制提供了可扩展、可解释且注重安全性的解决方案，能够在提高紧急车辆通行效率的同时，保持常规交通条件下的高性能表现。

Abstract: Traffic signal control (TSC) is a core challenge in urban mobility, where
real-time decisions must balance efficiency and safety. Existing methods -
ranging from rule-based heuristics to reinforcement learning (RL) - often
struggle to generalize to complex, dynamic, and safety-critical scenarios. We
introduce VLMLight, a novel TSC framework that integrates vision-language
meta-control with dual-branch reasoning. At the core of VLMLight is the first
image-based traffic simulator that enables multi-view visual perception at
intersections, allowing policies to reason over rich cues such as vehicle type,
motion, and spatial density. A large language model (LLM) serves as a
safety-prioritized meta-controller, selecting between a fast RL policy for
routine traffic and a structured reasoning branch for critical cases. In the
latter, multiple LLM agents collaborate to assess traffic phases, prioritize
emergency vehicles, and verify rule compliance. Experiments show that VLMLight
reduces waiting times for emergency vehicles by up to 65% over RL-only systems,
while preserving real-time performance in standard conditions with less than 1%
degradation. VLMLight offers a scalable, interpretable, and safety-aware
solution for next-generation traffic signal control.

</details>
