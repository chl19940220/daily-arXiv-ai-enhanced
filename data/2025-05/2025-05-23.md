<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: 这篇文章是一篇立场论文，批判性地综述了人类与AI智能体协作的广泛经验性进展。文章提出了一种新的概念架构（Hierarchical Exploration-Exploitation Net），旨在系统整合多智能体协调、知识管理、控制论反馈环路和高层控制机制等技术细节。通过映射现有的贡献到该框架上，文章促进了传统方法的修订，并激发了融合定性和定量范式的新型研究。


<details>
  <summary>Details</summary>
Motivation: 当前关于人类与AI智能体协作的研究虽然取得了许多技术成就，但仍然存在持久的差距。特别是缺乏一种统一的理论框架，能够连贯地整合这些不同的研究，尤其是在处理开放性复杂任务时。因此，需要提出一个新的概念架构来解决这一问题。

Method: 文章提出了一个名为 Hierarchical Exploration-Exploitation Net 的新概念架构。该架构系统地将多智能体协调、知识管理、控制论反馈环路和高层控制机制等技术细节联系起来。同时，它还涵盖了从符号AI技术、基于连接主义的大语言模型智能体到混合组织实践的各种现有贡献。

Result: 通过使用提出的框架，文章成功地对现有研究进行了重新审视，揭示了传统方法的不足，并为未来的研究提供了方向，特别是在融合定性和定量范式方面。此外，文章结构灵活，可以从任何部分阅读，既可以作为技术实现的批判性评论，也可以作为设计或扩展人机共生关系的前瞻性参考。

Conclusion: 文章总结认为，所提出的概念框架为更深层次的人类认知与AI能力共进化提供了一个重要的踏脚石。通过促进不同研究领域的整合，文章推动了人机协作领域的发展。

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO](https://arxiv.org/abs/2505.17017)
*Chengzhuo Tong,Ziyu Guo,Renrui Zhang,Wenyu Shan,Xinyu Wei,Zhenghao Xing,Hongsheng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 本文探讨了在自回归图像生成中应用强化学习（RL）算法（如GRPO和DPO）以提升链式思维（CoT）推理能力的潜力，分析了不同奖励模型对这些算法的影响，并提出了三种扩展策略以增强其性能。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在大语言模型中的链式思维推理方面取得显著进展，但在自回归图像生成领域，尚未深入研究如何有效利用RL算法应对特定挑战（如文本-图像一致性、图像美学质量等）。因此，有必要评估不同RL算法在此领域的表现以及奖励模型对其性能的影响。

Method: 作者通过实验评估了GRPO和DPO两种RL算法在自回归图像生成中的域内性能和域外泛化能力，同时分析了不同类型奖励模型对这两种算法的影响。此外，还系统地研究了三种扩展策略以提高算法的整体效能。

Result: 研究发现，GRPO和DPO各自具有独特的优势，且具有更强内在泛化能力的奖励模型能够显著提升所应用RL算法的泛化能力。此外，三种扩展策略为高效提升算法性能提供了新的见解。

Conclusion: 本研究为未来开发更有效的RL算法以实现自回归图像生成中的稳健链式思维推理奠定了基础，强调了设计具备强泛化能力的奖励模型的重要性，并为后续工作提供了理论指导与实践参考。

Abstract: Recent advancements underscore the significant role of Reinforcement Learning
(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large
language models (LLMs). Two prominent RL algorithms, Direct Preference
Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central
to these developments, showcasing different pros and cons. Autoregressive image
generation, also interpretable as a sequential CoT reasoning process, presents
unique challenges distinct from LLM-based CoT reasoning. These encompass
ensuring text-image consistency, improving image aesthetic quality, and
designing sophisticated reward models, rather than relying on simpler
rule-based rewards. While recent efforts have extended RL to this domain, these
explorations typically lack an in-depth analysis of the domain-specific
challenges and the characteristics of different RL strategies. To bridge this
gap, we provide the first comprehensive investigation of the GRPO and DPO
algorithms in autoregressive image generation, evaluating their in-domain
performance and out-of-domain generalization, while scrutinizing the impact of
different reward models on their respective capabilities. Our findings reveal
that GRPO and DPO exhibit distinct advantages, and crucially, that reward
models possessing stronger intrinsic generalization capabilities potentially
enhance the generalization potential of the applied RL algorithms. Furthermore,
we systematically explore three prevalent scaling strategies to enhance both
their in-domain and out-of-domain proficiency, deriving unique insights into
efficiently scaling performance for each paradigm. We hope our study paves a
new path for inspiring future work on developing more effective RL algorithms
to achieve robust CoT reasoning in the realm of autoregressive image
generation. Code is released at
https://github.com/ZiyuGuo99/Image-Generation-CoT

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Divide-Fuse-Conquer: Eliciting "Aha Moments" in Multi-Scenario Games](https://arxiv.org/abs/2505.16401)
*Xiaoqing Zhang,Huabin Zheng,Ang Lv,Yuhan Liu,Zirui Song,Flood Sung,Xiuying Chen,Rui Yan*

Main category: cs.LG

TL;DR: 提出了一种名为Divide-Fuse-Conquer的框架，用于增强多场景强化学习中的泛化能力。通过将游戏分组、训练专门模型并融合参数，该方法在TextArena游戏中显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在数学、编码和视觉任务中表现出色，但在多场景游戏中面临规则多样性、交互模式和环境复杂性的挑战，导致策略在一个场景中表现良好但在其他场景中无法泛化。

Method: 首先根据游戏的规则和难度等特征对游戏进行启发式分组；然后为每个组训练专门的模型以擅长该组的游戏（即分步）；接着从不同组融合模型参数形成新模型，并继续对其进行多组训练，直到所有组的场景都被攻克。

Result: 在18个TextArena游戏的实验中，使用Divide-Fuse-Conquer策略训练的Qwen2.5-32B-Align达到了与Claude3.5相当的性能水平，取得了7胜4平的成绩。

Conclusion: 希望Divide-Fuse-Conquer方法能够激励未来关于利用强化学习来提高大型语言模型泛化的研究。

Abstract: Large language models (LLMs) have been observed to suddenly exhibit advanced
reasoning abilities during reinforcement learning (RL), resembling an ``aha
moment'' triggered by simple outcome-based rewards. While RL has proven
effective in eliciting such breakthroughs in tasks involving mathematics,
coding, and vision, it faces significant challenges in multi-scenario games.
The diversity of game rules, interaction modes, and environmental complexities
often leads to policies that perform well in one scenario but fail to
generalize to others. Simply combining multiple scenarios during training
introduces additional challenges, such as training instability and poor
performance. To overcome these challenges, we propose Divide-Fuse-Conquer, a
framework designed to enhance generalization in multi-scenario RL. This
approach starts by heuristically grouping games based on characteristics such
as rules and difficulties. Specialized models are then trained for each group
to excel at games in the group is what we refer to as the divide step. Next, we
fuse model parameters from different groups as a new model, and continue
training it for multiple groups, until the scenarios in all groups are
conquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align
trained with the Divide-Fuse-Conquer strategy reaches a performance level
comparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can
inspire future research on using reinforcement learning to improve the
generalization of LLMs.

</details>


### [4] [SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning](https://arxiv.org/abs/2505.16368)
*Huanyu Liu,Jia Li,Hao Zhu,Kechi Zhang,Yihong Dong,Ge Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于SAT问题的强化学习框架Saturn，用于训练和评估大语言模型（LLMs）的推理能力。该框架解决了现有任务在可扩展性、可验证性和难度控制方面的局限性，并设计了课程学习管道以逐步提升LLM的推理能力。实验结果表明，Saturn在SAT问题、数学和编程任务上均显著提高了LLM的表现。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习任务存在三个主要局限：1）依赖大量人工标注或昂贵的LLM生成数据导致可扩展性差；2）LLM输出难以自动可靠地验证；3）大多数任务缺乏精细的难度控制，难以实现从简单到复杂的推理能力培养。因此，需要一种新的方法来克服这些限制，更有效地释放LLM的推理潜力。

Method: 作者提出了Saturn框架，利用布尔可满足性（SAT）问题来训练和评估LLM的推理能力。具体方法包括：1）通过SAT问题实现可扩展的任务构建；2）采用基于规则的验证机制确保输出的可靠性；3）设计精确的难度控制系统；4）构建课程学习管道，逐步增加任务难度，从而持续提升LLM的推理能力。此外，作者还引入了一个包含2660个不同难度SAT问题的数据集Saturn-2.6k，用于支持对LLM推理能力的评估。

Result: 实验结果表明：1）在SAT问题上，Saturn-1.5B和Saturn-7B分别实现了平均+14.0和+28.1的pass@3改进；2）在数学和编程任务上，Saturn-1.5B和Saturn-7B分别提升了+4.9和+1.8的基准分数；3）与现有最先进的RL任务构建方法相比，Saturn进一步提高了+8.8%的性能。

Conclusion: Saturn框架有效解决了现有强化学习任务的局限性，显著提升了LLM在SAT问题、数学和编程任务上的推理能力。作者开源了代码、数据和模型，为未来研究提供了支持。

Abstract: How to design reinforcement learning (RL) tasks that effectively unleash the
reasoning capability of large language models (LLMs) remains an open question.
Existing RL tasks (e.g., math, programming, and constructing reasoning tasks)
suffer from three key limitations: (1) Scalability. They rely heavily on human
annotation or expensive LLM synthesis to generate sufficient training data. (2)
Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3)
Controllable Difficulty. Most tasks lack fine-grained difficulty control,
making it hard to train LLMs to develop reasoning ability from easy to hard.
  To address these limitations, we propose Saturn, a SAT-based RL framework
that uses Boolean Satisfiability (SAT) problems to train and evaluate LLM
reasoning. Saturn enables scalable task construction, rule-based verification,
and precise difficulty control. Saturn designs a curriculum learning pipeline
that continuously improves LLMs' reasoning capability by constructing SAT tasks
of increasing difficulty and training LLMs from easy to hard. To ensure stable
training, we design a principled mechanism to control difficulty transitions.
  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying
difficulty. It supports the evaluation of how LLM reasoning changes with
problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain
Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT
problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of
+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B
and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,
AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in
constructing RL tasks, Saturn achieves further improvements of +8.8%. We
release the source code, data, and models to support future research.

</details>


### [5] [AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training](https://arxiv.org/abs/2505.16363)
*Huishuai Zhang,Bohan Wang,Luoxin Chen*

Main category: cs.LG

TL;DR: 提出了一种新的优化器AdamS，用于大语言模型的预训练和后训练。通过利用动量和当前梯度平方加权和的平方根作为分母，消除了对二阶矩估计的需求，从而在保持SGD+动量效率的同时提供了更优的优化性能。AdamS继承了AdamW的超参数，且与模型无关，易于集成到现有管道中。实验表明，AdamS在GPT-2和Llama2等模型的预训练及强化学习后训练任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 在Transformer目标函数中观察到的(L0, L1)平滑性特性启发了AdamS的设计。局部平滑性由梯度大小决定，而梯度大小可以进一步用动量大小近似。因此，AdamS旨在利用这一特性来设计一种简单、高效且理论上有保障的优化器。

Method: AdamS的核心方法是使用动量和当前梯度平方加权和的平方根作为分母，替代传统Adam中的二阶矩估计。这种方法不仅简化了计算，还使得AdamS具有与SGD+动量相同的内存和计算占用，同时保留了更好的优化性能。此外，AdamS可以直接继承AdamW的超参数，并且无需修改优化器API或架构即可无缝集成到现有模型中。

Result: 实验证明，AdamS在多种任务中表现优异，包括GPT-2和Llama2（多达13B参数）的预训练任务以及后训练阶段的强化学习任务。这表明AdamS不仅在理论上具有收敛性保证，而且在实际应用中也展现了强大的性能。

Conclusion: AdamS作为一种新的优化器，在效率、简单性和理论支持方面都优于现有的优化器。它不仅能直接继承AdamW的超参数，还能够在不修改优化器API或架构的情况下无缝集成到现有模型中，为大语言模型的预训练和后训练提供了一个有吸引力的替代方案。

Abstract: We introduce AdamS, a simple yet effective alternative to Adam for large
language model (LLM) pretraining and post-training. By leveraging a novel
denominator, i.e., the root of weighted sum of squares of the momentum and the
current gradient, AdamS eliminates the need for second-moment estimates. Hence,
AdamS is efficient, matching the memory and compute footprint of SGD with
momentum while delivering superior optimization performance. Moreover, AdamS is
easy to adopt: it can directly inherit hyperparameters of AdamW, and is
entirely model-agnostic, integrating seamlessly into existing pipelines without
modifications to optimizer APIs or architectures. The motivation behind AdamS
stems from the observed $(L_0, L_1)$ smoothness properties in transformer
objectives, where local smoothness is governed by gradient magnitudes that can
be further approximated by momentum magnitudes. We establish rigorous
theoretical convergence guarantees and provide practical guidelines for
hyperparameter selection. Empirically, AdamS demonstrates strong performance in
various tasks, including pre-training runs on GPT-2 and Llama2 (up to 13B
parameters) and reinforcement learning in post-training regimes. With its
efficiency, simplicity, and theoretical grounding, AdamS stands as a compelling
alternative to existing optimizers.

</details>


### [6] [Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning](https://arxiv.org/abs/2505.15311)
*Yurun Yuan,Fan Chen,Zeyu Jia,Alexander Rakhlin,Tengyang Xie*

Main category: cs.LG

TL;DR: 提出了一种名为轨迹贝尔曼残差最小化（TBRM）的算法，它是一种适用于大型语言模型（LLMs）的价值导向强化学习方法。通过实验表明，该方法在数学推理基准测试中优于基于策略的方法（如PPO和GRPO），同时具有较低的计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在大型语言模型中的应用主要以策略导向方法为主，而价值导向方法尚未得到充分探索。为了弥补这一空白，文章重新审视了经典的贝尔曼残差最小化方法，并尝试将其应用于LLMs。

Method: TBRM算法利用模型自身的logits作为Q值，优化单个轨迹级别的贝尔曼目标。该算法不需要批评者、重要性采样比率或裁剪，并且每个提示仅需一次rollout。此外，通过改进的轨迹测量变化分析，证明了从任意离线数据收敛到接近最优的KL正则化策略的可能性。

Result: 在标准数学推理基准上的实验表明，TBRM持续优于策略导向基线方法（如PPO和GRPO），并且在计算和内存开销上相当或更低。

Conclusion: 研究结果表明，价值导向的强化学习可能是提升LLMs推理能力的一种原则性和高效的替代方案。

Abstract: Policy-based methods currently dominate reinforcement learning (RL) pipelines
for large language model (LLM) reasoning, leaving value-based approaches
largely unexplored. We revisit the classical paradigm of Bellman Residual
Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an
algorithm that naturally adapts this idea to LLMs, yielding a simple yet
effective off-policy algorithm that optimizes a single trajectory-level Bellman
objective using the model's own logits as $Q$-values. TBRM removes the need for
critics, importance-sampling ratios, or clipping, and operates with only one
rollout per prompt. We prove convergence to the near-optimal KL-regularized
policy from arbitrary off-policy data via an improved
change-of-trajectory-measure analysis. Experiments on standard
mathematical-reasoning benchmarks show that TBRM consistently outperforms
policy-based baselines, like PPO and GRPO, with comparable or lower
computational and memory overhead. Our results indicate that value-based RL
might be a principled and efficient alternative for enhancing reasoning
capabilities in LLMs.

</details>


### [7] [Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One](https://arxiv.org/abs/2505.15306)
*Yiwen Song,Qianyue Hao,Qingmin Liao,Jian Yuan,Yong Li*

Main category: cs.LG

TL;DR: 在强化学习中，模型集成是一种有效的训练方法。尽管RL取得了广泛的成功，但训练高效的智能体仍然困难重重，需要仔细调整许多因素。模型集成通过将多个弱智能体组合成一个更强大的智能体来克服这些挑战。然而，现有的集成方法（如多数投票和Boltzmann加法）被设计为固定的策略，缺乏对特定任务的语义理解，限制了其适应性和有效性。为此，我们提出了LLM-Ens，这是一种利用大语言模型（LLMs）增强RL模型集成的新方法。LLM-Ens通过动态识别任务情况并切换到当前情况下表现最佳的智能体，确保在变化的任务条件下进行动态模型选择。实验结果表明，LLM-Ens显著提高了RL模型集成的效果，比已知基线高出20.9%。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习取得了成功，但训练高效智能体仍面临诸多挑战，包括算法选择、超参数设置和随机种子选择等。现有的模型集成方法虽然有效，但它们是固定策略，缺乏对特定任务的语义理解，这限制了其适应性和有效性。因此，研究者希望开发一种能够根据任务具体情况动态选择模型的方法，以进一步提升强化学习模型集成的效果。

Method: 研究者提出了LLM-Ens方法，首先设计了一个大语言模型（LLM），用于将任务中的状态分类为不同的'情境'，并结合任务条件的高级描述。然后，统计分析每个个体智能体在每种情境下的优劣势。在推理阶段，LLM-Ens动态识别任务情境，并切换到当前情境下表现最佳的智能体，从而实现动态模型选择。该方法兼容不同随机种子、超参数设置和各种RL算法训练的智能体。

Result: 通过在Atari基准上的广泛实验，LLM-Ens显著提升了RL模型集成的表现，相比知名基线方法最多提高了20.9%。这证明了LLM-Ens在强化学习任务中的有效性和优越性。

Conclusion: LLM-Ens是一种创新的方法，通过引入大语言模型驱动的任务特定语义理解，增强了强化学习模型集成的性能。这种方法不仅克服了现有集成方法的局限性，还展示了在动态任务条件下的高效适应能力。未来可以进一步探索LLM-Ens在更多复杂任务和实际应用中的潜力。

Abstract: Model ensemble is a useful approach in reinforcement learning (RL) for
training effective agents. Despite wide success of RL, training effective
agents remains difficult due to the multitude of factors requiring careful
tuning, such as algorithm selection, hyperparameter settings, and even random
seed choices, all of which can significantly influence an agent's performance.
Model ensemble helps overcome this challenge by combining multiple weak agents
into a single, more powerful one, enhancing overall performance. However,
existing ensemble methods, such as majority voting and Boltzmann addition, are
designed as fixed strategies and lack a semantic understanding of specific
tasks, limiting their adaptability and effectiveness. To address this, we
propose LLM-Ens, a novel approach that enhances RL model ensemble with
task-specific semantic understandings driven by large language models (LLMs).
Given a task, we first design an LLM to categorize states in this task into
distinct 'situations', incorporating high-level descriptions of the task
conditions. Then, we statistically analyze the strengths and weaknesses of each
individual agent to be used in the ensemble in each situation. During the
inference time, LLM-Ens dynamically identifies the changing task situation and
switches to the agent that performs best in the current situation, ensuring
dynamic model selection in the evolving task condition. Our approach is
designed to be compatible with agents trained with different random seeds,
hyperparameter settings, and various RL algorithms. Extensive experiments on
the Atari benchmark show that LLM-Ens significantly improves the RL model
ensemble, surpassing well-known baselines by up to 20.9%. For reproducibility,
our code is open-source at
https://anonymous.4open.science/r/LLM4RLensemble-F7EE.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [8] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 在大规模语言模型（LLMs）中，通过重复采样扩展推理计算会随着样本数量的增加而持续提高覆盖率。实验表明，在数学推理和事实知识两个领域中，基于训练集答案频率的基线方法在某些LLMs上优于重复采样，而在其他LLMs上与混合策略表现相当。该研究提供了一种更精确的度量方法，以评估重复采样在超越提示无关猜测时对覆盖率的改进程度。


<details>
  <summary>Details</summary>
Motivation: 作者观察到在大语言模型中，通过重复采样进行推理计算可以持续增加问题解决的覆盖率，并推测这种提升部分归因于标准评估基准中的答案分布偏向于较小的常见答案集合。为了验证这一假设，他们提出了一种基于训练集中答案频率的基线方法。

Method: 研究者定义了一个基线，该基线根据训练集中答案的频率枚举答案。然后在数学推理和事实知识两个领域进行了实验，比较了这个基线方法与重复模型采样以及一种混合策略的表现。混合策略通过仅使用10个模型样本获取答案，并通过枚举猜测剩余的答案。

Result: 实验结果表明，对于某些LLMs，基于训练集答案频率的基线方法比重复模型采样表现更好；而对于其他LLMs，其覆盖率与混合策略相当。这说明重复采样带来的覆盖率提升可能部分归因于答案分布的特性。

Conclusion: 这项研究提出了一种新的基线方法，能够更准确地衡量重复采样在大语言模型中的实际效果，排除了单纯依赖提示无关猜测的影响。这为未来关于大模型推理能力和评估方法的研究提供了重要参考。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [9] [SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation](https://arxiv.org/abs/2505.16637)
*Wenjie Yang,Mao Zheng,Mingyang Song,Zheng Li*

Main category: cs.CL

TL;DR: 提出了一种无需参考的简单自我奖励（SSR）强化学习框架，用于机器翻译任务。通过使用13K单语数据训练，基于Qwen-2.5-7B的模型SSR-Zero-7B在英语与中文互译任务中超越了多个现有模型。进一步结合外部监督信号（如COMET），最强模型SSR-X-Zero-7B达到SOTA性能，甚至超过闭源模型如GPT-4o和Gemini 1.5 Pro。研究表明自我奖励机制在机器翻译中的有效性，并展示了与训练好的奖励模型结合的互补优势。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的机器翻译专用大语言模型（LLMs）通常依赖昂贵且难以扩展的外部监督信号（如人工标注的参考数据或训练好的奖励模型）。为了解决这一问题，研究者提出了一种无需参考、完全在线且仅依赖自我判断奖励的强化学习框架——Simple Self-Rewarding (SSR) RL。

Method: SSR框架采用自我奖励机制进行强化学习，无需外部参考数据。通过使用13K单语例子和Qwen-2.5-7B作为基础模型进行训练，构建了SSR-Zero-7B模型。此外，还通过引入外部监督信号（如COMET）增强了SSR框架，构建了更强的SSR-X-Zero-7B模型。

Result: SSR-Zero-7B在英语与中文互译任务中超越了多个现有模型，包括专门的机器翻译LLMs和更大的通用LLMs。而SSR-X-Zero-7B达到了SOTA性能，超过了所有参数量小于72B的开源模型，甚至优于闭源模型如GPT-4o和Gemini 1.5 Pro。

Conclusion: 研究证明了自我奖励机制在机器翻译任务中的有效性，并展示了其与训练好的奖励模型结合时的互补优势。这些发现为自改进的强化学习方法提供了有价值的见解。同时，研究者公开发布了代码、数据和模型以供社区使用。

Abstract: Large language models (LLMs) have recently demonstrated remarkable
capabilities in machine translation (MT). However, most advanced MT-specific
LLMs heavily rely on external supervision signals during training, such as
human-annotated reference data or trained reward models (RMs), which are often
expensive to obtain and challenging to scale. To overcome this limitation, we
propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for
MT that is reference-free, fully online, and relies solely on self-judging
rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as
the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,
e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like
Qwen2.5-32B-Instruct in English $\leftrightarrow$ Chinese translation tasks
from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR
with external supervision from COMET, our strongest model, SSR-X-Zero-7B,
achieves state-of-the-art performance in English $\leftrightarrow$ Chinese
translation, surpassing all existing open-source models under 72B parameters
and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.
Our analysis highlights the effectiveness of the self-rewarding mechanism
compared to the external LLM-as-a-judge approach in MT and demonstrates its
complementary benefits when combined with trained RMs. Our findings provide
valuable insight into the potential of self-improving RL methods. We have
publicly released our code, data and models.

</details>


### [10] [WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.16421)
*Zhepei Wei,Wenlin Yao,Yao Liu,Weizhi Zhang,Qin Lu,Liang Qiu,Changlong Yu,Puyang Xu,Chao Zhang,Bing Yin,Hyokun Yun,Lihong Li*

Main category: cs.CL

TL;DR: 本文介绍了一种名为WebAgent-R1的端到端多轮强化学习框架，用于训练网络代理。通过在线与网络环境交互，该框架能够显著提高任务成功率，并在WebArena-Lite基准测试中超越现有最先进方法和强大的专有模型。深入分析表明，基于思考的提示策略和通过增加交互进行测试时扩展对网络任务的有效性。此外，还探讨了不同的强化学习初始化策略。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在提升大语言模型方面取得了显著成功，但主要集中在单轮任务上，如解决数学问题。对于涉及复杂长时间决策的动态网络界面多轮交互任务，训练有效的网络代理仍然具有挑战性。

Method: WebAgent-R1是一个简单的端到端多轮强化学习框架，直接从与网络环境的在线交互中学习。它通过异步生成多样化的轨迹进行学习，完全由任务成功与否决定的二元奖励引导。文章还介绍了两个变体：WebAgent-R1-Zero和WebAgent-R1-CoT，分别强调了行为克隆（warm-up training）的重要性以及将长链推理（CoT）融入网络代理中的见解。

Result: 实验结果表明，在WebArena-Lite基准测试中，WebAgent-R1将Qwen-2.5-3B的任务成功率从6.1%提高到33.9%，Llama-3.1-8B的任务成功率从8.5%提高到44.8%，显著优于现有的最先进方法和强大的专有模型。此外，基于思考的提示策略和测试时扩展通过增加交互对网络任务非常有效。

Conclusion: WebAgent-R1提供了一个简单而有效的框架来训练多轮网络代理，显著提高了任务成功率。研究揭示了行为克隆阶段的重要性以及如何将长链推理纳入网络代理中。这些发现为未来的研究提供了宝贵的见解和方向。

Abstract: While reinforcement learning (RL) has demonstrated remarkable success in
enhancing large language models (LLMs), it has primarily focused on single-turn
tasks such as solving math problems. Training effective web agents for
multi-turn interactions remains challenging due to the complexity of
long-horizon decision-making across dynamic web interfaces. In this work, we
present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework
for training web agents. It learns directly from online interactions with web
environments by asynchronously generating diverse trajectories, entirely guided
by binary rewards depending on task success. Experiments on the WebArena-Lite
benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task
success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to
44.8%, significantly outperforming existing state-of-the-art methods and strong
proprietary models such as OpenAI o3. In-depth analyses reveal the
effectiveness of the thinking-based prompting strategy and test-time scaling
through increased interactions for web tasks. We further investigate different
RL initialization policies by introducing two variants, namely WebAgent-R1-Zero
and WebAgent-R1-CoT, which highlight the importance of the warm-up training
stage (i.e., behavior cloning) and provide insights on incorporating long
chain-of-thought (CoT) reasoning in web agents.

</details>


### [11] [Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning](https://arxiv.org/abs/2505.16410)
*Guanting Dong,Yifei Chen,Xiaoxi Li,Jiajie Jin,Hongjin Qian,Yutao Zhu,Hangyu Mao,Guorui Zhou,Zhicheng Dou,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 本文提出了一种名为Tool-Star的强化学习框架，旨在使大型语言模型能够自主调用多个外部工具进行逐步推理。通过结合工具集成提示和基于提示的采样方法，Tool-Star解决工具使用数据稀缺的问题，并通过两阶段训练框架提高多工具协作推理能力，实验结果表明其在多个推理基准上的高效性和有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）通过大规模强化学习展示了出色的推理能力，但如何利用强化学习算法来实现有效的多工具协作推理仍然是一个开放性挑战。

Method: Tool-Star框架整合了六种类型的工具，并在数据合成和训练中引入了系统设计。具体方法包括：1) 提出通用工具集成推理数据合成流水线，自动扩展生成工具使用轨迹；2) 质量归一化和难度感知分类过程，过滤低质量样本并组织数据集；3) 两阶段训练框架，包含冷启动微调和多工具自我批评强化学习算法，以增强多工具协作推理。

Result: 实验分析显示，在超过10个具有挑战性的推理基准测试中，Tool-Star展现了其有效性和效率。

Conclusion: Tool-Star为大型语言模型提供了强大的多工具协作推理能力，解决了工具使用数据稀少的问题，并通过系统设计和两阶段训练框架显著提高了推理性能。代码已公开，供研究者进一步探索和改进。

Abstract: Recently, large language models (LLMs) have shown remarkable reasoning
capabilities via large-scale reinforcement learning (RL). However, leveraging
the RL algorithm to empower effective multi-tool collaborative reasoning in
LLMs remains an open challenge. In this paper, we introduce Tool-Star, an
RL-based framework designed to empower LLMs to autonomously invoke multiple
external tools during stepwise reasoning. Tool-Star integrates six types of
tools and incorporates systematic designs in both data synthesis and training.
To address the scarcity of tool-use data, we propose a general tool-integrated
reasoning data synthesis pipeline, which combines tool-integrated prompting
with hint-based sampling to automatically and scalably generate tool-use
trajectories. A subsequent quality normalization and difficulty-aware
classification process filters out low-quality samples and organizes the
dataset from easy to hard. Furthermore, we propose a two-stage training
framework to enhance multi-tool collaborative reasoning by: (1) cold-start
fine-tuning, which guides LLMs to explore reasoning patterns via
tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with
hierarchical reward design, which reinforces reward understanding and promotes
effective tool collaboration. Experimental analyses on over 10 challenging
reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.
The code is available at https://github.com/dongguanting/Tool-Star.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [12] [HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving](https://arxiv.org/abs/2505.15793)
*Zhiwen Chen,Bo Leng,Zhuoren Li,Hanming Deng,Guizhe Jin,Ran Yu,Huanxi Wen*

Main category: cs.RO

TL;DR: 将大型语言模型（LLM）与强化学习（RL）结合可以提升自动驾驶在复杂场景中的表现，但当前方法过于依赖LLM输出，容易产生幻觉问题。本文提出了一种新的LLM暗示的RL范式（LLM-Hinted RL），通过使用LLM生成语义提示来增强状态表示和优化策略，同时让RL代理通过策略学习抵消潜在的错误语义指示，从而实现优秀的驾驶性能。基于此范式，提出了HCRMP架构，在CARLA中的实验验证了其强大的整体驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 目前的LLM主导的RL方法过度依赖于LLM的输出，而LLM在关键驾驶任务上的非幻觉率仅为约57.95%，这可能导致驾驶策略性能下降。因此，需要一种新方法来解决LLM幻觉问题，并保持LLM与RL之间的相对独立性。

Method: 本文提出了一种名为LLM-Hinted RL的新范式，其中LLM用于生成语义提示以帮助RL代理进行动作规划，而RL代理则通过策略学习对抗可能的错误语义指示。具体来说，设计了HCRMP架构，包括：1) 增强语义表示模块，扩展状态空间；2) 上下文稳定性锚定模块，利用知识库信息提高多批评权重提示的可靠性；3) 语义缓存模块，无缝整合LLM低频指导与RL高频控制。

Result: 在CARLA仿真环境中的广泛实验证明了HCRMP的强大驾驶性能。在不同交通密度条件下，HCRMP的任务成功率高达80.3%。在安全关键驾驶条件下，HCRMP显著降低了11.4%的碰撞率，有效提升了复杂场景下的驾驶性能。

Conclusion: 本文提出的LLM-Hinted RL范式及HCRMP架构有效地解决了LLM幻觉问题，提高了自动驾驶在复杂场景中的性能。该研究为未来LLM与RL结合的自动驾驶技术提供了新的方向。

Abstract: Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can
enhance autonomous driving (AD) performance in complex scenarios. However,
current LLM-Dominated RL methods over-rely on LLM outputs, which are prone to
hallucinations. Evaluations show that state-of-the-art LLM indicates a
non-hallucination rate of only approximately 57.95% when assessed on essential
driving-related tasks. Thus, in these methods, hallucinations from the LLM can
directly jeopardize the performance of driving policies. This paper argues that
maintaining relative independence between the LLM and the RL is vital for
solving the hallucinations problem. Consequently, this paper is devoted to
propose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic
hints for state augmentation and policy optimization to assist RL agent in
motion planning, while the RL agent counteracts potential erroneous semantic
indications through policy learning to achieve excellent driving performance.
Based on this paradigm, we propose the HCRMP (LLM-Hinted Contextual
Reinforcement Learning Motion Planner) architecture, which is designed that
includes Augmented Semantic Representation Module to extend state space.
Contextual Stability Anchor Module enhances the reliability of multi-critic
weight hints by utilizing information from the knowledge base. Semantic Cache
Module is employed to seamlessly integrate LLM low-frequency guidance with RL
high-frequency control. Extensive experiments in CARLA validate HCRMP's strong
overall driving performance. HCRMP achieves a task success rate of up to 80.3%
under diverse driving conditions with different traffic densities. Under
safety-critical driving conditions, HCRMP significantly reduces the collision
rate by 11.4%, which effectively improves the driving performance in complex
scenarios.

</details>
