<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 3]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.CL](#cs.CL) [Total: 5]
- [econ.EM](#econ.EM) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: This position paper critically surveys recent developments in human-AI collaboration and proposes a new conceptual architecture to address the lack of a unifying theoretical framework.


<details>
  <summary>Details</summary>
Motivation: We observe a lack of a unifying theoretical framework that can coherently integrate these varied studies, especially when tackling open-ended, complex tasks. To address this, we propose a novel conceptual architecture: one that systematically interlinks the technical details of multi-agent coordination, knowledge management, cybernetic feedback loops, and higher-level control mechanisms.

Method: By mapping existing contributions, from symbolic AI techniques and connectionist LLM-based agents to hybrid organizational practices, onto this proposed framework (Hierarchical Exploration-Exploitation Net), our approach facilitates revision of legacy methods and inspires new work that fuses qualitative and quantitative paradigms.

Result: The paper's structure allows it to be read from any section, serving equally as a critical review of technical implementations and as a forward-looking reference for designing or extending human-AI symbioses.

Conclusion: Together, these insights offer a stepping stone toward deeper co-evolution of human cognition and AI capability.

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [2] [Artificial Expert Intelligence through PAC-reasoning](https://arxiv.org/abs/2412.02441)
*Shai Shalev-Shwartz,Amnon Shashua,Gal Beniamini,Yoav Levine,Or Sharir,Noam Wies,Ido Ben-Shaul,Tomer Nussbaum,Shir Granot Peled*

Main category: cs.AI

TL;DR: 本文提出了人工专家智能（AEI），通过整合领域专业知识和精确推理能力，克服现有AI系统的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的AI系统在预定义任务上表现出色，但在适应性和新问题解决的精确性方面存在不足，因此需要一种超越通用人工智能和狭义AI的新方法。

Method: 结合领域专业知识与类似顶级人类专家的精确推理能力，提出了一种新的“可能近似正确推理”范式，并将其称为System 3推理。

Result: AEI成功引入了System 3推理，为分解复杂问题提供了稳健的理论保证和控制推理精度的实用机制。

Conclusion: AEI通过引入“可能近似正确推理”框架，为复杂问题的解决提供了可靠的理论保障和实践机制，奠定了误差有界、推理时学习的基础。

Abstract: Artificial Expert Intelligence (AEI) seeks to transcend the limitations of
both Artificial General Intelligence (AGI) and narrow AI by integrating
domain-specific expertise with critical, precise reasoning capabilities akin to
those of top human experts. Existing AI systems often excel at predefined tasks
but struggle with adaptability and precision in novel problem-solving. To
overcome this, AEI introduces a framework for ``Probably Approximately Correct
(PAC) Reasoning". This paradigm provides robust theoretical guarantees for
reliably decomposing complex problems, with a practical mechanism for
controlling reasoning precision. In reference to the division of human thought
into System 1 for intuitive thinking and System 2 for reflective
reasoning~\citep{tversky1974judgment}, we refer to this new type of reasoning
as System 3 for precise reasoning, inspired by the rigor of the scientific
method. AEI thus establishes a foundation for error-bounded, inference-time
learning.

</details>


### [3] [Decomposing Elements of Problem Solving: What "Math" Does RL Teach?](https://arxiv.org/abs/2505.22756)
*Tian Qin,Core Francisco Park,Mujin Kwun,Aaron Walsman,Eran Malach,Nikhil Anand,Hidenori Tanaka,David Alvarez-Melis*

Main category: cs.AI

TL;DR: 这篇论文研究了强化学习（RL）在提升大型语言模型（LLM）数学推理能力方面的作用，发现RL主要增强执行技能，但面对全新问题时受限于规划能力不足的“覆盖墙”。通过设计合成任务验证这些发现，并探索了RL在改进探索和泛化能力方面的潜力，提出了克服当前限制的可能路径。


<details>
  <summary>Details</summary>
Motivation: 文章的研究动机源于对现有评估LLM推理能力方法的局限性的认识：仅依赖准确率指标无法揭示模型内部化的问题解决技能。通过更细致地分解问题解决能力，研究者希望更好地理解强化学习在提升这些能力方面的作用，尤其是在处理全新问题时的表现。

Method: 文章中使用的方法包括对问题解决能力的分解（Plan、Execute、Verify），以及基于GRPO等强化学习方法的实验分析。为了进一步探究RL的影响，作者设计了一个最小化的合成解决方案树导航任务作为数学问题求解的类比。这一受控设置帮助确认了RL在执行鲁棒性方面的贡献，并探索了在特定条件下RL如何可能突破“覆盖墙”。

Result: 研究结果显示，强化学习（如GRPO方法）主要提升了模型的执行技能，即在模型已知如何解决问题的情况下显著提高了执行的鲁棒性，这种现象被称为“温度蒸馏”（temperature distillation）。然而，RL训练的模型在面对本质上全新的问题时表现不佳，受到“覆盖墙”的限制，这归因于规划技能的不足。此外，在合成的任务设置中，作者验证了RL对执行鲁棒性的提升，并识别出RL在改进探索和泛化到新解决方案路径时可能突破“覆盖墙”的条件。

Conclusion: 该论文得出的结论是，强化学习（RL）在提升大型语言模型（LLM）的数学推理能力方面主要增强了执行技能的鲁棒性，但在面对全新问题时由于规划能力不足而面临“覆盖墙”（coverage wall）。通过构建一个最小化的解决方案树导航任务，作者验证了RL在提升执行鲁棒性方面的有效性，并识别了RL在改进探索和泛化到新解决方案路径时可能突破“覆盖墙”的条件。研究为理解RL在增强LLM推理中的作用提供了洞见，并提出了克服当前障碍的潜在路径。

Abstract: Mathematical reasoning tasks have become prominent benchmarks for assessing
the reasoning capabilities of LLMs, especially with reinforcement learning (RL)
methods such as GRPO showing significant performance gains. However, accuracy
metrics alone do not support fine-grained assessment of capabilities and fail
to reveal which problem-solving skills have been internalized. To better
understand these capabilities, we propose to decompose problem solving into
fundamental capabilities: Plan (mapping questions to sequences of steps),
Execute (correctly performing solution steps), and Verify (identifying the
correctness of a solution). Empirically, we find that GRPO mainly enhances the
execution skill-improving execution robustness on problems the model already
knows how to solve-a phenomenon we call temperature distillation. More
importantly, we show that RL-trained models struggle with fundamentally new
problems, hitting a 'coverage wall' due to insufficient planning skills. To
explore RL's impact more deeply, we construct a minimal, synthetic
solution-tree navigation task as an analogy for mathematical problem-solving.
This controlled setup replicates our empirical findings, confirming RL
primarily boosts execution robustness. Importantly, in this setting, we
identify conditions under which RL can potentially overcome the coverage wall
through improved exploration and generalization to new solution paths. Our
findings provide insights into the role of RL in enhancing LLM reasoning,
expose key limitations, and suggest a path toward overcoming these barriers.
Code is available at https://github.com/cfpark00/RL-Wall.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [MvDeCor: Multi-view Dense Correspondence Learning for Fine-grained 3D Segmentation](https://arxiv.org/abs/2208.08580)
*Gopal Sharma,Kangxue Yin,Subhransu Maji,Evangelos Kalogerakis,Or Litany,Sanja Fidler*

Main category: cs.CV

TL;DR: 文章提出了一种名为MvDeCor的新方法，用于精细的3D形状分割，通过在2D领域使用自监督技术，并结合3D几何推理，实现了更好的性能表现。


<details>
  <summary>Details</summary>
Motivation: 文章的动机是基于观察到基于视图的表面表示在建模高分辨率表面细节和纹理方面比基于点云或体素占用的3D对应方法更有效。

Method: 文章提出了一种基于对比学习框架的密集对应学习任务，通过从多个视角渲染3D形状来学习视图不变且几何一致的2D表示。

Result: 实验结果表明，该方法在细粒度部分分割任务中优于现有的替代方法，尤其是在训练时只有稀疏视图可用或形状带有纹理的情况下改进更为明显。

Conclusion: 文章得出结论，MvDeCor通过结合2D处理和3D几何推理，在有限标记数据的情况下优于其他利用2D或3D进行自监督学习的方法。

Abstract: We propose to utilize self-supervised techniques in the 2D domain for
fine-grained 3D shape segmentation tasks. This is inspired by the observation
that view-based surface representations are more effective at modeling
high-resolution surface details and texture than their 3D counterparts based on
point clouds or voxel occupancy. Specifically, given a 3D shape, we render it
from multiple views, and set up a dense correspondence learning task within the
contrastive learning framework. As a result, the learned 2D representations are
view-invariant and geometrically consistent, leading to better generalization
when trained on a limited number of labeled shapes compared to alternatives
that utilize self-supervision in 2D or 3D alone. Experiments on textured
(RenderPeople) and untextured (PartNet) 3D datasets show that our method
outperforms state-of-the-art alternatives in fine-grained part segmentation.
The improvements over baselines are greater when only a sparse set of views is
available for training or when shapes are textured, indicating that MvDeCor
benefits from both 2D processing and 3D geometric reasoning.

</details>


### [5] [Mix3D: Out-of-Context Data Augmentation for 3D Scenes](https://arxiv.org/abs/2110.02210)
*Alexey Nekrasov,Jonas Schult,Or Litany,Bastian Leibe,Francis Engelmann*

Main category: cs.CV

TL;DR: Mix3D是一种创新的数据增强技术，通过混合不同场景来提升3D场景分割的准确性，同时减少对全局上下文的过度依赖，取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前的3D场景分割方法过于依赖全局场景上下文，这可能导致模型误判（例如将行人误认为车辆）。因此，本文强调需要平衡全局上下文和局部几何结构，以超越训练集中存在的上下文先验知识。

Method: 提出了一种名为Mix3D的数据增强技术，将两个经过增强的场景进行混合，生成新的训练样本。这种方法迫使模型不仅依赖于全局场景上下文，还要从局部结构中推断语义信息，从而提高模型的泛化能力。

Result: 实验表明，使用Mix3D训练的模型在室内（ScanNet、S3DIS）和室外数据集（SemanticKITTI）上均实现了显著的性能提升。例如，结合Mix3D训练的MinkowskiNet在ScanNet测试基准上的mIoU达到78.1，显著超过了之前的所有方法。

Conclusion: Mix3D通过混合两个增强场景创建新的训练样本，有效平衡了全局场景上下文和局部几何结构的重要性，从而提高了模型的泛化能力。该方法在多个室内和室外数据集上展示了显著的性能提升，并且可以与现有方法结合使用，显著优于之前的技术水平。

Abstract: We present Mix3D, a data augmentation technique for segmenting large-scale 3D
scenes. Since scene context helps reasoning about object semantics, current
works focus on models with large capacity and receptive fields that can fully
capture the global context of an input 3D scene. However, strong contextual
priors can have detrimental implications like mistaking a pedestrian crossing
the street for a car. In this work, we focus on the importance of balancing
global scene context and local geometry, with the goal of generalizing beyond
the contextual priors in the training set. In particular, we propose a "mixing"
technique which creates new training samples by combining two augmented scenes.
By doing so, object instances are implicitly placed into novel out-of-context
environments and therefore making it harder for models to rely on scene context
alone, and instead infer semantics from local structure as well. We perform
detailed analysis to understand the importance of global context, local
structures and the effect of mixing scenes. In experiments, we show that models
trained with Mix3D profit from a significant performance boost on indoor
(ScanNet, S3DIS) and outdoor datasets (SemanticKITTI). Mix3D can be trivially
used with any existing method, e.g., trained with Mix3D, MinkowskiNet
outperforms all prior state-of-the-art methods by a significant margin on the
ScanNet test benchmark 78.1 mIoU. Code is available at:
https://nekrasov.dev/mix3d/

</details>


### [6] [cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning](https://arxiv.org/abs/2505.22914)
*Maksim Kolodiazhnyi,Denis Tarasov,Dmitrii Zhemchuzhnikov,Alexander Nikulin,Ilya Zisman,Anna Vorontsova,Anton Konushin,Vladislav Kurenkov,Danila Rukhovich*

Main category: cs.CV

TL;DR: 这篇文章提出了一种多模态CAD重建模型，利用监督微调和强化学习微调的方法，在多个数据集上实现了最先进的性能，尤其是在真实世界数据上的表现突出。


<details>
  <summary>Details</summary>
Motivation: 文章的动机在于现有的CAD重建方法通常只关注单一输入模态（如点云、图像或文本），这限制了它们的泛化能力和鲁棒性。通过结合视觉-语言模型（VLM）的最新进展，作者旨在开发一种能够同时处理多种输入模态的更通用和强大的CAD重建方法。

Method: 文章的方法受到大语言模型（LLM）训练范式的启发，采用了两阶段流程：首先对大规模程序生成数据进行监督微调（SFT），然后使用在线反馈进行强化学习微调。此外，他们引入了Group Relative Preference Optimization (GRPO)等在线RL算法来优化模型。

Result: 文章的结果显示，在DeepCAD基准测试中，他们的监督微调（SFT）模型在所有三种输入模态下都优于现有的单模态方法。更重要的是，在经过强化学习微调后，他们的方法（cadrille）在三个具有挑战性的数据集上取得了新的最先进成果，包括一个真实世界的数据集。

Conclusion: 文章的结论是，该研究提出了一个基于多模态输入的CAD重建模型，并首次探索了在线强化学习（RL）微调在CAD任务中的应用。结果表明，他们的方法在多个数据集中达到了最先进的性能，特别是在真实世界的数据集上。

Abstract: Computer-Aided Design (CAD) plays a central role in engineering and
manufacturing, making it possible to create precise and editable 3D models.
Using a variety of sensor or user-provided data as inputs for CAD
reconstruction can democratize access to design applications. However, existing
methods typically focus on a single input modality, such as point clouds,
images, or text, which limits their generalizability and robustness. Leveraging
recent advances in vision-language models (VLM), we propose a multi-modal CAD
reconstruction model that simultaneously processes all three input modalities.
Inspired by large language model (LLM) training paradigms, we adopt a two-stage
pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated
data, followed by reinforcement learning (RL) fine-tuning using online
feedback, obtained programatically. Furthermore, we are the first to explore RL
fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such
as Group Relative Preference Optimization (GRPO) outperform offline
alternatives. In the DeepCAD benchmark, our SFT model outperforms existing
single-modal approaches in all three input modalities simultaneously. More
importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three
challenging datasets, including a real-world one.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model](https://arxiv.org/abs/2505.23579)
*Adibvafa Fallahpour,Andrew Magnuson,Purav Gupta,Shihao Ma,Jack Naimer,Arnav Shah,Haonan Duan,Omar Ibrahim,Hani Goodarzi,Chris J. Maddison,Bo Wang*

Main category: cs.LG

TL;DR: BioReason首次将DNA基础模型与大型语言模型结合，实现基因组数据的多步骤推理，显著提升了生物学推断的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前的DNA基础模型在多步骤推理和生物可解释性方面存在不足，阻碍了科学发现。

Method: BioReason采用监督微调和针对性强化学习的方法，开发了能够处理基因组信息并生成逻辑清晰、生物一致性高的推论的架构。

Result: BioReason在生物学推理基准测试中表现出色，包括KEGG疾病通路预测准确率从88%提升至97%，以及平均15%的性能增益。

Conclusion: BioReason通过将DNA基础模型与大型语言模型结合，实现了对基因组数据的多步骤推理和生物合理性推断，为生物学中的AI应用提供了新的方法。

Abstract: Unlocking deep, interpretable biological reasoning from complex genomic data
is a major AI challenge hindering scientific discovery. Current DNA foundation
models, despite strong sequence representation, struggle with multi-step
reasoning and lack inherent transparent, biologically intuitive explanations.
We introduce BioReason, a pioneering architecture that, for the first time,
deeply integrates a DNA foundation model with a Large Language Model (LLM).
This novel connection enables the LLM to directly process and reason with
genomic information as a fundamental input, fostering a new form of multimodal
biological understanding. BioReason's sophisticated multi-step reasoning is
developed through supervised fine-tuning and targeted reinforcement learning,
guiding the system to generate logical, biologically coherent deductions. On
biological reasoning benchmarks including KEGG-based disease pathway prediction
- where accuracy improves from 88% to 97% - and variant effect prediction,
BioReason demonstrates an average 15% performance gain over strong
single-modality baselines. BioReason reasons over unseen biological entities
and articulates decision-making through interpretable, step-by-step biological
traces, offering a transformative approach for AI in biology that enables
deeper mechanistic insights and accelerates testable hypothesis generation from
genomic data. Data, code, and checkpoints are publicly available at
https://github.com/bowang-lab/BioReason

</details>


### [8] [Diversity-Aware Policy Optimization for Large Language Model Reasoning](https://arxiv.org/abs/2505.23433)
*Jian Yao,Ran Cheng,Xingyu Wu,Jibin Wu,Kay Chen Tan*

Main category: cs.LG

TL;DR: 本文研究了在基于强化学习的大型语言模型（LLM）训练中引入多样性的效果，提出了一种新的多样化感知策略优化方法，并证明该方法能显著提升LLM的推理能力和解决方案的多样性。


<details>
  <summary>Details</summary>
Motivation: 尽管多样性在强化学习中起着关键作用，但其对大型语言模型（LLM）推理能力的影响尚未得到充分研究。为了弥补这一空白，研究者们希望通过探索多样性在LLM训练中的作用，进一步提升模型的推理性能。

Method: 该研究设计了一种标记级别的多样性度量，并将其重新制定为一个实际的目标函数。然后，研究人员选择性地将该目标应用于正样本，并将其集成到R1-zero训练框架中，以验证其有效性。此外，他们还引入了一个新的评估指标“k潜力”（Potential at k），用于量化LLM的推理潜力。

Result: 通过对12个LLM进行评估，研究人员发现解决方案的多样性与高性能模型的“k潜力”之间存在显著正相关关系。将所提出的方法集成到R1-zero训练框架后，在四个数学推理基准测试中平均提升了3.5%的性能，同时生成了更丰富多样的解决方案。

Conclusion: 这项研究系统地探讨了在基于强化学习的LLM训练中多样性的影响，并提出了一种新的多样化感知策略优化方法。结果表明，通过增强RL训练中的多样性，可以有效提升大型语言模型在推理任务上的表现，同时生成更多样化和鲁棒的解决方案。

Abstract: The reasoning capabilities of large language models (LLMs) have advanced
rapidly, particularly following the release of DeepSeek R1, which has inspired
a surge of research into data quality and reinforcement learning (RL)
algorithms. Despite the pivotal role diversity plays in RL, its influence on
LLM reasoning remains largely underexplored. To bridge this gap, this work
presents a systematic investigation into the impact of diversity in RL-based
training for LLM reasoning, and proposes a novel diversity-aware policy
optimization method. Across evaluations on 12 LLMs, we observe a strong
positive correlation between the solution diversity and Potential at k (a novel
metric quantifying an LLM's reasoning potential) in high-performing models.
This finding motivates our method to explicitly promote diversity during RL
training. Specifically, we design a token-level diversity and reformulate it
into a practical objective, then we selectively apply it to positive samples.
Integrated into the R1-zero training framework, our method achieves a 3.5
percent average improvement across four mathematical reasoning benchmarks,
while generating more diverse and robust solutions.

</details>


### [9] [Towards Reward Fairness in RLHF: From a Resource Allocation Perspective](https://arxiv.org/abs/2505.23349)
*Sheng Ouyang,Yulan Hu,Ge Chen,Qingyang Li,Fuzheng Zhang,Yong Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法来解决强化学习中奖励函数存在的偏差问题，通过将偏好学习建模为资源分配问题，并引入公平正则化和公平系数两种方法，实现了更公平的大语言模型与人类偏好的对齐。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是奖励函数通常作为人类偏好的代理，在强化学习中起着关键作用，但这些奖励可能具有各种偏差，导致大语言模型与人类偏好的对齐效果下降。因此，需要一种通用的方法来处理奖励中的不公平问题。

Method: 文章的方法包括将偏好学习视为资源分配问题，提出了两种具体方法：公平正则化（Fairness Regularization）和公平系数（Fairness Coefficient），以在不专门针对每种偏差设计解决方案的情况下提升奖励的公平性。

Result: 实验结果表明，所提出的方法能够在验证和强化学习场景中有效地实现更公平的奖励分配，分别得到了公平的奖励模型和策略模型，从而更好地对齐大语言模型与人类偏好。

Conclusion: 文章的结论是，通过将偏好学习建模为资源分配问题，并使用提出的公平正则化和公平系数方法，可以更公平地实现大语言模型与人类偏好的对齐，从而有效缓解奖励中的各种偏差带来的负面影响。

Abstract: Rewards serve as proxies for human preferences and play a crucial role in
Reinforcement Learning from Human Feedback (RLHF). However, if these rewards
are inherently imperfect, exhibiting various biases, they can adversely affect
the alignment of large language models (LLMs). In this paper, we collectively
define the various biases present in rewards as the problem of reward
unfairness. We propose a bias-agnostic method to address the issue of reward
fairness from a resource allocation perspective, without specifically designing
for each type of bias, yet effectively mitigating them. Specifically, we model
preference learning as a resource allocation problem, treating rewards as
resources to be allocated while considering the trade-off between utility and
fairness in their distribution. We propose two methods, Fairness Regularization
and Fairness Coefficient, to achieve fairness in rewards. We apply our methods
in both verification and reinforcement learning scenarios to obtain a fairness
reward model and a policy model, respectively. Experiments conducted in these
scenarios demonstrate that our approach aligns LLMs with human preferences in a
more fair manner.

</details>


### [10] [Accelerating RLHF Training with Reward Variance Increase](https://arxiv.org/abs/2505.23247)
*Zonglin Yang,Zhexuan Gu,Houduo Qi,Yancheng Yuan*

Main category: cs.LG

TL;DR: 本文提出了一种新的奖励调整模型及高效算法，将其应用于GRPO算法中，显著提升了RLHF训练的速度与效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于GRPO的RLHF训练效率较低，而初始策略模型的高奖励方差能够加快训练速度。受此启发，文章旨在设计一种方法以提高奖励方差并提升训练效率。

Method: 设计了一种奖励调整模型，通过增加奖励方差来提升RLHF训练效率，并开发了一种O(n log n)算法用于求解非凸优化问题。将该模型集成到GRPO算法中，形成了更高效的GRPOVI算法。

Result: 提出的GRPOVI算法显著提高了RLHF训练效率，实验验证了其优于原始GRPO算法的效果。此外，还对基于规则奖励的GRPO方法的有效性进行了间接解释。

Conclusion: 本文提出了GRPOVI算法，通过奖励调整模型显著提升了RLHF训练的效率。实验结果表明，该方法在保持相对偏好和奖励期望的同时，有效加速了训练过程，并为现有方法的有效性提供了间接解释。

Abstract: Reinforcement learning from human feedback (RLHF) is an essential technique
for ensuring that large language models (LLMs) are aligned with human values
and preferences during the post-training phase. As an effective RLHF approach,
group relative policy optimization (GRPO) has demonstrated success in many
LLM-based applications. However, efficient GRPO-based RLHF training remains a
challenge. Recent studies reveal that a higher reward variance of the initial
policy model leads to faster RLHF training. Inspired by this finding, we
propose a practical reward adjustment model to accelerate RLHF training by
provably increasing the reward variance and preserving the relative preferences
and reward expectation. Our reward adjustment method inherently poses a
nonconvex optimization problem, which is NP-hard to solve in general. To
overcome the computational challenges, we design a novel $O(n \log n)$
algorithm to find a global solution of the nonconvex reward adjustment model by
explicitly characterizing the extreme points of the feasible set. As an
important application, we naturally integrate this reward adjustment model into
the GRPO algorithm, leading to a more efficient GRPO with reward variance
increase (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we
provide an indirect explanation for the empirical effectiveness of GRPO with
rule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment
results demonstrate that the GRPOVI algorithm can significantly improve the
RLHF training efficiency compared to the original GRPO algorithm.

</details>


### [11] [LLM-ODDR: A Large Language Model Framework for Joint Order Dispatching and Driver Repositioning](https://arxiv.org/abs/2505.22695)
*Tengfei Lyu,Siyuan Feng,Hao Liu,Hai Yang*

Main category: cs.LG

TL;DR: 本文提出了一种基于大语言模型的新框架 LLM-ODDR，用于解决网约车中的订单调度与司机重定位问题，兼顾平台收益与司机公平收入，同时提高系统适应性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理网约车订单调度和司机重定位问题时忽略了司机收入公平性、可解释性以及对现实环境动态的适应能力，因此需要一种更智能的解决方案。

Method: 提出了 LLM-ODDR 框架，包含多目标订单价值优化、公平感知的订单调度和时空需求感知的司机重定位三个模块，并开发了基于领域知识优化的 JointDR-GPT 模型。

Result: 实验表明，LLM-ODDR 在真实纽约出租车数据集上表现优异，特别是在应对异常情况和提升决策透明度方面优于现有方法。

Conclusion: LLM-ODDR 是首个将大语言模型作为决策代理应用于网约车订单调度和司机重新定位任务的框架，实验证明其在有效性、适应性和决策可解释性上显著优于传统方法。

Abstract: Ride-hailing platforms face significant challenges in optimizing order
dispatching and driver repositioning operations in dynamic urban environments.
Traditional approaches based on combinatorial optimization, rule-based
heuristics, and reinforcement learning often overlook driver income fairness,
interpretability, and adaptability to real-world dynamics. To address these
gaps, we propose LLM-ODDR, a novel framework leveraging Large Language Models
(LLMs) for joint Order Dispatching and Driver Repositioning (ODDR) in
ride-hailing services. LLM-ODDR framework comprises three key components: (1)
Multi-objective-guided Order Value Refinement, which evaluates orders by
considering multiple objectives to determine their overall value; (2)
Fairness-aware Order Dispatching, which balances platform revenue with driver
income fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning,
which optimizes idle vehicle placement based on historical patterns and
projected supply. We also develop JointDR-GPT, a fine-tuned model optimized for
ODDR tasks with domain knowledge. Extensive experiments on real-world datasets
from Manhattan taxi operations demonstrate that our framework significantly
outperforms traditional methods in terms of effectiveness, adaptability to
anomalous conditions, and decision interpretability. To our knowledge, this is
the first exploration of LLMs as decision-making agents in ride-hailing ODDR
tasks, establishing foundational insights for integrating advanced language
models within intelligent transportation systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains](https://arxiv.org/abs/2402.00559)
*Alon Jacovi,Yonatan Bitton,Bernd Bohnet,Jonathan Herzig,Or Honovich,Michael Tseng,Michael Collins,Roee Aharoni,Mor Geva*

Main category: cs.CL

TL;DR: 本文介绍了REVEAL数据集，旨在评估开放领域问答任务中复杂推理链的自动验证方法，并展示了验证器在逻辑正确性验证方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏细粒度的步骤级数据集来全面评估自动验证方法的有效性，这阻碍了这一领域的进展。

Method: 介绍了一个名为REVEAL的新数据集，用于基准测试开放领域问答设置中复杂思维链推理的自动验证器。该数据集包括对语言模型答案中每一步推理的相关性、证据引用和逻辑正确性的全面标签。

Result: 引入了REVEAL数据集，为复杂推理任务的自动验证方法提供了评估标准，并揭示了现有验证方法的局限性。

Conclusion: REVEAL评估的结果显示，验证器在验证推理链方面存在困难，尤其是在验证逻辑正确性和检测矛盾方面。

Abstract: Prompting language models to provide step-by-step answers (e.g.,
"Chain-of-Thought") is the prominent approach for complex reasoning tasks,
where more accurate reasoning chains typically improve downstream task
performance. Recent literature discusses automatic methods to verify reasoning
to evaluate and improve their correctness. However, no fine-grained step-level
datasets are available to enable thorough evaluation of such verification
methods, hindering progress in this direction. We introduce REVEAL: Reasoning
Verification Evaluation, a dataset to benchmark automatic verifiers of complex
Chain-of-Thought reasoning in open-domain question-answering settings. REVEAL
includes comprehensive labels for the relevance, attribution to evidence
passages, and logical correctness of each reasoning step in a language model's
answer, across a variety of datasets and state-of-the-art language models.
Evaluation on REVEAL shows that verifiers struggle at verifying reasoning
chains - in particular, verifying logical correctness and detecting
contradictions. Available at https://reveal-dataset.github.io/ .

</details>


### [13] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 本文研究了如何通过重复采样或基于答案频率的枚举方法来提高大语言模型的问题解决覆盖面，并发现某些情况下枚举方法比传统采样方法更有效。


<details>
  <summary>Details</summary>
Motivation: 文章的动机源于观察到增加大语言模型的采样次数可以提高问题解决的覆盖面，而作者推测这一现象可能与标准评估基准的答案分布偏向于少数常见答案有关。

Method: 文章通过实验比较了两种方法：一种是传统的重复采样方法，另一种是基于训练集中答案出现频率的枚举方法。实验涉及两个领域——数学推理和事实知识，并测试了不同模型的表现以及混合策略的效果。

Result: 结果显示，在某些情况下，基于答案频率的枚举方法优于重复采样方法；同时，对于其他模型，使用少量采样结合枚举猜测的混合策略也能达到类似的覆盖面表现。

Conclusion: 文章得出结论，重复采样在提高大语言模型解决问题的覆盖面方面有一定效果，但这种效果在部分情况下可以通过基于训练集中答案出现频率的枚举方法来实现。这表明对于某些设置，重复采样的优势可能并不如预期显著，并且需要更精确地衡量其带来的提升程度。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [14] [ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2505.23723)
*Zexi Liu,Jingyi Chai,Xinyu Zhu,Shuo Tang,Rui Ye,Bo Zhang,Lei Bai,Siheng Chen*

Main category: cs.CL

TL;DR: 本文提出一种新型基于强化学习的LLM代理框架，实现高效的自主机器学习并展示卓越泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型代理方法依赖手动提示工程，难以根据实验经验优化适应，因此提出了基于学习的新范式以提升自动化与效率。

Method: 提出了一种新的代理机器学习训练框架，包含探索增强微调、逐步强化学习和特定奖励模块三个关键组件，结合在线强化学习进行训练。

Result: 尽管仅训练于9个机器学习任务，7B规模的ML-Agent表现优于671B规模的DeepSeek-R1代理，同时实现了持续性能改进和出色的跨任务泛化能力。

Conclusion: 本文通过引入基于学习的代理机器学习范式，展示了ML-Agent在自主机器学习方面的巨大潜力，并证明了其在跨任务泛化和性能改进方面的显著优势。

Abstract: The emergence of large language model (LLM)-based agents has significantly
advanced the development of autonomous machine learning (ML) engineering.
However, most existing approaches rely heavily on manual prompt engineering,
failing to adapt and optimize based on diverse experimental experiences.
Focusing on this, for the first time, we explore the paradigm of learning-based
agentic ML, where an LLM agent learns through interactive experimentation on ML
tasks using online reinforcement learning (RL). To realize this, we propose a
novel agentic ML training framework with three key components: (1)
exploration-enriched fine-tuning, which enables LLM agents to generate diverse
actions for enhanced RL exploration; (2) step-wise RL, which enables training
on a single action step, accelerating experience collection and improving
training efficiency; (3) an agentic ML-specific reward module, which unifies
varied ML feedback signals into consistent rewards for RL optimization.
Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM
for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our
7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it
achieves continuous performance improvements and demonstrates exceptional
cross-task generalization capabilities.

</details>


### [15] [Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation](https://arxiv.org/abs/2505.23657)
*Hongxiang Zhang,Hao Chen,Tianyi Zhang,Muhao Chen*

Main category: cs.CL

TL;DR: 本文介绍了一种名为ActLCD的新解码策略，该策略通过强化学习和奖励感知分类器来优化大型语言模型的事实性，以减少生成文本中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的解码方法通过改进生成过程中的下一个标记选择提高了大型语言模型的事实性，但这些方法通常在较长的上下文中仍容易出现幻觉。

Method: 提出了一种新的解码策略ActLCD，它主动决定何时在生成过程中应用对比层，并利用强化学习政策优化事实性。

Result: 实验表明，ActLCD在五个基准测试中均超越了最先进的方法，有效减少了各种生成场景中的幻觉现象。

Conclusion: ActLCD通过将解码视为一个序列决策问题，并采用受奖励感知分类器指导的强化学习策略，在超过现有技术水平的五个基准测试中表现出色，展示了其在多样化生成场景中减少幻觉的有效性。

Abstract: Recent decoding methods improve the factuality of large language
models~(LLMs) by refining how the next token is selected during generation.
These methods typically operate at the token level, leveraging internal
representations to suppress superficial patterns. Nevertheless, LLMs remain
prone to hallucinations, especially over longer contexts. In this paper, we
propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy
that actively decides when to apply contrasting layers during generation. By
casting decoding as a sequential decision-making problem, ActLCD employs a
reinforcement learning policy guided by a reward-aware classifier to optimize
factuality beyond the token level. Our experiments demonstrate that ActLCD
surpasses state-of-the-art methods across five benchmarks, showcasing its
effectiveness in mitigating hallucinations in diverse generation scenarios.

</details>


### [16] [ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind](https://arxiv.org/abs/2505.22961)
*Peixuan Han,Zijia Liu,Jiaxuan You*

Main category: cs.CL

TL;DR: 本文介绍了一种新的说服性语言模型ToMAP，通过增强心智理论推理能力，显著提高了模型在多轮对话中的说服效果和多样性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在理解对手心理状态方面存在局限，而人类擅长动态建模对手思维，因此需要增强模型的心智理论推理能力。

Method: 引入了包含两种心智理论模块的ToMAP方法，通过预测对手立场并使用强化学习方案来训练说服者生成更有效的论点。

Result: 尽管ToMAP仅有3B参数，但其表现优于GPT-4o等更大的基线模型，在多个说服模型和不同语料库中相对增益达39.4%。

Conclusion: ToMAP方法在构建更具说服力的语言模型方面表现出色，展示了其在开发更具说服力的AI代理方面的潜力。

Abstract: Large language models (LLMs) have shown promising potential in persuasion,
but existing works on training LLM persuaders are still preliminary. Notably,
while humans are skilled in modeling their opponent's thoughts and opinions
proactively and dynamically, current LLMs struggle with such Theory of Mind
(ToM) reasoning, resulting in limited diversity and opponent awareness. To
address this limitation, we introduce Theory of Mind Augmented Persuader
(ToMAP), a novel approach for building more flexible persuader agents by
incorporating two theory of mind modules that enhance the persuader's awareness
and analysis of the opponent's mental state. Specifically, we begin by
prompting the persuader to consider possible objections to the target central
claim, and then use a text encoder paired with a trained MLP classifier to
predict the opponent's current stance on these counterclaims. Our carefully
designed reinforcement learning schema enables the persuader learns how to
analyze opponent-related information and utilize it to generate more effective
arguments. Experiments show that the ToMAP persuader, while containing only 3B
parameters, outperforms much larger baselines, like GPT-4o, with a relative
gain of 39.4% across multiple persuadee models and diverse corpora. Notably,
ToMAP exhibits complex reasoning chains and reduced repetition during training,
which leads to more diverse and effective arguments. The opponent-aware feature
of ToMAP also makes it suitable for long conversations and enables it to employ
more logical and opponent-aware strategies. These results underscore our
method's effectiveness and highlight its potential for developing more
persuasive language agents. Code is available at:
https://github.com/ulab-uiuc/ToMAP.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [17] [Prices and preferences in the electric vehicle market](https://arxiv.org/abs/2403.00458)
*Chung Yi See,Vasco Rato Santos,Lucas Woodley,Megan Yeo,Daniel Palmer,Shuheng Zhang,and Ashley Nunes*

Main category: econ.EM

TL;DR: 该论文探讨了电动汽车价格的主要影响因素及其对环境的影响，发现消费者偏好功能丰富和更强大的车辆，但这降低了燃油经济性和排放效益。


<details>
  <summary>Details</summary>
Motivation: 尽管电动汽车比汽油车污染少，但其推广受到较高采购价格的挑战。现有讨论强调电池成本是主要原因，并认为广泛采用取决于电池成本下降。本研究旨在审视这一论点。

Method: 通过收集2011年至2023年间的电动汽车属性和市场条件数据进行分析。

Result: 研究发现，电动汽车的价格受标准配置的设施、附加功能和经销商安装的配件数量影响较大，其次是马力；续航里程与价格呈负相关；电池容量与价格正相关；最后，这些偏好导致了较低的燃油经济性，减少了排放效益。

Conclusion: 研究结论指出，电动汽车较高的采购价格主要反映了消费者对功能丰富和更强大车辆的偏好，但这种偏好导致了较低的燃油经济性，减少了预期的生命周期排放效益。

Abstract: Although electric vehicles are less polluting than gasoline powered vehicles,
adoption is challenged by higher procurement prices. Existing discourse
emphasizes EV battery costs as being principally responsible for this price
differential and widespread adoption is routinely conditioned upon battery
costs declining. We scrutinize such reasoning by sourcing data on EV attributes
and market conditions between 2011 and 2023. Our findings are fourfold. First,
EV prices are influenced principally by the number of amenities, additional
features, and dealer-installed accessories sold as standard on an EV, and to a
lesser extent, by EV horsepower. Second, EV range is negatively correlated with
EV price implying that range anxiety concerns may be less consequential than
existing discourse suggests. Third, battery capacity is positively correlated
with EV price, due to more capacity being synonymous with the delivery of more
horsepower. Collectively, this suggests that higher procurement prices for EVs
reflects consumer preference for vehicles that are feature dense and more
powerful. Fourth and finally, accommodating these preferences have produced
vehicles with lower fuel economy, a shift that reduces envisioned lifecycle
emissions benefits by at least 3.26 percent, subject to the battery pack
chemistry leveraged and the carbon intensity of the electrical grid. These
findings warrant attention as decarbonization efforts increasingly emphasize
electrification as a pathway for complying with domestic and international
climate agreements.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [18] [Casper DPM: Cascaded Perceptual Dynamic Projection Mapping onto Hands](https://arxiv.org/abs/2409.04397)
*Yotam Erel,Or Kozlovsky-Mordenfeld,Daisuke Iwai,Kosuke Sato,Amit H. Bermano*

Main category: cs.GR

TL;DR: This paper presents a technique for dynamically projecting 3D content onto human hands with reduced perceived motion-to-photon latency.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is the challenge of accurately and quickly computing the pose and shape of human hands due to their articulated and deformable nature.

Method: The method involves combining a slower 3D coarse estimation of hand pose with high-speed 2D correction steps to improve projection alignment, surface area, and reduce latency.

Result: The results show that subjects are less sensitive to latency artifacts and perform tasks faster and more easily using the proposed method compared to naive projection approaches.

Conclusion: The paper concludes that their new technique for projecting 3D content onto human hands improves the user experience by reducing perceived latency and enhancing task performance compared to existing methods.

Abstract: We present a technique for dynamically projecting 3D content onto human hands
with short perceived motion-to-photon latency. Computing the pose and shape of
human hands accurately and quickly is a challenging task due to their
articulated and deformable nature. We combine a slower 3D coarse estimation of
the hand pose with high speed 2D correction steps which improve the alignment
of the projection to the hands, increase the projected surface area, and reduce
perceived latency. Since our approach leverages a full 3D reconstruction of the
hands, any arbitrary texture or reasonably performant effect can be applied,
which was not possible before. We conducted two user studies to assess the
benefits of using our method. The results show subjects are less sensitive to
latency artifacts and perform faster and with more ease a given associated task
over the naive approach of directly projecting rendered frames from the 3D pose
estimation. We demonstrate several novel use cases and applications.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [19] [Accelerating ab initio melting property calculations with machine learning: Application to the high entropy alloy TaVCrW](https://arxiv.org/abs/2408.08654)
*Li-Fang Zhu,Fritz Koermann,Qing Chen,Malin Selleby,Joerg Neugebauer,and Blazej Grabowski*

Main category: cond-mat.mtrl-sci

TL;DR: 这篇论文介绍了一种结合机器学习势函数的新型DFT方法，用于高效预测复杂合金的熔化性质，为材料设计提供了更快速且精确的工具。


<details>
  <summary>Details</summary>
Motivation: 熔化性质对于设计高性能耐高温材料至关重要，但实验测量非常困难，而传统的密度泛函理论（DFT）方法由于高昂的计算成本无法实现高通量计算。因此，开发一种高效且准确的计算方法成为迫切需求。

Method: 该研究采用了一种专门设计的机器学习势函数，用以替代传统的高成本热力学积分计算，通过自由能微扰法显著提升了计算效率。同时，利用DFT计算了高熵合金TaVCrW的熔化温度、熔化熵和焓变等关键性质。

Result: 该方法实现了计算资源的整体节约达80%，并能够准确预测高熵合金TaVCrW的熔化性质，包括熔化温度、熔化熵、熔化焓以及固态和液态下的热容，这些结果与calphad外推值合理吻合。

Conclusion: 该论文提出了一种高效的基于密度泛函理论（DFT）的方法，结合机器学习势函数来预测材料的熔化性质。这种方法相比传统方法节省了80%的计算资源，并成功应用于高熵合金TaVCrW的熔化性质预测，结果与实验外推值基本一致，展示了其在材料设计中的潜力。

Abstract: Melting properties are critical for designing novel materials, especially for
discovering high-performance, high-melting refractory materials. Experimental
measurements of these properties are extremely challenging due to their high
melting temperatures. Complementary theoretical predictions are, therefore,
indispensable. The conventional free energy approach using density functional
theory (DFT) has been a gold standard for such purposes because of its high
accuracy. However,it generally involves expensive thermodynamic integration
using ab initio molecular dynamic simulations. The high computational cost
makes high-throughput calculations infeasible. Here, we propose a highly
efficient DFT-based method aided by a specially designed machine learning
potential. As the machine learning potential can closely reproduce the ab
initio phase space, even for multi-component alloys, the costly thermodynamic
integration can be fully substituted with more efficient free energy
perturbation calculations. The method achieves overall savings of computational
resources by 80% compared to current alternatives. We apply the method to the
high-entropy alloy TaVCrW and calculate its melting properties, including
melting temperature, entropy and enthalpy of fusion, and volume change at the
melting point. Additionally, the heat capacities of solid and liquid TaVCrW are
calculated. The results agree reasonably with the calphad extrapolated values.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [20] [Model-Based AI planning and Execution Systems for Robotics](https://arxiv.org/abs/2505.04493)
*Or Wertheim,Ronen I. Brafman*

Main category: cs.RO

TL;DR: 本文综述了基于模型的机器人任务控制系统的最新发展，分析了其设计选择、解决方案，并展望了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 构建能够灵活完成多样任务的自主机器人需要一个有条理的方法，而基于模型的规划与执行系统提供了一个原则性的解决方案。虽然这一理念历史悠久，但直到近年来才在现代机器人平台上得到广泛应用。

Method: 文章通过回顾和分析现有的多种基于模型的规划与执行系统的设计选择、尝试解决的问题以及提出的解决方案，来探讨该领域的现状及未来发展路径。

Result: 文章梳理了现有系统的多样化设计、问题解决方法和方案，为未来研究提供了清晰的参考框架和发展建议。

Conclusion: 这篇文章总结了基于模型的机器人任务级控制系统的发展，指出现代机器人平台上集成通用系统如ROSPlan的出现促进了这一领域的发展，并提出了未来发展的方向。

Abstract: Model-based planning and execution systems offer a principled approach to
building flexible autonomous robots that can perform diverse tasks by
automatically combining a host of basic skills. This idea is almost as old as
modern robotics. Yet, while diverse general-purpose reasoning architectures
have been proposed since, general-purpose systems that are integrated with
modern robotic platforms have emerged only recently, starting with the
influential ROSPlan system. Since then, a growing number of model-based systems
for robot task-level control have emerged. In this paper, we consider the
diverse design choices and issues existing systems attempt to address, the
different solutions proposed so far, and suggest avenues for future
development.

</details>


### [21] [Hardware Design and Learning-Based Software Architecture of Musculoskeletal Wheeled Robot Musashi-W for Real-World Applications](https://arxiv.org/abs/2403.11729)
*Kento Kawaharazuka,Akihiro Miki,Masahiro Bando,Temma Suzuki,Yoshimoto Ribayashi,Yasunori Toshimitsu,Yuya Nagamatsu,Kei Okada,and Masayuki Inaba*

Main category: cs.RO

TL;DR: 本文介绍了一种新型肌骨骼轮式机器人 Musashi-W，通过结合轮式底座与肌骨骼上肢以及先进的软件系统，使其在多种现实任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的多体人形机器人虽然具有模仿人体的灵活性和冗余性，但在实际应用中仍面临双足行走困难的问题。因此，开发了结合轮式底座和肌骨骼上肢的 Musashi-W 机器人以解决这一问题。

Method: 结合静态和动态身体模式学习、反射控制和视觉识别构建 Musashi-W 的软件系统，并通过清洁、搬运重物和动态布料操作等任务进行验证。

Result: Musashi-W 在多个任务中展示了其优势，包括通过人工教学进行清洁、考虑肌肉添加的重物搬运以及通过可变刚度的动态布料操作来布置餐桌。

Conclusion: Musashi-W 的硬件和软件系统能够充分利用肌骨骼上肢的优势，适用于真实世界的任务。

Abstract: Various musculoskeletal humanoids have been developed so far. While these
humanoids have the advantage of their flexible and redundant bodies that mimic
the human body, they are still far from being applied to real-world tasks. One
of the reasons for this is the difficulty of bipedal walking in a flexible
body. Thus, we developed a musculoskeletal wheeled robot, Musashi-W, by
combining a wheeled base and musculoskeletal upper limbs for real-world
applications. Also, we constructed its software system by combining static and
dynamic body schema learning, reflex control, and visual recognition. We show
that the hardware and software of Musashi-W can make the most of the advantages
of the musculoskeletal upper limbs, through several tasks of cleaning by human
teaching, carrying a heavy object considering muscle addition, and setting a
table through dynamic cloth manipulation with variable stiffness.

</details>
