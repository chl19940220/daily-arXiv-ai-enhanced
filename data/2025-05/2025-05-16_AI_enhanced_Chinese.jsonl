{"id": "2505.00018", "keyword": "LLM Agent", "pdf": "https://arxiv.org/pdf/2505.00018", "abs": "https://arxiv.org/abs/2505.00018", "authors": ["Ju Wu", "Calvin K. L. Or"], "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability.", "keywords": ["LLM Agent"], "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u534f\u4f5c\u7684\u6700\u65b0\u7ecf\u9a8c\u53d1\u5c55\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u5ff5\u67b6\u6784\uff08Hierarchical Exploration-Exploitation Net\uff09\uff0c\u4ee5\u7cfb\u7edf\u5730\u6574\u5408\u591a\u4ee3\u7406\u534f\u8c03\u3001\u77e5\u8bc6\u7ba1\u7406\u3001\u63a7\u5236\u673a\u5236\u7b49\u6280\u672f\u7ec6\u8282\uff0c\u5e76\u4e3a\u672a\u6765\u8bbe\u8ba1\u6216\u6269\u5c55\u4eba\u7c7b-AI\u5171\u751f\u5173\u7cfb\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "motivation": "\u5f53\u524d\u5728\u5904\u7406\u5f00\u653e\u6027\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u6574\u5408\u5404\u79cd\u7814\u7a76\u548c\u6280\u672f\u6210\u679c\u3002", "method": "\u901a\u8fc7\u5c06\u73b0\u6709\u7684\u8d21\u732e\uff08\u5305\u62ec\u7b26\u53f7AI\u6280\u672f\u3001\u8fde\u63a5\u4e3b\u4e49LLM\u4ee3\u7406\u548c\u6df7\u5408\u7ec4\u7ec7\u5b9e\u8df5\uff09\u6620\u5c04\u5230\u63d0\u51fa\u7684\u6846\u67b6\u4e0a\uff0c\u7cfb\u7edf\u5730\u5206\u6790\u548c\u6574\u5408\u591a\u4ee3\u7406\u534f\u8c03\u3001\u77e5\u8bc6\u7ba1\u7406\u548c\u63a7\u5236\u673a\u5236\u7b49\u6280\u672f\u7ec6\u8282\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u4fee\u6b63\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u542f\u53d1\u878d\u5408\u5b9a\u6027\u548c\u5b9a\u91cf\u8303\u5f0f\u7684\u65b0\u5de5\u4f5c\uff0c\u4fc3\u8fdb\u4eba\u7c7b\u8ba4\u77e5\u4e0eAI\u80fd\u529b\u7684\u5171\u540c\u8fdb\u5316\u3002", "conclusion": "\u8fd9\u4e00\u65b0\u6846\u67b6\u4e3a\u8bbe\u8ba1\u6216\u6269\u5c55\u4eba\u7c7b-AI\u5171\u751f\u5173\u7cfb\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u63a8\u52a8\u4e86\u4e24\u8005\u66f4\u6df1\u5c42\u6b21\u7684\u5171\u540c\u8fdb\u5316\u3002"}}
{"id": "2410.15466", "keyword": "LLM reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing.", "keywords": ["LLM reasoning"], "AI": {"tldr": "\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\uff0c\u901a\u8fc7\u91cd\u590d\u91c7\u6837\u6765\u6269\u5c55\u63a8\u7406\u8ba1\u7b97\uff0c\u968f\u7740\u6837\u672c\u6570\u91cf\u7684\u589e\u52a0\uff0c\u8986\u76d6\u7387\uff08\u89e3\u51b3\u95ee\u9898\u7684\u6bd4\u4f8b\uff09\u6301\u7eed\u63d0\u9ad8\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u6539\u8fdb\u90e8\u5206\u662f\u7531\u4e8e\u6807\u51c6\u8bc4\u4f30\u57fa\u51c6\u7684\u7b54\u6848\u5206\u5e03\u504f\u5411\u4e8e\u5c11\u91cf\u5e38\u89c1\u7b54\u6848\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u4e8b\u5b9e\u77e5\u8bc6\u4e24\u4e2a\u9886\u57df\u4e2d\uff0c\u8be5\u57fa\u7ebf\u65b9\u6cd5\u5bf9\u4e8e\u67d0\u4e9bLLMs\u4f18\u4e8e\u91cd\u590d\u91c7\u6837\uff0c\u800c\u5bf9\u4e8e\u5176\u4ed6LLMs\u4e0e\u6df7\u5408\u7b56\u7565\u8868\u73b0\u76f8\u5f53\u3002\u6b64\u57fa\u7ebf\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u6d4b\u91cf\u91cd\u590d\u91c7\u6837\u5728\u63d0\u793a\u65e0\u5173\u731c\u6d4b\u4e4b\u5916\u5bf9\u8986\u76d6\u7387\u7684\u63d0\u5347\u3002", "motivation": "\u63a2\u7d22\u91cd\u590d\u91c7\u6837\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6548\u679c\u662f\u5426\u53d7\u5230\u8bc4\u4f30\u57fa\u51c6\u7b54\u6848\u5206\u5e03\u7684\u5f71\u54cd\uff0c\u5e76\u5c1d\u8bd5\u901a\u8fc7\u4e00\u79cd\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\u6765\u66f4\u51c6\u786e\u5730\u8861\u91cf\u91cd\u590d\u91c7\u6837\u7684\u5b9e\u9645\u4f5c\u7528\u3002", "method": "\u5b9a\u4e49\u4e00\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6839\u636e\u8bad\u7ec3\u96c6\u4e2d\u7b54\u6848\u7684\u9891\u7387\u679a\u4e3e\u7b54\u6848\uff1b\u5728\u6570\u5b66\u63a8\u7406\u548c\u4e8b\u5b9e\u77e5\u8bc6\u4e24\u4e2a\u9886\u57df\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u57fa\u7ebf\u65b9\u6cd5\u3001\u91cd\u590d\u91c7\u6837\u4ee5\u53ca\u6df7\u5408\u7b56\u7565\u7684\u8868\u73b0\u3002", "result": "\u57fa\u7ebf\u65b9\u6cd5\u5728\u67d0\u4e9bLLMs\u4e0a\u4f18\u4e8e\u91cd\u590d\u91c7\u6837\uff0c\u800c\u5728\u5176\u4ed6LLMs\u4e0a\u4e0e\u6df7\u5408\u7b56\u7565\u8868\u73b0\u76f8\u5f53\uff1b\u63ed\u793a\u4e86\u91cd\u590d\u91c7\u6837\u6548\u679c\u7684\u90e8\u5206\u539f\u56e0\u5728\u4e8e\u8bc4\u4f30\u57fa\u51c6\u7b54\u6848\u5206\u5e03\u7684\u504f\u5dee\u3002", "conclusion": "\u91cd\u590d\u91c7\u6837\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6548\u679c\u90e8\u5206\u5f52\u56e0\u4e8e\u8bc4\u4f30\u57fa\u51c6\u7b54\u6848\u5206\u5e03\u7684\u504f\u5dee\uff0c\u4f7f\u7528\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u91cd\u590d\u91c7\u6837\u7684\u5b9e\u9645\u8d21\u732e\u3002"}}
{"id": "2505.10425", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.10425", "abs": "https://arxiv.org/abs/2505.10425", "authors": ["Jingyao Wang", "Wenwen Qiang", "Zeen Song", "Changwen Zheng", "Hui Xiong"], "title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) excel at complex tasks thanks to advances in\nreasoning abilities. However, existing methods overlook the trade-off between\nreasoning effectiveness and computational efficiency, often encouraging\nunnecessarily long reasoning chains and wasting tokens. To address this, we\npropose Learning to Think (L2T), an information-theoretic reinforcement\nfine-tuning framework for LLMs to make the models achieve optimal reasoning\nwith fewer tokens. Specifically, L2T treats each query-response interaction as\na hierarchical session of multiple episodes and proposes a universal dense\nprocess reward, i.e., quantifies the episode-wise information gain in\nparameters, requiring no extra annotations or task-specific evaluators. We\npropose a method to quickly estimate this reward based on PAC-Bayes bounds and\nthe Fisher information matrix. Theoretical analyses show that it significantly\nreduces computational complexity with high estimation accuracy. By immediately\nrewarding each episode's contribution and penalizing excessive updates, L2T\noptimizes the model via reinforcement learning to maximize the use of each\nepisode and achieve effective updates. Empirical results on various reasoning\nbenchmarks and base models demonstrate the advantage of L2T across different\ntasks, boosting both reasoning effectiveness and efficiency.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5e73\u8861\u63a8\u7406\u6548\u679c\u548c\u8ba1\u7b97\u6548\u7387\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLearning to Think (L2T)\u7684\u4fe1\u606f\u7406\u8bba\u5f3a\u5316\u5fae\u8c03\u6846\u67b6\uff0c\u65e8\u5728\u4f7fLLMs\u4ee5\u66f4\u5c11\u7684token\u5b9e\u73b0\u6700\u4f18\u63a8\u7406\u3002L2T\u901a\u8fc7\u5feb\u901f\u4f30\u8ba1\u5956\u52b1\u3001\u4f18\u5316\u6bcf\u4e2a\u73af\u8282\u7684\u8d21\u732e\u5e76\u60e9\u7f5a\u8fc7\u5ea6\u66f4\u65b0\uff0c\u63d0\u9ad8\u4e86\u63a8\u7406\u7684\u6548\u679c\u548c\u6548\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cL2T\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u548c\u57fa\u7840\u6a21\u578b\u4e0a\u5747\u5c55\u73b0\u51fa\u4f18\u52bf\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u64c5\u957f\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u5e38\u5e38\u4ea7\u751f\u4e0d\u5fc5\u8981\u7684\u957f\u63a8\u7406\u94fe\uff0c\u6d6a\u8d39\u4e86token\u8d44\u6e90\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5e73\u8861\u63a8\u7406\u6548\u679c\u4e0e\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86Learning to Think (L2T)\uff0c\u4e00\u4e2a\u4fe1\u606f\u7406\u8bba\u5f3a\u5316\u5fae\u8c03\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u6bcf\u6b21\u67e5\u8be2-\u54cd\u5e94\u4ea4\u4e92\u89c6\u4e3a\u591a\u9636\u6bb5\u7684\u5206\u5c42\u4f1a\u8bdd\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u666e\u904d\u7684\u5bc6\u96c6\u8fc7\u7a0b\u5956\u52b1\uff0c\u7528\u4e8e\u91cf\u5316\u6bcf\u9636\u6bb5\u53c2\u6570\u7684\u4fe1\u606f\u589e\u76ca\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePAC-Bayes\u8fb9\u754c\u548cFisher\u4fe1\u606f\u77e9\u9635\u7684\u5feb\u901f\u5956\u52b1\u4f30\u8ba1\u65b9\u6cd5\u3002\u901a\u8fc7\u7acb\u5373\u5956\u52b1\u6bcf\u9636\u6bb5\u7684\u8d21\u732e\u5e76\u60e9\u7f5a\u8fc7\u591a\u66f4\u65b0\uff0cL2T\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6a21\u578b\uff0c\u6700\u5927\u5316\u6bcf\u9636\u6bb5\u7684\u4f7f\u7528\u5e76\u5b9e\u73b0\u6709\u6548\u66f4\u65b0\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5956\u52b1\u4f30\u8ba1\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u4f30\u8ba1\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cL2T\u5728\u5404\u79cd\u63a8\u7406\u57fa\u51c6\u548c\u57fa\u7840\u6a21\u578b\u4e0a\u90fd\u663e\u793a\u51fa\u4f18\u52bf\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u7684\u6548\u679c\u548c\u6548\u7387\u3002", "conclusion": "L2T\u4f5c\u4e3a\u4e00\u79cd\u4fe1\u606f\u7406\u8bba\u5f3a\u5316\u5fae\u8c03\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u548c\u6548\u7387\uff0c\u51cf\u5c11token\u6d88\u8017\u5e76\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2505.10320", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.10320", "abs": "https://arxiv.org/abs/2505.10320", "authors": ["Chenxi Whitehouse", "Tianlu Wang", "Ping Yu", "Xian Li", "Jason Weston", "Ilia Kulikov", "Swarnadeep Saha"], "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 8 tables, 11 figures", "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aJ1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u4ee5\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u673a\u5236\u4fc3\u8fdb\u601d\u8003\u5e76\u51cf\u5c11\u8bc4\u5224\u504f\u5dee\uff0c\u4e14\u5728\u76f8\u540c\u89c4\u6a21\u8bad\u7ec3\u4e0b\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u6a21\u578b\u3002", "motivation": "AI\u7684\u8fdb\u6b65\u53d7\u9650\u4e8e\u8bc4\u4f30\u8d28\u91cf\uff0c\u800c\u5f3a\u5927\u7684LLM-as-a-Judge\u6a21\u578b\u88ab\u8bc1\u660e\u662f\u6838\u5fc3\u89e3\u51b3\u65b9\u6848\u3002\u4e3a\u4e86\u63d0\u5347\u5224\u65ad\u80fd\u529b\uff0c\u9700\u8981\u627e\u5230\u6700\u4f73\u7684\u8bad\u7ec3\u65b9\u6cd5\u6765\u589e\u5f3a\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5f15\u5165J1\u65b9\u6cd5\uff0c\u5c06\u53ef\u9a8c\u8bc1\u548c\u4e0d\u53ef\u9a8c\u8bc1\u7684\u63d0\u793a\u8f6c\u6362\u4e3a\u5e26\u6709\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5224\u65ad\u4efb\u52a1\uff0c\u6fc0\u52b1\u6a21\u578b\u8fdb\u884c\u601d\u8003\u5e76\u51cf\u8f7b\u5224\u65ad\u504f\u5dee\u3002", "result": "J1\u65b9\u6cd5\u57288B\u621670B\u89c4\u6a21\u8bad\u7ec3\u65f6\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u6a21\u578b\uff0c\u5305\u62ec\u4eceDeepSeek-R1\u84b8\u998f\u51fa\u7684\u6a21\u578b\u3002\u5373\u4f7f\u8bad\u7ec3\u8f83\u5c0f\u7684\u6a21\u578b\uff0cJ1\u4e5f\u5728\u67d0\u4e9b\u57fa\u51c6\u4e0a\u8d85\u8d8ao1-mini\u548cR1\u3002", "conclusion": "\u901a\u8fc7\u5b66\u4e60\u6982\u8ff0\u8bc4\u4f30\u6807\u51c6\u3001\u4e0e\u81ea\u52a8\u751f\u6210\u7684\u53c2\u8003\u7b54\u6848\u6bd4\u8f83\u4ee5\u53ca\u91cd\u65b0\u8bc4\u4f30\u6a21\u578b\u54cd\u5e94\u7684\u6b63\u786e\u6027\uff0cJ1\u6a21\u578b\u80fd\u591f\u505a\u51fa\u66f4\u597d\u7684\u5224\u65ad\u3002"}}
{"id": "2505.10182", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.10182", "abs": "https://arxiv.org/abs/2505.10182", "authors": ["Yoichi Ishibashi", "Taro Yano", "Masafumi Oyamada"], "title": "Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant improvements in\nreasoning capabilities through supervised fine-tuning and reinforcement\nlearning. However, when training reasoning models, these approaches are\nprimarily applicable to specific domains such as mathematics and programming,\nwhich imposes fundamental constraints on the breadth and scalability of\ntraining data. In contrast, continual pretraining (CPT) offers the advantage of\nnot requiring task-specific signals. Nevertheless, how to effectively\nsynthesize training data for reasoning and how such data affect a wide range of\ndomains remain largely unexplored. This study provides a detailed evaluation of\nReasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden\nthought processes underlying texts, based on the premise that texts are the\nresult of the author's thinking process. Specifically, we apply Reasoning CPT\nto Gemma2-9B using synthetic data with hidden thoughts derived from STEM and\nLaw corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis\nreveals that Reasoning CPT consistently improves performance across all\nevaluated domains. Notably, reasoning skills acquired in one domain transfer\neffectively to others; the performance gap with conventional methods widens as\nproblem difficulty increases, with gains of up to 8 points on the most\nchallenging problems. Furthermore, models trained with hidden thoughts learn to\nadjust the depth of their reasoning according to problem difficulty.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff08Reasoning CPT\uff09\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5927\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u96be\u9898\u4e0a\u7684\u8868\u73b0\u3002\u6b64\u5916\uff0c\u6a21\u578b\u8fd8\u80fd\u6839\u636e\u95ee\u9898\u96be\u5ea6\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u3002", "motivation": "\u5f53\u524d\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u7279\u5b9a\u9886\u57df\uff08\u5982\u6570\u5b66\u548c\u7f16\u7a0b\uff09\uff0c\u4e14\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u5e7f\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u6709\u7ea6\u675f\u3002\u800c\u6301\u7eed\u9884\u8bad\u7ec3\uff08CPT\uff09\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u4fe1\u53f7\uff0c\u4f46\u5982\u4f55\u6709\u6548\u751f\u6210\u9002\u7528\u4e8e\u63a8\u7406\u7684\u8bad\u7ec3\u6570\u636e\u53ca\u5176\u5bf9\u591a\u9886\u57df\u7684\u5f71\u54cd\u5c1a\u5f85\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u63a8\u7406\u6301\u7eed\u9884\u8bad\u7ec3\uff08Reasoning CPT\uff09\uff0c\u5229\u7528STEM\u548c\u6cd5\u5f8b\u8bed\u6599\u5e93\u4e2d\u7684\u9690\u85cf\u601d\u7ef4\u8fc7\u7a0b\u6765\u91cd\u5efa\u6587\u672c\u80cc\u540e\u7684\u601d\u8003\u8fc7\u7a0b\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eGemma2-9B\u6a21\u578b\u3002\u7136\u540e\u5728MMLU\u57fa\u51c6\u4e0a\u4e0e\u6807\u51c6CPT\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "Reasoning CPT\u5728\u6240\u6709\u8bc4\u4f30\u9886\u57df\u4e2d\u5747\u8868\u73b0\u51fa\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u96be\u9898\u4e0a\u7684\u6539\u8fdb\u66f4\u4e3a\u663e\u8457\uff08\u6700\u9ad8\u53ef\u8fbe8\u5206\uff09\u3002\u540c\u65f6\uff0c\u4e00\u4e2a\u9886\u57df\u7684\u63a8\u7406\u6280\u80fd\u53ef\u4ee5\u6709\u6548\u8fc1\u79fb\u5230\u5176\u4ed6\u9886\u57df\uff0c\u6a21\u578b\u8fd8\u5b66\u4f1a\u4e86\u6839\u636e\u95ee\u9898\u96be\u5ea6\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u9a71\u52a8\u7684Reasoning CPT\u662f\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\uff0c\u80fd\u591f\u63d0\u5347\u5927\u6a21\u578b\u8de8\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2505.10010", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.10010", "abs": "https://arxiv.org/abs/2505.10010", "authors": ["Jing-Cheng Pang", "Kaiyuan Li", "Yidi Wang", "Si-Hang Yang", "Shengyi Jiang", "Yang Yu"], "title": "ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts", "categories": ["cs.LG"], "comment": null, "summary": "A central challenge in reinforcement learning (RL) is its dependence on\nextensive real-world interaction data to learn task-specific policies. While\nrecent work demonstrates that large language models (LLMs) can mitigate this\nlimitation by generating synthetic experience (noted as imaginary rollouts) for\nmastering novel tasks, progress in this emerging field is hindered due to the\nlack of a standard benchmark. To bridge this gap, we introduce ImagineBench,\nthe first comprehensive benchmark for evaluating offline RL algorithms that\nleverage both real rollouts and LLM-imaginary rollouts. The key features of\nImagineBench include: (1) datasets comprising environment-collected and\nLLM-imaginary rollouts; (2) diverse domains of environments covering\nlocomotion, robotic manipulation, and navigation tasks; and (3) natural\nlanguage task instructions with varying complexity levels to facilitate\nlanguage-conditioned policy learning. Through systematic evaluation of\nstate-of-the-art offline RL algorithms, we observe that simply applying\nexisting offline RL algorithms leads to suboptimal performance on unseen tasks,\nachieving 35.44% success rate in hard tasks in contrast to 64.37% of method\ntraining on real rollouts for hard tasks. This result highlights the need for\nalgorithm advancements to better leverage LLM-imaginary rollouts. Additionally,\nwe identify key opportunities for future research: including better utilization\nof imaginary rollouts, fast online adaptation and continual learning, and\nextension to multi-modal tasks. Our code is publicly available at\nhttps://github.com/LAMDA-RL/ImagineBench.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aImagineBench\u7684\u5168\u65b0\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u7ed3\u5408\u771f\u5b9e\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u7ecf\u9a8c\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002\u8be5\u57fa\u51c6\u5305\u62ec\u591a\u79cd\u73af\u5883\u6570\u636e\u96c6\u3001\u4efb\u52a1\u7c7b\u578b\u548c\u8bed\u8a00\u6307\u4ee4\uff0c\u901a\u8fc7\u8bc4\u4f30\u73b0\u6709\u7b97\u6cd5\u53d1\u73b0\u6027\u80fd\u6709\u5f85\u63d0\u5347\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u4f9d\u8d56\u5927\u91cf\u771f\u5b9e\u4ea4\u4e92\u6570\u636e\u7684\u95ee\u9898\uff0c\u800cLLMs\u53ef\u4ee5\u901a\u8fc7\u751f\u6210\u5408\u6210\u7ecf\u9a8c\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u963b\u788d\u4e86\u8fd9\u4e00\u9886\u57df\u7684\u53d1\u5c55\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u7efc\u5408\u6027\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u5229\u7528\u771f\u5b9e\u548c\u60f3\u8c61\u7ecf\u9a8c\u7684\u79bb\u7ebfRL\u7b97\u6cd5\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aImagineBench\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u542b\uff1a1\uff09\u771f\u5b9e\u548cLLM\u751f\u6210\u7684\u6570\u636e\u96c6\uff1b2\uff09\u6db5\u76d6\u8fd0\u52a8\u3001\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u5bfc\u822a\u7b49\u591a\u6837\u73af\u5883\uff1b3\uff09\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u6307\u4ee4\u3002\u4f7f\u7528\u6b64\u57fa\u51c6\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u79bb\u7ebfRL\u7b97\u6cd5\u3002", "result": "\u5355\u7eaf\u5e94\u7528\u73b0\u6709\u7684\u79bb\u7ebfRL\u7b97\u6cd5\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5728\u56f0\u96be\u4efb\u52a1\u4e0a\u7684\u6210\u529f\u7387\u4e3a35.44%\uff0c\u8fdc\u4f4e\u4e8e\u4ec5\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u7684\u65b9\u6cd564.37%\u7684\u6210\u529f\u7387\u3002\u8fd9\u8868\u660e\u9700\u8981\u6539\u8fdb\u7b97\u6cd5\u4ee5\u66f4\u597d\u5730\u5229\u7528LLM\u751f\u6210\u7684\u7ecf\u9a8c\u3002", "conclusion": "\u6587\u7ae0\u63d0\u51fa\u4e86ImagineBench\u4f5c\u4e3a\u9996\u4e2a\u8bc4\u4f30\u7ed3\u5408\u771f\u5b9e\u548c\u60f3\u8c61\u7ecf\u9a8c\u7684\u79bb\u7ebfRL\u7b97\u6cd5\u7684\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u7b97\u6cd5\u7684\u4e0d\u8db3\u4e4b\u5904\uff0c\u540c\u65f6\u6307\u51fa\u672a\u6765\u7814\u7a76\u7684\u673a\u4f1a\uff0c\u5982\u66f4\u597d\u5730\u5229\u7528\u60f3\u8c61\u7ecf\u9a8c\u3001\u5feb\u901f\u5728\u7ebf\u9002\u5e94\u548c\u6301\u7eed\u5b66\u4e60\u7b49\u3002"}}
{"id": "2505.09925", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.09925", "abs": "https://arxiv.org/abs/2505.09925", "authors": ["Yutao Yang", "Jie Zhou", "Junsong Li", "Qianjun Pan", "Bihao Zhan", "Qin Chen", "Xipeng Qiu", "Liang He"], "title": "Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper introduces an interactive continual learning paradigm where AI\nmodels dynamically learn new skills from real-time human feedback while\nretaining prior knowledge. This paradigm distinctively addresses two major\nlimitations of traditional continual learning: (1) dynamic model updates using\nstreaming, real-time human-annotated data, rather than static datasets with\nfixed labels, and (2) the assumption of clean labels, by explicitly handling\nthe noisy feedback common in real-world interactions. To tackle these problems,\nwe propose RiCL, a Reinforced interactive Continual Learning framework\nleveraging Large Language Models (LLMs) to learn new skills effectively from\ndynamic feedback. RiCL incorporates three key components: a temporal\nconsistency-aware purifier to automatically discern clean from noisy samples in\ndata streams; an interaction-aware direct preference optimization strategy to\nalign model behavior with human intent by reconciling AI-generated and\nhuman-provided feedback; and a noise-resistant contrastive learning module that\ncaptures robust representations by exploiting inherent data relationships, thus\navoiding reliance on potentially unreliable labels. Extensive experiments on\ntwo benchmark datasets (FewRel and TACRED), contaminated with realistic noise\npatterns, demonstrate that our RiCL approach substantially outperforms existing\ncombinations of state-of-the-art online continual learning and noisy-label\nlearning methods.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u4e92\u5f0f\u6301\u7eed\u5b66\u4e60\u6846\u67b6RiCL\uff0c\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ece\u52a8\u6001\u53cd\u9988\u4e2d\u6709\u6548\u5b66\u4e60\u65b0\u6280\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u4e24\u5927\u9650\u5236\uff1a\u9759\u6001\u6570\u636e\u96c6\u548c\u6e05\u6d01\u6807\u7b7e\u5047\u8bbe\u3002RiCL\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u65f6\u95f4\u4e00\u81f4\u6027\u611f\u77e5\u51c0\u5316\u5668\u3001\u4ea4\u4e92\u611f\u77e5\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7b56\u7565\u4ee5\u53ca\u6297\u566a\u58f0\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\u3002\u5b9e\u9a8c\u8868\u660e\uff0cRiCL\u5728\u5904\u7406\u542b\u566a\u6570\u636e\u65f6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u4f7f\u7528\u9759\u6001\u6570\u636e\u96c6\u800c\u975e\u5b9e\u65f6\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\uff1b2) \u5047\u8bbe\u6807\u7b7e\u6e05\u6d01\uff0c\u672a\u8003\u8651\u73b0\u5b9e\u4e16\u754c\u4e2d\u666e\u904d\u5b58\u5728\u7684\u566a\u58f0\u53cd\u9988\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86RiCL\u6846\u67b6\u3002", "method": "RiCL\u6846\u67b6\u5305\u542b\u4ee5\u4e0b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u65f6\u95f4\u4e00\u81f4\u6027\u611f\u77e5\u51c0\u5316\u5668\uff0c\u7528\u4e8e\u81ea\u52a8\u533a\u5206\u6570\u636e\u6d41\u4e2d\u7684\u5e72\u51c0\u6837\u672c\u4e0e\u566a\u58f0\u6837\u672c\uff1b2) \u4ea4\u4e92\u611f\u77e5\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u534f\u8c03AI\u751f\u6210\u53cd\u9988\u4e0e\u4eba\u7c7b\u63d0\u4f9b\u53cd\u9988\u6765\u5bf9\u9f50\u6a21\u578b\u884c\u4e3a\u4e0e\u4eba\u7c7b\u610f\u56fe\uff1b3) \u6297\u566a\u58f0\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\uff0c\u5229\u7528\u6570\u636e\u5185\u5728\u5173\u7cfb\u6355\u83b7\u9c81\u68d2\u8868\u793a\uff0c\u907f\u514d\u4f9d\u8d56\u4e0d\u53ef\u9760\u6807\u7b7e\u3002", "result": "\u5728FewRel\u548cTACRED\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0cRiCL\u65b9\u6cd5\u5728\u5904\u7406\u542b\u6709\u73b0\u5b9e\u566a\u58f0\u6a21\u5f0f\u7684\u6570\u636e\u65f6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u548c\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u65b9\u6cd5\u7ec4\u5408\u3002", "conclusion": "RiCL\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u4e24\u5927\u9650\u5236\uff0c\u5e76\u5728\u5904\u7406\u542b\u566a\u6570\u636e\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09820", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.09820", "abs": "https://arxiv.org/abs/2505.09820", "authors": ["Sajib Biswas", "Mao Nishino", "Samuel Jacob Chacko", "Xiuwen Liu"], "title": "Adversarial Attack on Large Language Models using Exponentiated Gradient Descent", "categories": ["cs.LG"], "comment": "Accepted to International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "As Large Language Models (LLMs) are widely used, understanding them\nsystematically is key to improving their safety and realizing their full\npotential. Although many models are aligned using techniques such as\nreinforcement learning from human feedback (RLHF), they are still vulnerable to\njailbreaking attacks. Some of the existing adversarial attack methods search\nfor discrete tokens that may jailbreak a target model while others try to\noptimize the continuous space represented by the tokens of the model's\nvocabulary. While techniques based on the discrete space may prove to be\ninefficient, optimization of continuous token embeddings requires projections\nto produce discrete tokens, which might render them ineffective. To fully\nutilize the constraints and the structures of the space, we develop an\nintrinsic optimization technique using exponentiated gradient descent with the\nBregman projection method to ensure that the optimized one-hot encoding always\nstays within the probability simplex. We prove the convergence of the technique\nand implement an efficient algorithm that is effective in jailbreaking several\nwidely used LLMs. We demonstrate the efficacy of the proposed technique using\nfive open-source LLMs on four openly available datasets. The results show that\nthe technique achieves a higher success rate with great efficiency compared to\nthree other state-of-the-art jailbreaking techniques. The source code for our\nimplementation is available at:\nhttps://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6307\u6570\u68af\u5ea6\u4e0b\u964d\u548cBregman\u6295\u5f71\u65b9\u6cd5\u7684\u5185\u5728\u4f18\u5316\u6280\u672f\uff0c\u80fd\u591f\u6709\u6548\u7834\u89e3\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6536\u655b\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6280\u672f\u5728\u4e94\u4e2a\u5f00\u6e90LLM\u4e0a\u76f8\u8f83\u4e8e\u5176\u4ed6\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u7834\u89e3\u6280\u672f\u5177\u6709\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u8bb8\u591aLLMs\u901a\u8fc7\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\u8fdb\u884c\u4e86\u5bf9\u9f50\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\u3002\u73b0\u6709\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u8981\u4e48\u5728\u79bb\u6563\u7a7a\u95f4\u4e2d\u641c\u7d22\u53ef\u80fd\u8d8a\u72f1\u76ee\u6807\u6a21\u578b\u7684token\uff0c\u8981\u4e48\u5c1d\u8bd5\u4f18\u5316\u7531\u6a21\u578b\u8bcd\u6c47\u8868\u4e2d\u7684token\u8868\u793a\u7684\u8fde\u7eed\u7a7a\u95f4\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5206\u522b\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u6216\u6548\u679c\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u79cd\u5185\u5728\u4f18\u5316\u6280\u672f\uff0c\u4f7f\u7528\u6307\u6570\u68af\u5ea6\u4e0b\u964d\u4e0eBregman\u6295\u5f71\u65b9\u6cd5\uff0c\u786e\u4fdd\u4f18\u5316\u7684\u4e00\u70ed\u7f16\u7801\u59cb\u7ec8\u4f4d\u4e8e\u6982\u7387\u5355\u7eaf\u5f62\u5185\u3002\u6b64\u65b9\u6cd5\u7ed3\u5408\u4e86\u79bb\u6563\u7a7a\u95f4\u548c\u8fde\u7eed\u7a7a\u95f4\u7684\u4f18\u70b9\uff0c\u5e76\u4e14\u4f5c\u8005\u8fd8\u8bc1\u660e\u4e86\u8be5\u6280\u672f\u7684\u6536\u655b\u6027\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6280\u672f\u5728\u7834\u89e3\u4e94\u4e2a\u5f00\u6e90LLM\u65f6\uff0c\u76f8\u8f83\u4e8e\u5176\u4ed6\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u8d8a\u72f1\u6280\u672f\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u66f4\u9ad8\u6548\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u5185\u5728\u4f18\u5316\u6280\u672f\u6709\u6548\u5730\u5229\u7528\u4e86\u7a7a\u95f4\u7684\u7ea6\u675f\u548c\u7ed3\u6784\uff0c\u5728\u7834\u89e3LLMs\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6e90\u4ee3\u7801\u5df2\u516c\u5f00\uff0c\u53ef\u4f9b\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2505.09395", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.09395", "abs": "https://arxiv.org/abs/2505.09395", "authors": ["Chen-Yu Liu", "Kuan-Cheng Chen", "Yi-Chien Chen", "Samuel Yen-Chi Chen", "Wei-Hao Huang", "Wei-Jia Huang", "Yen-Jui Chang"], "title": "Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": null, "summary": "Typhoon trajectory forecasting is essential for disaster preparedness but\nremains computationally demanding due to the complexity of atmospheric dynamics\nand the resource requirements of deep learning models. Quantum-Train (QT), a\nhybrid quantum-classical framework that leverages quantum neural networks\n(QNNs) to generate trainable parameters exclusively during training,\neliminating the need for quantum hardware at inference time. Building on QT's\nsuccess across multiple domains, including image classification, reinforcement\nlearning, flood prediction, and large language model (LLM) fine-tuning, we\nintroduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting\nmodel learning. Integrated with an Attention-based Multi-ConvGRU model, QPA\nenables parameter-efficient training while maintaining predictive accuracy.\nThis work represents the first application of quantum machine learning (QML) to\nlarge-scale typhoon trajectory prediction, offering a scalable and\nenergy-efficient approach to climate modeling. Our results demonstrate that QPA\nsignificantly reduces the number of trainable parameters while preserving\nperformance, making high-performance forecasting more accessible and\nsustainable through hybrid quantum-classical learning.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5QPA\uff08Quantum Parameter Adaptation\uff09\uff0c\u7ed3\u5408Attention-based Multi-ConvGRU\u6a21\u578b\uff0c\u9996\u6b21\u5c06\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u53f0\u98ce\u8f68\u8ff9\u9884\u6d4b\u4e2d\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u53f0\u98ce\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5927\u91cf\u8d44\u6e90\u3002\u4e3a\u4e86\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u9ad8\u6548\u7387\uff0c\u7814\u7a76\u8005\u5f15\u5165\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u5229\u7528Quantum-Train (QT)\u6846\u67b6\u6210\u529f\u5f00\u53d1\u51faQuantum Parameter Adaptation (QPA)\uff0c\u8be5\u65b9\u6cd5\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u63a8\u65ad\u65f6\u65e0\u9700\u91cf\u5b50\u786c\u4ef6\uff0c\u5e76\u5c06\u5176\u4e0eAttention-based Multi-ConvGRU\u6a21\u578b\u96c6\u6210\u3002", "result": "QPA\u5927\u5e45\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u8282\u80fd\u7684\u6c14\u5019\u5efa\u6a21\u65b9\u6cd5\u3002", "conclusion": "QPA\u4e3a\u9ad8\u6548\u5b66\u4e60\u53f0\u98ce\u9884\u62a5\u6a21\u578b\u63d0\u4f9b\u4e86\u53c2\u6570\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u5f0f\uff0c\u63a8\u52a8\u4e86\u6df7\u5408\u91cf\u5b50\u7ecf\u5178\u5b66\u4e60\u5728\u9ad8\u6027\u80fd\u91cf\u5316\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u548c\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2505.09024", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.09024", "abs": "https://arxiv.org/abs/2505.09024", "authors": ["Aaron Baughman", "Rahul Agarwal", "Eduardo Morales", "Gozde Akay"], "title": "Automated Meta Prompt Engineering for Alignment with the Theory of Mind", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "9 pages, 6 figures, 3 tables", "summary": "We introduce a method of meta-prompting that jointly produces fluent text for\ncomplex tasks while optimizing the similarity of neural states between a\nhuman's mental expectation and a Large Language Model's (LLM) neural\nprocessing. A technique of agentic reinforcement learning is applied, in which\nan LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,\nhow to produce content by interpreting the intended and unintended generated\ntext traits. To measure human mental beliefs around content production, users\nmodify long form AI-generated text articles before publication at the US Open\n2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)\nalignment problem by anticipating and including human edits within the creation\nof text from an LLM. Throughout experimentation and by interpreting the results\nof a live production system, the expectations of human content reviewers had\n100% of alignment with AI 53.8% of the time with an average iteration count of\n4.38. The geometric interpretation of content traits such as factualness,\nnovelty, repetitiveness, and relevancy over a Hilbert vector space combines\nspatial volume (all trait importance) with vertices alignment (individual trait\nrelevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an\nincrease in content quality by extending the coverage of tennis action. Our\nwork that was deployed at the US Open 2024 has been used across other live\nevents within sports and entertainment.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5143\u63d0\u793a\uff08meta-prompting\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u751f\u6210\u6d41\u7545\u6587\u672c\u548c\u4f18\u5316\u4eba\u7c7b\u5fc3\u7406\u9884\u671f\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u795e\u7ecf\u5904\u7406\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u6765\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u7f8e\u56fd\u7f51\u7403\u516c\u5f00\u8d5b2024\u7b49\u5b9e\u9645\u751f\u4ea7\u7cfb\u7edf\u4e2d\uff0cLLM\u4f5c\u4e3a\u5224\u65ad\u8005\uff08LLMaaJ\uff09\u80fd\u591f\u901a\u8fc7\u60c5\u5883\u5b66\u4e60\u6559\u5bfc\u5176\u4ed6LLM\u751f\u6210\u5185\u5bb9\uff0c\u5e76\u5728\u5e73\u57474.38\u6b21\u8fed\u4ee3\u540e\u8fbe\u523053.8%\u7684\u4eba\u7c7b\u8bc4\u5ba1\u9884\u671f\u5bf9\u9f50\u7387\u3002\u901a\u8fc7\u7ed3\u5408\u5e0c\u5c14\u4f2f\u7279\u5411\u91cf\u7a7a\u95f4\u4e2d\u7684\u5185\u5bb9\u7279\u8d28\u51e0\u4f55\u89e3\u91ca\uff0cLLMaaJ\u53ef\u4ee5\u4f18\u5316\u4eba\u7c7b\u7684\u7406\u8bba\u601d\u7ef4\uff08ToM\uff09\uff0c\u4ece\u800c\u63d0\u9ad8\u5185\u5bb9\u8d28\u91cf\u5e76\u6269\u5c55\u7f51\u7403\u8d5b\u4e8b\u62a5\u9053\u7684\u8986\u76d6\u8303\u56f4\u3002\u6b64\u65b9\u6cd5\u5df2\u63a8\u5e7f\u5230\u4f53\u80b2\u548c\u5a31\u4e50\u9886\u57df\u7684\u5176\u4ed6\u73b0\u573a\u6d3b\u52a8\u3002", "motivation": "\u4e3a\u4e86\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5185\u5bb9\u66f4\u7b26\u5408\u4eba\u7c7b\u7684\u5fc3\u7406\u9884\u671f\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5373\u901a\u8fc7\u5143\u63d0\u793a\u548c\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u8ba9\u4e00\u4e2aLLM\uff08\u4f5c\u4e3a\u5224\u65ad\u8005LLMaaJ\uff09\u6307\u5bfc\u53e6\u4e00\u4e2aLLM\u751f\u6210\u5185\u5bb9\uff0c\u540c\u65f6\u8003\u8651\u6709\u610f\u548c\u65e0\u610f\u751f\u6210\u7684\u6587\u672c\u7279\u6027\u3002\u76ee\u7684\u662f\u89e3\u51b3\u7406\u8bba\u601d\u7ef4\uff08ToM\uff09\u5bf9\u9f50\u95ee\u9898\uff0c\u9884\u6d4b\u5e76\u5305\u542b\u4eba\u7c7b\u7f16\u8f91\u610f\u89c1\u4e8e\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u4e2d\u3002", "method": "\u4f7f\u7528\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u5176\u4e2dLLMaaJ\u901a\u8fc7\u60c5\u5883\u5b66\u4e60\u6307\u5bfc\u53e6\u4e00\u4e2aLLM\u751f\u6210\u5185\u5bb9\uff1b\u7528\u6237\u5728\u7f8e\u56fd\u7f51\u7403\u516c\u5f00\u8d5b2024\u671f\u95f4\u4fee\u6539AI\u751f\u6210\u7684\u957f\u7bc7\u6587\u672c\u4ee5\u6d4b\u91cf\u4eba\u7c7b\u5fc3\u7406\u4fe1\u5ff5\uff1b\u5229\u7528\u5e0c\u5c14\u4f2f\u7279\u5411\u91cf\u7a7a\u95f4\u4e2d\u7684\u51e0\u4f55\u89e3\u91ca\u6765\u4f18\u5316\u5185\u5bb9\u7279\u8d28\uff08\u5982\u4e8b\u5b9e\u6027\u3001\u65b0\u9896\u6027\u3001\u91cd\u590d\u6027\u548c\u76f8\u5173\u6027\uff09\u3002", "result": "\u5728\u5b9e\u9a8c\u548c\u5b9e\u9645\u751f\u4ea7\u7cfb\u7edf\u7684\u7ed3\u679c\u5206\u6790\u4e2d\uff0c\u4eba\u7c7b\u5185\u5bb9\u8bc4\u5ba1\u8005\u7684\u671f\u671b\u4e0eAI\u751f\u6210\u5185\u5bb9\u7684\u5bf9\u9f50\u7387\u8fbe\u523053.8%\uff0c\u5e73\u5747\u8fed\u4ee3\u6b21\u6570\u4e3a4.38\u6b21\u3002\u8fd9\u79cd\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5185\u5bb9\u8d28\u91cf\uff0c\u6269\u5c55\u4e86\u7f51\u7403\u52a8\u4f5c\u7684\u62a5\u9053\u8986\u76d6\u8303\u56f4\u3002", "conclusion": "\u901a\u8fc7\u5143\u63d0\u793a\u548c\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\uff0cLLMaaJ\u53ef\u4ee5\u5728\u751f\u6210\u5185\u5bb9\u65f6\u6709\u6548\u9884\u6d4b\u5e76\u5305\u542b\u4eba\u7c7b\u7f16\u8f91\u610f\u89c1\uff0c\u4ece\u800c\u4f18\u5316\u4eba\u7c7b\u7684\u7406\u8bba\u601d\u7ef4\u5bf9\u9f50\uff0c\u63d0\u9ad8\u751f\u6210\u5185\u5bb9\u7684\u8d28\u91cf\u3002\u6b64\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u7f8e\u56fd\u7f51\u7403\u516c\u5f00\u8d5b2024\uff0c\u8fd8\u53ef\u63a8\u5e7f\u5230\u4f53\u80b2\u548c\u5a31\u4e50\u9886\u57df\u7684\u5176\u4ed6\u73b0\u573a\u6d3b\u52a8\u3002"}}
{"id": "2505.08849", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.08849", "abs": "https://arxiv.org/abs/2505.08849", "authors": ["Keyu Chen", "Hao Tang", "Qinglin Liu", "Yizhao Xu"], "title": "Improved Algorithms for Differentially Private Language Model Alignment", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Language model alignment is crucial for ensuring that large language models\n(LLMs) align with human preferences, yet it often involves sensitive user data,\nraising significant privacy concerns. While prior work has integrated\ndifferential privacy (DP) with alignment techniques, their performance remains\nlimited. In this paper, we propose novel algorithms for privacy-preserving\nalignment and rigorously analyze their effectiveness across varying privacy\nbudgets and models. Our framework can be deployed on two celebrated alignment\ntechniques, namely direct preference optimization (DPO) and reinforcement\nlearning from human feedback (RLHF). Through systematic experiments on\nlarge-scale language models, we demonstrate that our approach achieves\nstate-of-the-art performance. Notably, one of our algorithms, DP-AdamW,\ncombined with DPO, surpasses existing methods, improving alignment quality by\nup to 15% under moderate privacy budgets ({\\epsilon}=2-5). We further\ninvestigate the interplay between privacy guarantees, alignment efficacy, and\ncomputational demands, providing practical guidelines for optimizing these\ntrade-offs.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u65b0\u7684\u9690\u79c1\u4fdd\u62a4\u7b97\u6cd5\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\uff0c\u5e76\u5728\u4e0d\u540c\u9690\u79c1\u9884\u7b97\u4e0b\u8fdb\u884c\u4e86\u6709\u6548\u6027\u5206\u6790\uff0c\u5176\u4e2dDP-AdamW\u7b97\u6cd5\u7ed3\u5408DPO\u6280\u672f\u5728\u9002\u5ea6\u9690\u79c1\u9884\u7b97\u6761\u4ef6\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u63d0\u5347\u4e8615%\u7684\u5bf9\u9f50\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u6d89\u53ca\u654f\u611f\u7528\u6237\u6570\u636e\uff0c\u5b58\u5728\u9690\u79c1\u95ee\u9898\uff0c\u800c\u4ee5\u5f80\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u7684\u65b9\u6cd5\u6027\u80fd\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u548c\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u4e24\u79cd\u5bf9\u9f50\u6280\u672f\u7684\u65b0\u578b\u9690\u79c1\u4fdd\u62a4\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u65b0\u63d0\u51fa\u7684DP-AdamW\u7b97\u6cd5\u7ed3\u5408DPO\u6280\u672f\uff0c\u5728\u9002\u5ea6\u9690\u79c1\u9884\u7b97\uff08\u03b5=2-5\uff09\u4e0b\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u9ad8\u8fbe15%\u7684\u5bf9\u9f50\u8d28\u91cf\uff0c\u540c\u65f6\u7814\u7a76\u4e86\u9690\u79c1\u4fdd\u8bc1\u3001\u5bf9\u9f50\u6548\u679c\u548c\u8ba1\u7b97\u9700\u6c42\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u9690\u79c1\u4fdd\u62a4\u7b97\u6cd5\u4e3a\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u9690\u79c1\u4fdd\u969c\uff0c\u5e76\u7ed9\u51fa\u4e86\u4f18\u5316\u9690\u79c1-\u6548\u7528\u6743\u8861\u7684\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2505.08827", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.08827", "abs": "https://arxiv.org/abs/2505.08827", "authors": ["Toby Simonds", "Kevin Lopez", "Akira Yoshiyama", "Dominique Garmier"], "title": "Self Rewarding Self Improving", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We demonstrate that large language models can effectively self-improve\nthrough self-judging without requiring reference solutions, leveraging the\ninherent asymmetry between generating and verifying solutions. Our experiments\non Countdown puzzles and MIT Integration Bee problems show that models can\nprovide reliable reward signals without ground truth answers, enabling\nreinforcement learning in domains previously not possible. By implementing\nself-judging, we achieve significant performance gains maintaining alignment\nwith formal verification. When combined with synthetic question generation, we\nestablish a complete self-improvement loop where models generate practice\nproblems, solve them, and evaluate their own performance-achieving an 8%\nimprovement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on\nintegration tasks. Our findings demonstrate that LLM judges can provide\neffective reward signals for training models, unlocking many reinforcement\nlearning environments previously limited by the difficulty of creating\nprogrammatic rewards. This suggests a potential paradigm shift toward AI\nsystems that continuously improve through self-directed learning rather than\nhuman-guided training, potentially accelerating progress in domains with scarce\ntraining data or complex evaluation requirements.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u901a\u8fc7\u81ea\u6211\u8bc4\u5224\u673a\u5236\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5728\u6ca1\u6709\u53c2\u8003\u7b54\u6848\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728 Countdown \u6570\u5b57\u6e38\u620f\u548c MIT Integration Bee \u79ef\u5206\u7ade\u8d5b\u95ee\u9898\u4e0a\uff0c\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u53ef\u9760\u7684\u5956\u52b1\u4fe1\u53f7\u4ee5\u652f\u6301\u5f3a\u5316\u5b66\u4e60\u3002\u7ed3\u5408\u5408\u6210\u95ee\u9898\u751f\u6210\u6280\u672f\uff0c\u5f62\u6210\u4e86\u5b8c\u6574\u7684\u81ea\u6211\u6539\u8fdb\u5faa\u73af\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff08\u5982Qwen 2.5 7B\u76f8\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e868%\uff0c\u5e76\u5728\u79ef\u5206\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86GPT-4o\uff09\u3002\u6b64\u65b9\u6cd5\u53ef\u80fd\u5f15\u53d1\u4ece\u4eba\u7c7b\u5f15\u5bfc\u8bad\u7ec3\u5411\u81ea\u6211\u5bfc\u5411\u5b66\u4e60\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "\u5f53\u524d\u8bb8\u591a\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u53d7\u9650\u4e8e\u96be\u4ee5\u521b\u5efa\u7a0b\u5e8f\u5316\u5956\u52b1\u673a\u5236\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u6216\u8bc4\u4f30\u8981\u6c42\u590d\u6742\u7684\u9886\u57df\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u65e0\u9700\u771f\u5b9e\u7b54\u6848\u53c2\u8003\u7684\u81ea\u6211\u6539\u8fdb\u65b9\u6cd5\uff0c\u5229\u7528\u751f\u6210\u4e0e\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\u4e4b\u95f4\u7684\u56fa\u6709\u4e0d\u5bf9\u79f0\u6027\u3002", "method": "\u5728 Countdown \u6570\u5b57\u6e38\u620f\u548c MIT Integration Bee \u79ef\u5206\u7ade\u8d5b\u95ee\u9898\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u901a\u8fc7\u5b9e\u65bd\u81ea\u6211\u8bc4\u5224\u673a\u5236\uff0c\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u7ec3\u4e60\u95ee\u9898\u3001\u89e3\u51b3\u95ee\u9898\u5e76\u8bc4\u4f30\u81ea\u8eab\u8868\u73b0\u3002\u540c\u65f6\u7ed3\u5408\u5408\u6210\u95ee\u9898\u751f\u6210\u6280\u672f\u5f62\u6210\u5b8c\u6574\u7684\u81ea\u6211\u6539\u8fdb\u5faa\u73af\u3002", "result": "\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0cQwen 2.5 7B\u76f8\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e868%\uff0c\u5e76\u5728\u79ef\u5206\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86GPT-4o\u7684\u8868\u73b0\u3002\u8bc1\u660e\u4e86LLM\u8bc4\u5224\u8005\u80fd\u63d0\u4f9b\u6709\u6548\u7684\u5956\u52b1\u4fe1\u53f7\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u81ea\u6211\u8bc4\u5224\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\uff0c\u5e76\u53ef\u80fd\u63a8\u52a8\u4ece\u4eba\u7c7b\u5f15\u5bfc\u8bad\u7ec3\u5411\u81ea\u6211\u5bfc\u5411\u5b66\u4e60\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4ece\u800c\u52a0\u901f\u5728\u6570\u636e\u7a00\u7f3a\u6216\u590d\u6742\u8bc4\u4f30\u9700\u6c42\u9886\u57df\u7684\u8fdb\u6b65\u3002"}}
