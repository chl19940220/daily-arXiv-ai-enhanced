{"id": "2505.00018", "keyword": "LLM Agent", "pdf": "https://arxiv.org/pdf/2505.00018", "abs": "https://arxiv.org/abs/2505.00018", "authors": ["Ju Wu", "Calvin K. L. Or"], "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability."}
{"id": "2410.15466", "keyword": "LLM reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing."}
{"id": "2505.20065", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.20065", "abs": "https://arxiv.org/abs/2505.20065", "authors": ["Geon-Hyeong Kim", "Youngsoo Jang", "Yu Jin Kim", "Byoungjip Kim", "Honglak Lee", "Kyunghoon Bae", "Moontae Lee"], "title": "SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety", "categories": ["cs.LG", "cs.AI"], "comment": "34 pages", "summary": "As Large Language Models (LLMs) continue to advance and find applications\nacross a growing number of fields, ensuring the safety of LLMs has become\nincreasingly critical. To address safety concerns, recent studies have proposed\nintegrating safety constraints into Reinforcement Learning from Human Feedback\n(RLHF). However, these approaches tend to be complex, as they encompass\ncomplicated procedures in RLHF along with additional steps required by the\nsafety constraints. Inspired by Direct Preference Optimization (DPO), we\nintroduce a new algorithm called SafeDPO, which is designed to directly\noptimize the safety alignment objective in a single stage of policy learning,\nwithout requiring relaxation. SafeDPO introduces only one additional\nhyperparameter to further enhance safety and requires only minor modifications\nto standard DPO. As a result, it eliminates the need to fit separate reward and\ncost models or to sample from the language model during fine-tuning, while\nstill enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO\nachieves competitive performance compared to state-of-the-art safety alignment\nalgorithms, both in terms of aligning with human preferences and improving\nsafety."}
{"id": "2505.19954", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.19954", "abs": "https://arxiv.org/abs/2505.19954", "authors": ["Andrew Zamai", "Nathanael Fijalkow", "Boris Mansencal", "Laurent Simon", "Eloi Navet", "Pierrick Coupe"], "title": "An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The differential diagnosis of neurodegenerative dementias is a challenging\nclinical task, mainly because of the overlap in symptom presentation and the\nsimilarity of patterns observed in structural neuroimaging. To improve\ndiagnostic efficiency and accuracy, deep learning-based methods such as\nConvolutional Neural Networks and Vision Transformers have been proposed for\nthe automatic classification of brain MRIs. However, despite their strong\npredictive performance, these models find limited clinical utility due to their\nopaque decision making. In this work, we propose a framework that integrates\ntwo core components to enhance diagnostic transparency. First, we introduce a\nmodular pipeline for converting 3D T1-weighted brain MRIs into textual\nradiology reports. Second, we explore the potential of modern Large Language\nModels (LLMs) to assist clinicians in the differential diagnosis between\nFrontotemporal dementia subtypes, Alzheimer's disease, and normal aging based\non the generated reports. To bridge the gap between predictive accuracy and\nexplainability, we employ reinforcement learning to incentivize diagnostic\nreasoning in LLMs. Without requiring supervised reasoning traces or\ndistillation from larger models, our approach enables the emergence of\nstructured diagnostic rationales grounded in neuroimaging findings. Unlike\npost-hoc explainability methods that retrospectively justify model decisions,\nour framework generates diagnostic rationales as part of the inference\nprocess-producing causally grounded explanations that inform and guide the\nmodel's decision-making process. In doing so, our framework matches the\ndiagnostic performance of existing deep learning methods while offering\nrationales that support its diagnostic conclusions."}
{"id": "2505.19789", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.19789", "abs": "https://arxiv.org/abs/2505.19789", "authors": ["Jijia Liu", "Feng Gao", "Bingwen Wei", "Xinlei Chen", "Qingmin Liao", "Yi Wu", "Chao Yu", "Yu Wang"], "title": "What Can RL Bring to VLA Generalization? An Empirical Study", "categories": ["cs.LG"], "comment": null, "summary": "Large Vision-Language Action (VLA) models have shown significant potential\nfor embodied AI. However, their predominant training via supervised fine-tuning\n(SFT) limits generalization due to susceptibility to compounding errors under\ndistribution shifts. Reinforcement learning (RL) offers a path to overcome\nthese limitations by optimizing for task objectives via trial-and-error, yet a\nsystematic understanding of its specific generalization benefits for VLAs\ncompared to SFT is lacking. To address this, our study introduces a\ncomprehensive benchmark for evaluating VLA generalization and systematically\ninvestigates the impact of RL fine-tuning across diverse visual, semantic, and\nexecution dimensions. Our extensive experiments reveal that RL fine-tuning,\nparticularly with PPO, significantly enhances generalization in semantic\nunderstanding and execution robustness over SFT, while maintaining comparable\nvisual robustness. We identify PPO as a more effective RL algorithm for VLAs\nthan LLM-derived methods like DPO and GRPO. We also develop a simple recipe for\nefficient PPO training on VLAs, and demonstrate its practical utility for\nimproving VLA generalization. The project page is at https://rlvla.github.io"}
{"id": "2505.19714", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.19714", "abs": "https://arxiv.org/abs/2505.19714", "authors": ["Zhaopeng Feng", "Yupu Liang", "Shaosheng Cao", "Jiayuan Su", "Jiahan Ren", "Zhe Xu", "Yao Hu", "Wenxuan Huang", "Jian Wu", "Zuozhu Liu"], "title": "MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress", "summary": "Text Image Machine Translation (TIMT)-the task of translating textual content\nembedded in images-is critical for applications in accessibility, cross-lingual\ninformation access, and real-world document understanding. However, TIMT\nremains a complex challenge due to the need for accurate optical character\nrecognition (OCR), robust visual-text reasoning, and high-quality translation,\noften requiring cascading multi-stage pipelines. Recent advances in large-scale\nReinforcement Learning (RL) have improved reasoning in Large Language Models\n(LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is\nstill underexplored. To bridge this gap, we introduce MT$^{3}$, the first\nframework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts\na multi-task optimization paradigm targeting three key sub-skills: text\nrecognition, context-aware reasoning, and translation. It is trained using a\nnovel multi-mixed reward mechanism that adapts rule-based RL strategies to\nTIMT's intricacies, offering fine-grained, non-binary feedback across tasks.\nFurthermore, to facilitate the evaluation of TIMT in authentic cross-cultural\nand real-world social media contexts, we introduced XHSPost, the first social\nmedia TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on\nthe latest in-domain MIT-10M benchmark, outperforming strong baselines such as\nQwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics.\nAdditionally, the model shows strong generalization to out-of-distribution\nlanguage pairs and datasets. In-depth analyses reveal how multi-task synergy,\nreinforcement learning initialization, curriculum design, and reward\nformulation contribute to advancing MLLM-driven TIMT."}
{"id": "2505.19590", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.19590", "abs": "https://arxiv.org/abs/2505.19590", "authors": ["Xuandong Zhao", "Zhewei Kang", "Aosong Feng", "Sergey Levine", "Dawn Song"], "title": "Learning to Reason without External Rewards", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) for complex reasoning via Reinforcement\nLearning with Verifiable Rewards (RLVR) is effective but limited by reliance on\ncostly, domain-specific supervision. We explore Reinforcement Learning from\nInternal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic\nsignals without external rewards or labeled data. We propose Intuitor, an RLIF\nmethod that uses a model's own confidence, termed self-certainty, as its sole\nreward signal. Intuitor replaces external rewards in Group Relative Policy\nOptimization (GRPO) with self-certainty scores, enabling fully unsupervised\nlearning. Experiments demonstrate that Intuitor matches GRPO's performance on\nmathematical benchmarks while achieving superior generalization to\nout-of-domain tasks like code generation, without requiring gold solutions or\ntest cases. Our findings show that intrinsic model signals can drive effective\nlearning across domains, offering a scalable alternative to RLVR for autonomous\nAI systems where verifiable rewards are unavailable. Code is available at\nhttps://github.com/sunblaze-ucb/Intuitor"}
{"id": "2505.19486", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.19486", "abs": "https://arxiv.org/abs/2505.19486", "authors": ["Maonan Wang", "Yirong Chen", "Aoyu Pang", "Yuxin Cai", "Chung Shue Chen", "Yuheng Kan", "Man-On Pun"], "title": "VLMLight: Traffic Signal Control via Vision-Language Meta-Control and Dual-Branch Reasoning", "categories": ["eess.SY", "cs.LG", "cs.MA", "cs.SY"], "comment": "25 pages, 15 figures", "summary": "Traffic signal control (TSC) is a core challenge in urban mobility, where\nreal-time decisions must balance efficiency and safety. Existing methods -\nranging from rule-based heuristics to reinforcement learning (RL) - often\nstruggle to generalize to complex, dynamic, and safety-critical scenarios. We\nintroduce VLMLight, a novel TSC framework that integrates vision-language\nmeta-control with dual-branch reasoning. At the core of VLMLight is the first\nimage-based traffic simulator that enables multi-view visual perception at\nintersections, allowing policies to reason over rich cues such as vehicle type,\nmotion, and spatial density. A large language model (LLM) serves as a\nsafety-prioritized meta-controller, selecting between a fast RL policy for\nroutine traffic and a structured reasoning branch for critical cases. In the\nlatter, multiple LLM agents collaborate to assess traffic phases, prioritize\nemergency vehicles, and verify rule compliance. Experiments show that VLMLight\nreduces waiting times for emergency vehicles by up to 65% over RL-only systems,\nwhile preserving real-time performance in standard conditions with less than 1%\ndegradation. VLMLight offers a scalable, interpretable, and safety-aware\nsolution for next-generation traffic signal control."}
{"id": "2505.19255", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.19255", "abs": "https://arxiv.org/abs/2505.19255", "authors": ["Mingyuan Wu", "Jingcheng Yang", "Jize Jiang", "Meitang Li", "Kaizhuo Yan", "Hanchao Yu", "Minjia Zhang", "Chengxiang Zhai", "Klara Nahrstedt"], "title": "VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement Learning Finetuning (RFT) has significantly advanced the\nreasoning capabilities of large language models (LLMs) by enabling long chains\nof thought, self-correction, and effective tool use. While recent works attempt\nto extend RFT to vision-language models (VLMs), these efforts largely produce\ntext-only reasoning conditioned on static image inputs, falling short of true\nmultimodal reasoning in the response. In contrast, test-time methods like\nVisual Sketchpad incorporate visual steps but lack training mechanisms.\n  We introduce VTool-R1, the first framework that trains VLMs to generate\nmultimodal chains of thought by interleaving text and intermediate visual\nreasoning steps. VTool-R1 integrates Python-based visual editing tools into the\nRFT process, enabling VLMs to learn when and how to generate visual reasoning\nsteps that benefit final reasoning. Trained with outcome-based rewards tied to\ntask accuracy, our approach elicits strategic visual tool use for reasoning\nwithout relying on process-based supervision. Experiments on structured visual\nquestion answering over charts and tables show that VTool-R1 enhances reasoning\nperformance by teaching VLMs to \"think with images\" and generate multimodal\nchain of thoughts with tools."}
{"id": "2505.18979", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.18979", "abs": "https://arxiv.org/abs/2505.18979", "authors": ["Zixuan Chen", "Hao Lin", "Ke Xu", "Xinghao Jiang", "Tanfeng Sun"], "title": "GhostPrompt: Jailbreaking Text-to-image Generative Models based on Dynamic Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Text-to-image (T2I) generation models can inadvertently produce\nnot-safe-for-work (NSFW) content, prompting the integration of text and image\nsafety filters. Recent advances employ large language models (LLMs) for\nsemantic-level detection, rendering traditional token-level perturbation\nattacks largely ineffective. However, our evaluation shows that existing\njailbreak methods are ineffective against these modern filters. We introduce\nGhostPrompt, the first automated jailbreak framework that combines dynamic\nprompt optimization with multimodal feedback. It consists of two key\ncomponents: (i) Dynamic Optimization, an iterative process that guides a large\nlanguage model (LLM) using feedback from text safety filters and CLIP\nsimilarity scores to generate semantically aligned adversarial prompts; and\n(ii) Adaptive Safety Indicator Injection, which formulates the injection of\nbenign visual cues as a reinforcement learning problem to bypass image-level\nfilters. GhostPrompt achieves state-of-the-art performance, increasing the\nShieldLM-7B bypass rate from 12.5\\% (Sneakyprompt) to 99.0\\%, improving CLIP\nscore from 0.2637 to 0.2762, and reducing the time cost by $4.2 \\times$.\nMoreover, it generalizes to unseen filters including GPT-4.1 and successfully\njailbreaks DALLE 3 to generate NSFW images in our evaluation, revealing\nsystemic vulnerabilities in current multimodal defenses. To support further\nresearch on AI safety and red-teaming, we will release code and adversarial\nprompts under a controlled-access protocol."}
{"id": "2505.18830", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.18830", "abs": "https://arxiv.org/abs/2505.18830", "authors": ["Wenlong Deng", "Yi Ren", "Muchen Li", "Danica J. Sutherland", "Xiaoxiao Li", "Christos Thrampoulidis"], "title": "On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has become popular in enhancing the reasoning\ncapabilities of large language models (LLMs), with Group Relative Policy\nOptimization (GRPO) emerging as a widely used algorithm in recent systems.\nDespite GRPO's widespread adoption, we identify a previously unrecognized\nphenomenon we term Lazy Likelihood Displacement (LLD), wherein the likelihood\nof correct responses marginally increases or even decreases during training.\nThis behavior mirrors a recently discovered misalignment issue in Direct\nPreference Optimization (DPO), attributed to the influence of negative\ngradients. We provide a theoretical analysis of GRPO's learning dynamic,\nidentifying the source of LLD as the naive penalization of all tokens in\nincorrect responses with the same strength. To address this, we develop a\nmethod called NTHR, which downweights penalties on tokens contributing to the\nLLD. Unlike prior DPO-based approaches, NTHR takes advantage of GRPO's\ngroup-based structure, using correct responses as anchors to identify\ninfluential tokens. Experiments on math reasoning benchmarks demonstrate that\nNTHR effectively mitigates LLD, yielding consistent performance gains across\nmodels ranging from 0.5B to 3B parameters."}
{"id": "2505.18656", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.18656", "abs": "https://arxiv.org/abs/2505.18656", "authors": ["Dev Gurung", "Shiva Raj Pokhrel"], "title": "LLM-QFL: Distilling Large Language Model for Quantum Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Inspired by the power of large language models (LLMs), our research adapts\nthem to quantum federated learning (QFL) to boost efficiency and performance.\nWe propose a federated fine-tuning method that distills an LLM within QFL,\nallowing each client to locally adapt the model to its own data while\npreserving privacy and reducing unnecessary global updates. The fine-tuned LLM\nalso acts as a reinforcement agent, optimizing QFL by adjusting optimizer\nsteps, cutting down communication rounds, and intelligently selecting clients.\nExperiments show significant efficiency gains. We pioneer a synergy between LLM\nand QFL, offering: i) practical efficiency: Reduced communication costs and\nfaster convergence. ii) theoretical rigor: Provable guarantees for adaptive\nfederated optimization. iii) scalability: PEFT methods (LoRA, QLoRA) enable\ndeployment on resource-constrained quantum devices. Code implementation is\navailable here 1."}
