<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.CL](#cs.CL) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: 本文对人类与AI智能体协作的最新实证发展进行了全面调查，提出了一个新型概念架构：分层探索-利用网络（Hierarchical Exploration-Exploitation Net），以系统地整合多智能体协调、知识管理、控制机制等技术细节，并推动人类认知和AI能力的共同进化。


<details>
  <summary>Details</summary>
Motivation: 目前在人类-AI协作领域缺乏一个统一的理论框架，特别是在处理开放性复杂任务时，技术成就与持续存在的差距需要被系统性解决。

Method: 通过提出一种新的概念架构——分层探索-利用网络（Hierarchical Exploration-Exploitation Net），将现有的多种贡献映射到该框架中，包括符号AI技术、基于LLM的连接主义智能体以及混合组织实践。此方法有助于修订传统方法并激发融合定性和定量范式的新工作。

Result: 该框架能够系统地整合不同研究，促进对遗留方法的改进，并为未来设计或扩展人类-AI共生关系提供了参考。

Conclusion: 这些见解为更深入的人类认知和AI能力共进化提供了基础，文章既是对现有技术实现的批判性回顾，也是对未来工作的展望。

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [2] [Automated Meta Prompt Engineering for Alignment with the Theory of Mind](https://arxiv.org/abs/2505.09024)
*Aaron Baughman,Rahul Agarwal,Eduardo Morales,Gozde Akay*

Main category: cs.AI

TL;DR: 本研究介绍了一种元提示方法，通过联合生成流畅文本并优化人类心理预期与大型语言模型（LLM）神经处理之间的相似性，解决心智理论（ToM）对齐问题。实验表明，在美国公开赛2024网球大满贯赛事中，AI生成的内容与人类评审的期望有53.8%的时间完全一致，平均迭代次数为4.38次。该技术已扩展应用于体育和娱乐领域的其他实时活动。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在生成复杂任务文本时，难以完全匹配人类的心理预期。为了提高AI生成内容的质量并更好地对齐人类心智模型，需要一种新方法来优化这一过程。

Method: 研究使用了代理强化学习技术，其中作为'评判者'的LLM（LLMaaJ）通过上下文学习指导另一个LLM生成内容，并解释有意和无意生成的文本特征。同时，通过用户在美国公开赛2024期间对AI生成文章的修改，测量人类对于内容生成的心理信念。LLMaaJ通过预测和包含人类编辑建议来解决心智理论对齐问题。

Result: 实验结果表明，在实际生产系统中，人类内容评审者的期望有53.8%的时间与AI完全对齐，平均需要迭代4.38次。此外，通过结合希尔伯特向量空间中的内容特征（如事实性、新颖性、重复性和相关性），LLMaaJ能够优化人类心智模型的对齐度，从而提升内容质量。

Conclusion: 通过元提示方法和代理强化学习技术，可以显著提高AI生成内容的质量，并使其更符合人类的心理预期。该方法不仅适用于网球赛事报道，还可在体育和娱乐领域的其他实时活动中推广使用。

Abstract: We introduce a method of meta-prompting that jointly produces fluent text for
complex tasks while optimizing the similarity of neural states between a
human's mental expectation and a Large Language Model's (LLM) neural
processing. A technique of agentic reinforcement learning is applied, in which
an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,
how to produce content by interpreting the intended and unintended generated
text traits. To measure human mental beliefs around content production, users
modify long form AI-generated text articles before publication at the US Open
2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)
alignment problem by anticipating and including human edits within the creation
of text from an LLM. Throughout experimentation and by interpreting the results
of a live production system, the expectations of human content reviewers had
100% of alignment with AI 53.8% of the time with an average iteration count of
4.38. The geometric interpretation of content traits such as factualness,
novelty, repetitiveness, and relevancy over a Hilbert vector space combines
spatial volume (all trait importance) with vertices alignment (individual trait
relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an
increase in content quality by extending the coverage of tennis action. Our
work that was deployed at the US Open 2024 has been used across other live
events within sports and entertainment.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs](https://arxiv.org/abs/2505.10425)
*Jingyao Wang,Wenwen Qiang,Zeen Song,Changwen Zheng,Hui Xiong*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）在复杂任务中表现出色，但现有方法往往忽略了推理效果与计算效率之间的权衡。为了解决这个问题，本文提出了学习思考（L2T），这是一种基于信息论的强化微调框架，使LLMs能够在使用更少token的情况下实现最优推理。L2T通过量化每个阶段的信息增益并奖励有效更新，同时惩罚过度更新，从而优化模型的推理效率和效果。实验结果表明，L2T在各种推理基准和基础模型上具有优势，提升了推理的效果和效率。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM推理方法存在推理链过长、计算资源浪费的问题，因此需要一种方法来平衡推理效果与计算效率。

Method: 提出了一种名为Learning to Think (L2T) 的信息理论强化微调框架，将每次查询-响应交互视为多个阶段的分层会话，并引入了普遍密集过程奖励以量化各阶段的信息增益。该方法不需要额外注释或特定任务评估器，通过基于PAC-Bayes界限和Fisher信息矩阵快速估计奖励，显著降低计算复杂度。

Result: 理论分析显示，该方法能大幅减少计算复杂度，同时保持高估测准确性。实验证明，L2T在不同任务和模型上的推理效果和效率都有所提升。

Conclusion: L2T框架能够有效提高LLM的推理效率和效果，在减少token使用的同时达到更好的性能。

Abstract: Large language models (LLMs) excel at complex tasks thanks to advances in
reasoning abilities. However, existing methods overlook the trade-off between
reasoning effectiveness and computational efficiency, often encouraging
unnecessarily long reasoning chains and wasting tokens. To address this, we
propose Learning to Think (L2T), an information-theoretic reinforcement
fine-tuning framework for LLMs to make the models achieve optimal reasoning
with fewer tokens. Specifically, L2T treats each query-response interaction as
a hierarchical session of multiple episodes and proposes a universal dense
process reward, i.e., quantifies the episode-wise information gain in
parameters, requiring no extra annotations or task-specific evaluators. We
propose a method to quickly estimate this reward based on PAC-Bayes bounds and
the Fisher information matrix. Theoretical analyses show that it significantly
reduces computational complexity with high estimation accuracy. By immediately
rewarding each episode's contribution and penalizing excessive updates, L2T
optimizes the model via reinforcement learning to maximize the use of each
episode and achieve effective updates. Empirical results on various reasoning
benchmarks and base models demonstrate the advantage of L2T across different
tasks, boosting both reasoning effectiveness and efficiency.

</details>


### [4] [ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts](https://arxiv.org/abs/2505.10010)
*Jing-Cheng Pang,Kaiyuan Li,Yidi Wang,Si-Hang Yang,Shengyi Jiang,Yang Yu*

Main category: cs.LG

TL;DR: 文章介绍了一个新的基准测试ImagineBench，用于评估结合真实和想象轨迹的离线强化学习算法。通过评估现有算法，发现其在未见任务上的表现不佳，并指出了未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: 强化学习依赖大量真实世界交互数据，而大型语言模型可以生成合成经验（想象轨迹）来克服这一限制。但由于缺乏标准基准测试，该领域的进展受到阻碍。

Method: 提出ImagineBench，包含环境收集和LLM想象轨迹的数据集、多样化的环境领域以及自然语言任务指令。用于系统评估离线强化学习算法。

Result: 现有离线RL算法在未见任务上表现次优，在困难任务中成功率为35.44%，远低于基于真实轨迹训练的方法（64.37%）。

Conclusion: 需要算法改进以更好地利用想象轨迹，同时提出了包括快速在线适应、持续学习和多模态任务扩展在内的未来研究方向。

Abstract: A central challenge in reinforcement learning (RL) is its dependence on
extensive real-world interaction data to learn task-specific policies. While
recent work demonstrates that large language models (LLMs) can mitigate this
limitation by generating synthetic experience (noted as imaginary rollouts) for
mastering novel tasks, progress in this emerging field is hindered due to the
lack of a standard benchmark. To bridge this gap, we introduce ImagineBench,
the first comprehensive benchmark for evaluating offline RL algorithms that
leverage both real rollouts and LLM-imaginary rollouts. The key features of
ImagineBench include: (1) datasets comprising environment-collected and
LLM-imaginary rollouts; (2) diverse domains of environments covering
locomotion, robotic manipulation, and navigation tasks; and (3) natural
language task instructions with varying complexity levels to facilitate
language-conditioned policy learning. Through systematic evaluation of
state-of-the-art offline RL algorithms, we observe that simply applying
existing offline RL algorithms leads to suboptimal performance on unseen tasks,
achieving 35.44% success rate in hard tasks in contrast to 64.37% of method
training on real rollouts for hard tasks. This result highlights the need for
algorithm advancements to better leverage LLM-imaginary rollouts. Additionally,
we identify key opportunities for future research: including better utilization
of imaginary rollouts, fast online adaptation and continual learning, and
extension to multi-modal tasks. Our code is publicly available at
https://github.com/LAMDA-RL/ImagineBench.

</details>


### [5] [Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback](https://arxiv.org/abs/2505.09925)
*Yutao Yang,Jie Zhou,Junsong Li,Qianjun Pan,Bihao Zhan,Qin Chen,Xipeng Qiu,Liang He*

Main category: cs.LG

TL;DR: 本文介绍了一种交互式的持续学习范式，其中AI模型通过实时的人类反馈动态地学习新技能，同时保留先前的知识。为了解决传统持续学习的两大主要限制，提出了RiCL框架，该框架利用大型语言模型从动态反馈中有效学习新技能，并在两个基准数据集上表现出显著优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前持续学习方法主要依赖静态数据集和固定标签，且假设标签清洁，这与现实世界中的动态、嘈杂反馈不符，因此需要一种新的范式来解决这些问题。

Method: 提出RiCL框架，包含三个关键组件：时间一致性感知净化器、交互感知直接偏好优化策略和抗噪对比学习模块，以分别处理噪声样本、对齐模型行为与人类意图以及捕获鲁棒表示。

Result: 在FewRel和TACRED两个基准数据集上的广泛实验表明，RiCL方法在处理现实噪声模式时显著优于现有的在线持续学习和噪声标签学习方法组合。

Conclusion: RiCL框架成功解决了传统持续学习中的关键问题，展示了在动态、嘈杂反馈环境下学习新技能的有效性，为未来的研究提供了新的方向。

Abstract: This paper introduces an interactive continual learning paradigm where AI
models dynamically learn new skills from real-time human feedback while
retaining prior knowledge. This paradigm distinctively addresses two major
limitations of traditional continual learning: (1) dynamic model updates using
streaming, real-time human-annotated data, rather than static datasets with
fixed labels, and (2) the assumption of clean labels, by explicitly handling
the noisy feedback common in real-world interactions. To tackle these problems,
we propose RiCL, a Reinforced interactive Continual Learning framework
leveraging Large Language Models (LLMs) to learn new skills effectively from
dynamic feedback. RiCL incorporates three key components: a temporal
consistency-aware purifier to automatically discern clean from noisy samples in
data streams; an interaction-aware direct preference optimization strategy to
align model behavior with human intent by reconciling AI-generated and
human-provided feedback; and a noise-resistant contrastive learning module that
captures robust representations by exploiting inherent data relationships, thus
avoiding reliance on potentially unreliable labels. Extensive experiments on
two benchmark datasets (FewRel and TACRED), contaminated with realistic noise
patterns, demonstrate that our RiCL approach substantially outperforms existing
combinations of state-of-the-art online continual learning and noisy-label
learning methods.

</details>


### [6] [Adversarial Attack on Large Language Models using Exponentiated Gradient Descent](https://arxiv.org/abs/2505.09820)
*Sajib Biswas,Mao Nishino,Samuel Jacob Chacko,Xiuwen Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于指数梯度下降和Bregman投影方法的内在优化技术，用于有效攻击大型语言模型（LLMs）的越狱问题，并在五个开源LLM和四个公开数据集上验证了其高效性和高成功率。


<details>
  <summary>Details</summary>
Motivation: 尽管许多LLM通过如人类反馈强化学习（RLHF）等技术进行对齐，但它们仍然容易受到越狱攻击。现有的对抗攻击方法要么在离散空间中搜索可能引发越狱的token，效率较低；要么在连续空间中优化token嵌入，需要投影回离散空间，可能导致效果不佳。

Method: 作者开发了一种内在优化技术，使用指数梯度下降和Bregman投影方法来确保优化的一次编码始终保持在概率单纯形内。这种方法充分利用了空间的约束和结构，避免了传统方法中的不足。

Result: 该技术在越狱攻击中表现出更高的成功率和效率，相较于其他三种最先进的越狱技术有显著优势。实验在五个开源LLM和四个公开数据集上进行了验证。

Conclusion: 提出的指数梯度下降优化技术为LLM的越狱攻击提供了一种新的、高效的解决方案，具有理论收敛性保证，同时源代码已开源供进一步研究。

Abstract: As Large Language Models (LLMs) are widely used, understanding them
systematically is key to improving their safety and realizing their full
potential. Although many models are aligned using techniques such as
reinforcement learning from human feedback (RLHF), they are still vulnerable to
jailbreaking attacks. Some of the existing adversarial attack methods search
for discrete tokens that may jailbreak a target model while others try to
optimize the continuous space represented by the tokens of the model's
vocabulary. While techniques based on the discrete space may prove to be
inefficient, optimization of continuous token embeddings requires projections
to produce discrete tokens, which might render them ineffective. To fully
utilize the constraints and the structures of the space, we develop an
intrinsic optimization technique using exponentiated gradient descent with the
Bregman projection method to ensure that the optimized one-hot encoding always
stays within the probability simplex. We prove the convergence of the technique
and implement an efficient algorithm that is effective in jailbreaking several
widely used LLMs. We demonstrate the efficacy of the proposed technique using
five open-source LLMs on four openly available datasets. The results show that
the technique achieves a higher success rate with great efficiency compared to
three other state-of-the-art jailbreaking techniques. The source code for our
implementation is available at:
https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack

</details>


### [7] [Self Rewarding Self Improving](https://arxiv.org/abs/2505.08827)
*Toby Simonds,Kevin Lopez,Akira Yoshiyama,Dominique Garmier*

Main category: cs.LG

TL;DR: 研究展示了大语言模型通过自我评判实现有效自我改进的能力，无需参考答案。通过在 Countdown 谜题和 MIT 积分蜂问题上的实验，表明模型可以在没有真实答案的情况下提供可靠的奖励信号，并结合合成问题生成建立完整的自我改进循环，实现了性能的显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型训练通常依赖于人类指导或参考答案，但在某些领域（如数据稀缺或评估复杂的情况）中，这种方式受到限制。因此，探索一种无需外部参考答案、仅依靠模型自身能力进行自我改进的方法具有重要意义。

Method: 利用生成与验证解决方案之间的固有不对称性，让模型通过自我评判提供奖励信号，结合强化学习优化性能。同时引入合成问题生成技术，形成一个完整的自我改进循环：生成练习问题、解决问题并评估自身表现。

Result: 在 Countdown 谜题和 MIT 积分蜂问题上取得了显著的性能提升，例如 Qwen 2.5 7B 在积分任务上超越了 GPT-4o 的表现，并实现了 8% 的改进。

Conclusion: 该研究表明，大型语言模型可以通过自我评判提供有效的奖励信号，推动模型在缺乏程序化奖励的环境中实现持续改进。这可能引发 AI 系统从人类引导训练向自我导向学习范式的转变，加速数据稀缺或评估复杂的领域的进步。

Abstract: We demonstrate that large language models can effectively self-improve
through self-judging without requiring reference solutions, leveraging the
inherent asymmetry between generating and verifying solutions. Our experiments
on Countdown puzzles and MIT Integration Bee problems show that models can
provide reliable reward signals without ground truth answers, enabling
reinforcement learning in domains previously not possible. By implementing
self-judging, we achieve significant performance gains maintaining alignment
with formal verification. When combined with synthetic question generation, we
establish a complete self-improvement loop where models generate practice
problems, solve them, and evaluate their own performance-achieving an 8%
improvement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on
integration tasks. Our findings demonstrate that LLM judges can provide
effective reward signals for training models, unlocking many reinforcement
learning environments previously limited by the difficulty of creating
programmatic rewards. This suggests a potential paradigm shift toward AI
systems that continuously improve through self-directed learning rather than
human-guided training, potentially accelerating progress in domains with scarce
training data or complex evaluation requirements.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [8] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 在大规模语言模型（LLMs）中，通过重复采样来扩展推理计算，可以随着样本数量的增加而持续提高覆盖度（解决问题的比例）。实验表明，这种提升部分是因为评估基准的答案分布偏向少量常见答案。研究提出了一种基于训练集答案频率的基线方法，在数学推理和事实知识两个领域中，该方法在某些LLMs上优于重复采样，而在其他模型上与混合策略相当。此基线有助于更精确地衡量重复采样在提示无关猜测之外的覆盖度提升。


<details>
  <summary>Details</summary>
Motivation: 作者观察到在大语言模型中，通过增加重复采样的次数能够持续提高问题解决的覆盖度，并推测这一现象可能与标准评估基准中答案分布的偏斜有关，即答案集中于一小部分常见答案。为了验证这一假设，他们设计了一个基于训练集中答案频率的基线方法进行对比分析。

Method: 研究者定义了一个基线方法，该方法根据训练集中答案的出现频率枚举答案。然后，他们在数学推理和事实知识两个领域进行了实验，比较了该基线方法、重复模型采样以及一种混合策略的表现，其中混合策略结合了少量模型采样和枚举猜测。

Result: 实验结果表明，在某些大语言模型中，基于训练集答案频率的基线方法表现优于重复采样；而在其他模型中，其覆盖度与混合策略相当，即使用少量模型采样并结合枚举猜测可以达到类似的效果。

Conclusion: 研究证实了重复采样覆盖度的提升部分源于评估基准答案分布的偏斜。提出的基线方法为更准确地测量重复采样在提示无关猜测之外的实际效果提供了一种有效手段。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [9] [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320)
*Chenxi Whitehouse,Tianlu Wang,Ping Yu,Xian Li,Jason Weston,Ilia Kulikov,Swarnadeep Saha*

Main category: cs.CL

TL;DR: 本文提出了一种名为J1的强化学习方法，用于训练能够进行深度推理判断的AI模型。该方法通过可验证奖励机制将各种提示转化为判断任务，从而提升模型推理和判断能力，并在多个基准测试中超越了其他同规模或更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 当前AI的发展受限于评估质量，而强大的LLM-as-a-Judge模型被认为是核心解决方案。为了提高模型的判断能力，需要探索最佳训练策略以增强其链式思维推理能力。

Method: 引入了J1方法，一种基于强化学习的模型训练方式。此方法能将可验证与不可验证的提示转化为带有可验证奖励的判断任务，从而激励模型思考并减少判断偏差。

Result: J1方法在8B和70B规模的模型上表现优于其他现有模型，包括从DeepSeek-R1蒸馏出的模型。即使在较小规模的模型上，J1也超过了o1-mini和R1等模型。此外，作者通过多种实验比较了不同训练策略的效果。

Conclusion: J1方法显著提高了模型的判断能力，主要通过学习制定评估标准、对比自动生成的参考答案以及重新评估模型响应的正确性来实现。这表明强化学习是一种有效的方法来训练具备更好推理和判断能力的AI模型。

Abstract: The progress of AI is bottlenecked by the quality of evaluation, and powerful
LLM-as-a-Judge models have proved to be a core solution. Improved judgment
ability is enabled by stronger chain-of-thought reasoning, motivating the need
to find the best recipes for training such models to think. In this work we
introduce J1, a reinforcement learning approach to training such models. Our
method converts both verifiable and non-verifiable prompts to judgment tasks
with verifiable rewards that incentivize thinking and mitigate judgment bias.
In particular, our approach outperforms all other existing 8B or 70B models
when trained at those sizes, including models distilled from DeepSeek-R1. J1
also outperforms o1-mini, and even R1 on some benchmarks, despite training a
smaller model. We provide analysis and ablations comparing Pairwise-J1 vs
Pointwise-J1 models, offline vs online training recipes, reward strategies,
seed prompts, and variations in thought length and content. We find that our
models make better judgments by learning to outline evaluation criteria,
comparing against self-generated reference answers, and re-evaluating the
correctness of model responses.

</details>


### [10] [Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning](https://arxiv.org/abs/2505.10182)
*Yoichi Ishibashi,Taro Yano,Masafumi Oyamada*

Main category: cs.CL

TL;DR: 通过使用合成数据进行持续预训练（Reasoning CPT），可以有效提升大语言模型在多个领域内的推理能力，尤其是在难题上的表现。此外，跨领域的推理技能迁移效果显著，模型能够根据问题难度调整推理深度。


<details>
  <summary>Details</summary>
Motivation: 当前通过监督微调和强化学习提升大语言模型推理能力的方法受限于特定领域（如数学和编程），且缺乏对如何有效生成推理训练数据及其对多领域影响的深入研究。

Method: 采用持续预训练（CPT）方法，利用从STEM和法律语料中提取的隐藏思维过程生成合成数据，对Gemma2-9B模型进行训练，并与标准CPT方法在MMLU基准上进行比较。

Result: Reasoning CPT在所有评估领域内均提升了模型性能，特别是在最难的问题上获得了高达8分的提升；并且在一个领域内习得的推理技能能有效迁移到其他领域。

Conclusion: Reasoning CPT是一种有效的提升大语言模型推理能力的方法，其优势在于能够跨领域迁移推理技能并根据问题难度调整推理深度。

Abstract: Large Language Models (LLMs) have demonstrated significant improvements in
reasoning capabilities through supervised fine-tuning and reinforcement
learning. However, when training reasoning models, these approaches are
primarily applicable to specific domains such as mathematics and programming,
which imposes fundamental constraints on the breadth and scalability of
training data. In contrast, continual pretraining (CPT) offers the advantage of
not requiring task-specific signals. Nevertheless, how to effectively
synthesize training data for reasoning and how such data affect a wide range of
domains remain largely unexplored. This study provides a detailed evaluation of
Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden
thought processes underlying texts, based on the premise that texts are the
result of the author's thinking process. Specifically, we apply Reasoning CPT
to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and
Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis
reveals that Reasoning CPT consistently improves performance across all
evaluated domains. Notably, reasoning skills acquired in one domain transfer
effectively to others; the performance gap with conventional methods widens as
problem difficulty increases, with gains of up to 8 points on the most
challenging problems. Furthermore, models trained with hidden thoughts learn to
adjust the depth of their reasoning according to problem difficulty.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [11] [Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting](https://arxiv.org/abs/2505.09395)
*Chen-Yu Liu,Kuan-Cheng Chen,Yi-Chien Chen,Samuel Yen-Chi Chen,Wei-Hao Huang,Wei-Jia Huang,Yen-Jui Chang*

Main category: quant-ph

TL;DR: 量子参数适应（QPA）首次将量子机器学习应用于大规模台风轨迹预测，结合Attention-based Multi-ConvGRU模型，显著减少可训练参数数量，同时保持预测准确性，提供了一种可扩展且节能的气候建模方法。


<details>
  <summary>Details</summary>
Motivation: 台风轨迹预测对于灾害准备至关重要，但由于大气动力学的复杂性和深度学习模型对资源的需求，计算成本仍然很高。本研究旨在利用量子机器学习提高台风预测模型的效率和可持续性。

Method: 通过引入Quantum Parameter Adaptation（QPA），与Attention-based Multi-ConvGRU模型集成，仅在训练期间使用量子神经网络生成可训练参数，推理时无需量子硬件，从而实现高效参数训练并保持预测准确性。

Result: QPA显著减少了可训练参数的数量，同时保留了预测性能，使得高性能预报更加便捷和可持续。

Conclusion: 这是量子机器学习首次应用于大规模台风轨迹预测，展示了混合量子-经典学习方法在气候建模中的可扩展性和能源效率优势。

Abstract: Typhoon trajectory forecasting is essential for disaster preparedness but
remains computationally demanding due to the complexity of atmospheric dynamics
and the resource requirements of deep learning models. Quantum-Train (QT), a
hybrid quantum-classical framework that leverages quantum neural networks
(QNNs) to generate trainable parameters exclusively during training,
eliminating the need for quantum hardware at inference time. Building on QT's
success across multiple domains, including image classification, reinforcement
learning, flood prediction, and large language model (LLM) fine-tuning, we
introduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting
model learning. Integrated with an Attention-based Multi-ConvGRU model, QPA
enables parameter-efficient training while maintaining predictive accuracy.
This work represents the first application of quantum machine learning (QML) to
large-scale typhoon trajectory prediction, offering a scalable and
energy-efficient approach to climate modeling. Our results demonstrate that QPA
significantly reduces the number of trainable parameters while preserving
performance, making high-performance forecasting more accessible and
sustainable through hybrid quantum-classical learning.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [12] [Improved Algorithms for Differentially Private Language Model Alignment](https://arxiv.org/abs/2505.08849)
*Keyu Chen,Hao Tang,Qinglin Liu,Yizhao Xu*

Main category: cs.CR

TL;DR: 本文提出新的隐私保护对齐算法，结合DPO和RLHF，在适度隐私预算下性能优越，提升对齐质量最高达15%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的对齐需要敏感用户数据，存在隐私问题，而现有结合差分隐私（DP）的方法性能有限，因此需要更有效的隐私保护对齐方法。

Method: 提出新算法用于隐私保护对齐，并可在直接偏好优化（DPO）和基于人类反馈的强化学习（RLHF）两种技术上部署；通过系统实验验证其在大规模语言模型上的效果。

Result: 所提方法达到业界领先水平，其中DP-AdamW与DPO结合在中等隐私预算（ε=2-5）下对齐质量提高15%。此外，研究了隐私保证、对齐效果和计算需求之间的关系，提供优化权衡的实用指南。

Conclusion: 新提出的隐私保护对齐算法在不同隐私预算和模型下表现良好，为实际应用提供了有效工具和指导。

Abstract: Language model alignment is crucial for ensuring that large language models
(LLMs) align with human preferences, yet it often involves sensitive user data,
raising significant privacy concerns. While prior work has integrated
differential privacy (DP) with alignment techniques, their performance remains
limited. In this paper, we propose novel algorithms for privacy-preserving
alignment and rigorously analyze their effectiveness across varying privacy
budgets and models. Our framework can be deployed on two celebrated alignment
techniques, namely direct preference optimization (DPO) and reinforcement
learning from human feedback (RLHF). Through systematic experiments on
large-scale language models, we demonstrate that our approach achieves
state-of-the-art performance. Notably, one of our algorithms, DP-AdamW,
combined with DPO, surpasses existing methods, improving alignment quality by
up to 15% under moderate privacy budgets ({\epsilon}=2-5). We further
investigate the interplay between privacy guarantees, alignment efficacy, and
computational demands, providing practical guidelines for optimizing these
trade-offs.

</details>
