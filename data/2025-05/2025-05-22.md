<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: 本文是一篇立场论文，批判性地综述了人类与AI智能体协作的广泛经验发展，强调其技术成就和持续存在的差距。文章提出了一种新的概念架构（分层探索-利用网络），系统地将多智能体协调、知识管理、控制论反馈环路和高级控制机制相互关联。通过将现有贡献映射到该框架上，促进对传统方法的修订，并启发融合定性和定量范式的新研究。文章结构灵活，可从任何部分阅读，既是对技术实现的批判性回顾，也是设计或扩展人机共生关系的前瞻性参考。


<details>
  <summary>Details</summary>
Motivation: 当前关于人类与AI智能体协作的研究缺乏一个统一的理论框架，尤其是在处理开放性、复杂任务时，这使得各种研究难以系统整合。因此，需要一种能够综合不同研究方向和技术细节的新架构，以推动这一领域的深入发展。

Method: 文章提出了一个名为分层探索-利用网络（Hierarchical Exploration-Exploitation Net）的概念架构，该架构系统地整合了多智能体协调、知识管理、控制论反馈环路和高级控制机制等技术细节。通过将现有的符号AI技术、连接主义的大语言模型智能体以及混合组织实践映射到这一框架中，重新审视传统方法并激发新研究。

Result: 该方法为现有研究成果提供了一个统一的理论框架，促进了对传统方法的反思和改进，并激励了融合定性和定量范式的新研究方向。同时，文章还提供了对技术实现的批判性评价和对未来设计的指导。

Conclusion: 本文提出的分层探索-利用网络为人类与AI智能体协作研究提供了一个重要的理论基础，有助于更深层次的人类认知与AI能力的共同进化。文章不仅总结了当前的技术进展，还为未来的设计和扩展提供了有价值的参考。

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [2] [Self-Evolving Curriculum for LLM Reasoning](https://arxiv.org/abs/2505.14970)
*Xiaoyin Chen,Jiarui Lu,Minsu Kim,Dinghuai Zhang,Jian Tang,Alexandre Piché,Nicolas Gontier,Yoshua Bengio,Ehsan Kamalloo*

Main category: cs.AI

TL;DR: 提出了一种名为Self-Evolving Curriculum（SEC）的自动课程学习方法，该方法通过将课程选择建模为非平稳多臂老虎机问题，并结合强化学习微调过程来动态优化训练顺序。实验表明，SEC能显著提升大语言模型在规划、归纳推理和数学等领域的推理能力，并改善跨多个推理领域的技能平衡。


<details>
  <summary>Details</summary>
Motivation: 当前用于强化学习微调大语言模型的课程设计方法存在不足：随机课程效果不佳，手动设计依赖启发式规则，而在线过滤方法计算成本过高。因此需要一种更高效的自动课程生成方法以优化训练过程并提升模型性能。

Method: SEC将课程选择视为一个非平稳多臂老虎机问题，其中每个问题类别被视为一个独立的手臂。利用策略梯度方法中的绝对优势作为即时学习收益的代理指标，在每一步训练中，课程策略通过TD(0)方法更新，以选择能够最大化奖励信号的问题类别。

Result: 实验结果表明，SEC在三个不同的推理领域（规划、归纳推理和数学）上显著提高了模型的推理能力，促进了对更难的、分布外测试问题的泛化，并在同时微调多个推理领域时实现了更好的技能平衡。

Conclusion: Self-Evolving Curriculum作为一种自动课程学习方法，展示了其在强化学习微调大语言模型中的潜力，特别是在提升模型推理能力和跨领域技能平衡方面表现优异。

Abstract: Reinforcement learning (RL) has proven effective for fine-tuning large
language models (LLMs), significantly enhancing their reasoning abilities in
domains such as mathematics and code generation. A crucial factor influencing
RL fine-tuning success is the training curriculum: the order in which training
problems are presented. While random curricula serve as common baselines, they
remain suboptimal; manually designed curricula often rely heavily on
heuristics, and online filtering methods can be computationally prohibitive. To
address these limitations, we propose Self-Evolving Curriculum (SEC), an
automatic curriculum learning method that learns a curriculum policy
concurrently with the RL fine-tuning process. Our approach formulates
curriculum selection as a non-stationary Multi-Armed Bandit problem, treating
each problem category (e.g., difficulty level or problem type) as an individual
arm. We leverage the absolute advantage from policy gradient methods as a proxy
measure for immediate learning gain. At each training step, the curriculum
policy selects categories to maximize this reward signal and is updated using
the TD(0) method. Across three distinct reasoning domains: planning, inductive
reasoning, and mathematics, our experiments demonstrate that SEC significantly
improves models' reasoning capabilities, enabling better generalization to
harder, out-of-distribution test problems. Additionally, our approach achieves
better skill balance when fine-tuning simultaneously on multiple reasoning
domains. These findings highlight SEC as a promising strategy for RL
fine-tuning of LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning](https://arxiv.org/abs/2505.15311)
*Yurun Yuan,Fan Chen,Zeyu Jia,Alexander Rakhlin,Tengyang Xie*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于轨迹的贝尔曼残差最小化（TBRM）算法，用于大语言模型（LLMs）中的强化学习。该方法通过优化单个轨迹级别的贝尔曼目标，并使用模型自身的logits作为Q值，避免了对批评者、重要性采样比率或剪辑的需求。实验表明，TBRM在数学推理基准测试中优于基于策略的方法（如PPO和GRPO），并且计算和内存开销更低。


<details>
  <summary>Details</summary>
Motivation: 当前，策略优化方法主导了大语言模型（LLMs）推理中的强化学习（RL）流程，而基于价值的方法尚未被充分探索。因此，研究者重新审视了经典的贝尔曼残差最小化方法，并试图将其应用于LLMs以改进其推理能力。

Method: TBRM算法通过利用模型自身的logits作为Q值，优化了一个轨迹级别的贝尔曼目标函数。这种方法无需依赖批评者、重要性采样比率或剪辑，并且每个提示只需要一次模拟（rollout）。此外，作者还通过改进的轨迹度量变化分析证明了从任意离线数据收敛到接近最优的KL正则化策略的可能性。

Result: 在标准的数学推理基准测试中，TBRM算法的表现始终优于基于策略的基线方法（如PPO和GRPO），同时具有相当甚至更低的计算和内存开销。这表明基于价值的强化学习可能是增强LLMs推理能力的一种有原则且高效的替代方案。

Conclusion: 本文提出的TBRM算法为基于价值的强化学习在LLMs中的应用提供了一种新思路。通过简化算法设计并减少计算资源需求，TBRM展示了其在提升LLMs推理能力方面的潜力，为未来进一步研究基于价值的RL方法提供了方向。

Abstract: Policy-based methods currently dominate reinforcement learning (RL) pipelines
for large language model (LLM) reasoning, leaving value-based approaches
largely unexplored. We revisit the classical paradigm of Bellman Residual
Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an
algorithm that naturally adapts this idea to LLMs, yielding a simple yet
effective off-policy algorithm that optimizes a single trajectory-level Bellman
objective using the model's own logits as $Q$-values. TBRM removes the need for
critics, importance-sampling ratios, or clipping, and operates with only one
rollout per prompt. We prove convergence to the near-optimal KL-regularized
policy from arbitrary off-policy data via an improved
change-of-trajectory-measure analysis. Experiments on standard
mathematical-reasoning benchmarks show that TBRM consistently outperforms
policy-based baselines, like PPO and GRPO, with comparable or lower
computational and memory overhead. Our results indicate that value-based RL
might be a principled and efficient alternative for enhancing reasoning
capabilities in LLMs.

</details>


### [4] [Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One](https://arxiv.org/abs/2505.15306)
*Yiwen Song,Qianyue Hao,Qingmin Liao,Jian Yuan,Yong Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为LLM-Ens的新方法，通过大型语言模型（LLMs）增强强化学习中的模型集成技术。该方法能够根据任务的具体情况动态选择表现最佳的代理，从而显著提高整体性能。实验表明，LLM-Ens在Atari基准测试中超越了已知基线20.9%。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习取得了广泛的成功，但训练有效的代理仍然困难重重。这是因为诸如算法选择、超参数设置和随机种子选择等诸多因素都需要仔细调整，这些因素都会显著影响代理的性能。现有的集成方法如多数投票和Boltzmann加法是固定的策略，缺乏对特定任务的语义理解，这限制了它们的适应性和有效性。因此需要一种新的方法来克服这一挑战。

Method: 我们提出了LLM-Ens，这是一种通过大型语言模型（LLMs）增强RL模型集成的新方法。给定一个任务，首先设计一个LLM将任务中的状态分为不同的“情境”，并结合任务条件的高级描述。然后统计分析每个个体代理在每种情境下的优劣势。在推理阶段，LLM-Ens动态识别变化的任务情境，并切换到在当前情境下表现最佳的代理，确保动态模型选择。此方法兼容使用不同随机种子、超参数设置和各种RL算法训练的代理。

Result: 广泛的实验证明，在Atari基准上，LLM-Ens显著改进了RL模型集成，相较于知名基线提升了高达20.9%。此外，为了可重复性，作者开源了代码。

Conclusion: LLM-Ens是一种新颖且有效的方法，它通过利用大型语言模型增强了强化学习模型集成的性能。这种方法不仅能够动态适应任务情境的变化，还能够在不同训练条件下兼容多种代理，极大地提高了模型集成的效果。

Abstract: Model ensemble is a useful approach in reinforcement learning (RL) for
training effective agents. Despite wide success of RL, training effective
agents remains difficult due to the multitude of factors requiring careful
tuning, such as algorithm selection, hyperparameter settings, and even random
seed choices, all of which can significantly influence an agent's performance.
Model ensemble helps overcome this challenge by combining multiple weak agents
into a single, more powerful one, enhancing overall performance. However,
existing ensemble methods, such as majority voting and Boltzmann addition, are
designed as fixed strategies and lack a semantic understanding of specific
tasks, limiting their adaptability and effectiveness. To address this, we
propose LLM-Ens, a novel approach that enhances RL model ensemble with
task-specific semantic understandings driven by large language models (LLMs).
Given a task, we first design an LLM to categorize states in this task into
distinct 'situations', incorporating high-level descriptions of the task
conditions. Then, we statistically analyze the strengths and weaknesses of each
individual agent to be used in the ensemble in each situation. During the
inference time, LLM-Ens dynamically identifies the changing task situation and
switches to the agent that performs best in the current situation, ensuring
dynamic model selection in the evolving task condition. Our approach is
designed to be compatible with agents trained with different random seeds,
hyperparameter settings, and various RL algorithms. Extensive experiments on
the Atari benchmark show that LLM-Ens significantly improves the RL model
ensemble, surpassing well-known baselines by up to 20.9%. For reproducibility,
our code is open-source at
https://anonymous.4open.science/r/LLM4RLensemble-F7EE.

</details>


### [5] [LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models](https://arxiv.org/abs/2505.15293)
*Qianyue Hao,Yiwen Song,Qingmin Liao,Jian Yuan,Yong Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为LLM-Explorer的方法，利用大语言模型（LLMs）生成特定于任务的探索策略以增强强化学习中的策略探索。该方法通过采样代理的学习轨迹，提示LLM分析当前策略学习状态并生成未来策略探索的概率分布。此设计作为插件模块与多种广泛使用的RL算法兼容，并在Atari和MuJoCo基准测试中表现出平均性能提升37.27%。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习策略探索方法使用预设的随机过程，在所有类型的RL任务中不加区分地应用，未考虑影响策略探索的任务特定特征。此外，在RL训练期间，这些随机过程的演变是僵化的，通常仅包含方差衰减，无法根据代理的实际学习状态灵活调整。

Method: 作者设计了LLM-Explorer，它在给定任务的RL训练过程中采样代理的学习轨迹，并提示LLM分析代理当前的策略学习状态，然后为未来的策略探索生成概率分布。通过周期性更新概率分布，推导出专门针对特定任务且动态调整以适应学习过程的随机过程。此设计作为一个插件模块，与包括DQN系列、DDPG、TD3在内的多种广泛使用的RL算法兼容。

Result: 通过在Atari和MuJoCo基准上的广泛实验，证明了LLM-Explorer能够增强RL策略探索，实现了高达37.27%的平均性能提升。

Conclusion: LLM-Explorer通过结合大语言模型的能力，为特定任务生成自适应的探索策略，显著提高了强化学习的性能。这一方法作为插件模块，可以轻松集成到现有RL算法中，具有广泛的适用性和显著的性能改进效果。

Abstract: Policy exploration is critical in reinforcement learning (RL), where existing
approaches include greedy, Gaussian process, etc. However, these approaches
utilize preset stochastic processes and are indiscriminately applied in all
kinds of RL tasks without considering task-specific features that influence
policy exploration. Moreover, during RL training, the evolution of such
stochastic processes is rigid, which typically only incorporates a decay in the
variance, failing to adjust flexibly according to the agent's real-time
learning status. Inspired by the analyzing and reasoning capability of large
language models (LLMs), we design LLM-Explorer to adaptively generate
task-specific exploration strategies with LLMs, enhancing the policy
exploration in RL. In our design, we sample the learning trajectory of the
agent during the RL training in a given task and prompt the LLM to analyze the
agent's current policy learning status and then generate a probability
distribution for future policy exploration. Updating the probability
distribution periodically, we derive a stochastic process specialized for the
particular task and dynamically adjusted to adapt to the learning process. Our
design is a plug-in module compatible with various widely applied RL
algorithms, including the DQN series, DDPG, TD3, and any possible variants
developed based on them. Through extensive experiments on the Atari and MuJoCo
benchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy
exploration, achieving an average performance improvement up to 37.27%. Our
code is open-source at https://anonymous.4open.science/r/LLM-Explorer-19BE for
reproducibility.

</details>


### [6] [Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems](https://arxiv.org/abs/2505.15201)
*Christian Walder,Deep Karkhanis*

Main category: cs.LG

TL;DR: 本文提出了一种名为Pass-at-k Policy Optimization (PKPO)的新方法，通过优化样本集合的联合效用，提升强化学习中pass@k性能。与传统方法不同，PKPO可以灵活选择任意k≤n，并且在训练过程中可以通过退火机制调整k值，从而同时提高pass@1和pass@k性能。实验表明，PKPO不仅减少了方差，还能解决更多复杂问题，尤其在传统pass@1优化停滞时展现出更好的探索能力。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习算法通常独立奖励每个采样解，优化目标是pass@1性能，即优先考虑单个样本的强度，而忽略了样本集合的多样性和联合效用。这种策略限制了采样容量的利用，影响了探索能力和对更难问题的改进潜力。因此，需要一种新方法来直接优化样本集合的整体效用，以提升pass@k性能。

Method: 作者提出了Pass-at-k Policy Optimization (PKPO)，这是一种通过转换最终奖励来直接优化pass@k性能的方法。该方法的核心是推导出针对pass@k及其梯度的低方差无偏估计器，适用于二元和连续奖励设置。PKPO将优化问题转化为标准强化学习问题，但使用经过稳定高效转换函数处理的联合奖励。此外，PKPO首次实现了对任意k≤n的鲁棒优化，并允许在训练过程中退火k值，从而兼顾pass@1和pass@k性能。

Result: 通过玩具实验验证了PKPO奖励转换的有效性，证明了其具有降低方差的特性。在实际应用中，使用开源大语言模型GEMMA-2进行测试，结果表明PKPO能够有效优化目标k值。更高的k值有助于解决更多和更难的问题，而k值退火机制则进一步提升了pass@1和pass@k性能。特别是在传统pass@1优化难以取得进展的任务集合上，PKPO通过优先考虑样本集合的联合效用，显著增强了学习能力。

Conclusion: Pass-at-k Policy Optimization (PKPO)为强化学习提供了一种新的优化方向，通过直接优化样本集合的联合效用，显著提高了pass@k性能。相比传统方法，PKPO不仅支持任意k≤n的选择，还引入了k值退火机制，在提升整体性能的同时保持了pass@1的竞争力。这种方法在解决复杂问题和增强探索能力方面表现出色，为强化学习领域提供了重要的技术进步。

Abstract: Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts
for each problem and reward them independently. This optimizes for pass@1
performance and prioritizes the strength of isolated samples at the expense of
the diversity and collective utility of sets of samples. This under-utilizes
the sampling capacity, limiting exploration and eventual improvement on harder
examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a
transformation on the final rewards which leads to direct optimization of
pass@k performance, thus optimizing for sets of samples that maximize reward
when considered jointly. Our contribution is to derive novel low variance
unbiased estimators for pass@k and its gradient, in both the binary and
continuous reward settings. We show optimization with our estimators reduces to
standard RL with rewards that have been jointly transformed by a stable and
efficient transformation function.
  While previous efforts are restricted to k=n, ours is the first to enable
robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of
trading off pass@1 performance for pass@k gains, our method allows annealing k
during training, optimizing both metrics and often achieving strong pass@1
numbers alongside significant pass@k gains.
  We validate our reward transformations on toy experiments, which reveal the
variance reducing properties of our formulations. We also include real-world
examples using the open-source LLM, GEMMA-2. We find that our transformation
effectively optimizes for the target k. Furthermore, higher k values enable
solving more and harder problems, while annealing k boosts both the pass@1 and
pass@k . Crucially, for challenging task sets where conventional pass@1
optimization stalls, our pass@k approach unblocks learning, likely due to
better exploration by prioritizing joint utility over the utility of individual
samples.

</details>


### [7] [The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning](https://arxiv.org/abs/2505.15134)
*Shivam Agarwal,Zimin Zhang,Lifan Yuan,Jiawei Han,Hao Peng*

Main category: cs.LG

TL;DR: 本文研究了熵最小化(EM)方法对大语言模型在数学、物理和编程任务上的性能提升效果，发现无需标注数据即可显著改善模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型在复杂推理任务（如数学、物理和编程）上仍有不足，而传统的指令微调或强化学习方法通常依赖大量标注数据，这增加了训练成本。因此，探索一种不依赖标注数据的方法来提升模型性能具有重要意义。

Method: 本文提出了三种基于熵最小化的改进方法：1) EM-FT：通过最小化令牌级熵对模型进行微调；2) EM-RL：使用负熵作为唯一奖励的强化学习方法；3) EM-INF：无需训练数据或参数更新，在推理阶段调整logit以降低熵。

Result: 在Qwen-7B模型上，EM-RL方法在无任何标注数据的情况下，表现与基于60K标注样本训练的强基线方法相当甚至更优。此外，EM-INF使Qwen-32B在SciCode基准测试中匹配或超越GPT-4o等专有模型的表现，同时效率提高了3倍。

Conclusion: 研究表明，许多预训练的大语言模型具备未被充分认识的推理能力，这些能力可以通过熵最小化技术有效激发，且无需任何标注数据或参数更新。

Abstract: Entropy minimization (EM) trains the model to concentrate even more
probability mass on its most confident outputs. We show that this simple
objective alone, without any labeled data, can substantially improve large
language models' (LLMs) performance on challenging math, physics, and coding
tasks. We explore three approaches: (1) EM-FT minimizes token-level entropy
similarly to instruction finetuning, but on unlabeled outputs drawn from the
model; (2) EM-RL: reinforcement learning with negative entropy as the only
reward to maximize; (3) EM-INF: inference-time logit adjustment to reduce
entropy without any training data or parameter updates. On Qwen-7B, EM-RL,
without any labeled data, achieves comparable or better performance than strong
RL baselines such as GRPO and RLOO that are trained on 60K labeled examples.
Furthermore, EM-INF enables Qwen-32B to match or exceed the performance of
proprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the
challenging SciCode benchmark, while being 3x more efficient than
self-consistency and sequential refinement. Our findings reveal that many
pretrained LLMs possess previously underappreciated reasoning capabilities that
can be effectively elicited through entropy minimization alone, without any
labeled data or even any parameter updates.

</details>


### [8] [RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning](https://arxiv.org/abs/2505.15034)
*Kaiwen Zha,Zhengqi Gao,Maohao Shen,Zhang-Wei Hong,Duane S. Boning,Dina Katabi*

Main category: cs.LG

TL;DR: 本文提出了一种名为Tango的新框架，通过强化学习同时训练大型语言模型生成器和验证器，解决了现有方法中的奖励欺骗和泛化问题。实验表明，Tango在数学推理和跨领域推理任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习（RL）后训练方法中的验证器要么是固定的（基于规则或冻结的预训练模型），要么是通过监督微调（SFT）训练的判别模型。这些设计容易导致奖励欺骗，并且在超出训练分布的情况下泛化能力较差。

Method: Tango框架采用一种生成式的、过程级别的LLM验证器，该验证器通过强化学习与生成器交错训练。验证器仅根据结果级别的验证正确性奖励进行训练，无需显式的过程级注释。生成器和验证器共同进化，形成有效的相互强化关系。

Result: Tango的生成器在五个竞赛级别的数学基准测试和四个具有挑战性的跨领域推理任务中取得了最佳性能；验证器在ProcessBench数据集上领先。特别是在最难的数学推理问题上，两个组件都表现出显著的改进。

Conclusion: Tango框架展示了其生成器和验证器在7B/8B规模模型中的优越性能，特别是在困难的数学推理问题上。这种新框架为提升LLM的推理能力提供了一种有效的方法，并在多个基准测试中达到了最先进的水平。

Abstract: Reinforcement learning (RL) has recently emerged as a compelling approach for
enhancing the reasoning capabilities of large language models (LLMs), where an
LLM generator serves as a policy guided by a verifier (reward model). However,
current RL post-training methods for LLMs typically use verifiers that are
fixed (rule-based or frozen pretrained) or trained discriminatively via
supervised fine-tuning (SFT). Such designs are susceptible to reward hacking
and generalize poorly beyond their training distributions. To overcome these
limitations, we propose Tango, a novel framework that uses RL to concurrently
train both an LLM generator and a verifier in an interleaved manner. A central
innovation of Tango is its generative, process-level LLM verifier, which is
trained via RL and co-evolves with the generator. Importantly, the verifier is
trained solely based on outcome-level verification correctness rewards without
requiring explicit process-level annotations. This generative RL-trained
verifier exhibits improved robustness and superior generalization compared to
deterministic or SFT-trained verifiers, fostering effective mutual
reinforcement with the generator. Extensive experiments demonstrate that both
components of Tango achieve state-of-the-art results among 7B/8B-scale models:
the generator attains best-in-class performance across five competition-level
math benchmarks and four challenging out-of-domain reasoning tasks, while the
verifier leads on the ProcessBench dataset. Remarkably, both components exhibit
particularly substantial improvements on the most difficult mathematical
reasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.

</details>


### [9] [TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning](https://arxiv.org/abs/2505.14625)
*Zhangchen Xu,Yuetai Li,Fengqing Jiang,Bhaskar Ramasubramanian,Luyao Niu,Bill Yuchen Lin,Radha Poovendran*

Main category: cs.LG

TL;DR: 本文研究了强化学习中验证器产生的假阴性问题对大型语言模型（LLMs）训练的影响，并提出了一种轻量级的LLM验证器tinyV来缓解该问题，显著提高了数学推理基准测试的表现和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 强化学习的成功依赖于奖励信号的可靠性，然而当前验证器存在大量假阴性问题，即错误地拒绝了正确的模型输出。这一问题严重影响了模型的训练效果和收敛速度，因此需要找到一种方法来减少假阴性问题。

Method: 通过分析Big-Math-RL-Verified数据集，作者发现38%以上的模型生成响应受到假阴性问题影响。为了解决这一问题，作者提出了tinyV，一种基于轻量级LLM的验证器，它可以动态识别潜在的假阴性并恢复有效响应，从而提供更准确的奖励估计。

Result: 在多个数学推理基准测试中，集成tinyV后，通过率提高了高达10%，并且相对于基线方法加速了收敛。

Conclusion: 本文揭示了验证器假阴性问题对强化学习训练的严重影响，并通过提出tinyV提供了一个实用的解决方案，可以有效改善基于RL的LLMs微调效果。代码已开源至https://github.com/uw-nsl/TinyV。

Abstract: Reinforcement Learning (RL) has become a powerful tool for enhancing the
reasoning abilities of large language models (LLMs) by optimizing their
policies with reward signals. Yet, RL's success relies on the reliability of
rewards, which are provided by verifiers. In this paper, we expose and analyze
a widespread problem--false negatives--where verifiers wrongly reject correct
model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals
that over 38% of model-generated responses suffer from false negatives, where
the verifier fails to recognize correct answers. We show, both empirically and
theoretically, that these false negatives severely impair RL training by
depriving the model of informative gradient signals and slowing convergence. To
mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments
existing rule-based methods, which dynamically identifies potential false
negatives and recovers valid responses to produce more accurate reward
estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts
pass rates by up to 10% and accelerates convergence relative to the baseline.
Our findings highlight the critical importance of addressing verifier false
negatives and offer a practical approach to improve RL-based fine-tuning of
LLMs. Our code is available at https://github.com/uw-nsl/TinyV.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [10] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 在大规模语言模型（LLMs）中，通过重复采样来扩展推理计算可以持续增加覆盖率。本文研究了这种提升部分是由于评估基准答案分布的偏斜，并提出了一种基于训练集答案频率的基线方法，用于更准确地测量重复采样的效果。


<details>
  <summary>Details</summary>
Motivation: 重复采样在大语言模型中能够持续提高问题解决的覆盖率，但这种提升可能部分归因于标准评估基准中的答案分布偏斜（偏向少量常见答案）。为了验证这一猜想并更准确地评估重复采样的效果，需要一种新的基线方法。

Method: 定义了一种基线方法，该方法根据训练集中答案的出现频率枚举答案。通过在数学推理和事实知识两个领域进行实验，比较了此基线方法与重复模型采样以及混合策略的表现。

Result: 在某些大语言模型中，提出的基线方法优于重复模型采样；而在其他模型中，其表现与一种混合策略相当，该策略仅使用10次模型采样并结合枚举猜测剩余的答案。

Conclusion: 提出的基线方法提供了一种更准确的方式，用于测量在提示无关猜测之外，重复采样对覆盖率的实际提升效果。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [11] [DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data](https://arxiv.org/abs/2505.15074)
*Yuhang Zhou,Jing Zhu,Shengyi Qian,Zhuokai Zhao,Xiyao Wang,Xiaoyu Liu,Ming Li,Paiheng Xu,Wei Ai,Furong Huang*

Main category: cs.CL

TL;DR: DISCO是一种改进的GRPO方法，通过领域感知奖励缩放和难度感知奖励缩放解决数据不平衡问题，提升多领域对齐性能。


<details>
  <summary>Details</summary>
Motivation: 尽管GRPO方法简单且性能强大，但其在处理多领域、不平衡数据时存在优化偏向主导领域的问题，忽视了代表性不足的领域，导致泛化能力和公平性较差。因此，需要一种新的方法来解决这种不平衡问题。

Method: 提出Domain-Informed Self-Consistency Policy Optimization (DISCO) 方法，包含两个关键创新：1) 领域感知奖励缩放，通过根据领域频率重新加权优化来对抗频率偏差；2) 难度感知奖励缩放，利用提示级别的一致性识别并优先处理具有更大学习价值的不确定提示。

Result: 广泛的实验表明，DISCO提高了泛化能力，在Qwen3模型上比现有的GRPO变体高出5%，并在多领域对齐基准测试中取得了新的最先进结果。

Conclusion: DISCO通过解决领域间不平衡问题，促进了跨领域的更公平和有效的策略学习，成为多领域对齐任务中的新标准方法。

Abstract: Large Language Models (LLMs) are increasingly aligned with human preferences
through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods,
Group Relative Policy Optimization (GRPO) has gained attention for its
simplicity and strong performance, notably eliminating the need for a learned
value function. However, GRPO implicitly assumes a balanced domain distribution
and uniform semantic alignment across groups - assumptions that rarely hold in
real-world datasets. When applied to multi-domain, imbalanced data, GRPO
disproportionately optimizes for dominant domains, neglecting underrepresented
ones and resulting in poor generalization and fairness. We propose
Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled
extension to GRPO that addresses inter-group imbalance with two key
innovations. Domain-aware reward scaling counteracts frequency bias by
reweighting optimization based on domain prevalence. Difficulty-aware reward
scaling leverages prompt-level self-consistency to identify and prioritize
uncertain prompts that offer greater learning value. Together, these strategies
promote more equitable and effective policy learning across domains. Extensive
experiments across multiple LLMs and skewed training distributions show that
DISCO improves generalization, outperforms existing GRPO variants by 5% on
Qwen3 models, and sets new state-of-the-art results on multi-domain alignment
benchmarks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [12] [HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving](https://arxiv.org/abs/2505.15793)
*Zhiwen Chen,Bo Leng,Zhuoren Li,Hanming Deng,Guizhe Jin,Ran Yu,Huanxi Wen*

Main category: cs.RO

TL;DR: 将大语言模型（LLM）与强化学习（RL）结合可以提升自动驾驶在复杂场景中的性能。本文提出了一种新的LLM辅助的RL范式，通过语义提示增强状态表示和策略优化，并设计了HCRMP架构以解决LLM幻觉问题并实现优秀的驾驶性能。实验表明，HCRMP在不同交通密度下任务成功率达到80.3%，并在关键安全驾驶条件下减少11.4%的碰撞率。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM主导的强化学习方法过于依赖LLM输出，而LLM在关键驾驶任务上的非幻觉率仅为约57.95%，可能导致驾驶策略性能下降。因此，需要一种新方法来保持LLM与RL之间的相对独立性，从而缓解LLM幻觉问题并提升驾驶性能。

Method: 本文提出了LLM-Hinted RL范式，其中LLM用于生成语义提示以辅助RL代理进行运动规划，同时RL代理通过策略学习对抗潜在的错误语义指示。具体来说，设计了HCRMP架构，包括：1) 增强语义表示模块，扩展状态空间；2) 上下文稳定性锚定模块，利用知识库信息提高多批评权重提示的可靠性；3) 语义缓存模块，无缝集成LLM低频指导与RL高频控制。

Result: 在CARLA仿真环境中进行的广泛实验证明了HCRMP的有效性。结果表明，在不同交通密度下，HCRMP的任务成功率高达80.3%。在关键安全驾驶条件下，HCRMP显著降低了11.4%的碰撞率，有效提升了复杂场景下的驾驶性能。

Conclusion: 本文提出了一种新的LLM-Hinted RL范式及相应的HCRMP架构，解决了LLM幻觉问题并实现了卓越的自动驾驶性能。未来研究可以进一步探索如何更高效地结合LLM与RL，以应对更多复杂的驾驶挑战。

Abstract: Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can
enhance autonomous driving (AD) performance in complex scenarios. However,
current LLM-Dominated RL methods over-rely on LLM outputs, which are prone to
hallucinations.Evaluations show that state-of-the-art LLM indicates a
non-hallucination rate of only approximately 57.95% when assessed on essential
driving-related tasks. Thus, in these methods, hallucinations from the LLM can
directly jeopardize the performance of driving policies. This paper argues that
maintaining relative independence between the LLM and the RL is vital for
solving the hallucinations problem. Consequently, this paper is devoted to
propose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic
hints for state augmentation and policy optimization to assist RL agent in
motion planning, while the RL agent counteracts potential erroneous semantic
indications through policy learning to achieve excellent driving performance.
Based on this paradigm, we propose the HCRMP (LLM-Hinted Contextual
Reinforcement Learning Motion Planner) architecture, which is designed that
includes Augmented Semantic Representation Module to extend state space.
Contextual Stability Anchor Module enhances the reliability of multi-critic
weight hints by utilizing information from the knowledge base. Semantic Cache
Module is employed to seamlessly integrate LLM low-frequency guidance with RL
high-frequency control. Extensive experiments in CARLA validate HCRMP's strong
overall driving performance. HCRMP achieves a task success rate of up to 80.3%
under diverse driving conditions with different traffic densities. Under
safety-critical driving conditions, HCRMP significantly reduces the collision
rate by 11.4%, which effectively improves the driving performance in complex
scenarios.

</details>
