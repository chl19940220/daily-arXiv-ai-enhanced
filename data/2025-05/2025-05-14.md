<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: This paper surveys recent developments in human-AI collaboration, identifies gaps, and proposes a new conceptual framework called Hierarchical Exploration-Exploitation Net to integrate varied studies and inspire future work.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is the lack of a unifying theoretical framework that can coherently integrate various studies on human-AI agent collaboration, especially for complex tasks.

Method: The method involves critically surveying a broad spectrum of recent empirical developments on human-AI agents collaboration and proposing a novel conceptual architecture named Hierarchical Exploration-Exploitation Net which interlinks multi-agent coordination, knowledge management, cybernetic feedback loops, and higher-level control mechanisms.

Result: By mapping existing contributions onto the proposed framework, the approach facilitates revision of legacy methods and inspires new work that fuses qualitative and quantitative paradigms.

Conclusion: These insights provide a stepping stone toward deeper co-evolution of human cognition and AI capability, allowing the paper to serve as both a critical review and a forward-looking reference.

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [2] [The Steganographic Potentials of Language Models](https://arxiv.org/abs/2505.03439)
*Artem Karpov,Tinuade Adeleke,Seong Hah Cho,Natalia Perez-Campanero*

Main category: cs.AI

TL;DR: The abstract explores the steganographic capabilities of LLMs fine-tuned via reinforcement learning, revealing rudimentary abilities in security and capacity that can be enhanced with algorithmic guidance.


<details>
  <summary>Details</summary>
Motivation: To understand and address the challenge posed by LLMs' ability to hide messages within plain text, undermining the faithfulness of their reasoning.

Method: Fine-tuning LLMs via reinforcement learning to develop covert encoding schemes, engage in steganography when prompted, and utilize it in realistic scenarios. Detecting LLMs' intention to hide reasoning and evaluating their steganography performance.

Result: Current models exhibit basic steganographic abilities which can be significantly improved with explicit algorithmic guidance.

Conclusion: LLMs have potential steganographic capabilities that need to be understood and managed to ensure the faithfulness of their reasoning.

Abstract: The potential for large language models (LLMs) to hide messages within plain
text (steganography) poses a challenge to detection and thwarting of unaligned
AI agents, and undermines faithfulness of LLMs reasoning. We explore the
steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)
to: (1) develop covert encoding schemes, (2) engage in steganography when
prompted, and (3) utilize steganography in realistic scenarios where hidden
reasoning is likely, but not prompted. In these scenarios, we detect the
intention of LLMs to hide their reasoning as well as their steganography
performance. Our findings in the fine-tuning experiments as well as in
behavioral non fine-tuning evaluations reveal that while current models exhibit
rudimentary steganographic abilities in terms of security and capacity,
explicit algorithmic guidance markedly enhances their capacity for information
concealment.

</details>


### [3] [Enhancing LLMs' Clinical Reasoning with Real-World Data from a Nationwide Sepsis Registry](https://arxiv.org/abs/2505.02722)
*Junu Kim,Chaeeun Shim,Sungjin Park,Su Yeon Lee,Gee Young Suh,Chae-Man Lim,Seong Jin Choi,Song Mi Moon,Kyoung-Ho Song,Eu Suk Kim,Hong Bin Kim,Sejoong Kim,Chami Im,Dong-Wan Kang,Yong Soo Kim,Hee-Joon Bae,Sung Yoon Lim,Han-Gil Jeong,Edward Choi*

Main category: cs.AI

TL;DR: Although LLMs show good reasoning abilities in general domains, their effectiveness in clinical practice is limited. The researchers propose enhancing the clinical reasoning capabilities of LLMs by using real-world clinical data. They constructed questions from a sepsis registry and fine-tuned Phi-4 with reinforcement learning to create C-Reason, which showed strong clinical reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: To improve the clinical reasoning capabilities of large language models that have been limited due to insufficient exposure to real-world clinical data during training because of privacy concerns.

Method: Constructed reasoning-intensive questions from a nationwide sepsis registry and fine-tuned Phi-4 on these questions using reinforcement learning to produce C-Reason.

Result: C-Reason exhibited strong clinical reasoning capabilities on the in-domain test set and its enhanced reasoning capabilities generalized to other tasks and diseases.

Conclusion: Future research should focus on training LLMs with large-scale, multi-disease clinical datasets to develop more powerful, general-purpose clinical reasoning models.

Abstract: Although large language models (LLMs) have demonstrated impressive reasoning
capabilities across general domains, their effectiveness in real-world clinical
practice remains limited. This is likely due to their insufficient exposure to
real-world clinical data during training, as such data is typically not
included due to privacy concerns. To address this, we propose enhancing the
clinical reasoning capabilities of LLMs by leveraging real-world clinical data.
We constructed reasoning-intensive questions from a nationwide sepsis registry
and fine-tuned Phi-4 on these questions using reinforcement learning, resulting
in C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the
in-domain test set, as evidenced by both quantitative metrics and expert
evaluations. Furthermore, its enhanced reasoning capabilities generalized to a
sepsis dataset involving different tasks and patient cohorts, an open-ended
consultations on antibiotics use task, and other diseases. Future research
should focus on training LLMs with large-scale, multi-disease clinical datasets
to develop more powerful, general-purpose clinical reasoning models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [DSADF: Thinking Fast and Slow for Decision Making](https://arxiv.org/abs/2505.08189)
*Alex Zhihao Dou,Dongfei Cui,Jun Yan,Weida Wang,Benteng Chen,Haoming Wang,Zeke Xie,Shufei Zhang*

Main category: cs.LG

TL;DR: 尽管强化学习（RL）代理在定义良好的环境中表现良好，但它们通常难以将其学到的策略推广到动态设置中。为了解决这个问题，我们提出了双系统自适应决策框架（DSADF），该框架结合了快速直观决策的RL代理和深度分析推理的视觉语言模型（VLM）。通过这种方式，DSADF促进了高效和适应性的决策。实证研究证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: RL代理在动态环境中的泛化能力不足，现有结合基础模型的方法缺乏无缝协调，导致不合理决策和效率瓶颈。

Method: 受到Kahneman的快思考（系统1）和慢思考（系统2）理论的启发，提出了一种双系统自适应决策框架（DSADF）。该框架包括两个互补模块：系统1（由RL代理和记忆空间组成，用于快速直观决策）和系统2（由VLM驱动，用于深度分析推理）。

Result: 在Crafter和Housekeep视频游戏环境中的实证研究表明，所提出的方法在已知任务和未知任务中都显著提高了决策能力。

Conclusion: DSADF通过结合两种系统的优点，实现了高效和适应性的决策，平衡了直觉和深入推理，从而在复杂世界中实现敏捷决策。

Abstract: Although Reinforcement Learning (RL) agents are effective in well-defined
environments, they often struggle to generalize their learned policies to
dynamic settings due to their reliance on trial-and-error interactions. Recent
work has explored applying Large Language Models (LLMs) or Vision Language
Models (VLMs) to boost the generalization of RL agents through policy
optimization guidance or prior knowledge. However, these approaches often lack
seamless coordination between the RL agent and the foundation model, leading to
unreasonable decision-making in unfamiliar environments and efficiency
bottlenecks. Making full use of the inferential capabilities of foundation
models and the rapid response capabilities of RL agents and enhancing the
interaction between the two to form a dual system is still a lingering
scientific question. To address this problem, we draw inspiration from
Kahneman's theory of fast thinking (System 1) and slow thinking (System 2),
demonstrating that balancing intuition and deep reasoning can achieve nimble
decision-making in a complex world. In this study, we propose a Dual-System
Adaptive Decision Framework (DSADF), integrating two complementary modules:
System 1, comprising an RL agent and a memory space for fast and intuitive
decision making, and System 2, driven by a VLM for deep and analytical
reasoning. DSADF facilitates efficient and adaptive decision-making by
combining the strengths of both systems. The empirical study in the video game
environment: Crafter and Housekeep demonstrates the effectiveness of our
proposed method, showing significant improvements in decision abilities for
both unseen and known tasks.

</details>


### [5] [MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering](https://arxiv.org/abs/2505.07782)
*Rushi Qiang,Yuchen Zhuang,Yinghao Li,Dingu Sagar V K,Rongzhi Zhang,Changhao Li,Ian Shu-Hei Wong,Sherry Yang,Percy Liang,Chao Zhang,Bo Dai*

Main category: cs.LG

TL;DR: The paper introduces MLE-Dojo, an interactive framework for training and evaluating autonomous LLM agents in MLE workflows. Built on real-world Kaggle challenges, it supports iterative experimentation and reveals limitations in current LLMs.


<details>
  <summary>Details</summary>
Motivation: To systematically improve autonomous LLM agents through iterative experimentation and structured feedback loops in realistic machine learning engineering scenarios.

Method: Developed a Gym-style framework named MLE-Dojo based on over 200 real-world Kaggle challenges, providing diverse tasks and a fully executable environment supporting both supervised fine-tuning and reinforcement learning.

Result: Evaluations of eight frontier LLMs show meaningful iterative improvements but significant limitations in generating long-term solutions and resolving complex errors efficiently.

Conclusion: MLE-Dojo's flexible architecture promotes interoperability and reproducibility, and the open-source framework aims to drive community innovation for next-generation MLE agents.

Abstract: We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement
learning, evaluating, and improving autonomous large language model (LLM)
agents in iterative machine learning engineering (MLE) workflows. Unlike
existing benchmarks that primarily rely on static datasets or single-attempt
evaluations, MLE-Dojo provides an interactive environment enabling agents to
iteratively experiment, debug, and refine solutions through structured feedback
loops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,
open-ended MLE tasks carefully curated to reflect realistic engineering
scenarios such as data processing, architecture search, hyperparameter tuning,
and code debugging. Its fully executable environment supports comprehensive
agent training via both supervised fine-tuning and reinforcement learning,
facilitating iterative experimentation, realistic data sampling, and real-time
outcome verification. Extensive evaluations of eight frontier LLMs reveal that
while current models achieve meaningful iterative improvements, they still
exhibit significant limitations in autonomously generating long-horizon
solutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's
flexible and extensible architecture seamlessly integrates diverse data
sources, tools, and evaluation protocols, uniquely enabling model-based agent
tuning and promoting interoperability, scalability, and reproducibility. We
open-source our framework and benchmarks to foster community-driven innovation
towards next-generation MLE agents.

</details>


### [6] [Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains](https://arxiv.org/abs/2505.07274)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: The paper introduces a cache-efficient framework for integrating LLMs in RL, reducing computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To make the integration of large language models in reinforcement learning more computationally feasible without sacrificing performance.

Method: An adaptive caching mechanism is used where cache parameters are meta-optimized using surrogate gradients from policy performance. This method applies to both discrete text environments and continuous control domains.

Result: Achieved 3.8-4.7 times reduction in LLM queries and 4.0-12.0 times lower median latencies with minimal performance loss (96-98% of uncached performance). The CQL-Prior variant improved offline RL performance by 14-29% and reduced training time by 38-40%.

Conclusion: This principled framework enables efficient inference and demonstrates generalizability across different tasks, making LLM-guided RL practical in resource-constrained settings.

Abstract: Integrating large language models (LLMs) as priors in reinforcement learning
(RL) offers significant advantages but comes with substantial computational
costs. We present a principled cache-efficient framework for posterior sampling
with LLM-derived priors that dramatically reduces these costs while maintaining
high performance. At the core of our approach is an adaptive caching mechanism,
where cache parameters are meta-optimized using surrogate gradients derived
from policy performance. This design enables efficient inference across both
discrete text environments (e.g., TextWorld, ALFWorld) and continuous control
domains (e.g., MuJoCo), achieving a 3.8--4.7$\times$ reduction in LLM queries
and 4.0--12.0$\times$ lower median latencies (85--93\,ms on a consumer GPU)
while retaining 96--98\% of uncached performance. Our theoretical analysis
provides KL divergence bounds on approximation quality, validated empirically.
The framework extends to offline RL, where our CQL-Prior variant improves
performance by 14--29\% and reduces training time by 38--40\%. Extensive
evaluations across a diverse suite of eight tasks demonstrate the
generalizability and practical viability of LLM-guided RL in
resource-constrained settings.

</details>


### [7] [Remote Rowhammer Attack using Adversarial Observations on Federated Learning Clients](https://arxiv.org/abs/2505.06335)
*Jinsheng Yuan,Yuhang Hao,Weisi Guo,Yun Wu,Chongyan Gu*

Main category: cs.LG

TL;DR: In this paper, the authors explore a new type of attack on Federated Learning (FL) systems where an attacker can remotely initiate a rowhammer attack on the server memory by targeting certain clients. This leads to bit flips that corrupt the server memory, potentially causing disruptions or privilege escalation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate client-facing attacks on FL servers which have been less well researched, specifically focusing on how repetitive memory updates caused by certain clients can lead to remote rowhammer attacks.

Method: By using a reinforcement learning (RL) attacker that manipulates the client's sensor observations, the method aims at maximizing server repetitive memory updates leading to bit flips in the server DRAM.

Result: The adversarial attacking agent achieved around 70% repeated update rate (RUR) in the targeted server model, effectively inducing bit flips on server DRAM.

Conclusion: This research highlights the security implications for FL systems and calls for further investigation into practical mitigation strategies in FL and hardware design.

Abstract: Federated Learning (FL) has the potential for simultaneous global learning
amongst a large number of parallel agents, enabling emerging AI such as LLMs to
be trained across demographically diverse data. Central to this being efficient
is the ability for FL to perform sparse gradient updates and remote direct
memory access at the central server. Most of the research in FL security
focuses on protecting data privacy at the edge client or in the communication
channels between the client and server. Client-facing attacks on the server are
less well investigated as the assumption is that a large collective of clients
offer resilience.
  Here, we show that by attacking certain clients that lead to a high frequency
repetitive memory update in the server, we can remote initiate a rowhammer
attack on the server memory. For the first time, we do not need backdoor access
to the server, and a reinforcement learning (RL) attacker can learn how to
maximize server repetitive memory updates by manipulating the client's sensor
observation. The consequence of the remote rowhammer attack is that we are able
to achieve bit flips, which can corrupt the server memory. We demonstrate the
feasibility of our attack using a large-scale FL automatic speech recognition
(ASR) systems with sparse updates, our adversarial attacking agent can achieve
around 70\% repeated update rate (RUR) in the targeted server model,
effectively inducing bit flips on server DRAM. The security implications are
that can cause disruptions to learning or may inadvertently cause elevated
privilege. This paves the way for further research on practical mitigation
strategies in FL and hardware design.

</details>


### [8] [DMRL: Data- and Model-aware Reward Learning for Data Extraction](https://arxiv.org/abs/2505.06284)
*Zhiqiang Wang,Ruoxi Cheng*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）容易发生非故意的隐私泄露，因此需要系统性的研究来开发强大的防御机制。现有的数据提取方法存在一些限制，如依赖数据集副本、提示工程和随机搜索对抗生成。为了解决这些问题，我们提出了DMRL，一种数据和模型感知的奖励学习方法，用于从LLMs中提取数据。DMRL使用逆向强化学习，并包含两个主要组成部分：构建内省推理数据集以捕捉泄漏心态，以及使用组相对策略优化（GRPO）训练奖励模型。实验表明，DMRL在数据提取性能上优于所有基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易发生非故意的隐私泄露，而当前的数据提取方法存在多种局限性，包括依赖数据集副本、提示工程和随机搜索对抗生成等，这些方法要么可以通过去重解决，要么已经被检测和防御手段所应对。因此，需要一种新的方法来更有效地进行数据提取。

Method: 提出了一种名为DMRL（Data- and Model-aware Reward Learning）的方法，该方法利用逆向强化学习进行数据提取。具体来说，DMRL包含两个主要部分：1) 构建一个内省推理数据集，用来捕捉可能导致数据泄漏的心态，从而指导模型行为；2) 使用组相对策略优化（GRPO）训练奖励模型，根据任务在数据和模型层面的难度动态调整优化过程。

Result: 通过广泛的实验验证，DMRL在各种大型语言模型上的数据提取性能均优于所有基线方法。

Conclusion: DMRL是一种有效的数据提取方法，能够克服现有方法的诸多限制，并且在多个LLM上的实验证明了其优越性。

Abstract: Large language models (LLMs) are inherently vulnerable to unintended privacy
breaches. Consequently, systematic red-teaming research is essential for
developing robust defense mechanisms. However, current data extraction methods
suffer from several limitations: (1) rely on dataset duplicates (addressable
via deduplication), (2) depend on prompt engineering (now countered by
detection and defense), and (3) rely on random-search adversarial generation.
To address these challenges, we propose DMRL, a Data- and Model-aware Reward
Learning approach for data extraction. This technique leverages inverse
reinforcement learning to extract sensitive data from LLMs. Our method consists
of two main components: (1) constructing an introspective reasoning dataset
that captures leakage mindsets to guide model behavior, and (2) training reward
models with Group Relative Policy Optimization (GRPO), dynamically tuning
optimization based on task difficulty at both the data and model levels.
Comprehensive experiments across various LLMs demonstrate that DMRL outperforms
all baseline methods in data extraction performance.

</details>


### [9] [Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers](https://arxiv.org/abs/2505.04842)
*Kusha Sareen,Morgane M Moss,Alessandro Sordoni,Rishabh Agarwal,Arian Hosseini*

Main category: cs.LG

TL;DR: 提出RL^V方法，增强无价值函数的强化学习技术，通过联合训练提升推理和验证能力，显著提高数学精度、测试时间计算扩展效率及任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前主流的微调LLM推理器的强化学习方法（如GRPO或Leave-one-out PPO）舍弃了已学习的价值函数，转而使用经验估计回报，这阻碍了依赖价值函数进行验证的测试时间计算扩展。

Method: 提出RL^V方法，将任意无价值函数的强化学习技术增强，通过联合训练LLM作为推理器和生成式验证器，使用强化学习生成的数据添加验证功能，且不增加显著开销。

Result: RL^V在并行采样的情况下将数学准确率提高了20%以上，相比基础RL方法实现了8-32倍更高效的测试时间计算扩展，并表现出强大的任务泛化能力，包括从简单到复杂以及领域外的任务；当与长推理R1模型联合扩展并行和顺序测试时间计算时，性能高出1.2-1.6倍。

Conclusion: RL^V方法有效增强了无价值函数的强化学习技术，提供了验证能力，提升了测试时间计算扩展效率和任务泛化能力。

Abstract: Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners,
such as GRPO or Leave-one-out PPO, abandon the learned value function in favor
of empirically estimated returns. This hinders test-time compute scaling that
relies on using the value-function for verification. In this work, we propose
RL$^V$ that augments any ``value-free'' RL method by jointly training the LLM
as both a reasoner and a generative verifier using RL-generated data, adding
verification capabilities without significant overhead. Empirically, RL$^V$
boosts MATH accuracy by over 20\% with parallel sampling and enables
$8-32\times$ efficient test-time compute scaling compared to the base RL
method. RL$^V$ also exhibits strong generalization capabilities for both
easy-to-hard and out-of-domain tasks. Furthermore, RL$^V$ achieves
$1.2-1.6\times$ higher performance when jointly scaling parallel and sequential
test-time compute with a long reasoning R1 model.

</details>


### [10] [DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning](https://arxiv.org/abs/2505.03209)
*Borui Wang,Kathleen McKeown,Rex Ying*

Main category: cs.LG

TL;DR: The paper introduces DYSTIL, a new reinforcement learning framework combined with LLMs that enhances policy generalization and sample efficiency, outperforming current methods by 17.75% in average success rate.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning from expert demonstrations is challenging due to poor generalization, low sample efficiency, and poor model interpretability of existing methods.

Method: DYSTIL dynamically queries a strategy-generating LLM to induce textual strategies based on advantage estimations and expert demonstrations, then internalizes these strategies into the RL agent through policy optimization.

Result: Empirical tests on Minigrid and BabyAI environments show DYSTIL outperforms state-of-the-art baselines by 17.75% in average success rate with higher sample efficiency.

Conclusion: DYSTIL provides a promising approach to improve reinforcement learning performance, generalization, and interpretability.

Abstract: Reinforcement learning from expert demonstrations has long remained a
challenging research problem, and existing state-of-the-art methods using
behavioral cloning plus further RL training often suffer from poor
generalization, low sample efficiency, and poor model interpretability.
Inspired by the strong reasoning abilities of large language models (LLMs), we
propose a novel strategy-based reinforcement learning framework integrated with
LLMs called DYnamic STrategy Induction with Llms for reinforcement learning
(DYSTIL) to overcome these limitations. DYSTIL dynamically queries a
strategy-generating LLM to induce textual strategies based on advantage
estimations and expert demonstrations, and gradually internalizes induced
strategies into the RL agent through policy optimization to improve its
performance through boosting policy generalization and enhancing sample
efficiency. It also provides a direct textual channel to observe and interpret
the evolution of the policy's underlying strategies during training. We test
DYSTIL over challenging RL environments from Minigrid and BabyAI, and
empirically demonstrate that DYSTIL significantly outperforms state-of-the-art
baseline methods by 17.75% in average success rate while also enjoying higher
sample efficiency during the learning process.

</details>


### [11] [VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making](https://arxiv.org/abs/2505.03181)
*Jake Grigsby,Yuke Zhu,Michael Ryoo,Juan Carlos Niebles*

Main category: cs.LG

TL;DR: Recent research aims to integrate the capabilities of large language models (LLMs) and vision-language models (VLMs) into agents for interactive environments. While VLMs lag behind LLMs in certain skills, supervised fine-tuning (SFT) can help overcome these limitations. This work explores an off-policy reinforcement learning (RL) solution that allows VLMs to self-improve and learn from low-quality datasets, demonstrating this technique with two open-weight VLMs across three multi-modal agent domains.


<details>
  <summary>Details</summary>
Motivation: To enhance the abilities of vision-language models (VLMs) in interactive environments so they can accomplish user-specified goals as effectively as large language models (LLMs).

Method: Using an off-policy reinforcement learning (RL) approach to fine-tune VLMs, allowing them to learn from both successful and unsuccessful decisions, including those from larger models. This method retains the stability and simplicity of the supervised fine-tuning (SFT) workflow while enabling self-improvement.

Result: Demonstrated the effectiveness of the off-policy RL technique using two open-weight VLMs across three multi-modal agent domains, showing potential for VLMs to improve and perform better on specific tasks.

Conclusion: An off-policy RL solution provides a viable path for enhancing VLM capabilities in multi-modal agent domains, allowing them to learn effectively from various data sources and improve their performance.

Abstract: Recent research looks to harness the general knowledge and reasoning of large
language models (LLMs) into agents that accomplish user-specified goals in
interactive environments. Vision-language models (VLMs) extend LLMs to
multi-modal data and provide agents with the visual reasoning necessary for new
applications in areas such as computer automation. However, agent tasks
emphasize skills where accessible open-weight VLMs lag behind their LLM
equivalents. For example, VLMs are less capable of following an environment's
strict output syntax requirements and are more focused on open-ended question
answering. Overcoming these limitations requires supervised fine-tuning (SFT)
on task-specific expert demonstrations. Our work approaches these challenges
from an offline-to-online reinforcement learning (RL) perspective. RL lets us
fine-tune VLMs to agent tasks while learning from the unsuccessful decisions of
our own model or more capable (larger) models. We explore an off-policy RL
solution that retains the stability and simplicity of the widely used SFT
workflow while allowing our agent to self-improve and learn from low-quality
datasets. We demonstrate this technique with two open-weight VLMs across three
multi-modal agent domains.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 在大规模语言模型（LLMs）中，通过重复采样来扩展推理计算，可以随着样本数量的增加持续提高覆盖率。实验表明，这种提升部分是由于答案分布偏向少量常见答案。研究提出了一种基于训练集答案频率的基线方法，在某些LLMs上优于重复采样，且能更准确评估重复采样的效果。


<details>
  <summary>Details</summary>
Motivation: 研究者观察到在大规模语言模型中，通过重复采样扩大推理计算能够持续增加问题解决的覆盖率，并推测这一现象部分归因于标准评估基准中的答案分布偏向少数常见答案。为了验证这个假设，他们设计了一个基于训练集中答案频率的基线方法。

Method: 定义一个基线，该基线根据训练集中答案的出现频率枚举答案。然后在两个领域（数学推理和事实知识）进行实验，比较此基线与重复模型采样以及混合策略（结合少量模型采样和枚举猜测）的表现。

Result: 实验结果表明，对于某些LLMs，基线方法的表现优于重复模型采样；而对于其他LLMs，其覆盖率与一种混合策略相当，该策略通过仅使用10个模型样本并枚举剩余的答案来获得k个答案。

Conclusion: 提出的基线方法使我们能够更精确地测量在提示无关的猜测之外，重复采样对覆盖率的改进程度。这有助于更好地理解重复采样在不同LLMs上的效果。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>
