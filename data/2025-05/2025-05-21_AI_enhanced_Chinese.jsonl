{"id": "2410.15466", "keyword": "LLM reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing.", "keywords": ["LLM reasoning"], "AI": {"tldr": "\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\uff0c\u901a\u8fc7\u91cd\u590d\u91c7\u6837\u6765\u6269\u5c55\u63a8\u7406\u8ba1\u7b97\u53ef\u4ee5\u63d0\u9ad8\u8986\u76d6\u8303\u56f4\uff08\u89e3\u51b3\u95ee\u9898\u7684\u6bd4\u4f8b\uff09\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u6539\u8fdb\u90e8\u5206\u53ef\u80fd\u662f\u56e0\u4e3a\u6807\u51c6\u8bc4\u4f30\u57fa\u51c6\u7684\u7b54\u6848\u5206\u5e03\u504f\u5411\u4e8e\u5c11\u91cf\u5e38\u89c1\u7b54\u6848\u3002\u7814\u7a76\u901a\u8fc7\u5b9a\u4e49\u4e00\u4e2a\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\uff0c\u53d1\u73b0\u8be5\u57fa\u7ebf\u5728\u67d0\u4e9bLLMs\u4e2d\u4f18\u4e8e\u91cd\u590d\u91c7\u6837\uff0c\u800c\u5728\u5176\u4ed6LLMs\u4e2d\u4e0e\u6df7\u5408\u7b56\u7565\u76f8\u5f53\u3002\u8fd9\u4e3a\u66f4\u51c6\u786e\u5730\u8861\u91cf\u91cd\u590d\u91c7\u6837\u7684\u6548\u679c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u8005\u89c2\u5bdf\u5230\uff0c\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u968f\u7740\u6837\u672c\u6570\u91cf\u7684\u589e\u52a0\uff0c\u901a\u8fc7\u91cd\u590d\u91c7\u6837\u6301\u7eed\u6269\u5c55\u63a8\u7406\u8ba1\u7b97\u4f1a\u589e\u52a0\u8986\u76d6\u8303\u56f4\u3002\u4ed6\u4eec\u63a8\u6d4b\u8fd9\u79cd\u6539\u8fdb\u90e8\u5206\u662f\u7531\u4e8e\u6807\u51c6\u8bc4\u4f30\u57fa\u51c6\u4e2d\u7684\u7b54\u6848\u5206\u5e03\u504f\u5411\u4e8e\u5c11\u91cf\u5e38\u89c1\u7b54\u6848\uff0c\u56e0\u6b64\u5e0c\u671b\u9a8c\u8bc1\u8fd9\u4e00\u731c\u60f3\uff0c\u5e76\u63a2\u7d22\u91cd\u590d\u91c7\u6837\u7684\u5b9e\u9645\u6548\u679c\u3002", "method": "\u7814\u7a76\u8005\u5b9a\u4e49\u4e86\u4e00\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6839\u636e\u8bad\u7ec3\u96c6\u4e2d\u7b54\u6848\u7684\u51fa\u73b0\u9891\u7387\u679a\u4e3e\u7b54\u6848\u3002\u7136\u540e\u5728\u4e24\u4e2a\u9886\u57df\uff08\u6570\u5b66\u63a8\u7406\u548c\u4e8b\u5b9e\u77e5\u8bc6\uff09\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u8be5\u57fa\u7ebf\u4e0e\u91cd\u590d\u6a21\u578b\u91c7\u6837\u4ee5\u53ca\u4e00\u79cd\u6df7\u5408\u7b56\u7565\u7684\u8868\u73b0\uff0c\u5176\u4e2d\u6df7\u5408\u7b56\u7565\u901a\u8fc7\u4ec5\u4f7f\u752810\u4e2a\u6a21\u578b\u6837\u672c\u83b7\u53d6\u7b54\u6848\uff0c\u5e76\u901a\u8fc7\u679a\u4e3e\u731c\u6d4b\u5269\u4f59\u7684\u5c1d\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u67d0\u4e9bLLMs\u4e2d\uff0c\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\u4f18\u4e8e\u91cd\u590d\u91c7\u6837\uff1b\u800c\u5728\u5176\u4ed6LLMs\u4e2d\uff0c\u5176\u8868\u73b0\u4e0e\u6df7\u5408\u7b56\u7565\u76f8\u5f53\u3002\u8fd9\u8868\u660e\u91cd\u590d\u91c7\u6837\u7684\u6548\u679c\u53ef\u80fd\u53d7\u5230\u7b54\u6848\u5206\u5e03\u7684\u5f71\u54cd\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7814\u7a76\u4e3a\u66f4\u51c6\u786e\u5730\u6d4b\u91cf\u91cd\u590d\u91c7\u6837\u5728\u63d0\u793a\u65e0\u5173\u731c\u6d4b\u4e4b\u5916\u5bf9\u8986\u76d6\u8303\u56f4\u7684\u63d0\u5347\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002\u8fd9\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u91cd\u590d\u91c7\u6837\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2505.00018", "keyword": "LLM Agent", "pdf": "https://arxiv.org/pdf/2505.00018", "abs": "https://arxiv.org/abs/2505.00018", "authors": ["Ju Wu", "Calvin K. L. Or"], "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability.", "keywords": ["LLM Agent"], "AI": {"tldr": "\u8fd9\u7bc7\u6587\u7ae0\u662f\u4e00\u7bc7\u7acb\u573a\u8bba\u6587\uff0c\u6279\u5224\u6027\u5730\u7efc\u8ff0\u4e86\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u534f\u4f5c\u7684\u5e7f\u6cdb\u7ecf\u9a8c\u53d1\u5c55\uff0c\u5f3a\u8c03\u4e86\u5176\u6280\u672f\u6210\u5c31\u548c\u6301\u7eed\u5b58\u5728\u7684\u5dee\u8ddd\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u5ff5\u67b6\u6784\uff0c\u7cfb\u7edf\u5730\u5c06\u591a\u4ee3\u7406\u534f\u8c03\u3001\u77e5\u8bc6\u7ba1\u7406\u3001\u63a7\u5236\u8bba\u53cd\u9988\u56de\u8def\u548c\u9ad8\u5c42\u63a7\u5236\u673a\u5236\u76f8\u4e92\u5173\u8054\uff0c\u4e3a\u73b0\u6709\u7814\u7a76\u63d0\u4f9b\u4e86\u6620\u5c04\u6846\u67b6\u3002\u6587\u7ae0\u7ed3\u6784\u7075\u6d3b\uff0c\u53ef\u4ece\u4efb\u4f55\u90e8\u5206\u9605\u8bfb\uff0c\u65e2\u662f\u5bf9\u6280\u672f\u5b9e\u73b0\u7684\u6279\u5224\u6027\u56de\u987e\uff0c\u4e5f\u662f\u8bbe\u8ba1\u6216\u6269\u5c55\u4eba\u7c7b-AI\u5171\u751f\u5173\u7cfb\u7684\u524d\u77bb\u6027\u53c2\u8003\u3002", "motivation": "\u5f53\u524d\u5173\u4e8e\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u534f\u4f5c\u7684\u7814\u7a76\u867d\u7136\u53d6\u5f97\u4e86\u8bb8\u591a\u6280\u672f\u6210\u5c31\uff0c\u4f46\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u6574\u5408\u8fd9\u4e9b\u591a\u6837\u5316\u7684\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5f00\u653e\u6027\u3001\u590d\u6742\u4efb\u52a1\u65f6\u3002\u8fd9\u4fc3\u4f7f\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6982\u5ff5\u67b6\u6784\u4ee5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u5206\u5c42\u63a2\u7d22-\u5229\u7528\u7f51\u7edc\uff08Hierarchical Exploration-Exploitation Net\uff09\u7684\u65b0\u6982\u5ff5\u67b6\u6784\u3002\u8be5\u67b6\u6784\u7cfb\u7edf\u5730\u8fde\u63a5\u4e86\u591a\u4ee3\u7406\u534f\u8c03\u3001\u77e5\u8bc6\u7ba1\u7406\u3001\u63a7\u5236\u8bba\u53cd\u9988\u56de\u8def\u548c\u9ad8\u5c42\u63a7\u5236\u673a\u5236\u7b49\u6280\u672f\u7ec6\u8282\u3002\u901a\u8fc7\u5c06\u73b0\u6709\u7684\u8d21\u732e\uff0c\u5305\u62ec\u7b26\u53f7AI\u6280\u672f\u3001\u57fa\u4e8e\u8fde\u63a5\u4e3b\u4e49\u7684LLM\u4ee3\u7406\u4ee5\u53ca\u6df7\u5408\u7ec4\u7ec7\u5b9e\u8df5\uff0c\u6620\u5c04\u5230\u8fd9\u4e2a\u6846\u67b6\u4e0a\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u5bf9\u4f20\u7edf\u65b9\u6cd5\u7684\u4fee\u8ba2\uff0c\u5e76\u6fc0\u53d1\u4e86\u878d\u5408\u5b9a\u6027\u548c\u5b9a\u91cf\u8303\u5f0f\u7684\u65b0\u5de5\u4f5c\u3002", "result": "\u8be5\u65b0\u6982\u5ff5\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u89d2\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u73b0\u6709\u7684\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u534f\u4f5c\u7814\u7a76\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u4eba\u7c7b\u8ba4\u77e5\u548cAI\u80fd\u529b\u7684\u5171\u540c\u8fdb\u5316\u3002", "conclusion": "\u6587\u7ae0\u603b\u7ed3\u4e86\u5f53\u524d\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u534f\u4f5c\u7814\u7a76\u7684\u6280\u672f\u6210\u5c31\u548c\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u63d0\u51fa\u4e00\u4e2a\u65b0\u7684\u6982\u5ff5\u67b6\u6784\uff0c\u4e3a\u672a\u6765\u7684\u6df1\u5165\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u8fd9\u4e00\u67b6\u6784\u4e0d\u4ec5\u6709\u52a9\u4e8e\u4fee\u8ba2\u4f20\u7edf\u65b9\u6cd5\uff0c\u8fd8\u4e3a\u8bbe\u8ba1\u6216\u6269\u5c55\u4eba\u7c7b-AI\u5171\u751f\u5173\u7cfb\u63d0\u4f9b\u4e86\u524d\u77bb\u6027\u7684\u53c2\u8003\uff0c\u63a8\u52a8\u4e86\u4eba\u7c7b\u8ba4\u77e5\u548cAI\u80fd\u529b\u7684\u5171\u540c\u8fdb\u5316\u3002"}}
{"id": "2505.14625", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.14625", "abs": "https://arxiv.org/abs/2505.14625", "authors": ["Zhangchen Xu", "Yuetai Li", "Fengqing Jiang", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Radha Poovendran"], "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u9a8c\u8bc1\u5668\u9519\u8bef\u62d2\u7edd\u6b63\u786e\u6a21\u578b\u8f93\u51fa\uff08false negatives\uff09\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684LLM\u9a8c\u8bc1\u5668tinyV\u6765\u7f13\u89e3\u8be5\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u7684\u901a\u8fc7\u7387\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u5956\u52b1\u4fe1\u53f7\u7684\u53ef\u9760\u6027\uff0c\u7136\u800c\u73b0\u6709\u7684\u9a8c\u8bc1\u5668\u5b58\u5728\u5927\u91cf\u9519\u8bef\u62d2\u7edd\u6b63\u786e\u6a21\u578b\u8f93\u51fa\u7684\u73b0\u8c61\uff0c\u8fd9\u4f1a\u4e25\u91cd\u5f71\u54cdRL\u8bad\u7ec3\u7684\u6548\u679c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u89e3\u51b3\u9a8c\u8bc1\u5668\u4e2d\u7684false negatives\u95ee\u9898\u4ee5\u63d0\u5347RL\u8bad\u7ec3\u7684\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u5bf9Big-Math-RL-Verified\u6570\u636e\u96c6\u7684\u6df1\u5165\u5206\u6790\uff0c\u53d1\u73b0\u8d85\u8fc738%\u7684\u6a21\u578b\u751f\u6210\u54cd\u5e94\u53d7\u5230false negatives\u7684\u5f71\u54cd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684LLM\u9a8c\u8bc1\u5668tinyV\uff0c\u5b83\u80fd\u591f\u52a8\u6001\u8bc6\u522b\u6f5c\u5728\u7684false negatives\u5e76\u6062\u590d\u6709\u6548\u7684\u54cd\u5e94\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u5956\u52b1\u4f30\u8ba1\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u96c6\u6210tinyV\u540e\uff0c\u901a\u8fc7\u7387\u63d0\u9ad8\u4e86\u9ad8\u8fbe10%\uff0c\u5e76\u4e14\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u52a0\u901f\u4e86\u6536\u655b\u3002", "conclusion": "\u672c\u6587\u63ed\u793a\u4e86\u9a8c\u8bc1\u5668false negatives\u5bf9RL\u8bad\u7ec3\u7684\u4e25\u91cd\u5371\u5bb3\uff0c\u5e76\u901a\u8fc7\u63d0\u51fatinyV\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u9645\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u6539\u5584\u4e86\u57fa\u4e8eRL\u7684LLM\u5fae\u8c03\u6548\u679c\u3002"}}
{"id": "2505.14552", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.14552", "abs": "https://arxiv.org/abs/2505.14552", "authors": ["Jiajun Shi", "Jian Yang", "Jiaheng Liu", "Xingyuan Bu", "Jiangjie Chen", "Junting Zhou", "Kaijing Ma", "Zhoufutu Wen", "Bingli Wang", "Yancheng He", "Liang Song", "Hualei Zhu", "Shilong Li", "Xingjian Wang", "Wei Zhang", "Ruibin Yuan", "Yifan Yao", "Wenjun Yang", "Yunli Wang", "Siyuan Fang", "Siyu Yuan", "Qianyu He", "Xiangru Tang", "Yingshui Tan", "Wangchunshu Zhou", "Zhaoxiang Zhang", "Zhoujun Li", "Wenhao Huang", "Ge Zhang"], "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "22 pages", "summary": "Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u6700\u8fd1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\u8868\u660e\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u51c6\u786e\u8bc4\u4f30\u5176\u63a8\u7406\u80fd\u529b\u3002\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u662f\u9886\u57df\u7279\u5b9a\u7684\uff0c\u56e0\u6b64\u65e0\u6cd5\u5b8c\u5168\u6355\u6349LLM\u7684\u4e00\u822c\u63a8\u7406\u6f5c\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u77e5\u8bc6\u6b63\u4ea4\u63a8\u7406\u4f53\u80b2\u9986\uff08KORGym\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u53d7KOR-Bench\u548cGymnasium\u542f\u53d1\u7684\u52a8\u6001\u8bc4\u4f30\u5e73\u53f0\u3002KORGym\u63d0\u4f9b\u4e86\u8d85\u8fc7\u4e94\u5341\u79cd\u6587\u672c\u6216\u89c6\u89c9\u683c\u5f0f\u7684\u6e38\u620f\uff0c\u5e76\u652f\u6301\u5e26\u6709\u5f3a\u5316\u5b66\u4e60\u573a\u666f\u7684\u4ea4\u4e92\u5f0f\u3001\u591a\u8f6e\u8bc4\u4f30\u3002\u901a\u8fc7KORGym\uff0c\u6211\u4eec\u5bf919\u4e2aLLMs\u548c8\u4e2aVLMs\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5bb6\u65cf\u5185\u90e8\u4e00\u81f4\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u5e76\u5c55\u793a\u4e86\u95ed\u6e90\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u80fd\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u7814\u7a76\u4e86\u6a21\u6001\u3001\u63a8\u7406\u7b56\u7565\u3001\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u548c\u54cd\u5e94\u957f\u5ea6\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u6211\u4eec\u671f\u671bKORGym\u6210\u4e3a\u63a8\u52a8LLM\u63a8\u7406\u7814\u7a76\u548c\u5f00\u53d1\u9002\u5408\u590d\u6742\u4ea4\u4e92\u73af\u5883\u7684\u8bc4\u4f30\u65b9\u6cd5\u7684\u5b9d\u8d35\u8d44\u6e90\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u7684\u5e94\u7528\u8d8a\u6765\u8d8a\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u7684\u8bc4\u4f30\u57fa\u51c6\u5f80\u5f80\u5c40\u9650\u4e8e\u7279\u5b9a\u9886\u57df\uff0c\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u6a21\u578b\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u5f00\u53d1\u4e00\u4e2a\u66f4\u52a0\u7efc\u5408\u548c\u7075\u6d3b\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u4ee5\u9002\u5e94\u4e0d\u65ad\u53d1\u5c55\u7684\u6a21\u578b\u9700\u6c42\u3002", "method": "\u7814\u7a76\u8005\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u540d\u4e3aKORGym\u7684\u52a8\u6001\u8bc4\u4f30\u5e73\u53f0\uff0c\u8be5\u5e73\u53f0\u5305\u62ec\u8d85\u8fc7\u4e94\u5341\u79cd\u6e38\u620f\uff0c\u6db5\u76d6\u6587\u672c\u548c\u89c6\u89c9\u5f62\u5f0f\u3002KORGym\u652f\u6301\u4ea4\u4e92\u5f0f\u3001\u591a\u8f6e\u6b21\u8bc4\u4f30\uff0c\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u6a21\u578b\u5206\u6790\u3002\u901a\u8fc7\u5bf919\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u548c8\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u8be5\u5e73\u53f0\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cKORGym\u80fd\u591f\u6709\u6548\u63ed\u793a\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u4e4b\u95f4\u7684\u63a8\u7406\u6a21\u5f0f\u5dee\u5f02\uff0c\u5e76\u8bc1\u660e\u4e86\u95ed\u6e90\u6a21\u578b\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u4f18\u52bf\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u6a21\u6001\u3001\u63a8\u7406\u7b56\u7565\u3001\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u53ca\u54cd\u5e94\u957f\u5ea6\u7b49\u56e0\u7d20\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5177\u4f53\u5f71\u54cd\u3002", "conclusion": "KORGym\u4f5c\u4e3a\u4e00\u4e2a\u65b0\u9896\u4e14\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u5c06\u6709\u52a9\u4e8e\u63a8\u52a8\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u540c\u65f6\u4e3a\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e0b\u7684\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\u3002"}}
{"id": "2505.14264", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.14264", "abs": "https://arxiv.org/abs/2505.14264", "authors": ["Jian Xiong", "Jingbo Zhou", "Jingyong Ye", "Dejing Dou"], "title": "AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum", "categories": ["cs.LG", "cs.CL"], "comment": "14 pages, 7 figures", "summary": "Reinforcement learning (RL) has emerged as an effective approach for\nenhancing the reasoning capabilities of large language models (LLMs),\nespecially in scenarios where supervised fine-tuning (SFT) falls short due to\nlimited chain-of-thought (CoT) data. Among RL-based post-training methods,\ngroup relative advantage estimation, as exemplified by Group Relative Policy\nOptimization (GRPO), has attracted considerable attention for eliminating the\ndependency on the value model, thereby simplifying training compared to\ntraditional approaches like Proximal Policy Optimization (PPO). However, we\nobserve that exsiting group relative advantage estimation method still suffers\nfrom training inefficiencies, particularly when the estimated advantage\napproaches zero. To address this limitation, we propose Advantage-Augmented\nPolicy Optimization (AAPO), a novel RL algorithm that optimizes the\ncross-entropy (CE) loss using advantages enhanced through a momentum-based\nestimation scheme. This approach effectively mitigates the inefficiencies\nassociated with group relative advantage estimation. Experimental results on\nmultiple mathematical reasoning benchmarks demonstrate the superior performance\nof AAPO.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5AAPO\uff0c\u7528\u4e8e\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u5728\u94fe\u5f0f\u601d\u7ef4\u6570\u636e\u6709\u9650\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u7fa4\u4f53\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u8bad\u7ec3\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u4f30\u8ba1\u7684\u4f18\u52bf\u63a5\u8fd1\u96f6\u65f6\u3002\u4e3a\u4e86\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u5e76\u7b80\u5316\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u9700\u8981\u4e00\u79cd\u6539\u8fdb\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Advantage-Augmented Policy Optimization (AAPO)\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u52a8\u91cf\u4f30\u8ba1\u65b9\u6848\u589e\u5f3a\u4f18\u52bf\u6765\u4f18\u5316\u4ea4\u53c9\u71b5\u635f\u5931\u7684\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u4e0e\u7fa4\u4f53\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\u76f8\u5173\u7684\u6548\u7387\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAAPO\u7684\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "AAPO\u4f5c\u4e3a\u4e00\u79cd\u65b0\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u8bad\u7ec3\u6548\u7387\u95ee\u9898\uff0c\u5e76\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5c55\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2505.14185", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.14185", "abs": "https://arxiv.org/abs/2505.14185", "authors": ["Kaustubh Ponkshe", "Shaan Shah", "Raghav Singhal", "Praneeth Vepakomma"], "title": "Safety Subspaces are Not Distinct: A Fine-Tuning Case Study", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Kaustubh Ponkshe, Shaan Shah, and Raghav Singhal contributed equally\n  to this work", "summary": "Large Language Models (LLMs) rely on safety alignment to produce socially\nacceptable responses. This is typically achieved through instruction tuning and\nreinforcement learning from human feedback. However, this alignment is known to\nbe brittle: further fine-tuning, even on benign or lightly contaminated data,\ncan degrade safety and reintroduce harmful behaviors. A growing body of work\nsuggests that alignment may correspond to identifiable geometric directions in\nweight space, forming subspaces that could, in principle, be isolated or\npreserved to defend against misalignment. In this work, we conduct a\ncomprehensive empirical study of this geometric perspective. We examine whether\nsafety-relevant behavior is concentrated in specific subspaces, whether it can\nbe separated from general-purpose learning, and whether harmfulness arises from\ndistinguishable patterns in internal representations. Across both parameter and\nactivation space, our findings are consistent: subspaces that amplify safe\nbehaviors also amplify unsafe ones, and prompts with different safety\nimplications activate overlapping representations. We find no evidence of a\nsubspace that selectively governs safety. These results challenge the\nassumption that alignment is geometrically localized. Rather than residing in\ndistinct directions, safety appears to emerge from entangled, high-impact\ncomponents of the model's broader learning dynamics. This suggests that\nsubspace-based defenses may face fundamental limitations and underscores the\nneed for alternative strategies to preserve alignment under continued training.\nWe corroborate these findings through multiple experiments on five open-source\nLLMs. Our code is publicly available at:\nhttps://github.com/CERT-Lab/safety-subspaces.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u4e0e\u5b89\u5168\u6027\u76f8\u5173\u7684\u884c\u4e3a\u662f\u5426\u96c6\u4e2d\u5728\u7279\u5b9a\u7684\u5b50\u7a7a\u95f4\u5185\uff0c\u5e76\u53d1\u73b0\u5b89\u5168\u6027\u884c\u4e3a\u548c\u4e0d\u5b89\u5168\u6027\u884c\u4e3a\u5728\u540c\u4e00\u5b50\u7a7a\u95f4\u5185\u88ab\u653e\u5927\uff0c\u4e14\u4e0d\u540c\u5b89\u5168\u542b\u4e49\u7684\u63d0\u793a\u6fc0\u6d3b\u4e86\u91cd\u53e0\u7684\u8868\u793a\u3002\u8fd9\u8868\u660e\u5bf9\u9f50\u5e76\u975e\u51e0\u4f55\u4e0a\u5c40\u90e8\u5316\uff0c\u800c\u662f\u7531\u6a21\u578b\u66f4\u5e7f\u6cdb\u5b66\u4e60\u52a8\u6001\u4e2d\u7684\u7ea0\u7f20\u3001\u9ad8\u5f71\u54cd\u7ec4\u4ef6\u6240\u4ea7\u751f\u3002\u56e0\u6b64\uff0c\u57fa\u4e8e\u5b50\u7a7a\u95f4\u7684\u9632\u5fa1\u53ef\u80fd\u9762\u4e34\u6839\u672c\u9650\u5236\uff0c\u9700\u8981\u63a2\u7d22\u66ff\u4ee3\u7b56\u7565\u6765\u5728\u6301\u7eed\u8bad\u7ec3\u4e2d\u4fdd\u6301\u5bf9\u9f50\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u5bf9\u9f50\u901a\u5e38\u901a\u8fc7\u6307\u4ee4\u5fae\u8c03\u548c\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\uff0c\u4f46\u8fd9\u79cd\u5bf9\u9f50\u5bb9\u6613\u53d7\u5230\u8fdb\u4e00\u6b65\u5fae\u8c03\u7684\u5f71\u54cd\uff0c\u751a\u81f3\u53ef\u80fd\u5bfc\u81f4\u6709\u5bb3\u884c\u4e3a\u7684\u91cd\u65b0\u51fa\u73b0\u3002\u5df2\u6709\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u9f50\u53ef\u80fd\u5bf9\u5e94\u4e8e\u6743\u91cd\u7a7a\u95f4\u4e2d\u7684\u53ef\u8bc6\u522b\u51e0\u4f55\u65b9\u5411\uff0c\u5f62\u6210\u53ef\u4ee5\u9694\u79bb\u6216\u4fdd\u62a4\u4ee5\u9632\u6b62\u9519\u4f4d\u7684\u5b50\u7a7a\u95f4\u3002\u672c\u6587\u65e8\u5728\u4ece\u51e0\u4f55\u89c6\u89d2\u5168\u9762\u7814\u7a76\u8fd9\u4e9b\u5047\u8bbe\u7684\u6709\u6548\u6027\u3002", "method": "\u7814\u7a76\u8005\u901a\u8fc7\u591a\u4e2a\u5b9e\u9a8c\u5206\u6790\u4e86\u53c2\u6570\u7a7a\u95f4\u548c\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u5b50\u7a7a\u95f4\u7279\u6027\uff0c\u6d4b\u8bd5\u4e86\u5b89\u5168\u6027\u76f8\u5173\u884c\u4e3a\u662f\u5426\u96c6\u4e2d\u4e8e\u7279\u5b9a\u5b50\u7a7a\u95f4\uff0c\u4ee5\u53ca\u662f\u5426\u6709\u53ef\u533a\u5206\u7684\u5185\u90e8\u8868\u793a\u6a21\u5f0f\u5bfc\u81f4\u6709\u5bb3\u884c\u4e3a\u3002\u5b9e\u9a8c\u6d89\u53ca\u4e94\u79cd\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u653e\u5927\u5b89\u5168\u884c\u4e3a\u7684\u5b50\u7a7a\u95f4\u540c\u65f6\u4e5f\u653e\u5927\u4e86\u4e0d\u5b89\u5168\u884c\u4e3a\uff0c\u4e14\u5177\u6709\u4e0d\u540c\u5b89\u5168\u542b\u4e49\u7684\u63d0\u793a\u6fc0\u6d3b\u4e86\u91cd\u53e0\u7684\u8868\u793a\u3002\u6ca1\u6709\u53d1\u73b0\u9009\u62e9\u6027\u63a7\u5236\u5b89\u5168\u6027\u7684\u5b50\u7a7a\u95f4\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86\u51e0\u4f55\u5c40\u90e8\u5316\u5bf9\u9f50\u7684\u5047\u8bbe\uff0c\u6307\u51fa\u5b89\u5168\u6027\u662f\u7531\u6a21\u578b\u66f4\u5e7f\u6cdb\u5b66\u4e60\u52a8\u6001\u4e2d\u7684\u7ea0\u7f20\u7ec4\u4ef6\u4ea7\u751f\u7684\u3002\u57fa\u4e8e\u5b50\u7a7a\u95f4\u7684\u9632\u5fa1\u53ef\u80fd\u9762\u4e34\u57fa\u672c\u9650\u5236\uff0c\u9700\u8981\u63a2\u7d22\u5176\u4ed6\u65b9\u6cd5\u6765\u5728\u6301\u7eed\u8bad\u7ec3\u4e2d\u4fdd\u6301\u5bf9\u9f50\u3002"}}
{"id": "2505.13697", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.13697", "abs": "https://arxiv.org/abs/2505.13697", "authors": ["Soumya Rani Samineni", "Durgesh Kalwar", "Karthik Valmeekam", "Kaya Stechly", "Subbarao Kambhampati"], "title": "RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning-based post-training of large language models (LLMs)\nhas recently gained attention, particularly following the release of DeepSeek\nR1, which applied GRPO for fine-tuning. Amid the growing hype around improved\nreasoning abilities attributed to RL post-training, we critically examine the\nformulation and assumptions underlying these methods. We start by highlighting\nthe popular structural assumptions made in modeling LLM training as a Markov\nDecision Process (MDP), and show how they lead to a degenerate MDP that doesn't\nquite need the RL/GRPO apparatus. The two critical structural assumptions\ninclude (1) making the MDP states be just a concatenation of the actions-with\nstates becoming the context window and the actions becoming the tokens in LLMs\nand (2) splitting the reward of a state-action trajectory uniformly across the\ntrajectory. Through a comprehensive analysis, we demonstrate that these\nsimplifying assumptions make the approach effectively equivalent to an\noutcome-driven supervised learning. Our experiments on benchmarks including\nGSM8K and Countdown using Qwen-2.5 base models show that iterative supervised\nfine-tuning, incorporating both positive and negative samples, achieves\nperformance comparable to GRPO-based training. We will also argue that the\nstructural assumptions indirectly incentivize the RL to generate longer\nsequences of intermediate tokens-which in turn feeds into the narrative of \"RL\ngenerating longer thinking traces.\" While RL may well be a very useful\ntechnique for improving the reasoning abilities of LLMs, our analysis shows\nthat the simplistic structural assumptions made in modeling the underlying MDP\nrender the popular LLM RL frameworks and their interpretations questionable.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u8fd1\u671f\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u540e\u8bad\u7ec3\u65b9\u6cd5\u53d7\u5230\u5173\u6ce8\uff0c\u672c\u6587\u901a\u8fc7\u5206\u6790RL/GRPO\u65b9\u6cd5\u80cc\u540e\u7684\u5047\u8bbe\u548c\u516c\u5f0f\u5316\u95ee\u9898\uff0c\u6307\u51fa\u5176\u5728\u7279\u5b9a\u5047\u8bbe\u4e0b\u7b49\u4ef7\u4e8e\u76d1\u7763\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\u8fed\u4ee3\u76d1\u7763\u5fae\u8c03\u53ef\u8fbe\u5230\u4e0eGRPO\u7c7b\u4f3c\u7684\u6548\u679c\u3002\u867d\u7136RL\u53ef\u80fd\u5bf9\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u6709\u7528\uff0c\u4f46\u5f53\u524d\u6d41\u884c\u7684\u5efa\u6a21\u65b9\u5f0f\u5b58\u5728\u7f3a\u9677\u3002", "motivation": "\u968f\u7740DeepSeek R1\u7b49\u5e94\u7528\u4e86GRPO\u65b9\u6cd5\u7684LLM\u53d1\u5e03\uff0c\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u65b9\u6cd5\u56e0\u88ab\u8ba4\u4e3a\u80fd\u663e\u8457\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u800c\u5907\u53d7\u5173\u6ce8\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5c06LLM\u8bad\u7ec3\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u65f6\u6240\u4f5c\u7684\u7b80\u5316\u5047\u8bbe\u5c1a\u672a\u88ab\u5145\u5206\u5ba1\u89c6\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u6df1\u5165\u63a2\u8ba8\u8fd9\u4e9b\u5047\u8bbe\u7684\u5f71\u54cd\u53ca\u5176\u5408\u7406\u6027\u3002", "method": "\u6587\u7ae0\u9996\u5148\u6307\u51fa\u4e86\u5f53\u524d\u6d41\u884c\u7684\u4e24\u79cd\u5173\u952e\u7ed3\u6784\u5047\u8bbe\uff1a(1) \u5c06MDP\u72b6\u6001\u5b9a\u4e49\u4e3a\u52a8\u4f5c\u7684\u7b80\u5355\u62fc\u63a5\uff0c\u5373\u72b6\u6001\u5bf9\u5e94\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u52a8\u4f5c\u4e3aLLM\u4e2d\u7684token\uff1b(2) \u5c06\u8f68\u8ff9\u5956\u52b1\u5747\u5300\u5206\u914d\u5230\u6bcf\u4e2a\u6b65\u9aa4\u3002\u968f\u540e\uff0c\u4f5c\u8005\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\uff0c\u5728\u8fd9\u4e9b\u5047\u8bbe\u4e0b\uff0c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b9e\u9645\u4e0a\u9000\u5316\u4e3a\u4e00\u79cd\u7ed3\u679c\u9a71\u52a8\u7684\u76d1\u7763\u5b66\u4e60\u3002\u63a5\u7740\uff0c\u4f5c\u8005\u901a\u8fc7\u5728GSM8K\u548cCountdown\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f7f\u7528Qwen-2.5\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u8fed\u4ee3\u76d1\u7763\u5fae\u8c03\uff08\u7ed3\u5408\u6b63\u8d1f\u6837\u672c\uff09\u4e0eGRPO\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fed\u4ee3\u76d1\u7763\u5fae\u8c03\u53ef\u4ee5\u5b9e\u73b0\u4e0eGRPO\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u6307\u51fa\u8fd9\u4e9b\u7ed3\u6784\u5047\u8bbe\u95f4\u63a5\u9f13\u52b1\u751f\u6210\u66f4\u957f\u7684\u4e2d\u95f4token\u5e8f\u5217\uff0c\u4ece\u800c\u652f\u6301\u4e86\u201cRL\u751f\u6210\u66f4\u957f\u601d\u8003\u75d5\u8ff9\u201d\u7684\u53d9\u8ff0\u3002\u8fd9\u8868\u660e\u5f53\u524d\u7684RL\u6846\u67b6\u53ef\u80fd\u5b58\u5728\u89e3\u91ca\u4e0a\u7684\u504f\u5dee\u3002", "conclusion": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u53ef\u80fd\u662f\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u5de5\u5177\uff0c\u4f46\u672c\u6587\u7684\u7814\u7a76\u8868\u660e\uff0c\u5f53\u524d\u6d41\u884c\u7684\u7b80\u5316\u7ed3\u6784\u5047\u8bbe\u4f7f\u5f97RL\u65b9\u6cd5\u7684\u5b9e\u9645\u6548\u679c\u4e0e\u76d1\u7763\u5b66\u4e60\u76f8\u4f3c\uff0c\u4e14\u5176\u89e3\u91ca\u53ef\u80fd\u5b58\u5728\u8bef\u5bfc\u6027\u3002\u56e0\u6b64\uff0c\u672a\u6765\u7814\u7a76\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u8fd9\u4e9b\u5047\u8bbe\u5e76\u63a2\u7d22\u66f4\u5408\u7406\u7684\u5efa\u6a21\u65b9\u5f0f\u3002"}}
{"id": "2505.13638", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.13638", "abs": "https://arxiv.org/abs/2505.13638", "authors": ["Massimo Fioravanti", "Giovanni Agosta"], "title": "4Hammer: a board-game reinforcement learning environment for the hour long time frame", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong performance on tasks\nwith short time frames, but struggle with tasks requiring longer durations.\nWhile datasets covering extended-duration tasks, such as software engineering\ntasks or video games, do exist, there are currently few implementations of\ncomplex board games specifically designed for reinforcement learning and LLM\nevaluation. To address this gap, we propose the 4Hammer reinforcement learning\nenvironment, a digital twin simulation of a subset of Warhammer 40,000-a\ncomplex, zero-sum board game. Warhammer 40,000 features intricate rules,\nrequiring human players to thoroughly read and understand over 50 pages of\ndetailed natural language rules, grasp the interactions between their game\npieces and those of their opponents, and independently track and communicate\nthe evolving game state.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a4Hammer\u7684\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u68cb\u76d8\u6e38\u620f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u77ed\u671f\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u8f83\u957f\u65f6\u95f4\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u590d\u6742\u68cb\u76d8\u6e38\u620f\u8bbe\u8ba1\u7684\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u4ee5\u8bc4\u4f30LLMs\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u521b\u5efa4Hammer\u8fd9\u4e00\u57fa\u4e8eWarhammer 40,000\u5b50\u96c6\u7684\u6570\u5b57\u5b6a\u751f\u6a21\u62df\u73af\u5883\uff0c\u8be5\u73af\u5883\u5177\u6709\u590d\u6742\u7684\u89c4\u5219\u548c\u96f6\u548c\u535a\u5f08\u7279\u6027\uff0c\u8981\u6c42\u73a9\u5bb6\u7406\u89e3\u8be6\u7ec6\u89c4\u5219\u3001\u638c\u63e1\u6e38\u620f\u5355\u4f4d\u95f4\u7684\u4e92\u52a8\u5e76\u8ddf\u8e2a\u6e38\u620f\u72b6\u6001\u3002", "result": "\u5c1a\u672a\u660e\u786e\u7ed9\u51fa\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u6b64\u73af\u5883\u7684\u8bbe\u8ba1\u4e3a\u672a\u6765\u8bc4\u4f30LLMs\u548c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u4e86\u65b0\u5e73\u53f0\u3002", "conclusion": "4Hammer\u4f5c\u4e3a\u4e00\u6b3e\u4e13\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e0eLLM\u8bc4\u4f30\u8bbe\u8ba1\u7684\u590d\u6742\u68cb\u76d8\u6e38\u620f\u73af\u5883\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u7684\u7a7a\u767d\uff0c\u5e76\u4e3a\u957f\u671f\u4efb\u52a1\u7684\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2505.13508", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.13508", "abs": "https://arxiv.org/abs/2505.13508", "authors": ["Zijia Liu", "Peixuan Han", "Haofei Yu", "Haoru Li", "Jiaxuan You"], "title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate impressive capabilities but lack\nrobust temporal intelligence, struggling to integrate reasoning about the past\nwith predictions and plausible generations of the future. Meanwhile, existing\nmethods typically target isolated temporal skills, such as question answering\nabout past events or basic forecasting, and exhibit poor generalization,\nparticularly when dealing with events beyond their knowledge cutoff or\nrequiring creative foresight. To address these limitations, we introduce\n\\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter)\nLLM with comprehensive temporal abilities: understanding, prediction, and\ncreative generation. Our approach features a novel three-stage development\npath; the first two constitute a \\textit{reinforcement learning (RL)\ncurriculum} driven by a meticulously designed dynamic rule-based reward system.\nThis framework progressively builds (1) foundational temporal understanding and\nlogical event-time mappings from historical data, (2) future event prediction\nskills for events beyond its knowledge cutoff, and finally (3) enables\nremarkable generalization to creative future scenario generation without any\nfine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms\nmodels over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,\non highly challenging future event prediction and creative scenario generation\nbenchmarks. This work provides strong evidence that thoughtfully engineered,\nprogressive RL fine-tuning allows smaller, efficient models to achieve superior\ntemporal performance, offering a practical and scalable path towards truly\ntime-aware AI. To foster further research, we also release \\textit{Time-Bench},\na large-scale multi-task temporal reasoning dataset derived from 10 years of\nnews data, and our series of \\textit{Time-R1} checkpoints.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aTime-R1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8d4b\u4e88\u4e2d\u7b49\u89c4\u6a21\uff083B\u53c2\u6570\uff09\u7684\u8bed\u8a00\u6a21\u578b\u5168\u9762\u7684\u65f6\u95f4\u5904\u7406\u80fd\u529b\uff0c\u5305\u62ec\u7406\u89e3\u3001\u9884\u6d4b\u548c\u521b\u9020\u6027\u751f\u6210\u3002\u8be5\u6846\u67b6\u5206\u4e09\u4e2a\u9636\u6bb5\u5f00\u53d1\uff0c\u524d\u4e24\u4e2a\u9636\u6bb5\u57fa\u4e8e\u52a8\u6001\u89c4\u5219\u5956\u52b1\u7cfb\u7edf\u6784\u5efa\u65f6\u95f4\u7406\u89e3\u548c\u672a\u6765\u4e8b\u4ef6\u9884\u6d4b\u80fd\u529b\uff0c\u7b2c\u4e09\u9636\u6bb5\u5b9e\u73b0\u5bf9\u672a\u6765\u573a\u666f\u7684\u521b\u9020\u6027\u751f\u6210\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTime-R1\u5728\u590d\u6742\u672a\u6765\u4e8b\u4ef6\u9884\u6d4b\u548c\u521b\u9020\u6027\u573a\u666f\u751f\u6210\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86200\u500d\u5927\u7684\u6a21\u578b\uff0c\u5982671B\u53c2\u6570\u7684DeepSeek-R1\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u53d1\u5e03\u4e86\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u65f6\u95f4\u63a8\u7406\u6570\u636e\u96c6Time-Bench\u53caTime-R1\u7684\u68c0\u67e5\u70b9\uff0c\u4ee5\u63a8\u52a8\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u529f\u80fd\u5f3a\u5927\uff0c\u4f46\u5728\u65f6\u95f4\u667a\u80fd\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u5c06\u5bf9\u8fc7\u53bb\u7684\u63a8\u7406\u4e0e\u672a\u6765\u7684\u9884\u6d4b\u548c\u5408\u7406\u751f\u6210\u7ed3\u5408\u8d77\u6765\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u53ea\u9488\u5bf9\u5b64\u7acb\u7684\u65f6\u95f4\u6280\u80fd\uff0c\u4f8b\u5982\u56de\u7b54\u8fc7\u53bb\u4e8b\u4ef6\u7684\u95ee\u9898\u6216\u57fa\u672c\u9884\u6d4b\uff0c\u4e14\u5728\u5904\u7406\u8d85\u51fa\u77e5\u8bc6\u622a\u6b62\u65e5\u671f\u7684\u4e8b\u4ef6\u6216\u9700\u8981\u521b\u9020\u6027\u9884\u89c1\u7684\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5168\u9762\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u65f6\u95f4\u5904\u7406\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "Time-R1\u6846\u67b6\u91c7\u7528\u4e09\u9636\u6bb5\u5f00\u53d1\u8def\u5f84\uff1a\n1. \u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u5386\u53f2\u6570\u636e\u6784\u5efa\u57fa\u7840\u7684\u65f6\u95f4\u7406\u89e3\u80fd\u529b\u548c\u903b\u8f91\u4e8b\u4ef6-\u65f6\u95f4\u6620\u5c04\u3002\n2. \u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u8bfe\u7a0b\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u52a8\u6001\u89c4\u5219\u5956\u52b1\u7cfb\u7edf\uff0c\u8bad\u7ec3\u6a21\u578b\u5bf9\u672a\u6765\u4e8b\u4ef6\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u7279\u522b\u662f\u8d85\u51fa\u5176\u77e5\u8bc6\u622a\u6b62\u65e5\u671f\u7684\u4e8b\u4ef6\u3002\n3. \u7b2c\u4e09\u9636\u6bb5\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u5bf9\u672a\u6765\u573a\u666f\u7684\u521b\u9020\u6027\u751f\u6210\u3002\n\u6574\u4e2a\u8fc7\u7a0b\u7531\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\uff0c\u9010\u6b65\u63d0\u5347\u6a21\u578b\u7684\u65f6\u95f4\u5904\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTime-R1\u5728\u9ad8\u5ea6\u590d\u6742\u7684\u672a\u6765\u4e8b\u4ef6\u9884\u6d4b\u548c\u521b\u9020\u6027\u573a\u666f\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u663e\u8457\u4f18\u4e8e\u8d85\u8fc7200\u500d\u5927\u7684\u6a21\u578b\uff0c\u5982671B\u53c2\u6570\u7684DeepSeek-R1\u3002\u8fd9\u8bc1\u660e\u4e86\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u53ef\u4ee5\u4f7f\u8f83\u5c0f\u3001\u9ad8\u6548\u7684\u6a21\u578b\u8fbe\u5230\u4f18\u8d8a\u7684\u65f6\u95f4\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u8f83\u5c0f\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u6bd4\u5927\u89c4\u6a21\u6a21\u578b\u66f4\u4f18\u8d8a\u7684\u65f6\u95f4\u5904\u7406\u80fd\u529b\u3002Time-R1\u4e3a\u771f\u6b63\u5177\u5907\u65f6\u95f4\u611f\u77e5\u80fd\u529b\u7684AI\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002\u4e3a\u4e86\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u4f5c\u8005\u53d1\u5e03\u4e86\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u65f6\u95f4\u63a8\u7406\u6570\u636e\u96c6Time-Bench\u4ee5\u53caTime-R1\u7684\u4e00\u7cfb\u5217\u68c0\u67e5\u70b9\u3002"}}
{"id": "2505.13438", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.13438", "abs": "https://arxiv.org/abs/2505.13438", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cAnytimeReasoner\u901a\u8fc7\u4f18\u5316\u4efb\u610f\u65f6\u523b\u63a8\u7406\u6027\u80fd\u548c\u5f15\u5165\u65b0\u7684\u65b9\u5dee\u7f29\u51cf\u6280\u672fBRPO\uff0c\u5728\u4e0d\u540c\u5148\u9a8c\u5206\u5e03\u4e0b\u6240\u6709\u63a8\u7406\u9884\u7b97\u8303\u56f4\u5185\u5747\u4f18\u4e8eGRPO\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u6807\u8bb0\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u6700\u5927\u5316\u63a8\u7406\u8fc7\u7a0b\u7ed3\u675f\u65f6\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u4ec5\u9488\u5bf9\u5927\u4e14\u56fa\u5b9a\u7684\u6807\u8bb0\u9884\u7b97\u4e0b\u7684\u6700\u7ec8\u6027\u80fd\u8fdb\u884c\u4f18\u5316\uff0c\u5bfc\u81f4\u8bad\u7ec3\u548c\u90e8\u7f72\u6548\u7387\u4f4e\u4e0b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u5347\u6807\u8bb0\u6548\u7387\u5e76\u9002\u5e94\u4e0d\u540c\u6807\u8bb0\u9884\u7b97\u7ea6\u675f\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAnytimeReasoner\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u622a\u65ad\u5b8c\u6574\u7684\u601d\u8003\u8fc7\u7a0b\u4ee5\u9002\u5e94\u4ece\u5148\u9a8c\u5206\u5e03\u91c7\u6837\u7684\u6807\u8bb0\u9884\u7b97\uff0c\u5e76\u4fc3\u4f7f\u6a21\u578b\u4e3a\u6bcf\u4e2a\u622a\u65ad\u7684\u601d\u8003\u751f\u6210\u6700\u4f18\u7b54\u6848\u6458\u8981\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4ece\u800c\u5f15\u5165\u5bc6\u96c6\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u65b9\u5dee\u7f29\u51cf\u6280\u672fBudget Relative Policy Optimization (BRPO)\uff0c\u7528\u4e8e\u589e\u5f3a\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u52a0\u5f3a\u601d\u8003\u7b56\u7565\u65f6\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5404\u79cd\u5148\u9a8c\u5206\u5e03\u548c\u6240\u6709\u601d\u8003\u9884\u7b97\u8303\u56f4\u5185\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8eGRPO\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u8bad\u7ec3\u548c\u6807\u8bb0\u6548\u7387\u3002", "conclusion": "AnytimeReasoner\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u4efb\u610f\u65f6\u523b\u63a8\u7406\u6027\u80fd\u548c\u5f15\u5165BRPO\u6280\u672f\uff0c\u6210\u529f\u63d0\u5347\u4e86\u6807\u8bb0\u6548\u7387\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u4e3a\u66f4\u9ad8\u6548\u5730\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u9884\u7b97\u7ea6\u675f\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.13026", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.13026", "abs": "https://arxiv.org/abs/2505.13026", "authors": ["Jack Chen", "Fazhong Liu", "Naruto Liu", "Yuhan Luo", "Erqu Qin", "Harry Zheng", "Tian Dong", "Haojin Zhu", "Yan Meng", "Xiao Wang"], "title": "Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel at mathematical reasoning and logical\nproblem-solving. The current popular training paradigms primarily use\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the\nmodels' reasoning abilities. However, when using SFT or RL alone, there are\nrespective challenges: SFT may suffer from overfitting, while RL is prone to\nmode collapse. The state-of-the-art methods have proposed hybrid training\nschemes. However, static switching faces challenges such as poor generalization\nacross different tasks and high dependence on data quality. In response to\nthese challenges, inspired by the curriculum learning-quiz mechanism in human\nreasoning cultivation, We propose SASR, a step-wise adaptive hybrid training\nframework that theoretically unifies SFT and RL and dynamically balances the\ntwo throughout optimization. SASR uses SFT for initial warm-up to establish\nbasic reasoning skills, and then uses an adaptive dynamic adjustment algorithm\nbased on gradient norm and divergence relative to the original distribution to\nseamlessly integrate SFT with the online RL method GRPO. By monitoring the\ntraining status of LLMs and adjusting the training process in sequence, SASR\nensures a smooth transition between training schemes, maintaining core\nreasoning abilities while exploring different paths. Experimental results\ndemonstrate that SASR outperforms SFT, RL, and static hybrid training methods.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSASR\u7684\u9010\u6b65\u81ea\u9002\u5e94\u6df7\u5408\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\uff0c\u89e3\u51b3\u4e86\u5355\u72ec\u4f7f\u7528SFT\u6216RL\u65f6\u9762\u4e34\u7684\u8fc7\u62df\u5408\u548c\u6a21\u5f0f\u574d\u584c\u95ee\u9898\u3002SASR\u901a\u8fc7\u68af\u5ea6\u8303\u6570\u548c\u5206\u5e03\u504f\u79bb\u7684\u81ea\u9002\u5e94\u52a8\u6001\u8c03\u6574\u7b97\u6cd5\uff0c\u5c06SFT\u4e0e\u5728\u7ebfRL\u65b9\u6cd5GRPO\u65e0\u7f1d\u96c6\u6210\uff0c\u4ece\u800c\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u5b9e\u73b0\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u51cf\u5c11\u5bf9\u6570\u636e\u8d28\u91cf\u7684\u4f9d\u8d56\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSASR\u5728\u6570\u5b66\u63a8\u7406\u548c\u903b\u8f91\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u4f18\u4e8eSFT\u3001RL\u548c\u9759\u6001\u6df7\u5408\u8bad\u7ec3\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u901a\u8fc7\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u6765\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5355\u72ec\u4f7f\u7528\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u5b58\u5728\u7f3a\u9677\uff1aSFT\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u800cRL\u5bb9\u6613\u51fa\u73b0\u6a21\u5f0f\u574d\u584c\u3002\u73b0\u6709\u7684\u9759\u6001\u6df7\u5408\u8bad\u7ec3\u65b9\u6848\u867d\u7136\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u6570\u636e\u8d28\u91cf\u7684\u4f9d\u8d56\u6027\u4e0a\u4ecd\u9762\u4e34\u6311\u6218\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u5e73\u8861SFT\u548cRL\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u4ee5\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u5e76\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "SASR\u6846\u67b6\u9996\u5148\u5229\u7528SFT\u8fdb\u884c\u9884\u70ed\uff0c\u5efa\u7acb\u57fa\u672c\u7684\u63a8\u7406\u6280\u80fd\uff0c\u7136\u540e\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u8303\u6570\u548c\u5206\u5e03\u504f\u79bb\u7684\u81ea\u9002\u5e94\u52a8\u6001\u8c03\u6574\u7b97\u6cd5\uff0c\u5c06SFT\u4e0e\u5728\u7ebfRL\u65b9\u6cd5GRPO\u65e0\u7f1d\u7ed3\u5408\u3002\u5177\u4f53\u800c\u8a00\uff0cSASR\u4f1a\u6839\u636e\u8bad\u7ec3\u72b6\u6001\u76d1\u6d4b\u7ed3\u679c\uff0c\u52a8\u6001\u8c03\u6574SFT\u548cRL\u4e4b\u95f4\u7684\u6743\u91cd\uff0c\u786e\u4fdd\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5e73\u6ed1\u8fc7\u6e21\uff0c\u540c\u65f6\u7ef4\u6301\u6838\u5fc3\u63a8\u7406\u80fd\u529b\u5e76\u63a2\u7d22\u4e0d\u540c\u7684\u4f18\u5316\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSASR\u5728\u6570\u5b66\u63a8\u7406\u548c\u903b\u8f91\u95ee\u9898\u89e3\u51b3\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528SFT\u3001RL\u4ee5\u53ca\u9759\u6001\u6df7\u5408\u8bad\u7ec3\u65b9\u6cd5\u3002SASR\u4e0d\u4ec5\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8fd8\u51cf\u5c11\u4e86\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u7a33\u5b9a\u7684\u65b9\u6cd5\u3002", "conclusion": "SASR\u4f5c\u4e3a\u4e00\u79cd\u9010\u6b65\u81ea\u9002\u5e94\u6df7\u5408\u8bad\u7ec3\u6846\u67b6\uff0c\u6210\u529f\u5730\u7edf\u4e00\u4e86SFT\u548cRL\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u5e73\u8861\u4e24\u79cd\u65b9\u6cd5\u5728\u6574\u4e2a\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002\u76f8\u6bd4\u73b0\u6709\u7684\u9759\u6001\u6df7\u5408\u8bad\u7ec3\u65b9\u6848\uff0cSASR\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u964d\u4f4e\u4e86\u5bf9\u6570\u636e\u8d28\u91cf\u7684\u654f\u611f\u6027\uff0c\u4e3a\u672a\u6765\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u601d\u8def\u3002"}}
{"id": "2505.12951", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.12951", "abs": "https://arxiv.org/abs/2505.12951", "authors": ["Xuerui Su", "Liya Guo", "Yue Wang", "Yi Zhu", "Zhiming Ma", "Zun Wang", "Yuting Liu"], "title": "DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Inference scaling further accelerates Large Language Models (LLMs) toward\nArtificial General Intelligence (AGI), with large-scale Reinforcement Learning\n(RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning\napproaches usually rely on handcrafted rule-based reward functions. However,\nthe tarde-offs of exploration and exploitation in RL algorithms involves\nmultiple complex considerations, and the theoretical and empirical impacts of\nmanually designed reward functions remain insufficiently explored. In this\npaper, we propose Decoupled Group Reward Optimization (DGRO), a general RL\nalgorithm for LLM reasoning. On the one hand, DGRO decouples the traditional\nregularization coefficient into two independent hyperparameters: one scales the\npolicy gradient term, and the other regulates the distance from the sampling\npolicy. This decoupling not only enables precise control over balancing\nexploration and exploitation, but also can be seamlessly extended to Online\nPolicy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward\nOptimization. On the other hand, we observe that reward variance significantly\naffects both convergence speed and final model performance. We conduct both\ntheoretical analysis and extensive empirical validation to assess DGRO,\nincluding a detailed ablation study that investigates its performance and\noptimization dynamics. Experimental results show that DGRO achieves\nstate-of-the-art performance on the Logic dataset with an average accuracy of\n96.9\\%, and demonstrates strong generalization across mathematical benchmarks.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5Decoupled Group Reward Optimization (DGRO)\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u4efb\u52a1\u3002\u901a\u8fc7\u89e3\u8026\u4f20\u7edf\u7684\u6b63\u5219\u5316\u7cfb\u6570\uff0cDGRO\u53ef\u4ee5\u7cbe\u786e\u63a7\u5236\u63a2\u7d22\u4e0e\u5229\u7528\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u5e76\u5728\u903b\u8f91\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8696.9%\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u624b\u5de5\u8bbe\u8ba1\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u4e0e\u5229\u7528\u6743\u8861\u65b9\u9762\u5b58\u5728\u590d\u6742\u6027\uff0c\u4e14\u7406\u8bba\u548c\u7ecf\u9a8c\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6765\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "DGRO\u7b97\u6cd5\u5c06\u4f20\u7edf\u7684\u6b63\u5219\u5316\u7cfb\u6570\u5206\u89e3\u4e3a\u4e24\u4e2a\u72ec\u7acb\u7684\u8d85\u53c2\u6570\uff1a\u4e00\u4e2a\u7528\u4e8e\u7f29\u653e\u7b56\u7565\u68af\u5ea6\u9879\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u8c03\u8282\u91c7\u6837\u7b56\u7565\u7684\u8ddd\u79bb\u3002\u6b64\u5916\uff0c\u8be5\u7b97\u6cd5\u8fd8\u7814\u7a76\u4e86\u5956\u52b1\u65b9\u5dee\u5bf9\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDGRO\u5728\u903b\u8f91\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8696.9%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u5e76\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DGRO\u7b97\u6cd5\u4e0d\u4ec5\u80fd\u591f\u7cbe\u786e\u63a7\u5236\u63a2\u7d22\u4e0e\u5229\u7528\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u8fd8\u53ef\u4ee5\u65e0\u7f1d\u6269\u5c55\u5230\u5728\u7ebf\u7b56\u7565\u955c\u50cf\u4e0b\u964d\u7b97\u6cd5\u548c\u76f4\u63a5\u5956\u52b1\u4f18\u5316\u3002\u8fd9\u4f7f\u5f97DGRO\u6210\u4e3a\u4e00\u79cd\u9002\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4efb\u52a1\u7684\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002"}}
{"id": "2410.15466", "keyword": "LLM reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing.", "keywords": ["LLM reasoning"], "AI": {"tldr": "\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\uff0c\u901a\u8fc7\u91cd\u590d\u91c7\u6837\u6765\u6269\u5c55\u63a8\u7406\u8ba1\u7b97\uff0c\u53ef\u4ee5\u968f\u7740\u6837\u672c\u6570\u91cf\u7684\u589e\u52a0\u800c\u6301\u7eed\u63d0\u9ad8\u8986\u76d6\u7387\uff08\u89e3\u51b3\u95ee\u9898\u7684\u6bd4\u4f8b\uff09\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u6539\u8fdb\u90e8\u5206\u53ef\u80fd\u662f\u56e0\u4e3a\u6807\u51c6\u8bc4\u4f30\u57fa\u51c6\u4e2d\u7684\u7b54\u6848\u5206\u5e03\u504f\u5411\u4e8e\u5c11\u91cf\u5e38\u89c1\u7b54\u6848\u3002\u7814\u7a76\u901a\u8fc7\u5b9a\u4e49\u4e00\u4e2a\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\uff0c\u53d1\u73b0\u8be5\u57fa\u7ebf\u5728\u67d0\u4e9bLLMs\u4e0a\u4f18\u4e8e\u91cd\u590d\u91c7\u6837\uff0c\u5e76\u4e14\u5bf9\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5176\u8868\u73b0\u4e0e\u4e00\u79cd\u6df7\u5408\u7b56\u7565\u76f8\u5f53\u3002\u8fd9\u4f7f\u5f97\u6211\u4eec\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8861\u91cf\u91cd\u590d\u91c7\u6837\u5728\u63d0\u793a\u65e0\u5173\u731c\u6d4b\u4e4b\u5916\u7684\u6539\u8fdb\u7a0b\u5ea6\u3002", "motivation": "\u867d\u7136\u91cd\u590d\u91c7\u6837\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u88ab\u8bc1\u660e\u80fd\u63d0\u9ad8\u95ee\u9898\u89e3\u51b3\u7684\u8986\u76d6\u7387\uff0c\u4f46\u8fd9\u79cd\u63d0\u5347\u662f\u5426\u5b8c\u5168\u5f52\u56e0\u4e8e\u6a21\u578b\u6027\u80fd\u7684\u589e\u5f3a\u5c1a\u4e0d\u6e05\u695a\u3002\u7531\u4e8e\u8bb8\u591a\u8bc4\u4f30\u57fa\u51c6\u7684\u7b54\u6848\u5206\u5e03\u5b58\u5728\u504f\u5dee\uff0c\u7814\u7a76\u8005\u63a8\u6d4b\u90e8\u5206\u63d0\u5347\u53ef\u80fd\u4ec5\u4ec5\u662f\u7531\u4e8e\u7b54\u6848\u96c6\u4e2d\u4e8e\u5c11\u6570\u5e38\u89c1\u9009\u9879\u3002\u56e0\u6b64\uff0c\u4ed6\u4eec\u5e0c\u671b\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u7b80\u5355\u57fa\u7ebf\uff0c\u6765\u5206\u6790\u91cd\u590d\u91c7\u6837\u7684\u5b9e\u9645\u8d21\u732e\u3002", "method": "\u7814\u7a76\u8005\u5b9a\u4e49\u4e86\u4e00\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6839\u636e\u8bad\u7ec3\u96c6\u4e2d\u7b54\u6848\u7684\u9891\u7387\u679a\u4e3e\u7b54\u6848\u3002\u7136\u540e\uff0c\u4ed6\u4eec\u5728\u4e24\u4e2a\u9886\u57df\uff08\u6570\u5b66\u63a8\u7406\u548c\u4e8b\u5b9e\u77e5\u8bc6\uff09\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u8fd9\u4e00\u57fa\u7ebf\u4e0e\u91cd\u590d\u91c7\u6837\u4ee5\u53ca\u4e00\u79cd\u6df7\u5408\u7b56\u7565\u7684\u8868\u73b0\u3002\u6df7\u5408\u7b56\u7565\u901a\u8fc7\u4ec5\u4f7f\u752810\u4e2a\u6a21\u578b\u6837\u672c\u83b7\u53d6\u7b54\u6848\uff0c\u5e76\u5bf9\u5269\u4f59\u7684\u5c1d\u8bd5\u8fdb\u884c\u679a\u4e3e\u731c\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u67d0\u4e9bLLMs\u4e0a\uff0c\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\u4f18\u4e8e\u91cd\u590d\u91c7\u6837\uff1b\u800c\u5728\u5176\u4ed6\u6a21\u578b\u4e0a\uff0c\u5176\u8868\u73b0\u4e0e\u6df7\u5408\u7b56\u7565\u76f8\u5f53\u3002\u8fd9\u8bf4\u660e\u91cd\u590d\u91c7\u6837\u7684\u6548\u679c\u53ef\u80fd\u5e76\u4e0d\u5982\u9884\u671f\u663e\u8457\uff0c\u90e8\u5206\u63d0\u5347\u53ef\u80fd\u53ea\u662f\u56e0\u4e3a\u7b54\u6848\u5206\u5e03\u7684\u504f\u5dee\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\uff0c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u51c6\u786e\u7684\u65b9\u6cd5\u6765\u8861\u91cf\u91cd\u590d\u91c7\u6837\u5728\u63d0\u793a\u65e0\u5173\u731c\u6d4b\u4e4b\u5916\u5bf9\u8986\u76d6\u7387\u7684\u5b9e\u9645\u6539\u8fdb\u3002\u8fd9\u4e3a\u672a\u6765\u8bc4\u4f30LLMs\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u5de5\u5177\u3002"}}
{"id": "2505.00018", "keyword": "LLM Agent", "pdf": "https://arxiv.org/pdf/2505.00018", "abs": "https://arxiv.org/abs/2505.00018", "authors": ["Ju Wu", "Calvin K. L. Or"], "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability.", "keywords": ["LLM Agent"], "AI": {"tldr": "\u8fd9\u7bc7\u6587\u7ae0\u662f\u4e00\u7bc7\u7acb\u573a\u8bba\u6587\uff0c\u5e7f\u6cdb\u8c03\u67e5\u4e86\u4eba\u7c7b\u4e0eAI\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u6700\u65b0\u7ecf\u9a8c\u53d1\u5c55\u3002\u6587\u7ae0\u6307\u51fa\u76ee\u524d\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u6574\u5408\u8fd9\u4e9b\u591a\u6837\u5316\u7684\u7814\u7a76\uff0c\u7279\u522b\u662f\u9762\u5bf9\u5f00\u653e\u6027\u3001\u590d\u6742\u7684\u4efb\u52a1\u65f6\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u5ff5\u67b6\u6784\uff1a\u7cfb\u7edf\u5730\u5c06\u591a\u667a\u80fd\u4f53\u534f\u8c03\u3001\u77e5\u8bc6\u7ba1\u7406\u3001\u63a7\u5236\u53cd\u9988\u56de\u8def\u548c\u9ad8\u7ea7\u63a7\u5236\u673a\u5236\u76f8\u4e92\u5173\u8054\u3002\u901a\u8fc7\u5c06\u73b0\u6709\u7684\u8d21\u732e\uff08\u4ece\u7b26\u53f7AI\u6280\u672f\u3001\u57fa\u4e8e\u8fde\u63a5\u4e3b\u4e49\u7684\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u5230\u6df7\u5408\u7ec4\u7ec7\u5b9e\u8df5\uff09\u6620\u5c04\u5230\u8be5\u6846\u67b6\uff08\u5c42\u7ea7\u63a2\u7d22-\u5229\u7528\u7f51\u7edc\uff09\uff0c\u8fd9\u79cd\u65b9\u6cd5\u6709\u52a9\u4e8e\u4fee\u6b63\u4f20\u7edf\u65b9\u6cd5\u5e76\u6fc0\u53d1\u878d\u5408\u5b9a\u6027\u548c\u5b9a\u91cf\u8303\u5f0f\u7684\u65b0\u5de5\u4f5c\u3002\u6587\u7ae0\u7ed3\u6784\u7075\u6d3b\uff0c\u53ef\u4ece\u4efb\u4f55\u90e8\u5206\u9605\u8bfb\uff0c\u65e2\u662f\u5bf9\u73b0\u6709\u6280\u672f\u5b9e\u73b0\u7684\u6279\u5224\u6027\u56de\u987e\uff0c\u4e5f\u4e3a\u8bbe\u8ba1\u6216\u6269\u5c55\u4eba\u7c7b-AI\u5171\u751f\u5173\u7cfb\u63d0\u4f9b\u4e86\u524d\u77bb\u6027\u7684\u53c2\u8003\u3002\u8fd9\u4e9b\u89c1\u89e3\u4e3a\u4eba\u7c7b\u8ba4\u77e5\u548cAI\u80fd\u529b\u7684\u66f4\u6df1\u5c42\u6b21\u5171\u540c\u8fdb\u5316\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "motivation": "\u5f53\u524d\u5173\u4e8e\u4eba\u7c7b\u4e0eAI\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u7814\u7a76\u867d\u7136\u5728\u6280\u672f\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u5c31\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u6301\u7eed\u7684\u5dee\u8ddd\u3002\u7279\u522b\u5730\uff0c\u5728\u5904\u7406\u5f00\u653e\u6027\u3001\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u6574\u5408\u5404\u79cd\u4e0d\u540c\u7684\u7814\u7a76\u65b9\u6cd5\u548c\u6280\u672f\u6210\u679c\uff0c\u8fd9\u6210\u4e3a\u8fdb\u4e00\u6b65\u53d1\u5c55\u7684\u969c\u788d\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u7406\u8bba\u6846\u67b6\u4ee5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u5ff5\u67b6\u6784\uff0c\u79f0\u4e3a\u5c42\u7ea7\u63a2\u7d22-\u5229\u7528\u7f51\u7edc\uff08Hierarchical Exploration-Exploitation Net\uff09\u3002\u8be5\u6846\u67b6\u7cfb\u7edf\u5730\u5c06\u591a\u667a\u80fd\u4f53\u534f\u8c03\u3001\u77e5\u8bc6\u7ba1\u7406\u3001\u63a7\u5236\u53cd\u9988\u56de\u8def\u548c\u9ad8\u7ea7\u63a7\u5236\u673a\u5236\u76f8\u7ed3\u5408\u3002\u901a\u8fc7\u5c06\u73b0\u6709\u7814\u7a76\u6210\u679c\uff08\u5305\u62ec\u7b26\u53f7AI\u6280\u672f\u3001\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u4ee5\u53ca\u6df7\u5408\u7ec4\u7ec7\u5b9e\u8df5\uff09\u6620\u5c04\u5230\u8fd9\u4e00\u6846\u67b6\u4e2d\uff0c\u4f5c\u8005\u8bd5\u56fe\u63d0\u4f9b\u4e00\u79cd\u65b9\u6cd5\u6765\u4fee\u8ba2\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u542f\u53d1\u7ed3\u5408\u5b9a\u6027\u548c\u5b9a\u91cf\u8303\u5f0f\u7684\u65b0\u7814\u7a76\u65b9\u5411\u3002", "result": "\u901a\u8fc7\u63d0\u51fa\u7684\u65b0\u6846\u67b6\uff0c\u6587\u7ae0\u6210\u529f\u5730\u6574\u5408\u4e86\u591a\u79cd\u73b0\u6709\u7684\u7814\u7a76\u548c\u6280\u672f\uff0c\u4e3a\u672a\u6765\u7684\u4eba\u7c7b-AI\u534f\u4f5c\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u65b9\u5411\u3002\u65b0\u6846\u67b6\u4e0d\u4ec5\u4fc3\u8fdb\u4e86\u5bf9\u4f20\u7edf\u65b9\u6cd5\u7684\u91cd\u65b0\u5ba1\u89c6\uff0c\u8fd8\u6fc0\u52b1\u4e86\u878d\u5408\u4e0d\u540c\u7814\u7a76\u8303\u5f0f\u7684\u65b0\u5de5\u4f5c\u3002\u6b64\u5916\uff0c\u6587\u7ae0\u7684\u7ed3\u6784\u8bbe\u8ba1\u5141\u8bb8\u8bfb\u8005\u4ece\u4efb\u610f\u90e8\u5206\u5f00\u59cb\u9605\u8bfb\uff0c\u4f7f\u5176\u65e2\u9002\u7528\u4e8e\u6279\u5224\u6027\u56de\u987e\u73b0\u6709\u6280\u672f\uff0c\u4e5f\u9002\u5408\u4f5c\u4e3a\u524d\u77bb\u6027\u53c2\u8003\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u5ff5\u67b6\u6784\uff0c\u5373\u5c42\u7ea7\u63a2\u7d22-\u5229\u7528\u7f51\u7edc\uff0c\u4f5c\u4e3a\u7edf\u4e00\u4eba\u7c7b-AI\u534f\u4f5c\u7814\u7a76\u7684\u7406\u8bba\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5f3a\u8c03\u591a\u667a\u80fd\u4f53\u534f\u8c03\u3001\u77e5\u8bc6\u7ba1\u7406\u3001\u63a7\u5236\u53cd\u9988\u548c\u9ad8\u7ea7\u63a7\u5236\u673a\u5236\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u6574\u5408\u5b9a\u6027\u548c\u5b9a\u91cf\u8303\u5f0f\u63a8\u52a8\u7814\u7a76\u8fdb\u5c55\u3002\u6700\u7ec8\uff0c\u8fd9\u4e00\u6846\u67b6\u4e3a\u6df1\u5316\u4eba\u7c7b\u8ba4\u77e5\u548cAI\u80fd\u529b\u7684\u5171\u540c\u8fdb\u5316\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u540c\u65f6\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u65b9\u5411\u3002"}}
{"id": "2505.14625", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.14625", "abs": "https://arxiv.org/abs/2505.14625", "authors": ["Zhangchen Xu", "Yuetai Li", "Fengqing Jiang", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Radha Poovendran"], "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u9a8c\u8bc1\u5668\u9519\u8bef\u62d2\u7edd\uff08false negatives\uff09\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848tinyV\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u8bad\u7ec3\u6548\u679c\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6210\u529f\u4f9d\u8d56\u4e8e\u5956\u52b1\u4fe1\u53f7\u7684\u53ef\u9760\u6027\uff0c\u800c\u8fd9\u4e9b\u5956\u52b1\u7531\u9a8c\u8bc1\u5668\u63d0\u4f9b\u3002\u7136\u800c\uff0c\u9a8c\u8bc1\u5668\u53ef\u80fd\u4f1a\u9519\u8bef\u5730\u62d2\u7edd\u6b63\u786e\u7684\u6a21\u578b\u8f93\u51fa\uff08\u5373false negatives\uff09\uff0c\u8fd9\u4e25\u91cd\u5f71\u54cd\u4e86RL\u8bad\u7ec3\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002\u56e0\u6b64\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5bf9\u4e8e\u63d0\u5347LLMs\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u8005\u901a\u8fc7\u5206\u6790Big-Math-RL-Verified\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u8d85\u8fc738%\u7684\u6a21\u578b\u751f\u6210\u54cd\u5e94\u5b58\u5728false negatives\u3002\u4e3a\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4ed6\u4eec\u63d0\u51fa\u4e86tinyV\u2014\u2014\u4e00\u79cd\u57fa\u4e8e\u8f7b\u91cf\u7ea7LLM\u7684\u9a8c\u8bc1\u5668\uff0c\u80fd\u591f\u52a8\u6001\u8bc6\u522b\u6f5c\u5728\u7684false negatives\uff0c\u5e76\u6062\u590d\u6709\u6548\u54cd\u5e94\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u5956\u52b1\u4f30\u8ba1\u3002tinyV\u4f5c\u4e3a\u73b0\u6709\u89c4\u5219\u9a71\u52a8\u65b9\u6cd5\u7684\u8865\u5145\uff0c\u6539\u8fdb\u4e86\u5956\u52b1\u4fe1\u53f7\u7684\u8d28\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u96c6\u6210tinyV\u540e\uff0c\u6a21\u578b\u7684\u901a\u8fc7\u7387\u63d0\u5347\u4e86\u9ad8\u8fbe10%\uff0c\u540c\u65f6\u52a0\u901f\u4e86\u6536\u655b\u8fc7\u7a0b\u3002\u8fd9\u8868\u660etinyV\u663e\u8457\u6539\u5584\u4e86RL\u8bad\u7ec3\u7684\u6548\u679c\u548c\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u7684\u7814\u7a76\u5f3a\u8c03\u4e86\u89e3\u51b3\u9a8c\u8bc1\u5668false negatives\u95ee\u9898\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u5f15\u5165tinyV\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347RL\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5956\u52b1\u4fe1\u53f7\u7684\u51c6\u786e\u6027\uff0c\u4ece\u800c\u8fdb\u4e00\u6b65\u4f18\u5316LLMs\u7684\u6027\u80fd\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u4f9b\u7814\u7a76\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2505.14552", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.14552", "abs": "https://arxiv.org/abs/2505.14552", "authors": ["Jiajun Shi", "Jian Yang", "Jiaheng Liu", "Xingyuan Bu", "Jiangjie Chen", "Junting Zhou", "Kaijing Ma", "Zhoufutu Wen", "Bingli Wang", "Yancheng He", "Liang Song", "Hualei Zhu", "Shilong Li", "Xingjian Wang", "Wei Zhang", "Ruibin Yuan", "Yifan Yao", "Wenjun Yang", "Yunli Wang", "Siyuan Fang", "Siyu Yuan", "Qianyu He", "Xiangru Tang", "Yingshui Tan", "Wangchunshu Zhou", "Zhaoxiang Zhang", "Zhoujun Li", "Wenhao Huang", "Ge Zhang"], "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "22 pages", "summary": "Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u8fd1\u671f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\u8868\u660e\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u51c6\u786e\u8861\u91cf\u5176\u63a8\u7406\u80fd\u529b\u3002\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u662f\u9886\u57df\u7279\u5b9a\u7684\uff0c\u65e0\u6cd5\u5b8c\u5168\u6355\u6349LLM\u7684\u4e00\u822c\u63a8\u7406\u6f5c\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u77e5\u8bc6\u6b63\u4ea4\u63a8\u7406\u5065\u8eab\u623f\uff08KORGym\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u53d7KOR-Bench\u548cGymnasium\u542f\u53d1\u7684\u52a8\u6001\u8bc4\u4f30\u5e73\u53f0\u3002KORGym\u63d0\u4f9b\u4e86\u8d85\u8fc7\u4e94\u5341\u79cd\u6587\u672c\u6216\u89c6\u89c9\u683c\u5f0f\u7684\u6e38\u620f\uff0c\u5e76\u652f\u6301\u5e26\u6709\u5f3a\u5316\u5b66\u4e60\u573a\u666f\u7684\u4ea4\u4e92\u5f0f\u3001\u591a\u8f6e\u8bc4\u4f30\u3002\u901a\u8fc7KORGym\uff0c\u6211\u4eec\u5bf919\u4e2aLLM\u548c8\u4e2aVLM\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5bb6\u65cf\u5185\u90e8\u4e00\u81f4\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u5e76\u5c55\u793a\u4e86\u95ed\u6e90\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u80fd\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u7814\u7a76\u4e86\u6a21\u6001\u3001\u63a8\u7406\u7b56\u7565\u3001\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u548c\u54cd\u5e94\u957f\u5ea6\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u6211\u4eec\u671f\u671bKORGym\u6210\u4e3a\u63a8\u52a8LLM\u63a8\u7406\u7814\u7a76\u548c\u5f00\u53d1\u9002\u5408\u590d\u6742\u4ea4\u4e92\u73af\u5883\u7684\u8bc4\u4f30\u65b9\u6cd5\u7684\u5b9d\u8d35\u8d44\u6e90\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5b83\u4eec\u901a\u5e38\u5c40\u9650\u4e8e\u7279\u5b9a\u9886\u57df\uff0c\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u6a21\u578b\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u80fd\u591f\u6db5\u76d6\u591a\u79cd\u4efb\u52a1\u7c7b\u578b\u5e76\u652f\u6301\u590d\u6742\u7684\u4ea4\u4e92\u5f0f\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aKORGym\u7684\u52a8\u6001\u8bc4\u4f30\u5e73\u53f0\uff0c\u8be5\u5e73\u53f0\u6574\u5408\u4e86\u8d85\u8fc750\u79cd\u6587\u672c\u6216\u89c6\u89c9\u6e38\u620f\uff0c\u652f\u6301\u591a\u8f6e\u4ea4\u4e92\u8bc4\u4f30\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u573a\u666f\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf919\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u548c8\u4e2a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7814\u7a76\u8005\u5206\u6790\u4e86\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u7684\u63a8\u7406\u6a21\u5f0f\u53ca\u5176\u6027\u80fd\u8868\u73b0\u3002\u6b64\u5916\uff0c\u8fd8\u8003\u5bdf\u4e86\u6a21\u6001\u3001\u63a8\u7406\u7b56\u7565\u3001\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u53ca\u54cd\u5e94\u957f\u5ea6\u7b49\u56e0\u7d20\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u540c\u4e00\u6a21\u578b\u5bb6\u65cf\u5185\u7684\u6a21\u578b\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u4e14\u95ed\u6e90\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002\u540c\u65f6\uff0c\u7814\u7a76\u8868\u660e\u6a21\u6001\u3001\u63a8\u7406\u7b56\u7565\u3001\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u53ca\u54cd\u5e94\u957f\u5ea6\u7b49\u53d8\u91cf\u5bf9\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "KORGym\u4f5c\u4e3a\u4e00\u4e2a\u521b\u65b0\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e0b\u7684\u8bc4\u4f30\u9700\u6c42\u3002\u672a\u6765\uff0cKORGym\u6709\u671b\u4fc3\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u7814\u7a76\uff0c\u5e76\u63a8\u52a8\u66f4\u5148\u8fdb\u7684\u8bc4\u4f30\u65b9\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.14264", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.14264", "abs": "https://arxiv.org/abs/2505.14264", "authors": ["Jian Xiong", "Jingbo Zhou", "Jingyong Ye", "Dejing Dou"], "title": "AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum", "categories": ["cs.LG", "cs.CL"], "comment": "14 pages, 7 figures", "summary": "Reinforcement learning (RL) has emerged as an effective approach for\nenhancing the reasoning capabilities of large language models (LLMs),\nespecially in scenarios where supervised fine-tuning (SFT) falls short due to\nlimited chain-of-thought (CoT) data. Among RL-based post-training methods,\ngroup relative advantage estimation, as exemplified by Group Relative Policy\nOptimization (GRPO), has attracted considerable attention for eliminating the\ndependency on the value model, thereby simplifying training compared to\ntraditional approaches like Proximal Policy Optimization (PPO). However, we\nobserve that exsiting group relative advantage estimation method still suffers\nfrom training inefficiencies, particularly when the estimated advantage\napproaches zero. To address this limitation, we propose Advantage-Augmented\nPolicy Optimization (AAPO), a novel RL algorithm that optimizes the\ncross-entropy (CE) loss using advantages enhanced through a momentum-based\nestimation scheme. This approach effectively mitigates the inefficiencies\nassociated with group relative advantage estimation. Experimental results on\nmultiple mathematical reasoning benchmarks demonstrate the superior performance\nof AAPO.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5AAPO\uff0c\u7528\u4e8e\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u91cf\u4f30\u8ba1\u65b9\u6848\u589e\u5f3a\u4f18\u52bf\u51fd\u6570\uff0c\u4ece\u800c\u7f13\u89e3\u4e86\u73b0\u6709\u7fa4\u7ec4\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\u65b9\u6cd5\u5728\u8bad\u7ec3\u6548\u7387\u4e0a\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAAPO\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5c3d\u7ba1\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5b58\u5728\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4f46\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u7fa4\u7ec4\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\u65b9\u6cd5\uff08\u5982GRPO\uff09\uff0c\u56e0\u5176\u65e0\u9700\u4f9d\u8d56\u4ef7\u503c\u6a21\u578b\u800c\u53d7\u5230\u5173\u6ce8\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u7fa4\u7ec4\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\u65b9\u6cd5\u5728\u8bad\u7ec3\u6548\u7387\u4e0a\u4ecd\u6709\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u4f18\u52bf\u4f30\u8ba1\u63a5\u8fd1\u96f6\u65f6\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86Advantage-Augmented Policy Optimization (AAPO)\uff0c\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u4f7f\u7528\u52a8\u91cf\u4e3a\u57fa\u7840\u7684\u4f18\u52bf\u589e\u5f3a\u65b9\u6848\u6765\u4f18\u5316\u4ea4\u53c9\u71b5\uff08CE\uff09\u635f\u5931\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u4e0e\u7fa4\u7ec4\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\u76f8\u5173\u7684\u8bad\u7ec3\u4f4e\u6548\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAAPO\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "AAPO\u901a\u8fc7\u5f15\u5165\u52a8\u91cf\u4f30\u8ba1\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u8868\u73b0\u3002\u8fd9\u4e00\u6210\u679c\u4e3a\u63d0\u5347LLMs\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2505.14185", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.14185", "abs": "https://arxiv.org/abs/2505.14185", "authors": ["Kaustubh Ponkshe", "Shaan Shah", "Raghav Singhal", "Praneeth Vepakomma"], "title": "Safety Subspaces are Not Distinct: A Fine-Tuning Case Study", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Kaustubh Ponkshe, Shaan Shah, and Raghav Singhal contributed equally\n  to this work", "summary": "Large Language Models (LLMs) rely on safety alignment to produce socially\nacceptable responses. This is typically achieved through instruction tuning and\nreinforcement learning from human feedback. However, this alignment is known to\nbe brittle: further fine-tuning, even on benign or lightly contaminated data,\ncan degrade safety and reintroduce harmful behaviors. A growing body of work\nsuggests that alignment may correspond to identifiable geometric directions in\nweight space, forming subspaces that could, in principle, be isolated or\npreserved to defend against misalignment. In this work, we conduct a\ncomprehensive empirical study of this geometric perspective. We examine whether\nsafety-relevant behavior is concentrated in specific subspaces, whether it can\nbe separated from general-purpose learning, and whether harmfulness arises from\ndistinguishable patterns in internal representations. Across both parameter and\nactivation space, our findings are consistent: subspaces that amplify safe\nbehaviors also amplify unsafe ones, and prompts with different safety\nimplications activate overlapping representations. We find no evidence of a\nsubspace that selectively governs safety. These results challenge the\nassumption that alignment is geometrically localized. Rather than residing in\ndistinct directions, safety appears to emerge from entangled, high-impact\ncomponents of the model's broader learning dynamics. This suggests that\nsubspace-based defenses may face fundamental limitations and underscores the\nneed for alternative strategies to preserve alignment under continued training.\nWe corroborate these findings through multiple experiments on five open-source\nLLMs. Our code is publicly available at:\nhttps://github.com/CERT-Lab/safety-subspaces.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5b89\u5168\u5bf9\u9f50\u53ef\u80fd\u5e76\u975e\u7b80\u5355\u5730\u5bf9\u5e94\u4e8e\u6743\u91cd\u7a7a\u95f4\u4e2d\u7684\u7279\u5b9a\u51e0\u4f55\u5b50\u7a7a\u95f4\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u589e\u5f3a\u5b89\u5168\u884c\u4e3a\u7684\u5b50\u7a7a\u95f4\u540c\u65f6\u4e5f\u589e\u5f3a\u4e86\u4e0d\u5b89\u5168\u884c\u4e3a\uff0c\u5e76\u4e14\u4e0d\u540c\u7684\u5b89\u5168\u6027\u63d0\u793a\u6fc0\u6d3b\u4e86\u91cd\u53e0\u7684\u5185\u90e8\u8868\u793a\u3002\u8fd9\u8868\u660e\u5bf9\u9f50\u5e76\u975e\u51e0\u4f55\u5c40\u90e8\u5316\uff0c\u800c\u662f\u6a21\u578b\u6574\u4f53\u5b66\u4e60\u52a8\u6001\u4e2d\u7ea0\u7f20\u7684\u3001\u9ad8\u5f71\u54cd\u7684\u7ec4\u6210\u90e8\u5206\u3002\u6b64\u7ed3\u679c\u6311\u6218\u4e86\u51e0\u4f55\u5b50\u7a7a\u95f4\u9632\u5fa1\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u66ff\u4ee3\u7b56\u7565\u6765\u5728\u6301\u7eed\u8bad\u7ec3\u4e2d\u4fdd\u6301\u5bf9\u9f50\u3002", "motivation": "\u5c3d\u7ba1\u901a\u8fc7\u6307\u4ee4\u5fae\u8c03\u548c\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u5b9e\u73b0LLM\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u4f46\u8fd9\u79cd\u5bf9\u9f50\u662f\u8106\u5f31\u7684\uff0c\u5bb9\u6613\u56e0\u8fdb\u4e00\u6b65\u5fae\u8c03\u800c\u9000\u5316\u6216\u91cd\u65b0\u5f15\u5165\u6709\u5bb3\u884c\u4e3a\u3002\u6709\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u9f50\u53ef\u80fd\u4e0e\u6743\u91cd\u7a7a\u95f4\u4e2d\u7684\u53ef\u8bc6\u522b\u51e0\u4f55\u65b9\u5411\u76f8\u5173\u8054\uff0c\u8fd9\u4e9b\u65b9\u5411\u53ef\u80fd\u5f62\u6210\u53ef\u9694\u79bb\u6216\u4fdd\u62a4\u7684\u5b50\u7a7a\u95f4\u4ee5\u9632\u6b62\u9519\u4f4d\u3002\u7136\u800c\uff0c\u8fd9\u4e00\u5047\u8bbe\u5c1a\u672a\u5f97\u5230\u5145\u5206\u9a8c\u8bc1\u3002", "method": "\u672c\u7814\u7a76\u5bf9\u51e0\u4f55\u89c6\u89d2\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u8bc1\u5206\u6790\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\n1. \u68c0\u67e5\u5b89\u5168\u76f8\u5173\u884c\u4e3a\u662f\u5426\u96c6\u4e2d\u5728\u7279\u5b9a\u5b50\u7a7a\u95f4\u3002\n2. \u63a2\u8ba8\u5b89\u5168\u884c\u4e3a\u662f\u5426\u80fd\u4e0e\u901a\u7528\u5b66\u4e60\u5206\u79bb\u3002\n3. \u5206\u6790\u6709\u5bb3\u884c\u4e3a\u662f\u5426\u6e90\u81ea\u5185\u90e8\u8868\u793a\u4e2d\u7684\u53ef\u533a\u5206\u6a21\u5f0f\u3002\n\u7814\u7a76\u8986\u76d6\u4e86\u53c2\u6570\u7a7a\u95f4\u548c\u6fc0\u6d3b\u7a7a\u95f4\uff0c\u5e76\u5728\u4e94\u4e2a\u5f00\u6e90LLM\u4e0a\u8fdb\u884c\u4e86\u591a\u6b21\u5b9e\u9a8c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff1a\n- \u589e\u5f3a\u5b89\u5168\u884c\u4e3a\u7684\u5b50\u7a7a\u95f4\u540c\u65f6\u4e5f\u4f1a\u589e\u5f3a\u4e0d\u5b89\u5168\u884c\u4e3a\u3002\n- \u5177\u6709\u4e0d\u540c\u5b89\u5168\u5f71\u54cd\u7684\u63d0\u793a\u6fc0\u6d3b\u4e86\u91cd\u53e0\u7684\u5185\u90e8\u8868\u793a\u3002\n- \u6ca1\u6709\u8bc1\u636e\u8868\u660e\u5b58\u5728\u4e13\u95e8\u63a7\u5236\u5b89\u5168\u6027\u7684\u5b50\u7a7a\u95f4\u3002\n\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u5b89\u5168\u6027\u4f3c\u4e4e\u5e76\u4e0d\u662f\u7531\u6a21\u578b\u6743\u91cd\u7a7a\u95f4\u4e2d\u7684\u7279\u5b9a\u65b9\u5411\u51b3\u5b9a\uff0c\u800c\u662f\u6e90\u4e8e\u6a21\u578b\u66f4\u5e7f\u6cdb\u5b66\u4e60\u52a8\u6001\u4e2d\u7ea0\u7f20\u7684\u3001\u9ad8\u5f71\u54cd\u7684\u7ec4\u6210\u90e8\u5206\u3002", "conclusion": "\u8be5\u7814\u7a76\u6311\u6218\u4e86\u51e0\u4f55\u5c40\u90e8\u5316\u7684\u5047\u8bbe\uff0c\u6307\u51fa\u5b89\u5168\u6027\u5e76\u975e\u7b80\u5355\u5730\u5bf9\u5e94\u4e8e\u6743\u91cd\u7a7a\u95f4\u4e2d\u7684\u7279\u5b9a\u65b9\u5411\u3002\u57fa\u4e8e\u5b50\u7a7a\u95f4\u7684\u9632\u5fa1\u53ef\u80fd\u9762\u4e34\u6839\u672c\u9650\u5236\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u5176\u4ed6\u7b56\u7565\u4ee5\u5728\u6301\u7eed\u8bad\u7ec3\u4e2d\u4fdd\u6301\u5bf9\u9f50\u3002\u7814\u7a76\u6210\u679c\u5728\u591a\u4e2a\u5f00\u6e90LLM\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5e76\u5df2\u516c\u5f00\u4ee3\u7801\u4f9b\u793e\u533a\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.13697", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.13697", "abs": "https://arxiv.org/abs/2505.13697", "authors": ["Soumya Rani Samineni", "Durgesh Kalwar", "Karthik Valmeekam", "Kaya Stechly", "Subbarao Kambhampati"], "title": "RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning-based post-training of large language models (LLMs)\nhas recently gained attention, particularly following the release of DeepSeek\nR1, which applied GRPO for fine-tuning. Amid the growing hype around improved\nreasoning abilities attributed to RL post-training, we critically examine the\nformulation and assumptions underlying these methods. We start by highlighting\nthe popular structural assumptions made in modeling LLM training as a Markov\nDecision Process (MDP), and show how they lead to a degenerate MDP that doesn't\nquite need the RL/GRPO apparatus. The two critical structural assumptions\ninclude (1) making the MDP states be just a concatenation of the actions-with\nstates becoming the context window and the actions becoming the tokens in LLMs\nand (2) splitting the reward of a state-action trajectory uniformly across the\ntrajectory. Through a comprehensive analysis, we demonstrate that these\nsimplifying assumptions make the approach effectively equivalent to an\noutcome-driven supervised learning. Our experiments on benchmarks including\nGSM8K and Countdown using Qwen-2.5 base models show that iterative supervised\nfine-tuning, incorporating both positive and negative samples, achieves\nperformance comparable to GRPO-based training. We will also argue that the\nstructural assumptions indirectly incentivize the RL to generate longer\nsequences of intermediate tokens-which in turn feeds into the narrative of \"RL\ngenerating longer thinking traces.\" While RL may well be a very useful\ntechnique for improving the reasoning abilities of LLMs, our analysis shows\nthat the simplistic structural assumptions made in modeling the underlying MDP\nrender the popular LLM RL frameworks and their interpretations questionable.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5fae\u8c03\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u9488\u5bf9DeepSeek R1\u6240\u91c7\u7528\u7684GRPO\u6280\u672f\u3002\u6587\u7ae0\u6307\u51fa\uff0c\u5728\u5c06LLM\u8bad\u7ec3\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u65f6\uff0c\u5e38\u89c1\u7684\u7ed3\u6784\u5047\u8bbe\u4f7f\u5f97\u8be5\u65b9\u6cd5\u5b9e\u9645\u4e0a\u7b49\u540c\u4e8e\u4e00\u79cd\u7ed3\u679c\u9a71\u52a8\u7684\u76d1\u7763\u5b66\u4e60\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fed\u4ee3\u76d1\u7763\u5fae\u8c03\u53ef\u4ee5\u8fbe\u5230\u4e0eGRPO\u76f8\u4f3c\u7684\u6548\u679c\uff0c\u540c\u65f6\u4f5c\u8005\u8d28\u7591\u5f53\u524d\u6d41\u884c\u7684LLM RL\u6846\u67b6\u53ca\u5176\u89e3\u91ca\u7684\u6709\u6548\u6027\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u5bf9\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u540e\u8bad\u7ec3\uff08post-training\uff09\u7684\u65b9\u6cd5\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\uff0c\u5c24\u5176\u662f\u5728DeepSeek R1\u53d1\u5e03\u4e4b\u540e\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u80cc\u540e\u7684\u516c\u5f0f\u5316\u548c\u5047\u8bbe\u5c1a\u672a\u5f97\u5230\u6df1\u5165\u63a2\u8ba8\u3002\u672c\u6587\u65e8\u5728\u6279\u5224\u6027\u5730\u5206\u6790\u8fd9\u4e9b\u5047\u8bbe\uff0c\u5e76\u8bc4\u4f30\u5b83\u4eec\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "method": "\u4f5c\u8005\u9996\u5148\u6307\u51fa\u4e86\u5728\u5c06LLM\u8bad\u7ec3\u5efa\u6a21\u4e3aMDP\u65f6\u5e38\u7528\u7684\u4e24\u4e2a\u5173\u952e\u5047\u8bbe\uff1a(1) MDP\u72b6\u6001\u4ec5\u4ec5\u662f\u52a8\u4f5c\u7684\u8fde\u63a5\uff0c\u5176\u4e2d\u72b6\u6001\u6210\u4e3a\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u52a8\u4f5c\u6210\u4e3aLLM\u4e2d\u7684\u6807\u8bb0\uff1b(2) \u5c06\u8f68\u8ff9\u5956\u52b1\u5747\u5300\u62c6\u5206\u5230\u6574\u4e2a\u8f68\u8ff9\u4e0a\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8fd9\u4e9b\u7b80\u5316\u5047\u8bbe\u4f7f\u65b9\u6cd5\u672c\u8d28\u4e0a\u7b49\u540c\u4e8e\u7ed3\u679c\u9a71\u52a8\u7684\u76d1\u7763\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u4f7f\u7528Qwen-2.5\u57fa\u7840\u6a21\u578b\u5728GSM8K\u548cCountdown\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u8fed\u4ee3\u76d1\u7763\u5fae\u8c03\u4e0eGRPO\u65b9\u6cd5\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u7ed3\u5408\u6b63\u8d1f\u6837\u672c\u7684\u8fed\u4ee3\u76d1\u7763\u5fae\u8c03\u80fd\u591f\u5b9e\u73b0\u4e0eGRPO\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u7684\u7ed3\u6784\u5047\u8bbe\u95f4\u63a5\u4fc3\u4f7fRL\u751f\u6210\u66f4\u957f\u7684\u4e2d\u95f4\u6807\u8bb0\u5e8f\u5217\uff0c\u4ece\u800c\u652f\u6301\u4e86\u201cRL\u751f\u6210\u66f4\u957f\u601d\u8003\u75d5\u8ff9\u201d\u7684\u53d9\u8ff0\u3002\u8fd9\u8868\u660e\uff0c\u5c3d\u7ba1RL\u53ef\u80fd\u662f\u6539\u8fdbLLM\u63a8\u7406\u80fd\u529b\u7684\u6709\u7528\u5de5\u5177\uff0c\u4f46\u5176\u5f53\u524d\u7684\u7ed3\u6784\u5047\u8bbe\u9700\u8981\u8fdb\u4e00\u6b65\u5ba1\u89c6\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6d41\u884c\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\u5b58\u5728\u7684\u4e00\u4e9b\u95ee\u9898\u3002\u4f5c\u8005\u8ba4\u4e3a\uff0c\u8fc7\u4e8e\u7b80\u5316\u7684\u7ed3\u6784\u5047\u8bbe\u53ef\u80fd\u5bfc\u81f4\u8fd9\u4e9b\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u89e3\u91ca\u6027\u53d7\u5230\u8d28\u7591\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u66f4\u52a0\u5173\u6ce8\u6539\u8fdb\u8fd9\u4e9b\u5047\u8bbe\uff0c\u4ee5\u66f4\u597d\u5730\u53d1\u6325\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.13638", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.13638", "abs": "https://arxiv.org/abs/2505.13638", "authors": ["Massimo Fioravanti", "Giovanni Agosta"], "title": "4Hammer: a board-game reinforcement learning environment for the hour long time frame", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong performance on tasks\nwith short time frames, but struggle with tasks requiring longer durations.\nWhile datasets covering extended-duration tasks, such as software engineering\ntasks or video games, do exist, there are currently few implementations of\ncomplex board games specifically designed for reinforcement learning and LLM\nevaluation. To address this gap, we propose the 4Hammer reinforcement learning\nenvironment, a digital twin simulation of a subset of Warhammer 40,000-a\ncomplex, zero-sum board game. Warhammer 40,000 features intricate rules,\nrequiring human players to thoroughly read and understand over 50 pages of\ndetailed natural language rules, grasp the interactions between their game\npieces and those of their opponents, and independently track and communicate\nthe evolving game state.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a4Hammer\u7684\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30\u590d\u6742\u68cb\u76d8\u6e38\u620f\u4e2d\u7684LLM\u548c\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u96c6\u867d\u7136\u6db5\u76d6\u4e86\u957f\u65f6\u95f4\u4efb\u52a1\uff08\u5982\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u6216\u89c6\u9891\u6e38\u620f\uff09\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9\u590d\u6742\u68cb\u76d8\u6e38\u620f\u8bbe\u8ba1\u7684\u5f3a\u5316\u5b66\u4e60\u548cLLM\u8bc4\u4f30\u7684\u5177\u4f53\u5b9e\u73b0\u3002\u590d\u6742\u68cb\u76d8\u6e38\u620f\u9700\u8981\u73a9\u5bb6\u6df1\u5165\u7406\u89e3\u8be6\u7ec6\u89c4\u5219\u3001\u8ddf\u8e2a\u6e38\u620f\u72b6\u6001\u5e76\u4e0e\u5bf9\u624b\u4e92\u52a8\uff0c\u8fd9\u5bf9LLM\u548c\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u63d0\u51fa\u4e86\u72ec\u7279\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6a21\u62dfWarhammer 40,000\u7684\u4e00\u4e2a\u5b50\u96c6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a4Hammer\u7684\u6570\u5b57\u5b6a\u751f\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u3002\u8be5\u73af\u5883\u5177\u6709\u590d\u6742\u7684\u89c4\u5219\u548c\u96f6\u548c\u535a\u5f08\u7279\u6027\uff0c\u8981\u6c42\u73a9\u5bb6\u7406\u89e3\u8d85\u8fc750\u9875\u7684\u81ea\u7136\u8bed\u8a00\u89c4\u5219\uff0c\u5e76\u5904\u7406\u6e38\u620f\u68cb\u5b50\u4e4b\u95f4\u7684\u4ea4\u4e92\u548c\u52a8\u6001\u53d8\u5316\u7684\u6e38\u620f\u72b6\u6001\u3002", "result": "\u5c1a\u672a\u660e\u786e\u8bf4\u660e\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u8be5\u73af\u5883\u7684\u8bbe\u8ba1\u4e3a\u672a\u6765\u5728\u590d\u6742\u68cb\u76d8\u6e38\u620f\u4e2d\u8bc4\u4f30LLM\u548c\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "conclusion": "4Hammer\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u5e73\u53f0\uff0c\u4ee5\u8bc4\u4f30\u548c\u6539\u8fdbLLM\u53ca\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u5728\u957f\u65f6\u95f4\u3001\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u68cb\u76d8\u6e38\u620f\u9886\u57df\u3002"}}
{"id": "2505.13508", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.13508", "abs": "https://arxiv.org/abs/2505.13508", "authors": ["Zijia Liu", "Peixuan Han", "Haofei Yu", "Haoru Li", "Jiaxuan You"], "title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate impressive capabilities but lack\nrobust temporal intelligence, struggling to integrate reasoning about the past\nwith predictions and plausible generations of the future. Meanwhile, existing\nmethods typically target isolated temporal skills, such as question answering\nabout past events or basic forecasting, and exhibit poor generalization,\nparticularly when dealing with events beyond their knowledge cutoff or\nrequiring creative foresight. To address these limitations, we introduce\n\\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter)\nLLM with comprehensive temporal abilities: understanding, prediction, and\ncreative generation. Our approach features a novel three-stage development\npath; the first two constitute a \\textit{reinforcement learning (RL)\ncurriculum} driven by a meticulously designed dynamic rule-based reward system.\nThis framework progressively builds (1) foundational temporal understanding and\nlogical event-time mappings from historical data, (2) future event prediction\nskills for events beyond its knowledge cutoff, and finally (3) enables\nremarkable generalization to creative future scenario generation without any\nfine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms\nmodels over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,\non highly challenging future event prediction and creative scenario generation\nbenchmarks. This work provides strong evidence that thoughtfully engineered,\nprogressive RL fine-tuning allows smaller, efficient models to achieve superior\ntemporal performance, offering a practical and scalable path towards truly\ntime-aware AI. To foster further research, we also release \\textit{Time-Bench},\na large-scale multi-task temporal reasoning dataset derived from 10 years of\nnews data, and our series of \\textit{Time-R1} checkpoints.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTime-R1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bfe\u7a0b\u8d4b\u4e88\u4e2d\u7b49\u89c4\u6a21\uff083B\u53c2\u6570\uff09\u7684\u8bed\u8a00\u6a21\u578b\u5168\u9762\u7684\u65f6\u95f4\u80fd\u529b\uff0c\u5305\u62ec\u7406\u89e3\u3001\u9884\u6d4b\u548c\u521b\u9020\u6027\u751f\u6210\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6781\u5177\u6311\u6218\u6027\u7684\u672a\u6765\u4e8b\u4ef6\u9884\u6d4b\u548c\u521b\u9020\u6027\u573a\u666f\u751f\u6210\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4e86\u6bd4\u5176\u5927200\u500d\u4ee5\u4e0a\u7684\u6a21\u578b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u7136\u8868\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\uff0c\u4f46\u5728\u65f6\u95f4\u667a\u80fd\u65b9\u9762\u4ecd\u7136\u7f3a\u4e4f\u7a33\u5065\u6027\uff0c\u96be\u4ee5\u5c06\u5bf9\u8fc7\u53bb\u7684\u63a8\u7406\u4e0e\u5bf9\u672a\u6765\u9884\u6d4b\u548c\u5408\u7406\u751f\u6210\u76f8\u7ed3\u5408\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u5b64\u7acb\u7684\u65f6\u95f4\u6280\u80fd\uff0c\u5982\u5173\u4e8e\u8fc7\u53bb\u4e8b\u4ef6\u7684\u95ee\u9898\u56de\u7b54\u6216\u57fa\u672c\u9884\u6d4b\uff0c\u5e76\u4e14\u5728\u5904\u7406\u8d85\u51fa\u5176\u77e5\u8bc6\u622a\u6b62\u65e5\u671f\u6216\u9700\u8981\u521b\u9020\u6027\u9884\u89c1\u7684\u4e8b\u4ef6\u65f6\uff0c\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002", "method": "Time-R1\u91c7\u7528\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u4e09\u9636\u6bb5\u5f00\u53d1\u8def\u5f84\uff0c\u524d\u4e24\u4e2a\u9636\u6bb5\u6784\u6210\u4e00\u4e2a\u7531\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u52a8\u6001\u89c4\u5219\u5956\u52b1\u7cfb\u7edf\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bfe\u7a0b\u3002\u7b2c\u4e00\u9636\u6bb5\u5efa\u7acb\u57fa\u7840\u7684\u65f6\u95f4\u7406\u89e3\u548c\u903b\u8f91\u4e8b\u4ef6-\u65f6\u95f4\u6620\u5c04\uff1b\u7b2c\u4e8c\u9636\u6bb5\u53d1\u5c55\u5bf9\u672a\u6765\u4e8b\u4ef6\uff08\u5305\u62ec\u8d85\u51fa\u5176\u77e5\u8bc6\u622a\u6b62\u65e5\u671f\u7684\u4e8b\u4ef6\uff09\u7684\u9884\u6d4b\u80fd\u529b\uff1b\u7b2c\u4e09\u9636\u6bb5\u5b9e\u73b0\u5411\u521b\u9020\u6027\u672a\u6765\u60c5\u666f\u751f\u6210\u7684\u663e\u8457\u6cdb\u5316\uff0c\u65e0\u9700\u4efb\u4f55\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTime-R1\u5728\u9ad8\u5ea6\u6311\u6218\u6027\u7684\u672a\u6765\u4e8b\u4ef6\u9884\u6d4b\u548c\u521b\u9020\u6027\u573a\u666f\u751f\u6210\u57fa\u51c6\u4e0a\uff0c\u8868\u73b0\u4f18\u4e8e\u8d85\u8fc7\u5176200\u500d\u5927\u7684\u6a21\u578b\uff0c\u5305\u62ec\u6700\u5148\u8fdb\u7684671B DeepSeek-R1\u3002\u8fd9\u4e3a\u66f4\u5c0f\u3001\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u5b9e\u73b0\u4f18\u8d8a\u7684\u65f6\u95f4\u6027\u80fd\u63d0\u4f9b\u4e86\u5b9e\u9645\u548c\u53ef\u6269\u5c55\u7684\u9014\u5f84\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\uff0c\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u8f83\u5c0f\u7684\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u5353\u8d8a\u7684\u65f6\u95f4\u6027\u80fd\uff0c\u4ece\u800c\u4e3a\u771f\u6b63\u5177\u5907\u65f6\u95f4\u611f\u77e5\u80fd\u529b\u7684\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u9053\u8def\u3002\u4e3a\u4e86\u63a8\u52a8\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\uff0c\u4f5c\u8005\u8fd8\u53d1\u5e03\u4e86\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u65f6\u95f4\u63a8\u7406\u6570\u636e\u96c6Time-Bench\u4ee5\u53ca\u4e00\u7cfb\u5217Time-R1\u68c0\u67e5\u70b9\u3002"}}
{"id": "2505.13438", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.13438", "abs": "https://arxiv.org/abs/2505.13438", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAnytimeReasoner\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u4efb\u610f\u65f6\u523b\u63a8\u7406\u6027\u80fd\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u53ef\u9a8c\u8bc1\u7684\u5bc6\u96c6\u5956\u52b1\uff0c\u5e76\u91c7\u7528\u5206\u79bb\u4f18\u5316\u7b56\u7565\u548c\u65b0\u7684\u65b9\u5dee\u51cf\u5c11\u6280\u672f\uff08BRPO\uff09\uff0c\u4ece\u800c\u5728\u4e0d\u540c\u4ee4\u724c\u9884\u7b97\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u548c\u8bad\u7ec3\u6548\u679c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u5168\u9762\u4f18\u4e8eGRPO\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5728\u56fa\u5b9a\u7684\u5927\u4ee4\u724c\u9884\u7b97\u4e0b\u7684\u6700\u7ec8\u6027\u80fd\uff0c\u8fd9\u9650\u5236\u4e86\u63a8\u7406\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002\u4e3a\u4e86\u6539\u8fdb\u8fd9\u4e00\u70b9\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u52a8\u6001\u4ee4\u724c\u9884\u7b97\u4e0b\u6301\u7eed\u63d0\u4f9b\u826f\u597d\u63a8\u7406\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u9ad8\u4ee4\u724c\u4f7f\u7528\u6548\u7387\u548c\u63a8\u7406\u7684\u9002\u5e94\u6027\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86AnytimeReasoner\u6846\u67b6\uff0c\u901a\u8fc7\u622a\u65ad\u5b8c\u6574\u7684\u601d\u8003\u8fc7\u7a0b\u4ee5\u9002\u5e94\u91c7\u6837\u81ea\u5148\u9a8c\u5206\u5e03\u7684\u4ee4\u724c\u9884\u7b97\uff0c\u8feb\u4f7f\u6a21\u578b\u4e3a\u6bcf\u4e2a\u622a\u65ad\u751f\u6210\u6700\u4f18\u7b54\u6848\u6458\u8981\u3002\u8fd9\u79cd\u65b9\u6cd5\u5c06\u53ef\u9a8c\u8bc1\u7684\u5bc6\u96c6\u5956\u52b1\u5f15\u5165\u63a8\u7406\u8fc7\u7a0b\uff0c\u589e\u5f3a\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u6548\u679c\u3002\u6b64\u5916\uff0c\u601d\u8003\u7b56\u7565\u548c\u6458\u8981\u7b56\u7565\u88ab\u89e3\u8026\u5e76\u5206\u522b\u4f18\u5316\u4ee5\u6700\u5927\u5316\u7d2f\u79ef\u5956\u52b1\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u5dee\u51cf\u5c11\u6280\u672f\u2014\u2014Budget Relative Policy Optimization (BRPO)\uff0c\u4ee5\u589e\u5f3a\u5b66\u4e60\u8fc7\u7a0b\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cAnytimeReasoner\u65b9\u6cd5\u5728\u5404\u79cd\u5148\u9a8c\u5206\u5e03\u548c\u6240\u6709\u601d\u8003\u9884\u7b97\u4e0b\u5747\u663e\u8457\u4f18\u4e8eGRPO\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u8fd8\u5927\u5e45\u6539\u5584\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u4ee4\u724c\u4f7f\u7528\u6548\u7387\u3002", "conclusion": "AnytimeReasoner\u6846\u67b6\u6210\u529f\u5730\u4f18\u5316\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4efb\u610f\u65f6\u523b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ee4\u724c\u6548\u7387\u548c\u63a8\u7406\u7075\u6d3b\u6027\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u4ee4\u724c\u9884\u7b97\u73af\u5883\u3002\u6240\u63d0\u51fa\u7684BRPO\u6280\u672f\u4e5f\u6709\u6548\u589e\u5f3a\u4e86\u5b66\u4e60\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u4e0e\u6548\u7387\uff0c\u4e3a\u672a\u6765\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.13026", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.13026", "abs": "https://arxiv.org/abs/2505.13026", "authors": ["Jack Chen", "Fazhong Liu", "Naruto Liu", "Yuhan Luo", "Erqu Qin", "Harry Zheng", "Tian Dong", "Haojin Zhu", "Yan Meng", "Xiao Wang"], "title": "Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel at mathematical reasoning and logical\nproblem-solving. The current popular training paradigms primarily use\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the\nmodels' reasoning abilities. However, when using SFT or RL alone, there are\nrespective challenges: SFT may suffer from overfitting, while RL is prone to\nmode collapse. The state-of-the-art methods have proposed hybrid training\nschemes. However, static switching faces challenges such as poor generalization\nacross different tasks and high dependence on data quality. In response to\nthese challenges, inspired by the curriculum learning-quiz mechanism in human\nreasoning cultivation, We propose SASR, a step-wise adaptive hybrid training\nframework that theoretically unifies SFT and RL and dynamically balances the\ntwo throughout optimization. SASR uses SFT for initial warm-up to establish\nbasic reasoning skills, and then uses an adaptive dynamic adjustment algorithm\nbased on gradient norm and divergence relative to the original distribution to\nseamlessly integrate SFT with the online RL method GRPO. By monitoring the\ntraining status of LLMs and adjusting the training process in sequence, SASR\nensures a smooth transition between training schemes, maintaining core\nreasoning abilities while exploring different paths. Experimental results\ndemonstrate that SASR outperforms SFT, RL, and static hybrid training methods.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSASR\u7684\u9010\u6b65\u81ea\u9002\u5e94\u6df7\u5408\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\uff0c\u5e76\u52a8\u6001\u5e73\u8861\u4e24\u8005\u4ee5\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660eSASR\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5355\u72ec\u7684SFT\u3001RL\u548c\u9759\u6001\u6df7\u5408\u8bad\u7ec3\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u4e3b\u8981\u65b9\u6cd5\u5305\u62ec\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\uff0c\u4f46\u5404\u81ea\u5b58\u5728\u6311\u6218\uff1aSFT\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u800cRL\u5bb9\u6613\u51fa\u73b0\u6a21\u5f0f\u5d29\u6e83\u3002\u867d\u7136\u5df2\u6709\u6df7\u5408\u8bad\u7ec3\u65b9\u6848\uff0c\u4f46\u9759\u6001\u5207\u6362\u65b9\u6cd5\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u4e14\u5bf9\u6570\u636e\u8d28\u91cf\u4f9d\u8d56\u8f83\u9ad8\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u5e73\u8861SFT\u548cRL\u7684\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "SASR\u6846\u67b6\u9996\u5148\u4f7f\u7528SFT\u8fdb\u884c\u9884\u70ed\u4ee5\u5efa\u7acb\u57fa\u672c\u63a8\u7406\u6280\u80fd\uff0c\u7136\u540e\u91c7\u7528\u57fa\u4e8e\u68af\u5ea6\u8303\u6570\u548c\u76f8\u5bf9\u4e8e\u539f\u59cb\u5206\u5e03\u7684\u504f\u5dee\u7684\u81ea\u9002\u5e94\u52a8\u6001\u8c03\u6574\u7b97\u6cd5\uff0c\u5c06SFT\u4e0e\u5728\u7ebfRL\u65b9\u6cd5GRPO\u65e0\u7f1d\u96c6\u6210\u3002\u901a\u8fc7\u76d1\u63a7\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u72b6\u6001\u5e76\u6309\u987a\u5e8f\u8c03\u6574\u8bad\u7ec3\u8fc7\u7a0b\uff0cSASR\u786e\u4fdd\u4e86\u8bad\u7ec3\u65b9\u6848\u4e4b\u95f4\u7684\u5e73\u7a33\u8fc7\u6e21\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSASR\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5355\u72ec\u7684SFT\u3001RL\u4ee5\u53ca\u9759\u6001\u6df7\u5408\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "SASR\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u8bad\u7ec3\u6846\u67b6\uff0c\u6210\u529f\u5730\u52a8\u6001\u5e73\u8861\u4e86SFT\u548cRL\uff0c\u5e76\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u63a8\u7406\u6027\u80fd\u3002\u8fd9\u4e3a\u672a\u6765\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u5411\u3002"}}
{"id": "2505.12951", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.12951", "abs": "https://arxiv.org/abs/2505.12951", "authors": ["Xuerui Su", "Liya Guo", "Yue Wang", "Yi Zhu", "Zhiming Ma", "Zun Wang", "Yuting Liu"], "title": "DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Inference scaling further accelerates Large Language Models (LLMs) toward\nArtificial General Intelligence (AGI), with large-scale Reinforcement Learning\n(RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning\napproaches usually rely on handcrafted rule-based reward functions. However,\nthe tarde-offs of exploration and exploitation in RL algorithms involves\nmultiple complex considerations, and the theoretical and empirical impacts of\nmanually designed reward functions remain insufficiently explored. In this\npaper, we propose Decoupled Group Reward Optimization (DGRO), a general RL\nalgorithm for LLM reasoning. On the one hand, DGRO decouples the traditional\nregularization coefficient into two independent hyperparameters: one scales the\npolicy gradient term, and the other regulates the distance from the sampling\npolicy. This decoupling not only enables precise control over balancing\nexploration and exploitation, but also can be seamlessly extended to Online\nPolicy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward\nOptimization. On the other hand, we observe that reward variance significantly\naffects both convergence speed and final model performance. We conduct both\ntheoretical analysis and extensive empirical validation to assess DGRO,\nincluding a detailed ablation study that investigates its performance and\noptimization dynamics. Experimental results show that DGRO achieves\nstate-of-the-art performance on the Logic dataset with an average accuracy of\n96.9\\%, and demonstrates strong generalization across mathematical benchmarks.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDecoupled Group Reward Optimization (DGRO)\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\u3002\u901a\u8fc7\u89e3\u8026\u4f20\u7edf\u7684\u6b63\u5219\u5316\u7cfb\u6570\u4e3a\u4e24\u4e2a\u72ec\u7acb\u7684\u8d85\u53c2\u6570\uff0cDGRO\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u63a7\u5236\u63a2\u7d22\u4e0e\u5229\u7528\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u5e76\u5728\u903b\u8f91\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8696.9%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u548c\u5e7f\u6cdb\u7684\u6570\u5b66\u57fa\u51c6\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u624b\u5de5\u8bbe\u8ba1\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4f46\u8fd9\u4e9b\u51fd\u6570\u5728\u7406\u8bba\u548c\u7ecf\u9a8c\u4e0a\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u6b64\u5916\uff0c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e2d\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u6743\u8861\u6d89\u53ca\u591a\u4e2a\u590d\u6742\u7684\u8003\u8651\u56e0\u7d20\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u5956\u52b1\u673a\u5236\u5e76\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "method": "DGRO\u7b97\u6cd5\u7684\u6838\u5fc3\u5728\u4e8e\u5c06\u4f20\u7edf\u7684\u6b63\u5219\u5316\u7cfb\u6570\u5206\u89e3\u4e3a\u4e24\u4e2a\u72ec\u7acb\u7684\u8d85\u53c2\u6570\uff1a\u4e00\u4e2a\u7528\u4e8e\u7f29\u653e\u7b56\u7565\u68af\u5ea6\u9879\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u8c03\u8282\u91c7\u6837\u7b56\u7565\u7684\u8ddd\u79bb\u3002\u8fd9\u79cd\u89e3\u8026\u65b9\u5f0f\u53ef\u4ee5\u66f4\u597d\u5730\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u5e76\u4e14\u53ef\u4ee5\u65e0\u7f1d\u6269\u5c55\u5230\u5728\u7ebf\u7b56\u7565\u955c\u50cf\u4e0b\u964d\uff08OPMD\uff09\u7b97\u6cd5\u548c\u76f4\u63a5\u5956\u52b1\u4f18\u5316\u4e2d\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u89c2\u5bdf\u5230\u5956\u52b1\u65b9\u5dee\u663e\u8457\u5f71\u54cd\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6a21\u578b\u6027\u80fd\uff0c\u56e0\u6b64\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\u548c\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u5305\u62ec\u8be6\u7ec6\u7684\u6d88\u878d\u7814\u7a76\u4ee5\u8bc4\u4f30DGRO\u7684\u6027\u80fd\u548c\u4f18\u5316\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDGRO\u5728\u903b\u8f91\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8696.9%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u8bc1\u660e\u4e86DGRO\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "DGRO\u662f\u4e00\u79cd\u901a\u7528\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u6b63\u5219\u5316\u7cfb\u6570\u548c\u4f18\u5316\u5956\u52b1\u65b9\u5dee\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002\u8be5\u7b97\u6cd5\u4e0d\u4ec5\u5728\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u4e14\u5177\u6709\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
