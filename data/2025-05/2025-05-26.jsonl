{"id": "2410.15466", "keyword": "LLM reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing."}
{"id": "2505.00018", "keyword": "LLM Agent", "pdf": "https://arxiv.org/pdf/2505.00018", "abs": "https://arxiv.org/abs/2505.00018", "authors": ["Ju Wu", "Calvin K. L. Or"], "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability."}
{"id": "2505.18116", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.18116", "abs": "https://arxiv.org/abs/2505.18116", "authors": ["Huayu Chen", "Kaiwen Zheng", "Qinsheng Zhang", "Ganqu Cui", "Yin Cui", "Haotian Ye", "Tsung-Yi Lin", "Ming-Yu Liu", "Jun Zhu", "Haoxiang Wang"], "title": "Bridging Supervised Learning and Reinforcement Learning in Math Reasoning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reinforcement Learning (RL) has played a central role in the recent surge of\nLLMs' math abilities by enabling self-improvement through binary verifier\nsignals. In contrast, Supervised Learning (SL) is rarely considered for such\nverification-driven training, largely due to its heavy reliance on reference\nanswers and inability to reflect on mistakes. In this work, we challenge the\nprevailing notion that self-improvement is exclusive to RL and propose\nNegative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to\nreflect on their failures and improve autonomously with no external teachers.\nIn online training, instead of throwing away self-generated negative answers,\nNFT constructs an implicit negative policy to model them. This implicit policy\nis parameterized with the same positive LLM we target to optimize on positive\ndata, enabling direct policy optimization on all LLMs' generations. We conduct\nexperiments on 7B and 32B models in math reasoning tasks. Results consistently\nshow that through the additional leverage of negative feedback, NFT\nsignificantly improves over SL baselines like Rejection sampling Fine-Tuning,\nmatching or even surpassing leading RL algorithms like GRPO and DAPO.\nFurthermore, we demonstrate that NFT and GRPO are actually equivalent in\nstrict-on-policy training, even though they originate from entirely different\ntheoretical foundations. Our experiments and theoretical findings bridge the\ngap between SL and RL methods in binary-feedback learning systems."}
{"id": "2505.18086", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.18086", "abs": "https://arxiv.org/abs/2505.18086", "authors": ["Muzhi Dai", "Shixuan Liu", "Qingyi Si"], "title": "Stable Reinforcement Learning for Efficient Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "The success of Deepseek-R1 has drawn the LLM community's attention to\nreinforcement learning (RL) methods like GRPO. However, such rule-based 0/1\noutcome reward methods lack the capability to regulate the intermediate\nreasoning processes during chain-of-thought (CoT) generation, leading to severe\noverthinking phenomena. In response, recent studies have designed reward\nfunctions to reinforce models' behaviors in producing shorter yet correct\ncompletions. Nevertheless, we observe that these length-penalty reward\nfunctions exacerbate RL training instability: as the completion length\ndecreases, model accuracy abruptly collapses, often occurring early in\ntraining. To address this issue, we propose a simple yet effective solution\nGRPO-$\\lambda$, an efficient and stabilized variant of GRPO, which dynamically\nadjusts the reward strategy by monitoring the correctness ratio among\ncompletions within each query-sampled group. A low correctness ratio indicates\nthe need to avoid length penalty that compromises CoT quality, triggering a\nswitch to length-agnostic 0/1 rewards that prioritize reasoning capability. A\nhigh ratio maintains length penalties to boost efficiency. Experimental results\nshow that our approach avoids training instability caused by length penalty\nwhile maintaining the optimal accuracy-efficiency trade-off. On the GSM8K,\nGPQA, MATH-500, AMC 2023, and AIME 2024 benchmarks, it improves average\naccuracy by 1.48% while reducing CoT sequence length by 47.3%."}
{"id": "2505.17997", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.17997", "abs": "https://arxiv.org/abs/2505.17997", "authors": ["Jintian Shao", "Yiming Cheng", "Hongyi Huang", "Beiwen Zhang", "Zhiyu Wu", "You Shan", "Mingkai Zheng"], "title": "Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The VAPO framework has demonstrated significant empirical success in\nenhancing the efficiency and reliability of reinforcement learning for long\nchain-of-thought (CoT) reasoning tasks with large language models (LLMs). By\nsystematically addressing challenges such as value model bias, heterogeneous\nsequence lengths, and sparse reward signals, VAPO achieves state-of-the-art\nperformance. While its practical benefits are evident, a deeper theoretical\nunderstanding of its underlying mechanisms and potential limitations is crucial\nfor guiding future advancements. This paper aims to initiate such a discussion\nby exploring VAPO from a theoretical perspective, highlighting areas where its\nassumptions might be challenged and where further investigation could yield\nmore robust and generalizable reasoning agents. We delve into the intricacies\nof value function approximation in complex reasoning spaces, the optimality of\nadaptive advantage estimation, the impact of token-level optimization, and the\nenduring challenges of exploration and generalization."}
{"id": "2505.17989", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.17989", "abs": "https://arxiv.org/abs/2505.17989", "authors": ["Benjamin Turtel", "Danny Franklin", "Kris Skotheim", "Luke Hewitt", "Philipp Schoenegger"], "title": "Outcome-based Reinforcement Learning to Predict the Future", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has boosted math and\ncoding in large language models, yet there has been little effort to extend\nRLVR into messier, real-world domains like forecasting. One sticking point is\nthat outcome-based reinforcement learning for forecasting must learn from\nbinary, delayed, and noisy rewards, a regime where standard fine-tuning is\nbrittle. We show that outcome-only online RL on a 14B model can match\nfrontier-scale accuracy and surpass it in calibration and hypothetical\nprediction market betting by adapting two leading algorithms, Group-Relative\nPolicy Optimisation (GRPO) and ReMax, to the forecasting setting. Our\nadaptations remove per-question variance scaling in GRPO, apply\nbaseline-subtracted advantages in ReMax, hydrate training with 100k temporally\nconsistent synthetic questions, and introduce lightweight guard-rails that\npenalise gibberish, non-English responses and missing rationales, enabling a\nsingle stable pass over 110k events. Scaling ReMax to 110k questions and\nensembling seven predictions yields a 14B model that matches frontier baseline\no1 on accuracy on our holdout set (Brier = 0.193, p = 0.23) while beating it in\ncalibration (ECE = 0.042, p < 0.001). A simple trading rule turns this\ncalibration edge into \\$127 of hypothetical profit versus \\$92 for o1 (p =\n0.037). This demonstrates that refined RLVR methods can convert small-scale\nLLMs into potentially economically valuable forecasting tools, with\nimplications for scaling this to larger models."}
{"id": "2505.17697", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.17697", "abs": "https://arxiv.org/abs/2505.17697", "authors": ["Zekai Zhao", "Qi Liu", "Kun Zhou", "Zihan Liu", "Yifei Shao", "Zhiting Hu", "Biwei Huang"], "title": "Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite the remarkable reasoning performance, eliciting the long\nchain-of-thought (CoT) ability in large language models (LLMs) typically\nrequires costly reinforcement learning or supervised fine-tuning on\nhigh-quality distilled data. We investigate the internal mechanisms behind this\ncapability and show that a small set of high-impact activations in the last few\nlayers largely governs long-form reasoning attributes, such as output length\nand self-reflection. By simply amplifying these activations and inserting\n\"wait\" tokens, we can invoke the long CoT ability without any training,\nresulting in significantly increased self-reflection rates and accuracy.\nMoreover, we find that the activation dynamics follow predictable trajectories,\nwith a sharp rise after special tokens and a subsequent exponential decay.\nBuilding on these insights, we introduce a general training-free activation\ncontrol technique. It leverages a few contrastive examples to identify key\nactivations, and employs simple analytic functions to modulate their values at\ninference time to elicit long CoTs. Extensive experiments confirm the\neffectiveness of our method in efficiently eliciting long CoT reasoning in LLMs\nand improving their performance. Additionally, we propose a parameter-efficient\nfine-tuning method that trains only a last-layer activation amplification\nmodule and a few LoRA layers, outperforming full LoRA fine-tuning on reasoning\nbenchmarks with significantly fewer parameters. Our code and data are publicly\nreleased."}
{"id": "2505.17652", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.17652", "abs": "https://arxiv.org/abs/2505.17652", "authors": ["Deyang Kong", "Qi Guo", "Xiangyu Xi", "Wei Wang", "Jingang Wang", "Xunliang Cai", "Shikun Zhang", "Wei Ye"], "title": "Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning exhibits potential in enhancing the reasoning\nabilities of large language models, yet it is hard to scale for the low sample\nefficiency during the rollout phase. Existing methods attempt to improve\nefficiency by scheduling problems based on problem difficulties. However, these\napproaches suffer from unstable and biased estimations of problem difficulty\nand fail to capture the alignment between model competence and problem\ndifficulty in RL training, leading to suboptimal results. To tackle these\nlimitations, this paper introduces \\textbf{C}ompetence-\\textbf{D}ifficulty\n\\textbf{A}lignment \\textbf{S}ampling (\\textbf{CDAS}), which enables accurate\nand stable estimation of problem difficulties by aggregating historical\nperformance discrepancies of problems. Then the model competence is quantified\nto adaptively select problems whose difficulty is in alignment with the model's\ncurrent competence using a fixed-point system. Experimental results across a\nrange of challenging mathematical benchmarks show that CDAS achieves great\nimprovements in both accuracy and efficiency. CDAS attains the highest average\naccuracy against baselines and exhibits significant speed advantages compared\nto Dynamic Sampling, a competitive strategy in DAPO, which is \\textbf{2.33}\ntimes slower than CDAS."}
{"id": "2505.17621", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.17621", "abs": "https://arxiv.org/abs/2505.17621", "authors": ["Jingtong Gao", "Ling Pan", "Yejing Wang", "Rui Zhong", "Chi Lu", "Qingpeng Cai", "Peng Jiang", "Xiangyu Zhao"], "title": "Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has emerged as a pivotal method for improving the\nreasoning capabilities of Large Language Models (LLMs). However, prevalent RL\napproaches such as Proximal Policy Optimization (PPO) and Group-Regularized\nPolicy Optimization (GRPO) face critical limitations due to their reliance on\nsparse outcome-based rewards and inadequate mechanisms for incentivizing\nexploration. These limitations result in inefficient guidance for multi-step\nreasoning processes. Specifically, sparse reward signals fail to deliver\neffective or sufficient feedback, particularly for challenging problems.\nFurthermore, such reward structures induce systematic biases that prioritize\nexploitation of familiar trajectories over novel solution discovery. These\nshortcomings critically hinder performance in complex reasoning tasks, which\ninherently demand iterative refinement across ipntermediate steps. To address\nthese challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd\nfoR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense\nrewards and amplify explorations in the RL-based training paradigm. i-MENTOR\nintroduces three key innovations: trajectory-aware exploration rewards that\nmitigate bias in token-level strategies while maintaining computational\nefficiency; dynamic reward scaling to stabilize exploration and exploitation in\nlarge action spaces; and advantage-preserving reward implementation that\nmaintains advantage distribution integrity while incorporating exploratory\nguidance. Experiments across three public datasets demonstrate i-MENTOR's\neffectiveness with a 22.39% improvement on the difficult dataset Countdown-4."}
{"id": "2505.17508", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.17508", "abs": "https://arxiv.org/abs/2505.17508", "authors": ["Yifan Zhang", "Yifeng Liu", "Huizhuo Yuan", "Yang Yuan", "Quanquan Gu", "Andrew C Yao"], "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "53 pages, 17 figures", "summary": "Policy gradient algorithms have been successfully applied to enhance the\nreasoning capabilities of large language models (LLMs). Despite the widespread\nuse of Kullback-Leibler (KL) regularization in policy gradient algorithms to\nstabilize training, the systematic exploration of how different KL divergence\nformulations can be estimated and integrated into surrogate loss functions for\nonline reinforcement learning (RL) presents a nuanced and systematically\nexplorable design space. In this paper, we propose regularized policy gradient\n(RPG), a systematic framework for deriving and analyzing KL-regularized policy\ngradient methods in the online RL setting. We derive policy gradients and\ncorresponding surrogate loss functions for objectives regularized by both\nforward and reverse KL divergences, considering both normalized and\nunnormalized policy distributions. Furthermore, we present derivations for\nfully differentiable loss functions as well as REINFORCE-style gradient\nestimators, accommodating diverse algorithmic needs. We conduct extensive\nexperiments on RL for LLM reasoning using these methods, showing improved or\ncompetitive results in terms of training stability and performance compared to\nstrong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at\nhttps://github.com/complex-reasoning/RPG."}
{"id": "2505.17312", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.17312", "abs": "https://arxiv.org/abs/2505.17312", "authors": ["Xiangqi Wang", "Yue Huang", "Yanbo Wang", "Xiaonan Luo", "Kehan Guo", "Yujun Zhou", "Xiangliang Zhang"], "title": "AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "LLMs often need effective configurations, like temperature and reasoning\nsteps, to handle tasks requiring sophisticated reasoning and problem-solving,\nranging from joke generation to mathematical reasoning. Existing prompting\napproaches usually adopt general-purpose, fixed configurations that work 'well\nenough' across tasks but seldom achieve task-specific optimality. To address\nthis gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM\nto automate adaptive reasoning configurations for tasks requiring different\ntypes of thinking. AdaReasoner is trained using a reinforcement learning (RL)\nframework, combining a factorized action space with a targeted exploration\nstrategy, along with a pretrained reward model to optimize the policy model for\nreasoning configurations with only a few-shot guide. AdaReasoner is backed by\ntheoretical guarantees and experiments of fast convergence and a sublinear\npolicy gap. Across six different LLMs and a variety of reasoning tasks, it\nconsistently outperforms standard baselines, preserves out-of-distribution\nrobustness, and yield gains on knowledge-intensive tasks through tailored\nprompts."}
{"id": "2505.17249", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.17249", "abs": "https://arxiv.org/abs/2505.17249", "authors": ["Yuran Sun", "Susu Xu", "Chenguang Wang", "Xilei Zhao"], "title": "Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Big trajectory data hold great promise for human mobility analysis, but their\nutility is often constrained by the absence of critical traveler attributes,\nparticularly sociodemographic information. While prior studies have explored\npredicting such attributes from mobility patterns, they often overlooked\nunderlying cognitive mechanisms and exhibited low predictive accuracy. This\nstudy introduces SILIC, short for Sociodemographic Inference with LLM-guided\nInverse Reinforcement Learning (IRL) and Cognitive Chain Reasoning (CCR), a\ntheoretically grounded framework that leverages LLMs to infer sociodemographic\nattributes from observed mobility patterns by capturing latent behavioral\nintentions and reasoning through psychological constructs. Particularly, our\napproach explicitly follows the Theory of Planned Behavior (TPB), a\nfoundational behavioral framework in transportation research, to model\nindividuals' latent cognitive processes underlying travel decision-making. The\nLLMs further provide heuristic guidance to improve IRL reward function\ninitialization and update by addressing its ill-posedness and optimization\nchallenges arising from the vast and unstructured reward space. Evaluated in\nthe 2017 Puget Sound Regional Council Household Travel Survey, our method\nsubstantially outperforms state-of-the-art baselines and shows great promise\nfor enriching big trajectory data to support more behaviorally grounded\napplications in transportation planning and beyond."}
