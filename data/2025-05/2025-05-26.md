<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 4]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: 本文是一篇立场论文，批判性地综述了人类与AI智能体协作领域的广泛经验性进展。文章提出了一个新型的概念架构——分层探索-利用网络（Hierarchical Exploration-Exploitation Net），以系统地整合多智能体协调、知识管理、控制反馈回路和高层控制机制等技术细节。该框架旨在弥合当前研究中的技术成就与持续存在的差距，并为未来设计或扩展人机共生系统提供了前瞻性的参考。


<details>
  <summary>Details</summary>
Motivation: 目前在处理开放性复杂任务时，缺乏一个统一的理论框架来整合各种关于人类与AI智能体协作的研究成果。这种缺口限制了技术进步的进一步发展，因此需要提出一个新的概念架构来解决这一问题。

Method: 作者通过分析现有的贡献，包括符号AI技术、基于连接主义的大语言模型智能体以及混合组织实践，将这些方法映射到所提出的分层探索-利用网络框架中。此方法不仅修订了传统方法，还融合了定性和定量范式以激励新的研究方向。

Result: 通过提出的新框架，成功地促进了对遗留方法的修订，并启发了结合定性和定量范式的全新工作。这有助于更深入地理解人类认知与AI能力之间的协同进化。

Conclusion: 本文提供的框架和见解为人类与AI智能体之间的深度协同进化奠定了基础。它既可以作为对现有技术实现的批判性回顾，也可以作为未来设计或扩展人机共生系统的参考。

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [2] [Stable Reinforcement Learning for Efficient Reasoning](https://arxiv.org/abs/2505.18086)
*Muzhi Dai,Shixuan Liu,Qingyi Si*

Main category: cs.AI

TL;DR: 本文提出了一种改进的强化学习方法 GRPO-$\lambda$，通过动态调整奖励策略来解决长度惩罚导致的训练不稳定性问题，从而在提高准确率的同时显著缩短推理链长度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于规则的奖励方法（如GRPO）虽然成功应用于LLM（如Deepseek-R1），但在生成推理链时容易出现过度思考的现象。尽管已有研究尝试通过设计奖励函数来鼓励模型生成更短且正确的完成内容，但这些长度惩罚的奖励函数却加剧了强化学习训练的不稳定性，表现为随着完成长度的减少，模型准确性突然下降，通常发生在训练初期。

Method: 为了解决上述问题，作者提出了GRPO-$\lambda$，这是对GRPO的一种高效且稳定的变体。该方法通过监控每个查询采样组中完成内容的正确性比例，动态调整奖励策略。当正确性比例较低时，避免使用可能导致推理质量下降的长度惩罚，转而采用与长度无关的0/1奖励以优先考虑推理能力；当正确性比例较高时，则维持长度惩罚以提升效率。

Result: 实验结果表明，GRPO-$\lambda$能够有效避免由长度惩罚引起的训练不稳定性，同时保持准确性和效率之间的最佳平衡。在GSM8K、GPQA、MATH-500、AMC 2023和AIME 2024等多个基准测试上，该方法将平均准确率提高了1.48%，同时将推理链序列长度减少了47.3%。

Conclusion: GRPO-$\lambda$是一种简单有效的解决方案，能够稳定强化学习训练过程，并在减少推理链长度的同时保持甚至提升模型的准确性。这为未来大语言模型在推理任务中的优化提供了新的思路。

Abstract: The success of Deepseek-R1 has drawn the LLM community's attention to
reinforcement learning (RL) methods like GRPO. However, such rule-based 0/1
outcome reward methods lack the capability to regulate the intermediate
reasoning processes during chain-of-thought (CoT) generation, leading to severe
overthinking phenomena. In response, recent studies have designed reward
functions to reinforce models' behaviors in producing shorter yet correct
completions. Nevertheless, we observe that these length-penalty reward
functions exacerbate RL training instability: as the completion length
decreases, model accuracy abruptly collapses, often occurring early in
training. To address this issue, we propose a simple yet effective solution
GRPO-$\lambda$, an efficient and stabilized variant of GRPO, which dynamically
adjusts the reward strategy by monitoring the correctness ratio among
completions within each query-sampled group. A low correctness ratio indicates
the need to avoid length penalty that compromises CoT quality, triggering a
switch to length-agnostic 0/1 rewards that prioritize reasoning capability. A
high ratio maintains length penalties to boost efficiency. Experimental results
show that our approach avoids training instability caused by length penalty
while maintaining the optimal accuracy-efficiency trade-off. On the GSM8K,
GPQA, MATH-500, AMC 2023, and AIME 2024 benchmarks, it improves average
accuracy by 1.48% while reducing CoT sequence length by 47.3%.

</details>


### [3] [AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking](https://arxiv.org/abs/2505.17312)
*Xiangqi Wang,Yue Huang,Yanbo Wang,Xiaonan Luo,Kehan Guo,Yujun Zhou,Xiangliang Zhang*

Main category: cs.AI

TL;DR: 本论文提出了AdaReasoner，一个针对大型语言模型（LLM）设计的通用插件，用于自动化调整适应性推理配置。通过强化学习框架训练，结合因子化动作空间和定向探索策略，以及预训练奖励模型，AdaReasoner在多种推理任务中表现出色，超越了标准基线方法，并在知识密集型任务中通过定制提示提供了显著增益。


<details>
  <summary>Details</summary>
Motivation: 现有的提示方法通常采用适用于广泛任务的一般固定配置，虽然在跨任务中表现'足够好'，但很少能达到特定任务的最佳性能。因此，需要一种能够根据任务需求自适应调整推理配置的方法来弥补这一差距。

Method: AdaReasoner是一个与具体LLM无关的插件，它通过强化学习框架进行训练。该方法结合了因子化动作空间、定向探索策略以及预训练奖励模型，从而仅需少量示例指导即可优化推理配置的策略模型。此外，AdaReasoner具有理论保证，实验表明其收敛速度快且策略差距呈次线性。

Result: 在六个不同的LLM和各种推理任务上，AdaReasoner持续超越标准基线方法，展现出强大的泛化能力和稳健性。特别是在知识密集型任务中，通过定制提示显著提升了性能。

Conclusion: AdaReasoner为需要不同类型思考的任务提供了一种有效的自动化适配推理配置方案。其实验结果表明，在不同类型的LLM和推理任务中，它可以实现高性能、快速收敛和稳健的表现，同时保持出色的分布外鲁棒性。

Abstract: LLMs often need effective configurations, like temperature and reasoning
steps, to handle tasks requiring sophisticated reasoning and problem-solving,
ranging from joke generation to mathematical reasoning. Existing prompting
approaches usually adopt general-purpose, fixed configurations that work 'well
enough' across tasks but seldom achieve task-specific optimality. To address
this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM
to automate adaptive reasoning configurations for tasks requiring different
types of thinking. AdaReasoner is trained using a reinforcement learning (RL)
framework, combining a factorized action space with a targeted exploration
strategy, along with a pretrained reward model to optimize the policy model for
reasoning configurations with only a few-shot guide. AdaReasoner is backed by
theoretical guarantees and experiments of fast convergence and a sublinear
policy gap. Across six different LLMs and a variety of reasoning tasks, it
consistently outperforms standard baselines, preserves out-of-distribution
robustness, and yield gains on knowledge-intensive tasks through tailored
prompts.

</details>


### [4] [Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse Reinforcement Learning](https://arxiv.org/abs/2505.17249)
*Yuran Sun,Susu Xu,Chenguang Wang,Xilei Zhao*

Main category: cs.AI

TL;DR: 本研究提出了一种名为SILIC的框架，通过结合大型语言模型（LLM）、逆强化学习（IRL）和认知链推理（CCR），从人类移动模式中推断社会人口属性。该方法基于计划行为理论（TPB），能够捕捉潜在的行为意图，并通过心理结构进行推理，显著提高了预测准确性。在2017年普吉特湾地区委员会家庭旅行调查中的实验结果表明，SILIC大幅超越了现有方法，为交通规划和其他应用提供了更丰富的轨迹数据支持。


<details>
  <summary>Details</summary>
Motivation: 现有的大轨迹数据分析虽然潜力巨大，但由于缺乏关键的旅行者属性，尤其是社会人口统计信息，其效用受到限制。尽管已有研究尝试从移动模式中预测这些属性，但它们通常忽略了底层的认知机制，导致预测精度较低。因此，需要一种新的方法来更好地捕捉和解释人类移动行为背后的潜在心理过程。

Method: SILIC框架将大型语言模型、逆强化学习和认知链推理相结合，利用计划行为理论建模个体潜在的认知过程。具体来说，LLM用于提供启发式指导，以改善IRL奖励函数的初始化和更新过程，解决由于奖励空间庞大且无结构带来的优化难题。同时，通过CCR对心理学构建进行推理，进一步增强了对社会人口属性的推断能力。

Result: 在2017年普吉特湾地区委员会家庭旅行调查的数据集上，SILIC的表现明显优于当前最先进的基线方法，展现出在丰富大轨迹数据方面的巨大潜力，从而支持更多基于行为的应用场景。

Conclusion: SILIC框架为从移动模式中推断社会人口属性提供了一个理论基础扎实的新方法。它通过结合LLM、IRL和CCR，不仅捕捉了潜在的行为意图，还通过心理结构进行了推理，有效提升了预测精度。这一成果为交通规划及其他领域提供了更深入的行为洞察，展示了广泛的应用前景。

Abstract: Big trajectory data hold great promise for human mobility analysis, but their
utility is often constrained by the absence of critical traveler attributes,
particularly sociodemographic information. While prior studies have explored
predicting such attributes from mobility patterns, they often overlooked
underlying cognitive mechanisms and exhibited low predictive accuracy. This
study introduces SILIC, short for Sociodemographic Inference with LLM-guided
Inverse Reinforcement Learning (IRL) and Cognitive Chain Reasoning (CCR), a
theoretically grounded framework that leverages LLMs to infer sociodemographic
attributes from observed mobility patterns by capturing latent behavioral
intentions and reasoning through psychological constructs. Particularly, our
approach explicitly follows the Theory of Planned Behavior (TPB), a
foundational behavioral framework in transportation research, to model
individuals' latent cognitive processes underlying travel decision-making. The
LLMs further provide heuristic guidance to improve IRL reward function
initialization and update by addressing its ill-posedness and optimization
challenges arising from the vast and unstructured reward space. Evaluated in
the 2017 Puget Sound Regional Council Household Travel Survey, our method
substantially outperforms state-of-the-art baselines and shows great promise
for enriching big trajectory data to support more behaviorally grounded
applications in transportation planning and beyond.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Bridging Supervised Learning and Reinforcement Learning in Math Reasoning](https://arxiv.org/abs/2505.18116)
*Huayu Chen,Kaiwen Zheng,Qinsheng Zhang,Ganqu Cui,Yin Cui,Haotian Ye,Tsung-Yi Lin,Ming-Yu Liu,Jun Zhu,Haoxiang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Negative-aware Fine-Tuning (NFT)的监督学习方法，使大语言模型能够在无需外部教师的情况下通过反思错误自主改进，实验表明其效果与强化学习算法相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在提升大语言模型数学能力方面取得显著成果，但监督学习因其对参考答案的依赖和无法有效反思错误而较少被用于验证驱动训练。本研究旨在挑战强化学习独占自改进能力的传统观念，探索监督学习在这一领域的潜力。

Method: NFT方法通过在线训练过程中构建隐式负策略来模拟自动生成的错误答案，并使用相同的正向LLM参数化该策略，从而实现对所有生成内容的直接策略优化。这种方法充分利用了负反馈信息，而无需外部教师指导。

Result: 在7B和32B模型的数学推理任务实验中，NFT显著优于传统的监督学习基线方法（如拒绝采样微调），并在某些情况下匹配或超越领先的强化学习算法（如GRPO和DAPO）。此外，理论分析表明，在严格同策略训练条件下，NFT与GRPO等价。

Conclusion: 本研究通过实验和理论分析，弥合了二元反馈学习系统中监督学习和强化学习方法之间的差距，证明了监督学习在自改进能力方面的潜力，为未来的研究提供了新的视角。

Abstract: Reinforcement Learning (RL) has played a central role in the recent surge of
LLMs' math abilities by enabling self-improvement through binary verifier
signals. In contrast, Supervised Learning (SL) is rarely considered for such
verification-driven training, largely due to its heavy reliance on reference
answers and inability to reflect on mistakes. In this work, we challenge the
prevailing notion that self-improvement is exclusive to RL and propose
Negative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to
reflect on their failures and improve autonomously with no external teachers.
In online training, instead of throwing away self-generated negative answers,
NFT constructs an implicit negative policy to model them. This implicit policy
is parameterized with the same positive LLM we target to optimize on positive
data, enabling direct policy optimization on all LLMs' generations. We conduct
experiments on 7B and 32B models in math reasoning tasks. Results consistently
show that through the additional leverage of negative feedback, NFT
significantly improves over SL baselines like Rejection sampling Fine-Tuning,
matching or even surpassing leading RL algorithms like GRPO and DAPO.
Furthermore, we demonstrate that NFT and GRPO are actually equivalent in
strict-on-policy training, even though they originate from entirely different
theoretical foundations. Our experiments and theoretical findings bridge the
gap between SL and RL methods in binary-feedback learning systems.

</details>


### [6] [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/abs/2505.17997)
*Jintian Shao,Yiming Cheng,Hongyi Huang,Beiwen Zhang,Zhiyu Wu,You Shan,Mingkai Zheng*

Main category: cs.LG

TL;DR: VAPO框架通过解决价值模型偏差、异构序列长度和稀疏奖励信号等问题，在提升大语言模型（LLMs）在长链推理任务中的强化学习效率和可靠性方面取得了显著的实证成功。本文从理论角度探讨了VAPO，分析其潜在机制及局限性，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管VAPO框架在实践中表现出显著的优势，但对其底层机制的深入理论理解及其潜在限制的认识对于指导未来的进步至关重要。因此，本文旨在从理论角度探讨VAPO的工作原理和可能的改进空间。

Method: 本文从理论层面探讨VAPO框架，重点关注以下几个方面：1) 复杂推理空间中的价值函数近似；2) 自适应优势估计的最优性；3) 令牌级别优化的影响；4) 探索与泛化的持久挑战。通过这些分析，文章揭示了VAPO假设可能被质疑的地方以及需要进一步研究的方向。

Result: 通过对VAPO的理论分析，文章明确了其在复杂推理任务中的优势和局限性，为开发更强大和可泛化的推理代理提供了指导。此外，文章还指出了当前方法在探索和泛化方面的持续挑战。

Conclusion: 本文总结了VAPO框架的理论基础，强调了其在处理长链推理任务时的价值函数近似、自适应优势估计等方面的重要性。同时，文章也指出了一些关键的理论问题和潜在的研究方向，例如如何改进探索策略和增强泛化能力，以推动未来更高效和可靠的推理代理的发展。

Abstract: The VAPO framework has demonstrated significant empirical success in
enhancing the efficiency and reliability of reinforcement learning for long
chain-of-thought (CoT) reasoning tasks with large language models (LLMs). By
systematically addressing challenges such as value model bias, heterogeneous
sequence lengths, and sparse reward signals, VAPO achieves state-of-the-art
performance. While its practical benefits are evident, a deeper theoretical
understanding of its underlying mechanisms and potential limitations is crucial
for guiding future advancements. This paper aims to initiate such a discussion
by exploring VAPO from a theoretical perspective, highlighting areas where its
assumptions might be challenged and where further investigation could yield
more robust and generalizable reasoning agents. We delve into the intricacies
of value function approximation in complex reasoning spaces, the optimality of
adaptive advantage estimation, the impact of token-level optimization, and the
enduring challenges of exploration and generalization.

</details>


### [7] [Outcome-based Reinforcement Learning to Predict the Future](https://arxiv.org/abs/2505.17989)
*Benjamin Turtel,Danny Franklin,Kris Skotheim,Luke Hewitt,Philipp Schoenegger*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reinforcement learning with verifiable rewards (RLVR) has boosted math and
coding in large language models, yet there has been little effort to extend
RLVR into messier, real-world domains like forecasting. One sticking point is
that outcome-based reinforcement learning for forecasting must learn from
binary, delayed, and noisy rewards, a regime where standard fine-tuning is
brittle. We show that outcome-only online RL on a 14B model can match
frontier-scale accuracy and surpass it in calibration and hypothetical
prediction market betting by adapting two leading algorithms, Group-Relative
Policy Optimisation (GRPO) and ReMax, to the forecasting setting. Our
adaptations remove per-question variance scaling in GRPO, apply
baseline-subtracted advantages in ReMax, hydrate training with 100k temporally
consistent synthetic questions, and introduce lightweight guard-rails that
penalise gibberish, non-English responses and missing rationales, enabling a
single stable pass over 110k events. Scaling ReMax to 110k questions and
ensembling seven predictions yields a 14B model that matches frontier baseline
o1 on accuracy on our holdout set (Brier = 0.193, p = 0.23) while beating it in
calibration (ECE = 0.042, p < 0.001). A simple trading rule turns this
calibration edge into \$127 of hypothetical profit versus \$92 for o1 (p =
0.037). This demonstrates that refined RLVR methods can convert small-scale
LLMs into potentially economically valuable forecasting tools, with
implications for scaling this to larger models.

</details>


### [8] [Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective](https://arxiv.org/abs/2505.17652)
*Deyang Kong,Qi Guo,Xiangyu Xi,Wei Wang,Jingang Wang,Xunliang Cai,Shikun Zhang,Wei Ye*

Main category: cs.LG

TL;DR: 强化学习在提升大语言模型推理能力方面具有潜力，但扩展性受限于低样本效率。本文提出了一种新的方法CDAS（Competence-Difficulty Alignment Sampling），通过聚合历史性能差异来准确稳定地估计问题难度，并利用固定点系统量化模型能力以自适应选择与模型当前能力相匹配的问题。实验结果表明，CDAS在多个数学基准测试中显著提高了准确性和效率，比Dynamic Sampling快2.33倍。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法尝试通过基于问题难度的调度来提高效率，但由于问题难度估计不稳定且偏差较大，未能很好地捕捉模型能力和问题难度之间的对齐关系，导致次优结果。

Method: 本文引入了CDAS方法，首先通过聚合问题的历史性能差异实现对问题难度的准确和稳定估计，然后利用固定点系统量化模型能力，从而自适应地选择与模型当前能力相匹配的问题进行训练。

Result: 实验结果显示，CDAS在多个具有挑战性的数学基准测试中取得了最高的平均准确率，并且相比Dynamic Sampling等竞争策略展现出显著的速度优势，后者比CDAS慢2.33倍。

Conclusion: CDAS通过更精确和稳定的问题难度估计以及模型能力的量化，在提高强化学习训练的准确性和效率方面表现出色，为解决现有方法的局限性提供了一种有效的解决方案。

Abstract: Reinforcement learning exhibits potential in enhancing the reasoning
abilities of large language models, yet it is hard to scale for the low sample
efficiency during the rollout phase. Existing methods attempt to improve
efficiency by scheduling problems based on problem difficulties. However, these
approaches suffer from unstable and biased estimations of problem difficulty
and fail to capture the alignment between model competence and problem
difficulty in RL training, leading to suboptimal results. To tackle these
limitations, this paper introduces \textbf{C}ompetence-\textbf{D}ifficulty
\textbf{A}lignment \textbf{S}ampling (\textbf{CDAS}), which enables accurate
and stable estimation of problem difficulties by aggregating historical
performance discrepancies of problems. Then the model competence is quantified
to adaptively select problems whose difficulty is in alignment with the model's
current competence using a fixed-point system. Experimental results across a
range of challenging mathematical benchmarks show that CDAS achieves great
improvements in both accuracy and efficiency. CDAS attains the highest average
accuracy against baselines and exhibits significant speed advantages compared
to Dynamic Sampling, a competitive strategy in DAPO, which is \textbf{2.33}
times slower than CDAS.

</details>


### [9] [Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration](https://arxiv.org/abs/2505.17621)
*Jingtong Gao,Ling Pan,Yejing Wang,Rui Zhong,Chi Lu,Qingpeng Cai,Peng Jiang,Xiangyu Zhao*

Main category: cs.LG

TL;DR: 提出了一种新的方法i-MENTOR，通过引入轨迹感知探索奖励、动态奖励缩放和优势保持奖励实现机制，解决了现有强化学习方法在大型语言模型推理中的稀疏奖励和探索不足问题，实验表明其在复杂推理任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法（如PPO和GRPO）在改进大型语言模型（LLMs）推理能力时存在依赖稀疏奖励信号和激励探索机制不足的问题，导致多步推理过程的效率低下，特别是在复杂推理任务中表现受限。

Method: i-MENTOR方法包含三个关键创新点：1) 轨迹感知探索奖励，减少标记级别策略中的偏差同时保持计算效率；2) 动态奖励缩放，稳定大动作空间中的探索与利用平衡；3) 优势保持奖励实现，维持优势分布完整性的同时加入探索指导。

Result: 实验结果表明，在三个公开数据集上的测试显示了i-MENTOR的有效性，尤其在困难数据集Countdown-4上实现了22.39%的性能提升。

Conclusion: i-MENTOR通过提供密集奖励和增强探索，有效缓解了现有强化学习方法在LLM推理中的局限性，为复杂推理任务提供了更高效的解决方案。

Abstract: Reinforcement learning (RL) has emerged as a pivotal method for improving the
reasoning capabilities of Large Language Models (LLMs). However, prevalent RL
approaches such as Proximal Policy Optimization (PPO) and Group-Regularized
Policy Optimization (GRPO) face critical limitations due to their reliance on
sparse outcome-based rewards and inadequate mechanisms for incentivizing
exploration. These limitations result in inefficient guidance for multi-step
reasoning processes. Specifically, sparse reward signals fail to deliver
effective or sufficient feedback, particularly for challenging problems.
Furthermore, such reward structures induce systematic biases that prioritize
exploitation of familiar trajectories over novel solution discovery. These
shortcomings critically hinder performance in complex reasoning tasks, which
inherently demand iterative refinement across ipntermediate steps. To address
these challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd
foR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense
rewards and amplify explorations in the RL-based training paradigm. i-MENTOR
introduces three key innovations: trajectory-aware exploration rewards that
mitigate bias in token-level strategies while maintaining computational
efficiency; dynamic reward scaling to stabilize exploration and exploitation in
large action spaces; and advantage-preserving reward implementation that
maintains advantage distribution integrity while incorporating exploratory
guidance. Experiments across three public datasets demonstrate i-MENTOR's
effectiveness with a 22.39% improvement on the difficult dataset Countdown-4.

</details>


### [10] [On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning](https://arxiv.org/abs/2505.17508)
*Yifan Zhang,Yifeng Liu,Huizhuo Yuan,Yang Yuan,Quanquan Gu,Andrew C Yao*

Main category: cs.LG

TL;DR: 本文提出了一种系统化的框架——正则化策略梯度（RPG），用于推导和分析在线强化学习中基于KL散度正则化的策略梯度方法。通过实验验证，该方法在训练稳定性和性能上优于或媲美现有强基线模型。


<details>
  <summary>Details</summary>
Motivation: 尽管KL正则化在策略梯度算法中被广泛使用以稳定训练，但如何系统地探索不同的KL散度公式并将其整合到代理损失函数中仍是一个值得深入研究的设计空间。

Method: 作者提出了一个名为RPG的框架，推导了基于前向和反向KL散度正则化的策略梯度及其对应的代理损失函数，并考虑了归一化与非归一化的策略分布。此外，还提供了完全可微的损失函数和REINFORCE风格的梯度估计器，满足不同算法需求。

Result: 在大规模语言模型推理任务中的强化学习实验表明，RPG方法在训练稳定性及性能上相比如GRPO、REINFORCE++和DAPO等强大基线模型表现出改进或具有竞争力的结果。

Conclusion: 本文提出的RPG框架提供了一个系统化的方法来设计和分析KL正则化的策略梯度算法，其有效性通过实验证明，未来可以进一步探索其在其他任务中的应用。

Abstract: Policy gradient algorithms have been successfully applied to enhance the
reasoning capabilities of large language models (LLMs). Despite the widespread
use of Kullback-Leibler (KL) regularization in policy gradient algorithms to
stabilize training, the systematic exploration of how different KL divergence
formulations can be estimated and integrated into surrogate loss functions for
online reinforcement learning (RL) presents a nuanced and systematically
explorable design space. In this paper, we propose regularized policy gradient
(RPG), a systematic framework for deriving and analyzing KL-regularized policy
gradient methods in the online RL setting. We derive policy gradients and
corresponding surrogate loss functions for objectives regularized by both
forward and reverse KL divergences, considering both normalized and
unnormalized policy distributions. Furthermore, we present derivations for
fully differentiable loss functions as well as REINFORCE-style gradient
estimators, accommodating diverse algorithmic needs. We conduct extensive
experiments on RL for LLM reasoning using these methods, showing improved or
competitive results in terms of training stability and performance compared to
strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at
https://github.com/complex-reasoning/RPG.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 在大规模语言模型（LLMs）中，通过重复采样来扩展推理计算会随着样本数量的增加而持续提高覆盖率。实验表明，这种改进部分是由于评估基准中的答案分布偏向于少量常见答案。研究提出了一种基于训练集答案频率的基线方法，在数学推理和事实知识两个领域中，该基线方法的表现优于某些LLMs的重复采样，或与混合策略相当。这有助于更准确地测量重复采样在提示无关猜测之外对覆盖率的提升。


<details>
  <summary>Details</summary>
Motivation: 研究者观察到，在大语言模型中，通过重复采样进行推理计算可以持续提高问题解决的覆盖率，并推测这一现象部分归因于标准评估基准中答案分布的偏斜，即大部分答案集中在少数常见答案上。为了验证这一假设并更好地理解重复采样的效果，研究者设计了一个基线方法，该方法根据训练集中答案的频率枚举可能的答案。

Method: 研究者定义了一个基线方法，该方法根据训练集中答案的出现频率枚举答案。通过在数学推理和事实知识两个领域进行实验，比较了该基线方法与重复采样以及一种混合策略的性能。混合策略结合了少量模型采样和基于枚举的猜测。

Result: 实验结果表明，对于某些LLMs，基线方法的性能优于重复采样；而对于其他LLMs，其覆盖率与混合策略相当。这说明重复采样的效果在很大程度上依赖于答案分布的特点。

Conclusion: 该研究提供了一种新的基线方法，用于更准确地测量重复采样在提示无关猜测之外对覆盖率的提升。这为理解LLMs在不同任务上的表现提供了更有价值的参考，并强调了答案分布对模型性能评估的重要性。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [12] [Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models](https://arxiv.org/abs/2505.17697)
*Zekai Zhao,Qi Liu,Kun Zhou,Zihan Liu,Yifei Shao,Zhiting Hu,Biwei Huang*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）中长链推理（CoT）能力的内部机制，发现通过放大最后几层中的高影响力激活和插入'wait' tokens，无需训练即可激发长CoT能力。此外，还提出了一种无需训练的激活控制技术和参数高效的微调方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在推理方面表现出色，但通常需要昂贵的强化学习或高质量数据的监督微调来激发其长链推理能力。因此，研究其内部机制以找到更高效的方法成为动机。

Method: 通过分析最后几层的高影响力激活，发现它们主要控制长形式推理属性。利用这些发现，引入了一种无需训练的激活控制技术，使用对比样例识别关键激活，并用解析函数在推理时调节其值。同时提出了一种参数高效的微调方法，仅训练最后一层的激活放大模块和少量LoRA层。

Result: 实验表明，该方法能有效激发LLMs的长CoT推理能力并提高性能。参数高效的微调方法在推理基准上显著优于完整的LoRA微调，且使用的参数更少。

Conclusion: 本文揭示了LLMs中长形式推理的内部机制，并提出了有效的无训练激活控制技术和参数高效的微调策略，为提高LLMs的推理能力和效率提供了新途径。

Abstract: Despite the remarkable reasoning performance, eliciting the long
chain-of-thought (CoT) ability in large language models (LLMs) typically
requires costly reinforcement learning or supervised fine-tuning on
high-quality distilled data. We investigate the internal mechanisms behind this
capability and show that a small set of high-impact activations in the last few
layers largely governs long-form reasoning attributes, such as output length
and self-reflection. By simply amplifying these activations and inserting
"wait" tokens, we can invoke the long CoT ability without any training,
resulting in significantly increased self-reflection rates and accuracy.
Moreover, we find that the activation dynamics follow predictable trajectories,
with a sharp rise after special tokens and a subsequent exponential decay.
Building on these insights, we introduce a general training-free activation
control technique. It leverages a few contrastive examples to identify key
activations, and employs simple analytic functions to modulate their values at
inference time to elicit long CoTs. Extensive experiments confirm the
effectiveness of our method in efficiently eliciting long CoT reasoning in LLMs
and improving their performance. Additionally, we propose a parameter-efficient
fine-tuning method that trains only a last-layer activation amplification
module and a few LoRA layers, outperforming full LoRA fine-tuning on reasoning
benchmarks with significantly fewer parameters. Our code and data are publicly
released.

</details>
