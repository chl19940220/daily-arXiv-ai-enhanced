{"id": "2505.00018", "keyword": "LLM Agent", "pdf": "https://arxiv.org/pdf/2505.00018", "abs": "https://arxiv.org/abs/2505.00018", "authors": ["Ju Wu", "Calvin K. L. Or"], "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability."}
{"id": "2410.15466", "keyword": "LLM reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing."}
{"id": "2402.00559", "keyword": "Chain of Thoughts", "pdf": "https://arxiv.org/pdf/2402.00559", "abs": "https://arxiv.org/abs/2402.00559", "authors": ["Alon Jacovi", "Yonatan Bitton", "Bernd Bohnet", "Jonathan Herzig", "Or Honovich", "Michael Tseng", "Michael Collins", "Roee Aharoni", "Mor Geva"], "title": "A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains", "categories": ["cs.CL"], "comment": "Accepted to ACL 2024", "summary": "Prompting language models to provide step-by-step answers (e.g.,\n\"Chain-of-Thought\") is the prominent approach for complex reasoning tasks,\nwhere more accurate reasoning chains typically improve downstream task\nperformance. Recent literature discusses automatic methods to verify reasoning\nto evaluate and improve their correctness. However, no fine-grained step-level\ndatasets are available to enable thorough evaluation of such verification\nmethods, hindering progress in this direction. We introduce REVEAL: Reasoning\nVerification Evaluation, a dataset to benchmark automatic verifiers of complex\nChain-of-Thought reasoning in open-domain question-answering settings. REVEAL\nincludes comprehensive labels for the relevance, attribution to evidence\npassages, and logical correctness of each reasoning step in a language model's\nanswer, across a variety of datasets and state-of-the-art language models.\nEvaluation on REVEAL shows that verifiers struggle at verifying reasoning\nchains - in particular, verifying logical correctness and detecting\ncontradictions. Available at https://reveal-dataset.github.io/ ."}
{"id": "2505.04493", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2505.04493", "abs": "https://arxiv.org/abs/2505.04493", "authors": ["Or Wertheim", "Ronen I. Brafman"], "title": "Model-Based AI planning and Execution Systems for Robotics", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Model-based planning and execution systems offer a principled approach to\nbuilding flexible autonomous robots that can perform diverse tasks by\nautomatically combining a host of basic skills. This idea is almost as old as\nmodern robotics. Yet, while diverse general-purpose reasoning architectures\nhave been proposed since, general-purpose systems that are integrated with\nmodern robotic platforms have emerged only recently, starting with the\ninfluential ROSPlan system. Since then, a growing number of model-based systems\nfor robot task-level control have emerged. In this paper, we consider the\ndiverse design choices and issues existing systems attempt to address, the\ndifferent solutions proposed so far, and suggest avenues for future\ndevelopment."}
{"id": "2412.02441", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2412.02441", "abs": "https://arxiv.org/abs/2412.02441", "authors": ["Shai Shalev-Shwartz", "Amnon Shashua", "Gal Beniamini", "Yoav Levine", "Or Sharir", "Noam Wies", "Ido Ben-Shaul", "Tomer Nussbaum", "Shir Granot Peled"], "title": "Artificial Expert Intelligence through PAC-reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Artificial Expert Intelligence (AEI) seeks to transcend the limitations of\nboth Artificial General Intelligence (AGI) and narrow AI by integrating\ndomain-specific expertise with critical, precise reasoning capabilities akin to\nthose of top human experts. Existing AI systems often excel at predefined tasks\nbut struggle with adaptability and precision in novel problem-solving. To\novercome this, AEI introduces a framework for ``Probably Approximately Correct\n(PAC) Reasoning\". This paradigm provides robust theoretical guarantees for\nreliably decomposing complex problems, with a practical mechanism for\ncontrolling reasoning precision. In reference to the division of human thought\ninto System 1 for intuitive thinking and System 2 for reflective\nreasoning~\\citep{tversky1974judgment}, we refer to this new type of reasoning\nas System 3 for precise reasoning, inspired by the rigor of the scientific\nmethod. AEI thus establishes a foundation for error-bounded, inference-time\nlearning."}
{"id": "2410.15466", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing."}
{"id": "2409.04397", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2409.04397", "abs": "https://arxiv.org/abs/2409.04397", "authors": ["Yotam Erel", "Or Kozlovsky-Mordenfeld", "Daisuke Iwai", "Kosuke Sato", "Amit H. Bermano"], "title": "Casper DPM: Cascaded Perceptual Dynamic Projection Mapping onto Hands", "categories": ["cs.GR"], "comment": "Project page: https://yoterel.github.io/casper-project-page/", "summary": "We present a technique for dynamically projecting 3D content onto human hands\nwith short perceived motion-to-photon latency. Computing the pose and shape of\nhuman hands accurately and quickly is a challenging task due to their\narticulated and deformable nature. We combine a slower 3D coarse estimation of\nthe hand pose with high speed 2D correction steps which improve the alignment\nof the projection to the hands, increase the projected surface area, and reduce\nperceived latency. Since our approach leverages a full 3D reconstruction of the\nhands, any arbitrary texture or reasonably performant effect can be applied,\nwhich was not possible before. We conducted two user studies to assess the\nbenefits of using our method. The results show subjects are less sensitive to\nlatency artifacts and perform faster and with more ease a given associated task\nover the naive approach of directly projecting rendered frames from the 3D pose\nestimation. We demonstrate several novel use cases and applications."}
{"id": "2505.22334", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.22334", "abs": "https://arxiv.org/abs/2505.22334", "authors": ["Lai Wei", "Yuting Li", "Kaipeng Zheng", "Chen Wang", "Yue Wang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on\nMathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start."}
{"id": "2408.08654", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2408.08654", "abs": "https://arxiv.org/abs/2408.08654", "authors": ["Li-Fang Zhu", "Fritz Koermann", "Qing Chen", "Malin Selleby", "Joerg Neugebauer", "and Blazej Grabowski"], "title": "Accelerating ab initio melting property calculations with machine learning: Application to the high entropy alloy TaVCrW", "categories": ["cond-mat.mtrl-sci"], "comment": "14 pages, 6 figures", "summary": "Melting properties are critical for designing novel materials, especially for\ndiscovering high-performance, high-melting refractory materials. Experimental\nmeasurements of these properties are extremely challenging due to their high\nmelting temperatures. Complementary theoretical predictions are, therefore,\nindispensable. The conventional free energy approach using density functional\ntheory (DFT) has been a gold standard for such purposes because of its high\naccuracy. However,it generally involves expensive thermodynamic integration\nusing ab initio molecular dynamic simulations. The high computational cost\nmakes high-throughput calculations infeasible. Here, we propose a highly\nefficient DFT-based method aided by a specially designed machine learning\npotential. As the machine learning potential can closely reproduce the ab\ninitio phase space, even for multi-component alloys, the costly thermodynamic\nintegration can be fully substituted with more efficient free energy\nperturbation calculations. The method achieves overall savings of computational\nresources by 80% compared to current alternatives. We apply the method to the\nhigh-entropy alloy TaVCrW and calculate its melting properties, including\nmelting temperature, entropy and enthalpy of fusion, and volume change at the\nmelting point. Additionally, the heat capacities of solid and liquid TaVCrW are\ncalculated. The results agree reasonably with the calphad extrapolated values."}
{"id": "2505.22312", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.22312", "abs": "https://arxiv.org/abs/2505.22312", "authors": ["Jujie He", "Jiacai Liu", "Chris Yuhao Liu", "Rui Yan", "Chaojie Wang", "Peng Cheng", "Xiaoyu Zhang", "Fuxiang Zhang", "Jiacheng Xu", "Wei Shen", "Siyuan Li", "Liang Zeng", "Tianwen Wei", "Cheng Cheng", "Bo An", "Yang Liu", "Yahui Zhou"], "title": "Skywork Open Reasoner 1 Technical Report", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets."}
{"id": "2403.11729", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2403.11729", "abs": "https://arxiv.org/abs/2403.11729", "authors": ["Kento Kawaharazuka", "Akihiro Miki", "Masahiro Bando", "Temma Suzuki", "Yoshimoto Ribayashi", "Yasunori Toshimitsu", "Yuya Nagamatsu", "Kei Okada", "and Masayuki Inaba"], "title": "Hardware Design and Learning-Based Software Architecture of Musculoskeletal Wheeled Robot Musashi-W for Real-World Applications", "categories": ["cs.RO"], "comment": "Accepted at Humanoids2022", "summary": "Various musculoskeletal humanoids have been developed so far. While these\nhumanoids have the advantage of their flexible and redundant bodies that mimic\nthe human body, they are still far from being applied to real-world tasks. One\nof the reasons for this is the difficulty of bipedal walking in a flexible\nbody. Thus, we developed a musculoskeletal wheeled robot, Musashi-W, by\ncombining a wheeled base and musculoskeletal upper limbs for real-world\napplications. Also, we constructed its software system by combining static and\ndynamic body schema learning, reflex control, and visual recognition. We show\nthat the hardware and software of Musashi-W can make the most of the advantages\nof the musculoskeletal upper limbs, through several tasks of cleaning by human\nteaching, carrying a heavy object considering muscle addition, and setting a\ntable through dynamic cloth manipulation with variable stiffness."}
{"id": "2505.21908", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.21908", "abs": "https://arxiv.org/abs/2505.21908", "authors": ["Hanyin Wang", "Zhenbang Wu", "Gururaj Kolar", "Hariprasad Korsapati", "Brian Bartlett", "Bryan Hull", "Jimeng Sun"], "title": "Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement\nand operations but require labor-intensive assignment. Large Language Models\n(LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of\nthe task: pretraining corpora rarely contain private clinical or billing data.\nWe introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL)\nfor automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained\nwith Group Relative Policy Optimization (GRPO) using rule-based rewards,\nDRG-Sapphire introduces a series of RL enhancements to address domain-specific\nchallenges not seen in previous mathematical tasks. Our model achieves\nstate-of-the-art accuracy on the MIMIC-IV benchmark and generates\nphysician-validated reasoning for DRG assignments, significantly enhancing\nexplainability. Our study further sheds light on broader challenges of applying\nRL to knowledge-intensive, OOD tasks. We observe that RL performance scales\napproximately linearly with the logarithm of the number of supervised\nfine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally\nconstrained by the domain knowledge encoded in the base model. For OOD tasks\nlike DRG coding, strong RL performance requires sufficient knowledge infusion\nprior to RL. Consequently, scaling SFT may be more effective and\ncomputationally efficient than scaling RL alone for such tasks."}
{"id": "2403.00458", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2403.00458", "abs": "https://arxiv.org/abs/2403.00458", "authors": ["Chung Yi See", "Vasco Rato Santos", "Lucas Woodley", "Megan Yeo", "Daniel Palmer", "Shuheng Zhang", "and Ashley Nunes"], "title": "Prices and preferences in the electric vehicle market", "categories": ["econ.EM"], "comment": "Main paper: 5 tables, 2 figures", "summary": "Although electric vehicles are less polluting than gasoline powered vehicles,\nadoption is challenged by higher procurement prices. Existing discourse\nemphasizes EV battery costs as being principally responsible for this price\ndifferential and widespread adoption is routinely conditioned upon battery\ncosts declining. We scrutinize such reasoning by sourcing data on EV attributes\nand market conditions between 2011 and 2023. Our findings are fourfold. First,\nEV prices are influenced principally by the number of amenities, additional\nfeatures, and dealer-installed accessories sold as standard on an EV, and to a\nlesser extent, by EV horsepower. Second, EV range is negatively correlated with\nEV price implying that range anxiety concerns may be less consequential than\nexisting discourse suggests. Third, battery capacity is positively correlated\nwith EV price, due to more capacity being synonymous with the delivery of more\nhorsepower. Collectively, this suggests that higher procurement prices for EVs\nreflects consumer preference for vehicles that are feature dense and more\npowerful. Fourth and finally, accommodating these preferences have produced\nvehicles with lower fuel economy, a shift that reduces envisioned lifecycle\nemissions benefits by at least 3.26 percent, subject to the battery pack\nchemistry leveraged and the carbon intensity of the electrical grid. These\nfindings warrant attention as decarbonization efforts increasingly emphasize\nelectrification as a pathway for complying with domestic and international\nclimate agreements."}
{"id": "2505.21807", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.21807", "abs": "https://arxiv.org/abs/2505.21807", "authors": ["Tommy Xu", "Zhitian Zhang", "Xiangyu Sun", "Lauren Kelly Zung", "Hossein Hajimirsadeghi", "Greg Mori"], "title": "TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Predictive modeling on tabular data is the cornerstone of many real-world\napplications. Although gradient boosting machines and some recent deep models\nachieve strong performance on tabular data, they often lack interpretability.\nOn the other hand, large language models (LLMs) have demonstrated powerful\ncapabilities to generate human-like reasoning and explanations, but remain\nunder-performed for tabular data prediction. In this paper, we propose a new\napproach that leverages reasoning-based LLMs, trained using reinforcement\nlearning, to perform more accurate and explainable predictions on tabular data.\nOur method introduces custom reward functions that guide the model not only\ntoward high prediction accuracy but also toward human-understandable reasons\nfor its predictions. Experimental results show that our model achieves\npromising performance on financial benchmark datasets, outperforming most\nexisting LLMs."}
{"id": "2402.00559", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2402.00559", "abs": "https://arxiv.org/abs/2402.00559", "authors": ["Alon Jacovi", "Yonatan Bitton", "Bernd Bohnet", "Jonathan Herzig", "Or Honovich", "Michael Tseng", "Michael Collins", "Roee Aharoni", "Mor Geva"], "title": "A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains", "categories": ["cs.CL"], "comment": "Accepted to ACL 2024", "summary": "Prompting language models to provide step-by-step answers (e.g.,\n\"Chain-of-Thought\") is the prominent approach for complex reasoning tasks,\nwhere more accurate reasoning chains typically improve downstream task\nperformance. Recent literature discusses automatic methods to verify reasoning\nto evaluate and improve their correctness. However, no fine-grained step-level\ndatasets are available to enable thorough evaluation of such verification\nmethods, hindering progress in this direction. We introduce REVEAL: Reasoning\nVerification Evaluation, a dataset to benchmark automatic verifiers of complex\nChain-of-Thought reasoning in open-domain question-answering settings. REVEAL\nincludes comprehensive labels for the relevance, attribution to evidence\npassages, and logical correctness of each reasoning step in a language model's\nanswer, across a variety of datasets and state-of-the-art language models.\nEvaluation on REVEAL shows that verifiers struggle at verifying reasoning\nchains - in particular, verifying logical correctness and detecting\ncontradictions. Available at https://reveal-dataset.github.io/ ."}
{"id": "2505.21493", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.21493", "abs": "https://arxiv.org/abs/2505.21493", "authors": ["Xiangxin Zhou", "Zichen Liu", "Anya Sims", "Haonan Wang", "Tianyu Pang", "Chongxuan Li", "Liang Wang", "Min Lin", "Chao Du"], "title": "Reinforcing General Reasoning without Verifiers", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The recent paradigm shift towards training large language models (LLMs) using\nDeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has\nled to impressive advancements in code and mathematical reasoning. However,\nthis methodology is limited to tasks where rule-based answer verification is\npossible and does not naturally extend to real-world domains such as chemistry,\nhealthcare, engineering, law, biology, business, and economics. Current\npractical workarounds use an additional LLM as a model-based verifier; however,\nthis introduces issues such as reliance on a strong verifier LLM,\nsusceptibility to reward hacking, and the practical burden of maintaining the\nverifier model in memory during training. To address this and extend\nDeepSeek-R1-Zero-style training to general reasoning domains, we propose a\nverifier-free method (VeriFree) that bypasses answer verification and instead\nuses RL to directly maximize the probability of generating the reference\nanswer. We compare VeriFree with verifier-based methods and demonstrate that,\nin addition to its significant practical benefits and reduced compute\nrequirements, VeriFree matches and even surpasses verifier-based methods on\nextensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related\nbenchmarks. Moreover, we provide insights into this method from multiple\nperspectives: as an elegant integration of training both the policy and\nimplicit verifier in a unified model, and as a variational optimization\napproach. Code is available at https://github.com/sail-sg/VeriFree."}
{"id": "2208.08580", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2208.08580", "abs": "https://arxiv.org/abs/2208.08580", "authors": ["Gopal Sharma", "Kangxue Yin", "Subhransu Maji", "Evangelos Kalogerakis", "Or Litany", "Sanja Fidler"], "title": "MvDeCor: Multi-view Dense Correspondence Learning for Fine-grained 3D Segmentation", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "project page: https://nv-tlabs.github.io/MvDeCor/", "summary": "We propose to utilize self-supervised techniques in the 2D domain for\nfine-grained 3D shape segmentation tasks. This is inspired by the observation\nthat view-based surface representations are more effective at modeling\nhigh-resolution surface details and texture than their 3D counterparts based on\npoint clouds or voxel occupancy. Specifically, given a 3D shape, we render it\nfrom multiple views, and set up a dense correspondence learning task within the\ncontrastive learning framework. As a result, the learned 2D representations are\nview-invariant and geometrically consistent, leading to better generalization\nwhen trained on a limited number of labeled shapes compared to alternatives\nthat utilize self-supervision in 2D or 3D alone. Experiments on textured\n(RenderPeople) and untextured (PartNet) 3D datasets show that our method\noutperforms state-of-the-art alternatives in fine-grained part segmentation.\nThe improvements over baselines are greater when only a sparse set of views is\navailable for training or when shapes are textured, indicating that MvDeCor\nbenefits from both 2D processing and 3D geometric reasoning."}
{"id": "2505.21444", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.21444", "abs": "https://arxiv.org/abs/2505.21444", "authors": ["Sheikh Shafayat", "Fahim Tajwar", "Ruslan Salakhutdinov", "Jeff Schneider", "Andrea Zanette"], "title": "Can Large Reasoning Models Self-Train?", "categories": ["cs.LG"], "comment": "Project website: https://self-rewarding-llm-training.github.io/", "summary": "Scaling the performance of large language models (LLMs) increasingly depends\non methods that reduce reliance on human supervision. Reinforcement learning\nfrom automated verification offers an alternative, but it incurs scalability\nlimitations due to dependency upon human-designed verifiers. Self-training,\nwhere the model's own judgment provides the supervisory signal, presents a\ncompelling direction. We propose an online self-training reinforcement learning\nalgorithm that leverages the model's self-consistency to infer correctness\nsignals and train without any ground-truth supervision. We apply the algorithm\nto challenging mathematical reasoning tasks and show that it quickly reaches\nperformance levels rivaling reinforcement-learning methods trained explicitly\non gold-standard answers. Additionally, we analyze inherent limitations of the\nalgorithm, highlighting how the self-generated proxy reward initially\ncorrelated with correctness can incentivize reward hacking, where confidently\nincorrect outputs are favored. Our results illustrate how self-supervised\nimprovement can achieve significant performance gains without external labels,\nwhile also revealing its fundamental challenges."}
{"id": "2110.02210", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2110.02210", "abs": "https://arxiv.org/abs/2110.02210", "authors": ["Alexey Nekrasov", "Jonas Schult", "Or Litany", "Bastian Leibe", "Francis Engelmann"], "title": "Mix3D: Out-of-Context Data Augmentation for 3D Scenes", "categories": ["cs.CV"], "comment": "Accepted for publication at 3DV 2021. Camera-ready submission. Link\n  to code: https://github.com/kumuji/mix3d - Project page:\n  https://nekrasov.dev/mix3d/", "summary": "We present Mix3D, a data augmentation technique for segmenting large-scale 3D\nscenes. Since scene context helps reasoning about object semantics, current\nworks focus on models with large capacity and receptive fields that can fully\ncapture the global context of an input 3D scene. However, strong contextual\npriors can have detrimental implications like mistaking a pedestrian crossing\nthe street for a car. In this work, we focus on the importance of balancing\nglobal scene context and local geometry, with the goal of generalizing beyond\nthe contextual priors in the training set. In particular, we propose a \"mixing\"\ntechnique which creates new training samples by combining two augmented scenes.\nBy doing so, object instances are implicitly placed into novel out-of-context\nenvironments and therefore making it harder for models to rely on scene context\nalone, and instead infer semantics from local structure as well. We perform\ndetailed analysis to understand the importance of global context, local\nstructures and the effect of mixing scenes. In experiments, we show that models\ntrained with Mix3D profit from a significant performance boost on indoor\n(ScanNet, S3DIS) and outdoor datasets (SemanticKITTI). Mix3D can be trivially\nused with any existing method, e.g., trained with Mix3D, MinkowskiNet\noutperforms all prior state-of-the-art methods by a significant margin on the\nScanNet test benchmark 78.1 mIoU. Code is available at:\nhttps://nekrasov.dev/mix3d/"}
{"id": "2505.21097", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.21097", "abs": "https://arxiv.org/abs/2505.21097", "authors": ["Stephen Chung", "Wenyu Du", "Jie Fu"], "title": "Thinker: Learning to Think Fast and Slow", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.6; I.2.8; I.5.1"], "comment": "21 pages", "summary": "Recent studies show that the reasoning capabilities of Large Language Models\n(LLMs) can be improved by applying Reinforcement Learning (RL) to\nquestion-answering (QA) tasks in areas such as math and coding. With a long\ncontext length, LLMs may learn to perform search, as indicated by the\nself-correction behavior observed in DeepSeek R1. However, this search behavior\nis often imprecise and lacks confidence, resulting in long, redundant responses\nand highlighting deficiencies in intuition and verification. Inspired by the\nDual Process Theory in psychology, we introduce a simple modification to the QA\ntask that includes four stages: Fast Thinking, where the LLM must answer within\na strict token budget; Verification, where the model evaluates its initial\nresponse; Slow Thinking, where it refines the initial response with more\ndeliberation; and Summarization, where it distills the refinement from the\nprevious stage into precise steps. Our proposed task improves average accuracy\nfrom 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for\nDeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone\nachieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial\ninference efficiency gains. These findings suggest that intuition and\ndeliberative reasoning are distinct, complementary systems benefiting from\ntargeted training."}
{"id": "2505.20732", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.20732", "abs": "https://arxiv.org/abs/2505.20732", "authors": ["Hanlin Wang", "Chak Tou Leong", "Jiashuo Wang", "Jian Wang", "Wenjie Li"], "title": "SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) holds significant promise for training LLM agents\nto handle complex, goal-oriented tasks that require multi-step interactions\nwith external environments. However, a critical challenge when applying RL to\nthese agentic tasks arises from delayed rewards: feedback signals are typically\navailable only after the entire task is completed. This makes it non-trivial to\nassign delayed rewards to earlier actions, providing insufficient guidance\nregarding environmental constraints and hindering agent training. In this work,\nwe draw on the insight that the ultimate completion of a task emerges from the\ncumulative progress an agent makes across individual steps. We propose Stepwise\nProgress Attribution (SPA), a general reward redistribution framework that\ndecomposes the final reward into stepwise contributions, each reflecting its\nincremental progress toward overall task completion. To achieve this, we train\na progress estimator that accumulates stepwise contributions over a trajectory\nto match the task completion. During policy optimization, we combine the\nestimated per-step contribution with a grounding signal for actions executed in\nthe environment as the fine-grained, intermediate reward for effective agent\ntraining. Extensive experiments on common agent benchmarks (including Webshop,\nALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the\nstate-of-the-art method in both success rate (+2.5\\% on average) and grounding\naccuracy (+1.9\\% on average). Further analyses demonstrate that our method\nremarkably provides more effective intermediate rewards for RL training. Our\ncode is available at https://github.com/WangHanLinHenry/SPA-RL-Agent."}
{"id": "2505.20686", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.20686", "abs": "https://arxiv.org/abs/2505.20686", "authors": ["Kiant√© Brantley", "Mingyu Chen", "Zhaolin Gao", "Jason D. Lee", "Wen Sun", "Wenhao Zhan", "Xuezhou Zhang"], "title": "Accelerating RL for LLM Reasoning with Optimal Advantage Regression", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning\nlarge language models (LLMs) to improve complex reasoning abilities. However,\nstate-of-the-art policy optimization methods often suffer from high\ncomputational overhead and memory consumption, primarily due to the need for\nmultiple generations per prompt and the reliance on critic networks or\nadvantage estimates of the current policy. In this paper, we propose $A$*-PO, a\nnovel two-stage policy optimization framework that directly approximates the\noptimal advantage function and enables efficient training of LLMs for reasoning\ntasks. In the first stage, we leverage offline sampling from a reference policy\nto estimate the optimal value function $V$*, eliminating the need for costly\nonline value estimation. In the second stage, we perform on-policy updates\nusing a simple least-squares regression loss with only a single generation per\nprompt. Theoretically, we establish performance guarantees and prove that the\nKL-regularized RL objective can be optimized without requiring complex\nexploration strategies. Empirically, $A$*-PO achieves competitive performance\nacross a wide range of mathematical reasoning benchmarks, while reducing\ntraining time by up to 2$\\times$ and peak memory usage by over 30% compared to\nPPO, GRPO, and REBEL. Implementation of $A$*-PO can be found at\nhttps://github.com/ZhaolinGao/A-PO."}
{"id": "2505.20671", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.20671", "abs": "https://arxiv.org/abs/2505.20671", "authors": ["Heng Tan", "Hua Yan", "Yu Yang"], "title": "LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "While reinforcement learning (RL) has achieved notable success in various\ndomains, training effective policies for complex tasks remains challenging.\nAgents often converge to local optima and fail to maximize long-term rewards.\nExisting approaches to mitigate training bottlenecks typically fall into two\ncategories: (i) Automated policy refinement, which identifies critical states\nfrom past trajectories to guide policy updates, but suffers from costly and\nuncertain model training; and (ii) Human-in-the-loop refinement, where human\nfeedback is used to correct agent behavior, but this does not scale well to\nenvironments with large or continuous action spaces. In this work, we design a\nlarge language model-guided policy modulation framework that leverages LLMs to\nimprove RL training without additional model training or human intervention. We\nfirst prompt an LLM to identify critical states from a sub-optimal agent's\ntrajectories. Based on these states, the LLM then provides action suggestions\nand assigns implicit rewards to guide policy refinement. Experiments across\nstandard RL benchmarks demonstrate that our method outperforms state-of-the-art\nbaselines, highlighting the effectiveness of LLM-based explanations in\naddressing RL training bottlenecks."}
