<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: 本文是一篇立场论文，对人类与AI代理协作的近期经验性进展进行了批判性综述。文章指出当前缺乏一个统一的理论框架来整合这些多样化的研究，并提出了一个新颖的概念架构——分层探索-利用网络（Hierarchical Exploration-Exploitation Net），以系统地连接多代理协调、知识管理、控制反馈回路和高层控制机制等技术细节。通过将现有贡献映射到这一框架中，作者希望推动传统方法的改进并启发融合定性和定量范式的新型研究。文章结构灵活，可从任意部分阅读，既作为技术实现的批判性回顾，也为设计或扩展人类-AI共生关系提供了前瞻性的参考。


<details>
  <summary>Details</summary>
Motivation: 尽管人类与AI代理的协作在技术上取得了显著成就，但仍然存在持续的技术鸿沟，尤其是在处理开放性、复杂任务时。目前的研究领域缺乏一个能够统一整合各类研究的理论框架，因此需要一种新的概念架构来填补这一空白。

Method: 作者提出了一种名为分层探索-利用网络（Hierarchical Exploration-Exploitation Net）的新颖概念架构，该架构系统地连接了多代理协调、知识管理、控制反馈回路和高层控制机制。此外，通过将现有的研究成果（包括符号AI技术、基于连接主义的大语言模型代理以及混合组织实践）映射到这一框架中，文章为重新审视传统方法和开发新方法提供了路径。

Result: 该方法不仅促进了对现有技术的深入理解，还为未来研究指明了方向，特别是如何融合定性和定量范式来推动人类认知与AI能力的共同进化。

Conclusion: 本文提出的概念框架为深化人类认知与AI能力的共进化提供了一个重要的起点。通过整合多代理系统、知识管理和控制机制等方面的技术细节，文章为设计或扩展人类-AI共生关系提供了理论支持和实践指导。

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO](https://arxiv.org/abs/2505.17017)
*Chengzhuo Tong,Ziyu Guo,Renrui Zhang,Wenyu Shan,Xinyu Wei,Zhenghao Xing,Hongsheng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 本文探讨了强化学习（RL）算法在自回归图像生成中的应用，特别是Group Relative Policy Optimization (GRPO)和Direct Preference Optimization (DPO)两种算法的性能及奖励模型的影响。研究揭示了不同算法的优势，并提出了三种扩展策略以提升其领域内和领域外的泛化能力，为未来开发更有效的RL算法提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）已在增强大型语言模型（LLMs）的思维链（CoT）推理能力方面取得显著进展，但在自回归图像生成中，如何确保文本-图像一致性、提高图像美学质量以及设计复杂的奖励模型仍存在独特挑战。目前的研究对这些领域特定的挑战及不同RL策略的特性分析不足，因此需要深入探讨GRPO和DPO等算法在该领域的表现。

Method: 研究者对GRPO和DPO算法进行了全面评估，包括它们在自回归图像生成中的领域内性能和领域外泛化能力。同时，分析了不同奖励模型对其能力的影响，并系统地探索了三种流行的扩展策略，以提升这两种算法在领域内外的表现。

Result: 研究表明，GRPO和DPO算法各有优势，且具有更强内在泛化能力的奖励模型可以增强所应用RL算法的泛化潜力。此外，三种扩展策略能够有效提升算法的性能，为未来开发更高效的RL算法提供了新的见解。

Conclusion: 本研究首次深入分析了GRPO和DPO算法在自回归图像生成中的表现及其与奖励模型的关系。研究结果表明，选择合适的奖励模型和扩展策略对于提升RL算法在图像生成任务中的性能至关重要。这些发现为未来在自回归图像生成中实现鲁棒CoT推理的RL算法开发奠定了基础。

Abstract: Recent advancements underscore the significant role of Reinforcement Learning
(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large
language models (LLMs). Two prominent RL algorithms, Direct Preference
Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central
to these developments, showcasing different pros and cons. Autoregressive image
generation, also interpretable as a sequential CoT reasoning process, presents
unique challenges distinct from LLM-based CoT reasoning. These encompass
ensuring text-image consistency, improving image aesthetic quality, and
designing sophisticated reward models, rather than relying on simpler
rule-based rewards. While recent efforts have extended RL to this domain, these
explorations typically lack an in-depth analysis of the domain-specific
challenges and the characteristics of different RL strategies. To bridge this
gap, we provide the first comprehensive investigation of the GRPO and DPO
algorithms in autoregressive image generation, evaluating their in-domain
performance and out-of-domain generalization, while scrutinizing the impact of
different reward models on their respective capabilities. Our findings reveal
that GRPO and DPO exhibit distinct advantages, and crucially, that reward
models possessing stronger intrinsic generalization capabilities potentially
enhance the generalization potential of the applied RL algorithms. Furthermore,
we systematically explore three prevalent scaling strategies to enhance both
their in-domain and out-of-domain proficiency, deriving unique insights into
efficiently scaling performance for each paradigm. We hope our study paves a
new path for inspiring future work on developing more effective RL algorithms
to achieve robust CoT reasoning in the realm of autoregressive image
generation. Code is released at
https://github.com/ZiyuGuo99/Image-Generation-CoT

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Divide-Fuse-Conquer: Eliciting "Aha Moments" in Multi-Scenario Games](https://arxiv.org/abs/2505.16401)
*Xiaoqing Zhang,Huabin Zheng,Ang Lv,Yuhan Liu,Zirui Song,Flood Sung,Xiuying Chen,Rui Yan*

Main category: cs.LG

TL;DR: 本文提出了一种名为Divide-Fuse-Conquer的框架，旨在通过强化学习提升大型语言模型在多场景游戏中的泛化能力。实验表明，使用该策略训练的Qwen2.5-32B-Align模型在TextArena游戏中表现优异，可与Claude3.5媲美。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在数学、编程和视觉任务中取得了显著效果，但在多场景游戏中，由于规则、交互模式和环境复杂性的多样性，模型往往难以泛化到不同场景。为解决这一问题并提升模型的泛化能力，研究者提出了新的方法。

Method: Divide-Fuse-Conquer框架包含三个主要步骤：1) 将游戏基于规则和难度等特征进行分组（Divide）；2) 为每组训练专门的模型以优化其性能；3) 融合不同组模型的参数形成新模型，并继续跨组训练，直到所有场景都被有效覆盖（Fuse）。

Result: 实验结果表明，采用Divide-Fuse-Conquer策略训练的Qwen2.5-32B-Align模型在18个TextArena游戏中表现出色，达到与Claude3.5相当的水平，取得了7胜4平的成绩。

Conclusion: Divide-Fuse-Conquer框架为提升多场景强化学习中的泛化能力提供了有效的解决方案，未来的研究可以在此基础上进一步探索如何利用强化学习改进大型语言模型的泛化性能。

Abstract: Large language models (LLMs) have been observed to suddenly exhibit advanced
reasoning abilities during reinforcement learning (RL), resembling an ``aha
moment'' triggered by simple outcome-based rewards. While RL has proven
effective in eliciting such breakthroughs in tasks involving mathematics,
coding, and vision, it faces significant challenges in multi-scenario games.
The diversity of game rules, interaction modes, and environmental complexities
often leads to policies that perform well in one scenario but fail to
generalize to others. Simply combining multiple scenarios during training
introduces additional challenges, such as training instability and poor
performance. To overcome these challenges, we propose Divide-Fuse-Conquer, a
framework designed to enhance generalization in multi-scenario RL. This
approach starts by heuristically grouping games based on characteristics such
as rules and difficulties. Specialized models are then trained for each group
to excel at games in the group is what we refer to as the divide step. Next, we
fuse model parameters from different groups as a new model, and continue
training it for multiple groups, until the scenarios in all groups are
conquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align
trained with the Divide-Fuse-Conquer strategy reaches a performance level
comparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can
inspire future research on using reinforcement learning to improve the
generalization of LLMs.

</details>


### [4] [SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning](https://arxiv.org/abs/2505.16368)
*Huanyu Liu,Jia Li,Hao Zhu,Kechi Zhang,Yihong Dong,Ge Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于SAT问题的强化学习框架Saturn，旨在通过可扩展的任务构建、规则验证和精确难度控制来提升大语言模型（LLMs）的推理能力。实验表明，该方法在SAT问题、数学和编程任务上均显著提升了LLMs的表现。


<details>
  <summary>Details</summary>
Motivation: 目前的强化学习任务存在三个主要限制：(1) 可扩展性差，依赖大量人工标注或昂贵的LLM合成数据；(2) 验证困难，LLM输出难以自动可靠地验证；(3) 难度控制不足，大多数任务缺乏精细的难度分级，导致训练过程难以循序渐进。因此，需要一种新的方法来解决这些问题并有效释放LLMs的推理能力。

Method: 作者提出了Saturn框架，利用布尔可满足性（SAT）问题来训练和评估LLMs的推理能力。具体而言，Saturn具备以下特点：
- **可扩展任务构建**：通过自动生成SAT问题，减少对人工标注的依赖。
- **规则验证**：基于明确的规则自动验证模型输出，确保可靠性。
- **精确难度控制**：设计了课程学习管道，逐步增加任务难度，从简单到复杂训练LLMs。
此外，作者还引入了一个包含2,660个不同难度的SAT问题的数据集Saturn-2.6k，并开发了两个改进版模型Saturn-1.5B和Saturn-7B。

Result: 实验结果表明：
1. 在SAT问题上，Saturn-1.5B和Saturn-7B分别实现了平均+14.0和+28.1的pass@3提升。
2. 在数学和编程任务中，Saturn-1.5B和Saturn-7B分别在基准测试（如AIME、LiveCodeBench）中提高了平均分数+4.9和+1.8。
3. 与当前最先进的RL任务构建方法相比，Saturn进一步提升了+8.8%的性能。
这些结果证明了Saturn的有效性和优越性。

Conclusion: Saturn框架成功解决了现有RL任务中的关键限制，包括可扩展性、验证可靠性和难度控制。它通过SAT问题提供了一种新颖且高效的方法来训练和评估LLMs的推理能力。作者开源了代码、数据和模型，为未来的研究提供了有力支持。

Abstract: How to design reinforcement learning (RL) tasks that effectively unleash the
reasoning capability of large language models (LLMs) remains an open question.
Existing RL tasks (e.g., math, programming, and constructing reasoning tasks)
suffer from three key limitations: (1) Scalability. They rely heavily on human
annotation or expensive LLM synthesis to generate sufficient training data. (2)
Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3)
Controllable Difficulty. Most tasks lack fine-grained difficulty control,
making it hard to train LLMs to develop reasoning ability from easy to hard.
  To address these limitations, we propose Saturn, a SAT-based RL framework
that uses Boolean Satisfiability (SAT) problems to train and evaluate LLM
reasoning. Saturn enables scalable task construction, rule-based verification,
and precise difficulty control. Saturn designs a curriculum learning pipeline
that continuously improves LLMs' reasoning capability by constructing SAT tasks
of increasing difficulty and training LLMs from easy to hard. To ensure stable
training, we design a principled mechanism to control difficulty transitions.
  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying
difficulty. It supports the evaluation of how LLM reasoning changes with
problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain
Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT
problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of
+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B
and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,
AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in
constructing RL tasks, Saturn achieves further improvements of +8.8%. We
release the source code, data, and models to support future research.

</details>


### [5] [AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training](https://arxiv.org/abs/2505.16363)
*Huishuai Zhang,Bohan Wang,Luoxin Chen*

Main category: cs.LG

TL;DR: 提出了一种名为AdamS的新优化器，它通过新颖的分母设计去除了对二阶矩估计的需求，从而在大语言模型预训练和后训练中表现出高效性、简单性和优越的优化性能。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型优化器如Adam需要二阶矩估计，这增加了计算和内存负担。而观察到Transformer目标函数具有$(L_0, L_1)$平滑性特性，局部平滑性由梯度大小决定，且可以进一步用动量大小近似，因此提出了无需二阶矩估计的AdamS优化器。

Method: AdamS采用了一个新的分母，即动量和当前梯度平方的加权和的平方根，从而避免了对二阶矩的估计。这种方法与带有动量的SGD一样高效，同时保留了优于Adam的优化性能。并且可以直接继承AdamW的超参数，无需修改优化器API或架构。

Result: 在多项任务中，包括GPT-2和Llama2（高达13B参数）的预训练以及强化学习中的后训练场景下，AdamS均展现出强大的性能。

Conclusion: AdamS以其高效性、简单性和理论支持成为现有优化器的一个有力替代方案，适用于大语言模型的预训练和后训练阶段。

Abstract: We introduce AdamS, a simple yet effective alternative to Adam for large
language model (LLM) pretraining and post-training. By leveraging a novel
denominator, i.e., the root of weighted sum of squares of the momentum and the
current gradient, AdamS eliminates the need for second-moment estimates. Hence,
AdamS is efficient, matching the memory and compute footprint of SGD with
momentum while delivering superior optimization performance. Moreover, AdamS is
easy to adopt: it can directly inherit hyperparameters of AdamW, and is
entirely model-agnostic, integrating seamlessly into existing pipelines without
modifications to optimizer APIs or architectures. The motivation behind AdamS
stems from the observed $(L_0, L_1)$ smoothness properties in transformer
objectives, where local smoothness is governed by gradient magnitudes that can
be further approximated by momentum magnitudes. We establish rigorous
theoretical convergence guarantees and provide practical guidelines for
hyperparameter selection. Empirically, AdamS demonstrates strong performance in
various tasks, including pre-training runs on GPT-2 and Llama2 (up to 13B
parameters) and reinforcement learning in post-training regimes. With its
efficiency, simplicity, and theoretical grounding, AdamS stands as a compelling
alternative to existing optimizers.

</details>


### [6] [Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning](https://arxiv.org/abs/2505.15311)
*Yurun Yuan,Fan Chen,Zeyu Jia,Alexander Rakhlin,Tengyang Xie*

Main category: cs.LG

TL;DR: 本文提出了一种名为轨迹贝尔曼残差最小化（TBRM）的算法，将经典的贝尔曼残差最小化思想应用于大型语言模型（LLM），以优化单一轨迹级别的贝尔曼目标。该方法无需批评者、重要性采样比率或剪辑，并且在数学推理基准测试中表现优于基于策略的方法，如PPO和GRPO，同时具有相当或更低的计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习（RL）管道中，基于策略的方法主导了大型语言模型（LLM）推理领域，而基于价值的方法尚未得到充分探索。因此，作者重新审视了经典的贝尔曼残差最小化范式，并试图将其应用于LLM推理。

Method: TBRM算法通过使用模型自身的logits作为$Q$-值来优化单一轨迹级别的贝尔曼目标。它不需要批评者、重要性采样比率或剪辑，并且只需要每个提示进行一次rollout。此外，作者通过改进的轨迹度量分析证明了该算法从任意离线数据收敛到接近最优的KL正则化策略。

Result: 实验结果表明，在标准数学推理基准上，TBRM一致地优于基于策略的基线方法（如PPO和GRPO），并且具有相当或更低的计算和内存开销。这表明基于价值的RL可能是增强LLM推理能力的一种有原则且高效的替代方案。

Conclusion: 基于价值的强化学习方法（如TBRM）为提高大型语言模型的推理能力提供了一个新的、有效的方向。这种方法不仅简化了算法设计，还展示了与现有基于策略方法相比的竞争性能和效率优势。

Abstract: Policy-based methods currently dominate reinforcement learning (RL) pipelines
for large language model (LLM) reasoning, leaving value-based approaches
largely unexplored. We revisit the classical paradigm of Bellman Residual
Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an
algorithm that naturally adapts this idea to LLMs, yielding a simple yet
effective off-policy algorithm that optimizes a single trajectory-level Bellman
objective using the model's own logits as $Q$-values. TBRM removes the need for
critics, importance-sampling ratios, or clipping, and operates with only one
rollout per prompt. We prove convergence to the near-optimal KL-regularized
policy from arbitrary off-policy data via an improved
change-of-trajectory-measure analysis. Experiments on standard
mathematical-reasoning benchmarks show that TBRM consistently outperforms
policy-based baselines, like PPO and GRPO, with comparable or lower
computational and memory overhead. Our results indicate that value-based RL
might be a principled and efficient alternative for enhancing reasoning
capabilities in LLMs.

</details>


### [7] [Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One](https://arxiv.org/abs/2505.15306)
*Yiwen Song,Qianyue Hao,Qingmin Liao,Jian Yuan,Yong Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为LLM-Ens的新方法，通过大语言模型（LLMs）增强强化学习中的模型集成策略。该方法根据任务语义理解将状态分类为不同的情境，并动态选择在当前情境下表现最佳的代理。实验表明，LLM-Ens显著提升了RL模型集成的效果，超越了现有基线20.9%。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习取得了广泛的成功，但训练有效的智能体仍然面临诸多挑战，例如算法选择、超参数设置和随机种子选择等都会显著影响性能。现有的集成方法（如多数投票和Boltzmann加权）由于缺乏对特定任务的语义理解，其适应性和有效性受到限制。因此需要一种新的集成方法来克服这些限制。

Method: LLM-Ens方法首先利用大语言模型（LLMs）对任务中的状态进行分类，将其划分为不同的'情境'，并结合任务条件的高层描述。然后统计分析每个独立代理在不同情境下的优劣势。在推理阶段，LLM-Ens动态识别任务情境的变化，并切换到在当前情境下表现最佳的代理，从而实现动态模型选择。该方法兼容使用不同随机种子、超参数设置和多种RL算法训练的代理。

Result: 在Atari基准测试上的广泛实验表明，LLM-Ens显著提高了RL模型集成的性能，超越了已知的基线方法最多达20.9%。此外，为了促进可重复性研究，作者开源了代码。

Conclusion: LLM-Ens通过引入大语言模型的任务语义理解，成功增强了RL模型集成的效果，在多个任务中展现了优越性能和良好的适应性。这种方法为未来强化学习代理集成的研究提供了新方向。

Abstract: Model ensemble is a useful approach in reinforcement learning (RL) for
training effective agents. Despite wide success of RL, training effective
agents remains difficult due to the multitude of factors requiring careful
tuning, such as algorithm selection, hyperparameter settings, and even random
seed choices, all of which can significantly influence an agent's performance.
Model ensemble helps overcome this challenge by combining multiple weak agents
into a single, more powerful one, enhancing overall performance. However,
existing ensemble methods, such as majority voting and Boltzmann addition, are
designed as fixed strategies and lack a semantic understanding of specific
tasks, limiting their adaptability and effectiveness. To address this, we
propose LLM-Ens, a novel approach that enhances RL model ensemble with
task-specific semantic understandings driven by large language models (LLMs).
Given a task, we first design an LLM to categorize states in this task into
distinct 'situations', incorporating high-level descriptions of the task
conditions. Then, we statistically analyze the strengths and weaknesses of each
individual agent to be used in the ensemble in each situation. During the
inference time, LLM-Ens dynamically identifies the changing task situation and
switches to the agent that performs best in the current situation, ensuring
dynamic model selection in the evolving task condition. Our approach is
designed to be compatible with agents trained with different random seeds,
hyperparameter settings, and various RL algorithms. Extensive experiments on
the Atari benchmark show that LLM-Ens significantly improves the RL model
ensemble, surpassing well-known baselines by up to 20.9%. For reproducibility,
our code is open-source at
https://anonymous.4open.science/r/LLM4RLensemble-F7EE.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [8] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 在大规模语言模型（LLMs）中，通过重复采样扩展推理计算可以持续增加覆盖率（解决问题的比例）。本文研究了这种提升是否部分归因于标准评估基准中的答案分布偏向少数常见答案。为此，作者定义了一个基于训练集答案频率的基线方法，并在数学推理和事实知识两个领域进行了实验。结果表明，对于某些LLM，该基线优于重复采样；而对于其他模型，其表现与一种混合策略相当（该策略仅使用10次模型采样并结合枚举猜测剩余的答案）。此基线有助于更准确地衡量重复采样在提示无关猜测之外对覆盖率的改进。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型中，通过重复采样进行推理计算时，随着样本数量增加，问题解决覆盖率也相应提高。然而，研究者怀疑这一现象可能部分归因于评估基准中的答案分布偏向少数常见答案，因此需要设计一个基线来验证这一假设。

Method: 作者定义了一个基线方法，该方法根据训练集中答案的出现频率枚举答案。然后，在数学推理和事实知识两个领域对多个LLM进行了实验，比较了基线方法、重复采样以及一种混合策略的表现。

Result: 实验结果显示，对于某些LLM，基于训练集答案频率的基线方法优于重复采样；而对于其他模型，其覆盖率与混合策略相当，后者仅使用少量模型采样并结合枚举猜测其余答案。

Conclusion: 通过引入基于训练集答案频率的基线方法，研究者能够更准确地评估重复采样在提示无关猜测之外对覆盖率的实际改进效果。这为理解LLM在不同任务上的性能提供了新的视角。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [9] [SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation](https://arxiv.org/abs/2505.16637)
*Wenjie Yang,Mao Zheng,Mingyang Song,Zheng Li*

Main category: cs.CL

TL;DR: 提出了一种无需参考的简单自我奖励（SSR）强化学习框架，用于机器翻译任务。该框架利用自判断奖励信号进行训练，在减少对外部监督信号依赖的同时，提升了翻译性能。使用13K单语数据和Qwen-2.5-7B作为基础模型，SSR-Zero-7B在多个基准测试中超越了现有专用于机器翻译的大语言模型和其他更大规模的通用模型。进一步结合外部COMET监督，SSR-X-Zero-7B模型达到了最先进的英语与中文互译效果，超越了包括GPT-4o和Gemini 1.5 Pro在内的闭源模型。研究表明，自我奖励机制在机器翻译中的有效性优于外部LLM-as-a-judge方法，并且与训练好的奖励模型相结合时具有互补优势。此研究揭示了自我改进强化学习方法的潜力，并公开发布了代码、数据和模型。


<details>
  <summary>Details</summary>
Motivation: 当前先进的机器翻译专用大语言模型在训练过程中高度依赖昂贵且难以扩展的外部监督信号，例如人工标注的参考数据或训练好的奖励模型。为了降低这种依赖并提升效率，本文提出了一个无需参考、完全在线且仅依赖自我判断奖励的强化学习框架——Simple Self-Rewarding (SSR) RL。

Method: SSR框架通过自判断奖励信号对模型进行训练，无需参考数据或外部监督。具体而言，使用13K单语数据和Qwen-2.5-7B作为基础模型，构建了一个名为SSR-Zero-7B的模型。此外，还通过引入来自COMET的外部监督增强了SSR框架，形成了更强的SSR-X-Zero-7B模型。

Result: SSR-Zero-7B在WMT23、WMT24和Flores200等多个基准测试中，超越了现有的机器翻译专用大语言模型（如TowerInstruct-13B和GemmaX-28-9B），以及更大规模的通用大语言模型（如Qwen2.5-32B-Instruct）。而增强版SSR-X-Zero-7B模型更是达到了最先进的英语与中文互译效果，不仅超过了所有参数量小于72B的开源模型，还超越了闭源模型如GPT-4o和Gemini 1.5 Pro。

Conclusion: 研究表明，自我奖励机制在机器翻译任务中比外部LLM-as-a-judge方法更有效，并且当与训练好的奖励模型结合时表现出互补优势。这为未来自我改进的强化学习方法提供了宝贵的见解。同时，作者公开了代码、数据和模型，促进了相关领域的进一步研究和发展。

Abstract: Large language models (LLMs) have recently demonstrated remarkable
capabilities in machine translation (MT). However, most advanced MT-specific
LLMs heavily rely on external supervision signals during training, such as
human-annotated reference data or trained reward models (RMs), which are often
expensive to obtain and challenging to scale. To overcome this limitation, we
propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for
MT that is reference-free, fully online, and relies solely on self-judging
rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as
the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,
e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like
Qwen2.5-32B-Instruct in English $\leftrightarrow$ Chinese translation tasks
from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR
with external supervision from COMET, our strongest model, SSR-X-Zero-7B,
achieves state-of-the-art performance in English $\leftrightarrow$ Chinese
translation, surpassing all existing open-source models under 72B parameters
and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.
Our analysis highlights the effectiveness of the self-rewarding mechanism
compared to the external LLM-as-a-judge approach in MT and demonstrates its
complementary benefits when combined with trained RMs. Our findings provide
valuable insight into the potential of self-improving RL methods. We have
publicly released our code, data and models.

</details>


### [10] [WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.16421)
*Zhepei Wei,Wenlin Yao,Yao Liu,Weizhi Zhang,Qin Lu,Liang Qiu,Changlong Yu,Puyang Xu,Chao Zhang,Bing Yin,Hyokun Yun,Lihong Li*

Main category: cs.CL

TL;DR: 本研究提出了一种名为WebAgent-R1的端到端多轮强化学习框架，用于训练网络代理。通过在线与网络环境交互生成多样轨迹，并仅由任务成功与否的二元奖励引导。实验表明，该框架显著提高了Qwen-2.5-3B和Llama-3.1-8B在WebArena-Lite基准上的任务成功率，并超过现有最先进方法和强大的专有模型如OpenAI o3。深入分析揭示了基于思考的提示策略和测试时扩展的有效性，还探讨了不同的强化学习初始化策略及其变体WebAgent-R1-Zero和WebAgent-R1-CoT的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）在提升大语言模型（LLMs）方面取得了显著成就，但主要集中在单轮任务上，例如解决数学问题。对于需要长时间决策的动态网络界面多轮交互训练有效的网络代理仍然具有挑战性。因此，本文旨在开发一种简单而有效的端到端多轮强化学习框架来应对这一挑战。

Method: WebAgent-R1是一个端到端多轮强化学习框架，它直接从与网络环境的在线交互中学习。该框架通过异步生成多样轨迹进行训练，并完全由任务成功与否的二元奖励引导。此外，引入了两个变体：WebAgent-R1-Zero和WebAgent-R1-CoT，以研究不同的强化学习初始化策略。前者强调行为克隆（warm-up training stage）的重要性，后者提供了将长链推理（CoT）融入网络代理的见解。

Result: 在WebArena-Lite基准上的实验表明，WebAgent-R1大幅提升了Qwen-2.5-3B的任务成功率从6.1%至33.9%，Llama-3.1-8B的任务成功率从8.5%至44.8%，显著优于现有的最先进的方法和强大的专有模型如OpenAI o3。深入分析进一步证明了基于思考的提示策略和通过增加交互进行测试时扩展的有效性。

Conclusion: WebAgent-R1作为一种简单的端到端多轮强化学习框架，有效提升了网络代理在复杂、动态网络界面中的任务成功率。其通过在线交互学习和二元奖励引导的方式，不仅在实验中表现出色，而且揭示了行为克隆和长链推理在网络代理中的重要性。这为未来的研究提供了宝贵的启示和方向。

Abstract: While reinforcement learning (RL) has demonstrated remarkable success in
enhancing large language models (LLMs), it has primarily focused on single-turn
tasks such as solving math problems. Training effective web agents for
multi-turn interactions remains challenging due to the complexity of
long-horizon decision-making across dynamic web interfaces. In this work, we
present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework
for training web agents. It learns directly from online interactions with web
environments by asynchronously generating diverse trajectories, entirely guided
by binary rewards depending on task success. Experiments on the WebArena-Lite
benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task
success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to
44.8%, significantly outperforming existing state-of-the-art methods and strong
proprietary models such as OpenAI o3. In-depth analyses reveal the
effectiveness of the thinking-based prompting strategy and test-time scaling
through increased interactions for web tasks. We further investigate different
RL initialization policies by introducing two variants, namely WebAgent-R1-Zero
and WebAgent-R1-CoT, which highlight the importance of the warm-up training
stage (i.e., behavior cloning) and provide insights on incorporating long
chain-of-thought (CoT) reasoning in web agents.

</details>


### [11] [Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning](https://arxiv.org/abs/2505.16410)
*Guanting Dong,Yifei Chen,Xiaoxi Li,Jiajie Jin,Hongjin Qian,Yutao Zhu,Hangyu Mao,Guorui Zhou,Zhicheng Dou,Ji-Rong Wen*

Main category: cs.CL

TL;DR: Tool-Star是一个基于强化学习（RL）的框架，旨在增强大语言模型（LLMs）在逐步推理过程中自主调用多个外部工具的能力。它通过工具集成推理数据合成管道和两阶段训练框架来解决工具使用数据稀缺的问题，并提升多工具协作推理能力。实验表明其在多个推理基准上具有高效性和有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模强化学习使大语言模型展现出卓越的推理能力，但如何利用强化学习算法有效实现LLMs中的多工具协同推理仍是一个未解决的挑战。现有的方法在生成高质量工具使用数据和促进多工具协作方面存在不足。

Method: Tool-Star集成了六种类型的工具，并在数据合成和训练中采用了系统化设计：
1. 提出了一般工具集成推理数据合成管道，结合工具集成提示与基于提示的采样技术，自动生成工具使用轨迹。
2. 采用质量归一化和难度感知分类过程对样本进行过滤和组织。
3. 设计了两阶段训练框架：冷启动微调引导LLMs探索推理模式；多工具自我批评RL算法通过分层奖励设计加强奖励理解和工具协作。

Result: 实验分析显示，Tool-Star在超过10个具有挑战性的推理基准测试中表现出了高效性和有效性，证明了该框架在提升LLMs多工具协作推理能力方面的显著优势。

Conclusion: Tool-Star通过强化学习有效地增强了大语言模型在多工具协作推理方面的能力。其提出的工具集成推理数据合成管道和两阶段训练框架为解决工具使用数据稀缺问题提供了创新解决方案，同时促进了更有效的工具协作。项目代码已开源，可供进一步研究和应用。

Abstract: Recently, large language models (LLMs) have shown remarkable reasoning
capabilities via large-scale reinforcement learning (RL). However, leveraging
the RL algorithm to empower effective multi-tool collaborative reasoning in
LLMs remains an open challenge. In this paper, we introduce Tool-Star, an
RL-based framework designed to empower LLMs to autonomously invoke multiple
external tools during stepwise reasoning. Tool-Star integrates six types of
tools and incorporates systematic designs in both data synthesis and training.
To address the scarcity of tool-use data, we propose a general tool-integrated
reasoning data synthesis pipeline, which combines tool-integrated prompting
with hint-based sampling to automatically and scalably generate tool-use
trajectories. A subsequent quality normalization and difficulty-aware
classification process filters out low-quality samples and organizes the
dataset from easy to hard. Furthermore, we propose a two-stage training
framework to enhance multi-tool collaborative reasoning by: (1) cold-start
fine-tuning, which guides LLMs to explore reasoning patterns via
tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with
hierarchical reward design, which reinforces reward understanding and promotes
effective tool collaboration. Experimental analyses on over 10 challenging
reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.
The code is available at https://github.com/dongguanting/Tool-Star.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [12] [HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving](https://arxiv.org/abs/2505.15793)
*Zhiwen Chen,Bo Leng,Zhuoren Li,Hanming Deng,Guizhe Jin,Ran Yu,Huanxi Wen*

Main category: cs.RO

TL;DR: 本文提出了一种新的LLM-Hinted RL范式，通过结合大语言模型和强化学习来提升自动驾驶在复杂场景下的性能。该范式中的HCRMP架构包括增强语义表示模块、上下文稳定性锚定模块和语义缓存模块，能够在不同交通密度下实现高达80.3%的任务成功率，并在关键安全驾驶条件下降低11.4%的碰撞率。


<details>
  <summary>Details</summary>
Motivation: 当前以LLM为主导的强化学习方法过于依赖LLM输出，容易产生幻觉问题，这直接影响了驾驶策略的性能。为了解决这一问题，需要保持LLM与RL之间的相对独立性。

Method: 提出了LLM-Hinted RL范式及HCRMP架构：1) 增强语义表示模块扩展状态空间；2) 上下文稳定性锚定模块利用知识库信息提高多批评权重提示的可靠性；3) 语义缓存模块将LLM低频指导与RL高频控制无缝结合。

Result: 实验表明，HCRMP在多种驾驶条件下任务成功率达到80.3%，并在关键安全驾驶条件下将碰撞率降低了11.4%，显著提升了复杂场景下的驾驶性能。

Conclusion: LLM-Hinted RL范式能够有效解决LLM幻觉问题并提升自动驾驶性能，所提出的HCRMP架构展示了强大的整体驾驶能力，特别是在复杂和高密度交通场景中表现出色。

Abstract: Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can
enhance autonomous driving (AD) performance in complex scenarios. However,
current LLM-Dominated RL methods over-rely on LLM outputs, which are prone to
hallucinations. Evaluations show that state-of-the-art LLM indicates a
non-hallucination rate of only approximately 57.95% when assessed on essential
driving-related tasks. Thus, in these methods, hallucinations from the LLM can
directly jeopardize the performance of driving policies. This paper argues that
maintaining relative independence between the LLM and the RL is vital for
solving the hallucinations problem. Consequently, this paper is devoted to
propose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic
hints for state augmentation and policy optimization to assist RL agent in
motion planning, while the RL agent counteracts potential erroneous semantic
indications through policy learning to achieve excellent driving performance.
Based on this paradigm, we propose the HCRMP (LLM-Hinted Contextual
Reinforcement Learning Motion Planner) architecture, which is designed that
includes Augmented Semantic Representation Module to extend state space.
Contextual Stability Anchor Module enhances the reliability of multi-critic
weight hints by utilizing information from the knowledge base. Semantic Cache
Module is employed to seamlessly integrate LLM low-frequency guidance with RL
high-frequency control. Extensive experiments in CARLA validate HCRMP's strong
overall driving performance. HCRMP achieves a task success rate of up to 80.3%
under diverse driving conditions with different traffic densities. Under
safety-critical driving conditions, HCRMP significantly reduces the collision
rate by 11.4%, which effectively improves the driving performance in complex
scenarios.

</details>
