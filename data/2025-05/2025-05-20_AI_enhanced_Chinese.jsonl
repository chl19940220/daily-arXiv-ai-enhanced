{"id": "2410.15466", "keyword": "LLM reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing.", "keywords": ["LLM reasoning"], "AI": {"tldr": "\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\uff0c\u901a\u8fc7\u91cd\u590d\u91c7\u6837\u6269\u5c55\u63a8\u7406\u8ba1\u7b97\u4f1a\u968f\u7740\u6837\u672c\u6570\u91cf\u7684\u589e\u52a0\u800c\u6301\u7eed\u63d0\u9ad8\u8986\u76d6\u7387\uff08\u89e3\u51b3\u95ee\u9898\u7684\u6bd4\u4f8b\uff09\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u6539\u8fdb\u90e8\u5206\u5f52\u56e0\u4e8e\u8bc4\u4f30\u57fa\u51c6\u7b54\u6848\u5206\u5e03\u7684\u504f\u5dee\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8861\u91cf\u91cd\u590d\u91c7\u6837\u7684\u5b9e\u9645\u6548\u679c\u3002", "motivation": "\u91cd\u590d\u91c7\u6837\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u63a8\u7406\u4efb\u52a1\u4e2d\u80fd\u663e\u8457\u63d0\u5347\u95ee\u9898\u89e3\u51b3\u7684\u8986\u76d6\u7387\u3002\u7136\u800c\uff0c\u7814\u7a76\u8005\u63a8\u6d4b\u8fd9\u4e00\u63d0\u5347\u53ef\u80fd\u90e8\u5206\u6e90\u4e8e\u6807\u51c6\u8bc4\u4f30\u57fa\u51c6\u4e2d\u7b54\u6848\u5206\u5e03\u7684\u504f\u501a\uff08\u5373\u7b54\u6848\u96c6\u4e2d\u4e8e\u4e00\u5c0f\u90e8\u5206\u5e38\u89c1\u7b54\u6848\uff09\u3002\u4e3a\u4e86\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\u5e76\u66f4\u51c6\u786e\u5730\u8861\u91cf\u91cd\u590d\u91c7\u6837\u7684\u6548\u679c\uff0c\u9700\u8981\u8bbe\u8ba1\u4e00\u79cd\u57fa\u7ebf\u65b9\u6cd5\u6765\u6a21\u62df\u8fd9\u79cd\u7b54\u6848\u5206\u5e03\u7279\u6027\u3002", "method": "\u7814\u7a76\u8005\u5b9a\u4e49\u4e86\u4e00\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6839\u636e\u8bad\u7ec3\u96c6\u4e2d\u7b54\u6848\u7684\u9891\u7387\u679a\u4e3e\u7b54\u6848\u3002\u901a\u8fc7\u5728\u6570\u5b66\u63a8\u7406\u548c\u4e8b\u5b9e\u77e5\u8bc6\u4e24\u4e2a\u9886\u57df\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u6b64\u57fa\u7ebf\u65b9\u6cd5\u4e0e\u91cd\u590d\u6a21\u578b\u91c7\u6837\u7684\u8868\u73b0\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u6df7\u5408\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u4ec5\u4f7f\u752810\u6b21\u6a21\u578b\u91c7\u6837\uff0c\u5269\u4f59\u7684\u7b54\u6848\u5219\u901a\u8fc7\u679a\u4e3e\u731c\u6d4b\u5b8c\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u67d0\u4e9b\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\u4f18\u4e8e\u91cd\u590d\u6a21\u578b\u91c7\u6837\uff1b\u800c\u5728\u5176\u4ed6\u6a21\u578b\u4e2d\uff0c\u5176\u8868\u73b0\u4e0e\u6df7\u5408\u7b56\u7565\u76f8\u5f53\uff0c\u5373\u7528\u5c11\u91cf\u6a21\u578b\u91c7\u6837\u7ed3\u5408\u679a\u4e3e\u731c\u6d4b\u80fd\u8fbe\u5230\u76f8\u4f3c\u7684\u8986\u76d6\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7cbe\u786e\u7684\u65b9\u5f0f\uff0c\u7528\u4e8e\u6d4b\u91cf\u5728\u7279\u5b9a\u8bbe\u7f6e\u4e0b\u91cd\u590d\u91c7\u6837\u76f8\u5bf9\u4e8e\u65e0\u63d0\u793a\u731c\u6d4b\u6240\u80fd\u5e26\u6765\u7684\u989d\u5916\u8986\u76d6\u63d0\u5347\u3002\u8fd9\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u91cd\u590d\u91c7\u6837\u7684\u5b9e\u9645\u4f5c\u7528\u4ee5\u53ca\u8bc4\u4f30\u57fa\u51c6\u7b54\u6848\u5206\u5e03\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002"}}
{"id": "2505.00018", "keyword": "LLM Agent", "pdf": "https://arxiv.org/pdf/2505.00018", "abs": "https://arxiv.org/abs/2505.00018", "authors": ["Ju Wu", "Calvin K. L. Or"], "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability.", "keywords": ["LLM Agent"], "AI": {"tldr": "\u672c\u6587\u662f\u4e00\u7bc7\u7acb\u573a\u8bba\u6587\uff0c\u5168\u9762\u56de\u987e\u4e86\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u534f\u4f5c\u9886\u57df\u7684\u6700\u65b0\u5b9e\u8bc1\u8fdb\u5c55\u3002\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6982\u5ff5\u6846\u67b6\u2014\u2014\u5206\u5c42\u63a2\u7d22-\u5229\u7528\u7f51\u7edc\uff08Hierarchical Exploration-Exploitation Net\uff09\uff0c\u65e8\u5728\u7cfb\u7edf\u6574\u5408\u591a\u4ee3\u7406\u534f\u8c03\u3001\u77e5\u8bc6\u7ba1\u7406\u3001\u63a7\u5236\u8bba\u53cd\u9988\u5faa\u73af\u548c\u9ad8\u7ea7\u63a7\u5236\u673a\u5236\u7b49\u6280\u672f\u7ec6\u8282\u3002\u8be5\u6846\u67b6\u4e0d\u4ec5\u80fd\u591f\u4fee\u8ba2\u4f20\u7edf\u65b9\u6cd5\uff0c\u8fd8\u4e3a\u878d\u5408\u5b9a\u6027\u548c\u5b9a\u91cf\u8303\u5f0f\u7684\u65b0\u7814\u7a76\u63d0\u4f9b\u4e86\u7075\u611f\u3002\u6587\u7ae0\u7ed3\u6784\u7075\u6d3b\uff0c\u53ef\u4ece\u4efb\u4f55\u90e8\u5206\u5f00\u59cb\u9605\u8bfb\uff0c\u65e2\u662f\u5bf9\u73b0\u6709\u6280\u672f\u5b9e\u73b0\u7684\u6279\u5224\u6027\u7efc\u8ff0\uff0c\u4e5f\u4e3a\u672a\u6765\u8bbe\u8ba1\u6216\u6269\u5c55\u4eba\u673a\u5171\u751f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u524d\u77bb\u6027\u7684\u53c2\u8003\u3002", "motivation": "\u5f53\u524d\u5173\u4e8e\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u534f\u4f5c\u7684\u7814\u7a76\u867d\u7136\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6280\u672f\u6210\u5c31\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u8bb8\u591a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002\u7279\u522b\u662f\uff0c\u5728\u5904\u7406\u5f00\u653e\u6027\u3001\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u6574\u5408\u5404\u79cd\u4e0d\u540c\u7684\u7814\u7a76\u65b9\u5411\u548c\u6280\u672f\u6210\u679c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6982\u5ff5\u67b6\u6784\uff0c\u4ee5\u4fc3\u8fdb\u8fd9\u4e00\u9886\u57df\u7684\u53d1\u5c55\u5e76\u63a8\u52a8\u4eba\u673a\u534f\u540c\u8fdb\u5316\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5206\u6790\u73b0\u6709\u7684\u7814\u7a76\u6210\u679c\uff0c\u5305\u62ec\u7b26\u53f7AI\u6280\u672f\u3001\u57fa\u4e8e\u8fde\u63a5\u4e3b\u4e49\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u4ee5\u53ca\u6df7\u5408\u578b\u7ec4\u7ec7\u5b9e\u8df5\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a\u5206\u5c42\u63a2\u7d22-\u5229\u7528\u7f51\u7edc\uff08Hierarchical Exploration-Exploitation Net\uff09\u7684\u65b0\u6846\u67b6\u3002\u8be5\u6846\u67b6\u7cfb\u7edf\u5730\u8fde\u63a5\u4e86\u591a\u4ee3\u7406\u534f\u8c03\u3001\u77e5\u8bc6\u7ba1\u7406\u3001\u63a7\u5236\u8bba\u53cd\u9988\u56de\u8def\u548c\u9ad8\u5c42\u6b21\u63a7\u5236\u673a\u5236\u7b49\u591a\u4e2a\u65b9\u9762\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5143\u7d20\u6620\u5c04\u5230\u73b0\u6709\u8d21\u732e\u4e2d\uff0c\u4ece\u800c\u63ed\u793a\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u5e76\u542f\u53d1\u4e86\u65b0\u65b9\u5411\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u4e0d\u4ec5\u6709\u52a9\u4e8e\u91cd\u65b0\u5ba1\u89c6\u4f20\u7edf\u7684\u9057\u7559\u65b9\u6cd5\uff0c\u8fd8\u80fd\u591f\u6fc0\u52b1\u7814\u7a76\u8005\u5f00\u53d1\u878d\u5408\u5b9a\u6027\u4e0e\u5b9a\u91cf\u8303\u5f0f\u7684\u521b\u65b0\u5de5\u4f5c\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u4e3a\u8bbe\u8ba1\u6216\u6269\u5c55\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u4e4b\u95f4\u7684\u5171\u751f\u5173\u7cfb\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u8fdb\u4e00\u6b65\u63a8\u52a8\u4e86\u4eba\u673a\u8ba4\u77e5\u80fd\u529b\u7684\u5171\u540c\u8fdb\u5316\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u534f\u4f5c\u9886\u57df\u7684\u73b0\u72b6\uff0c\u6307\u51fa\u4e86\u5176\u6280\u672f\u6210\u5c31\u53ca\u5b58\u5728\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u5ff5\u6846\u67b6\u3002\u8be5\u6846\u67b6\u53ef\u4ee5\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u7684\u57fa\u7840\uff0c\u5e2e\u52a9\u5b9e\u73b0\u66f4\u6df1\u5c42\u6b21\u7684\u4eba\u7c7b\u8ba4\u77e5\u4e0eAI\u80fd\u529b\u7684\u534f\u540c\u8fdb\u5316\u3002"}}
{"id": "2505.13438", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.13438", "abs": "https://arxiv.org/abs/2505.13438", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u63d0\u5347\u63a8\u7406\u80fd\u529b\u5bf9\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u6700\u5927\u5316\u63a8\u7406\u8fc7\u7a0b\u7ed3\u675f\u65f6\u83b7\u5f97\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u4ec5\u4f18\u5316\u6700\u7ec8\u6027\u80fd\uff0c\u5bfc\u81f4\u8bad\u7ec3\u548c\u90e8\u7f72\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6AnytimeReasoner\uff0c\u65e8\u5728\u4f18\u5316\u968f\u65f6\u63a8\u7406\u6027\u80fd\uff0c\u63d0\u9ad8token\u6548\u7387\u548c\u63a8\u7406\u7075\u6d3b\u6027\u3002\u901a\u8fc7\u622a\u65ad\u5b8c\u6574\u601d\u8003\u8fc7\u7a0b\u5e76\u5f15\u5165\u53ef\u9a8c\u8bc1\u5bc6\u96c6\u5956\u52b1\uff0c\u6539\u8fdb\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u5dee\u51cf\u5c11\u6280\u672fBudget Relative Policy Optimization (BRPO)\uff0c\u4ee5\u589e\u5f3a\u5b66\u4e60\u8fc7\u7a0b\u7684\u7a33\u5065\u6027\u548c\u6548\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8eGRPO\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u548ctoken\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u867d\u7136\u80fd\u591f\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u4e3b\u8981\u5173\u6ce8\u5728\u56fa\u5b9a\u7684\u5927token\u9884\u7b97\u4e0b\u7684\u6700\u7ec8\u6027\u80fd\uff0c\u5ffd\u7565\u4e86\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4e0d\u540c\u9636\u6bb5\u7684\u4f18\u5316\u9700\u6c42\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u540ctoken\u9884\u7b97\u7ea6\u675f\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86AnytimeReasoner\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5148\u9a8c\u5206\u5e03\u4e2d\u91c7\u6837token\u9884\u7b97\u5e76\u622a\u65ad\u5b8c\u6574\u7684\u601d\u8003\u8fc7\u7a0b\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u6bcf\u4e2a\u622a\u65ad\u70b9\u751f\u6210\u6700\u4f18\u7b54\u6848\u6458\u8981\u4ee5\u4f9b\u9a8c\u8bc1\u3002\u6b64\u8fc7\u7a0b\u5f15\u5165\u4e86\u53ef\u9a8c\u8bc1\u7684\u5bc6\u96c6\u5956\u52b1\uff0c\u6709\u52a9\u4e8e\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u4fe1\u7528\u5206\u914d\u3002\u63a5\u7740\uff0c\u4f5c\u8005\u91c7\u7528\u5206\u79bb\u7684\u65b9\u5f0f\u4f18\u5316\u601d\u8003\u7b56\u7565\u548c\u6458\u8981\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u5316\u7d2f\u79ef\u5956\u52b1\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5b66\u4e60\u8fc7\u7a0b\u7684\u7a33\u5065\u6027\u548c\u6548\u7387\uff0c\u6587\u7ae0\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u5dee\u51cf\u5c11\u6280\u672f\u2014\u2014Budget Relative Policy Optimization (BRPO)\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u7684\u5b9e\u9a8c\u4e2d\uff0cAnytimeReasoner\u65b9\u6cd5\u5728\u5404\u79cd\u5148\u9a8c\u5206\u5e03\u548c\u4e0d\u540c\u601d\u8003\u9884\u7b97\u4e0b\u5747\u663e\u8457\u4f18\u4e8eGRPO\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\uff0c\u8fd8\u5927\u5e45\u63d0\u9ad8\u4e86token\u4f7f\u7528\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684AnytimeReasoner\u6846\u67b6\u6210\u529f\u4f18\u5316\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540ctoken\u9884\u7b97\u4e0b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86token\u6548\u7387\u548c\u63a8\u7406\u7075\u6d3b\u6027\u3002\u901a\u8fc7\u5f15\u5165\u53ef\u9a8c\u8bc1\u5bc6\u96c6\u5956\u52b1\u548c\u65b0\u7684\u65b9\u5dee\u51cf\u5c11\u6280\u672fBRPO\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u7684\u7a33\u5065\u6027\u548c\u6548\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2505.13026", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.13026", "abs": "https://arxiv.org/abs/2505.13026", "authors": ["Jack Chen", "Fazhong Liu", "Naruto Liu", "Yuhan Luo", "Erqu Qin", "Harry Zheng", "Tian Dong", "Haojin Zhu", "Yan Meng", "Xiao Wang"], "title": "Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel at mathematical reasoning and logical\nproblem-solving. The current popular training paradigms primarily use\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the\nmodels' reasoning abilities. However, when using SFT or RL alone, there are\nrespective challenges: SFT may suffer from overfitting, while RL is prone to\nmode collapse. The state-of-the-art methods have proposed hybrid training\nschemes. However, static switching faces challenges such as poor generalization\nacross different tasks and high dependence on data quality. In response to\nthese challenges, inspired by the curriculum learning-quiz mechanism in human\nreasoning cultivation, We propose SASR, a step-wise adaptive hybrid training\nframework that theoretically unifies SFT and RL and dynamically balances the\ntwo throughout optimization. SASR uses SFT for initial warm-up to establish\nbasic reasoning skills, and then uses an adaptive dynamic adjustment algorithm\nbased on gradient norm and divergence relative to the original distribution to\nseamlessly integrate SFT with the online RL method GRPO. By monitoring the\ntraining status of LLMs and adjusting the training process in sequence, SASR\nensures a smooth transition between training schemes, maintaining core\nreasoning abilities while exploring different paths. Experimental results\ndemonstrate that SASR outperforms SFT, RL, and static hybrid training methods.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u8bad\u7ec3\u6846\u67b6SASR\uff0c\u5b83\u52a8\u6001\u5e73\u8861\u4e86\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\uff0c\u89e3\u51b3\u4e86\u5355\u72ec\u4f7f\u7528\u8fd9\u4e24\u79cd\u65b9\u6cd5\u65f6\u7684\u6311\u6218\u3002\u901a\u8fc7\u68af\u5ea6\u8303\u6570\u548c\u5206\u5e03\u6563\u5ea6\u7684\u81ea\u9002\u5e94\u8c03\u6574\u7b97\u6cd5\uff0cSASR\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4f18\u4e8eSFT\u3001RL\u548c\u9759\u6001\u6df7\u5408\u8bad\u7ec3\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u6765\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5206\u522b\u9762\u4e34\u8fc7\u62df\u5408\u548c\u6a21\u5f0f\u5d29\u6e83\u7684\u95ee\u9898\u3002\u73b0\u6709\u7684\u9759\u6001\u6df7\u5408\u8bad\u7ec3\u65b9\u6848\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u4e14\u5bf9\u6570\u636e\u8d28\u91cf\u4f9d\u8d56\u8f83\u9ad8\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u5e73\u8861SFT\u548cRL\u7684\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002", "method": "SASR\u9996\u5148\u5229\u7528SFT\u8fdb\u884c\u521d\u59cb\u70ed\u542f\u52a8\uff0c\u4ee5\u5efa\u7acb\u57fa\u672c\u7684\u63a8\u7406\u6280\u80fd\uff1b\u7136\u540e\u91c7\u7528\u57fa\u4e8e\u68af\u5ea6\u8303\u6570\u548c\u539f\u59cb\u5206\u5e03\u6563\u5ea6\u7684\u81ea\u9002\u5e94\u52a8\u6001\u8c03\u6574\u7b97\u6cd5\uff0c\u5c06SFT\u4e0e\u5728\u7ebfRL\u65b9\u6cd5GRPO\u65e0\u7f1d\u7ed3\u5408\u3002\u901a\u8fc7\u76d1\u63a7\u6a21\u578b\u8bad\u7ec3\u72b6\u6001\u5e76\u6309\u987a\u5e8f\u8c03\u6574\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u786e\u4fdd\u4e86\u4e0d\u540c\u8bad\u7ec3\u65b9\u6848\u4e4b\u95f4\u7684\u5e73\u6ed1\u8fc7\u6e21\uff0c\u540c\u65f6\u4fdd\u6301\u6838\u5fc3\u63a8\u7406\u80fd\u529b\u5e76\u63a2\u7d22\u4e0d\u540c\u7684\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSASR\u5728\u6570\u5b66\u63a8\u7406\u548c\u903b\u8f91\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u8d85\u8d8a\u4e86\u5355\u72ec\u4f7f\u7528SFT\u3001RL\u4ee5\u53ca\u9759\u6001\u6df7\u5408\u8bad\u7ec3\u65b9\u6cd5\u7684\u8868\u73b0\uff0c\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "SASR\u4f5c\u4e3a\u4e00\u79cd\u6b65\u8fdb\u5f0f\u81ea\u9002\u5e94\u6df7\u5408\u8bad\u7ec3\u6846\u67b6\uff0c\u6210\u529f\u5730\u7edf\u4e00\u4e86SFT\u548cRL\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u5e73\u8861\u4e24\u79cd\u65b9\u6cd5\u514b\u670d\u4e86\u5b83\u4eec\u5404\u81ea\u7684\u5c40\u9650\u6027\u3002\u8be5\u6846\u67b6\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2505.12951", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.12951", "abs": "https://arxiv.org/abs/2505.12951", "authors": ["Xuerui Su", "Liya Guo", "Yue Wang", "Yi Zhu", "Zhiming Ma", "Zun Wang", "Yuting Liu"], "title": "DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Inference scaling further accelerates Large Language Models (LLMs) toward\nArtificial General Intelligence (AGI), with large-scale Reinforcement Learning\n(RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning\napproaches usually rely on handcrafted rule-based reward functions. However,\nthe tarde-offs of exploration and exploitation in RL algorithms involves\nmultiple complex considerations, and the theoretical and empirical impacts of\nmanually designed reward functions remain insufficiently explored. In this\npaper, we propose Decoupled Group Reward Optimization (DGRO), a general RL\nalgorithm for LLM reasoning. On the one hand, DGRO decouples the traditional\nregularization coefficient into two independent hyperparameters: one scales the\npolicy gradient term, and the other regulates the distance from the sampling\npolicy. This decoupling not only enables precise control over balancing\nexploration and exploitation, but also can be seamlessly extended to Online\nPolicy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward\nOptimization. On the other hand, we observe that reward variance significantly\naffects both convergence speed and final model performance. We conduct both\ntheoretical analysis and extensive empirical validation to assess DGRO,\nincluding a detailed ablation study that investigates its performance and\noptimization dynamics. Experimental results show that DGRO achieves\nstate-of-the-art performance on the Logic dataset with an average accuracy of\n96.9\\%, and demonstrates strong generalization across mathematical benchmarks.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5DGRO\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002\u901a\u8fc7\u89e3\u8026\u4f20\u7edf\u7684\u6b63\u5219\u5316\u7cfb\u6570\u548c\u4f18\u5316\u5956\u52b1\u65b9\u5dee\uff0c\u8be5\u65b9\u6cd5\u5728\u63a2\u7d22\u4e0e\u5229\u7528\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u5e76\u5728\u903b\u8f91\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8696.9%\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4f46\u8fd9\u4e9b\u51fd\u6570\u5728\u7406\u8bba\u548c\u7ecf\u9a8c\u4e0a\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u540c\u65f6RL\u7b97\u6cd5\u4e2d\u7684\u63a2\u7d22\u4e0e\u5229\u7528\u6743\u8861\u6d89\u53ca\u590d\u6742\u7684\u8003\u91cf\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u4e14\u9ad8\u6548\u7684RL\u7b97\u6cd5\u6765\u6539\u8fdbLLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u4e86Decoupled Group Reward Optimization (DGRO)\u7b97\u6cd5\uff0c\u5c06\u4f20\u7edf\u7684\u6b63\u5219\u5316\u7cfb\u6570\u5206\u89e3\u4e3a\u4e24\u4e2a\u72ec\u7acb\u7684\u8d85\u53c2\u6570\uff0c\u5206\u522b\u63a7\u5236\u7b56\u7565\u68af\u5ea6\u9879\u548c\u91c7\u6837\u7b56\u7565\u7684\u8ddd\u79bb\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5956\u52b1\u65b9\u5dee\u5bf9\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u6d88\u878d\u7814\u7a76\u4ee5\u8bc4\u4f30\u5176\u8868\u73b0\u548c\u4f18\u5316\u52a8\u6001\u3002", "result": "DGRO\u5728Logic\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8696.9%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "DGRO\u7b97\u6cd5\u901a\u8fc7\u89e3\u8026\u7b56\u7565\u68af\u5ea6\u9879\u548c\u91c7\u6837\u7b56\u7565\u8ddd\u79bb\uff0c\u4ee5\u53ca\u4f18\u5316\u5956\u52b1\u65b9\u5dee\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u826f\u597d\u7684\u6cdb\u5316\u6027\u3002\u8fd9\u4e3a\u672a\u6765LLM\u7684\u63a8\u7406\u80fd\u529b\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2505.12929", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.12929", "abs": "https://arxiv.org/abs/2505.12929", "authors": ["Zhihe Yang", "Xufang Luo", "Zilong Wang", "Dongqi Han", "Zhiyuan He", "Dongsheng Li", "Yunjian Xu"], "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 12 figures", "summary": "Reinforcement learning (RL) has become a cornerstone for enhancing the\nreasoning capabilities of large language models (LLMs), with recent innovations\nsuch as Group Relative Policy Optimization (GRPO) demonstrating exceptional\neffectiveness. In this study, we identify a critical yet underexplored issue in\nRL training: low-probability tokens disproportionately influence model updates\ndue to their large gradient magnitudes. This dominance hinders the effective\nlearning of high-probability tokens, whose gradients are essential for LLMs'\nperformance but are substantially suppressed. To mitigate this interference, we\npropose two novel methods: Advantage Reweighting and Low-Probability Token\nIsolation (Lopti), both of which effectively attenuate gradients from\nlow-probability tokens while emphasizing parameter updates driven by\nhigh-probability tokens. Our approaches promote balanced updates across tokens\nwith varying probabilities, thereby enhancing the efficiency of RL training.\nExperimental results demonstrate that they substantially improve the\nperformance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K\nLogic Puzzle reasoning tasks. Our implementation is available at\nhttps://github.com/zhyang2226/AR-Lopti.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u4f4e\u6982\u7387token\u5bf9\u6a21\u578b\u66f4\u65b0\u7684\u4e0d\u6210\u6bd4\u4f8b\u5f71\u54cd\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u65b9\u6cd5\uff1aAdvantage Reweighting\u548cLow-Probability Token Isolation (Lopti)\uff0c\u4ee5\u51cf\u5f31\u4f4e\u6982\u7387token\u7684\u68af\u5ea6\u5e76\u5f3a\u8c03\u9ad8\u6982\u7387token\u7684\u53c2\u6570\u66f4\u65b0\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4f7f\u7528GRPO\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4f4e\u6982\u7387token\u7531\u4e8e\u5176\u8f83\u5927\u7684\u68af\u5ea6\u5e45\u5ea6\u5bf9\u6a21\u578b\u66f4\u65b0\u4ea7\u751f\u8fc7\u5927\u7684\u5f71\u54cd\uff0c\u8fd9\u4e00\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u8fd9\u79cd\u5f71\u54cd\u538b\u5236\u4e86\u9ad8\u6982\u7387token\u7684\u68af\u5ea6\uff0c\u800c\u9ad8\u6982\u7387token\u5bf9\u4e8eLLMs\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff1a1. Advantage Reweighting\uff0c\u91cd\u65b0\u8c03\u6574\u4f18\u52bf\u51fd\u6570\u4ee5\u51cf\u5c11\u4f4e\u6982\u7387token\u7684\u5f71\u54cd\uff1b2. Low-Probability Token Isolation (Lopti)\uff0c\u9694\u79bb\u4f4e\u6982\u7387token\u4ee5\u907f\u514d\u5b83\u4eec\u5e72\u6270\u6a21\u578b\u66f4\u65b0\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u65e8\u5728\u5e73\u8861\u4e0d\u540c\u6982\u7387token\u4e4b\u95f4\u7684\u68af\u5ea6\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u57fa\u4e8eGRPO\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728K&K\u903b\u8f91\u8c1c\u9898\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe46.2%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165Advantage Reweighting\u548cLopti\uff0c\u53ef\u4ee5\u6709\u6548\u51cf\u8f7b\u4f4e\u6982\u7387token\u5bf9\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u540c\u65f6\u63d0\u9ad8\u9ad8\u6982\u7387token\u7684\u5b66\u4e60\u6548\u7387\uff0c\u4ece\u800c\u589e\u5f3aLLMs\u7684\u63a8\u7406\u80fd\u529b\u3002\u8fd9\u4e9b\u65b9\u6cd5\u4e3a\u66f4\u9ad8\u6548\u7684RL\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2505.12432", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.12432", "abs": "https://arxiv.org/abs/2505.12432", "authors": ["Zirun Guo", "Minjie Hong", "Tao Jin"], "title": "Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Reinforcement Learning (RL) has shown promise in improving the reasoning\nabilities of Large Language Models (LLMs). However, the specific challenges of\nadapting RL to multimodal data and formats remain relatively unexplored. In\nthis work, we present Observe-R1, a novel framework aimed at enhancing the\nreasoning capabilities of multimodal large language models (MLLMs). We draw\ninspirations from human learning progression--from simple to complex and easy\nto difficult, and propose a gradual learning paradigm for MLLMs. To this end,\nwe construct the NeuraLadder dataset, which is organized and sampled according\nto the difficulty and complexity of data samples for RL training. To tackle\nmultimodal tasks, we introduce a multimodal format constraint that encourages\ncareful observation of images, resulting in enhanced visual abilities and\nclearer and more structured responses. Additionally, we implement a bonus\nreward system that favors concise, correct answers within a length constraint,\nalongside a dynamic weighting mechanism that prioritizes uncertain and\nmedium-difficulty problems, ensuring that more informative samples have a\ngreater impact on training. Our experiments with the Qwen2.5-VL-3B and\nQwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that\nObserve-R1 outperforms a series of larger reasoning models on both reasoning\nand general benchmarks, achieving superior clarity and conciseness in reasoning\nchains. Ablation studies validate the effectiveness of our strategies,\nhighlighting the robustness and generalization of our approach. The dataset and\ncode will be released at https://github.com/zrguo/Observe-R1.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aObserve-R1\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u9010\u6b65\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u3002\u8be5\u6846\u67b6\u53d7\u5230\u4eba\u7c7b\u5b66\u4e60\u8fdb\u7a0b\u7684\u542f\u53d1\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aNeuraLadder\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165\u4e86\u591a\u6a21\u6001\u683c\u5f0f\u7ea6\u675f\u548c\u5956\u52b1\u673a\u5236\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u89c6\u89c9\u80fd\u529b\u548c\u63a8\u7406\u8d28\u91cf\u3002\u5b9e\u9a8c\u8868\u660e\uff0cObserve-R1\u5728\u63a8\u7406\u548c\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u5176\u7b56\u7565\u5728\u6d88\u878d\u7814\u7a76\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5c06RL\u9002\u5e94\u4e8e\u591a\u6a21\u6001\u6570\u636e\u548c\u683c\u5f0f\u7684\u5177\u4f53\u6311\u6218\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "Observe-R1\u6846\u67b6\u91c7\u7528\u4e86\u6e10\u8fdb\u5f0f\u5b66\u4e60\u8303\u5f0f\uff0c\u57fa\u4e8e\u6570\u636e\u6837\u672c\u7684\u96be\u5ea6\u548c\u590d\u6742\u6027\u7ec4\u7ec7\u548c\u91c7\u6837NeuraLadder\u6570\u636e\u96c6\u3002\u540c\u65f6\uff0c\u5f15\u5165\u591a\u6a21\u6001\u683c\u5f0f\u7ea6\u675f\u4ee5\u4fc3\u8fdb\u5bf9\u56fe\u50cf\u7684\u4ed4\u7ec6\u89c2\u5bdf\uff0c\u4ee5\u53ca\u5956\u52b1\u7cfb\u7edf\u548c\u52a8\u6001\u52a0\u6743\u673a\u5236\uff0c\u4f18\u5148\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u4e2d\u7b49\u96be\u5ea6\u7684\u95ee\u9898\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5171\u540c\u4f5c\u7528\uff0c\u786e\u4fdd\u8bad\u7ec3\u8fc7\u7a0b\u66f4\u6709\u6548\uff0c\u5e76\u751f\u6210\u66f4\u6e05\u6670\u3001\u7b80\u6d01\u7684\u63a8\u7406\u94fe\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cObserve-R1\u5728Qwen2.5-VL-3B\u548cQwen2.5-VL-7B\u6a21\u578b\u4e0a\u663e\u8457\u4f18\u4e8e\u4e00\u7cfb\u5217\u66f4\u5927\u7684\u63a8\u7406\u6a21\u578b\uff0c\u5728\u63a8\u7406\u548c\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u7406\u94fe\u66f4\u6e05\u6670\u3001\u7b80\u6d01\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b56\u7565\u662f\u6709\u6548\u7684\uff0c\u7a81\u663e\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Observe-R1\u6846\u67b6\u6210\u529f\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u6e10\u8fdb\u5f0f\u5b66\u4e60\u8303\u5f0f\u548c\u591a\u6a21\u6001\u683c\u5f0f\u7ea6\u675f\u7684\u6709\u6548\u6027\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u6269\u5c55\u8fd9\u79cd\u65b9\u6cd5\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4ee5\u89e3\u51b3\u66f4\u591a\u590d\u6742\u7684\u591a\u6a21\u6001\u4efb\u52a1\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u53d1\u5e03\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\u3002"}}
{"id": "2505.12380", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.12380", "abs": "https://arxiv.org/abs/2505.12380", "authors": ["Han Weng", "Boyi Liu", "Yuanfeng Song", "Dun Zeng", "Yingxiang Yang", "Yi Zhan", "Longjie Cui", "Xiaoming Yin", "Yang Sun"], "title": "Graph-Reward-SQL: Execution-Free Reinforcement Learning for Text-to-SQL via Graph Matching and Stepwise Reward", "categories": ["cs.LG", "cs.DB", "cs.PL"], "comment": null, "summary": "Reinforcement learning (RL) has been widely adopted to enhance the\nperformance of large language models (LLMs) on Text-to-SQL tasks. However,\nexisting methods often rely on execution-based or LLM-based Bradley-Terry\nreward models. The former suffers from high execution latency caused by\nrepeated database calls, whereas the latter imposes substantial GPU memory\noverhead, both of which significantly hinder the efficiency and scalability of\nRL pipelines. To this end, we propose a novel Text-to-SQL RL fine-tuning\nframework named Graph-Reward-SQL, which employs the GMNScore outcome reward\nmodel. We leverage SQL graph representations to provide accurate reward signals\nwhile significantly reducing inference time and GPU memory usage. Building on\nthis foundation, we further introduce StepRTM, a stepwise reward model that\nprovides intermediate supervision over Common Table Expression (CTE)\nsubqueries. This encourages both functional correctness and structural clarity\nof SQL. Extensive comparative and ablation experiments on standard benchmarks,\nincluding Spider and BIRD, demonstrate that our method consistently outperforms\nexisting reward models.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6587\u672c\u5230SQL\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6846\u67b6Graph-Reward-SQL\uff0c\u4f7f\u7528GMNScore\u7ed3\u679c\u5956\u52b1\u6a21\u578b\u548cStepRTM\u9010\u6b65\u5956\u52b1\u6a21\u578b\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\u548cGPU\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86SQL\u67e5\u8be2\u7684\u529f\u80fd\u6b63\u786e\u6027\u548c\u7ed3\u6784\u6e05\u6670\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6587\u672c\u5230SQL\u4efb\u52a1\u4e2d\uff0c\u8981\u4e48\u56e0\u9891\u7e41\u6570\u636e\u5e93\u8c03\u7528\u5bfc\u81f4\u6267\u884c\u5ef6\u8fdf\u9ad8\uff0c\u8981\u4e48\u56e0\u5927\u8bed\u8a00\u6a21\u578b\u5e26\u6765\u663e\u8457\u7684GPU\u5185\u5b58\u5f00\u9500\uff0c\u5f71\u54cd\u4e86\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e86Graph-Reward-SQL\u6846\u67b6\uff0c\u5229\u7528SQL\u56fe\u8868\u793a\u63d0\u4f9b\u7cbe\u786e\u5956\u52b1\u4fe1\u53f7\u5e76\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u548cGPU\u5185\u5b58\u4f7f\u7528\uff1b\u5f15\u5165StepRTM\u9010\u6b65\u5956\u52b1\u6a21\u578b\u5bf9\u516c\u5171\u8868\u8868\u8fbe\u5f0f\u5b50\u67e5\u8be2\u8fdb\u884c\u4e2d\u95f4\u76d1\u7763\uff0c\u4ee5\u63d0\u9ad8SQL\u67e5\u8be2\u7684\u529f\u80fd\u6b63\u786e\u6027\u548c\u7ed3\u6784\u6e05\u6670\u5ea6\u3002", "result": "\u901a\u8fc7\u5728Spider\u548cBIRD\u7b49\u6807\u51c6\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u6bd4\u8f83\u548c\u6d88\u878d\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u5956\u52b1\u6a21\u578b\u3002", "conclusion": "Graph-Reward-SQL\u6846\u67b6\u53ca\u5176\u7ec4\u4ef6StepRTM\u5728\u63d0\u5347\u6587\u672c\u5230SQL\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2505.12186", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.12186", "abs": "https://arxiv.org/abs/2505.12186", "authors": ["Yuhui Wang", "Rongyi Zhu", "Ting Wang"], "title": "Self-Destructive Language Model", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Harmful fine-tuning attacks pose a major threat to the security of large\nlanguage models (LLMs), allowing adversaries to compromise safety guardrails\nwith minimal harmful data. While existing defenses attempt to reinforce LLM\nalignment, they fail to address models' inherent \"trainability\" on harmful\ndata, leaving them vulnerable to stronger attacks with increased learning rates\nor larger harmful datasets. To overcome this critical limitation, we introduce\nSEAM, a novel alignment-enhancing defense that transforms LLMs into\nself-destructive models with intrinsic resilience to misalignment attempts.\nSpecifically, these models retain their capabilities for legitimate tasks while\nexhibiting substantial performance degradation when fine-tuned on harmful data.\nThe protection is achieved through a novel loss function that couples the\noptimization trajectories of benign and harmful data, enhanced with adversarial\ngradient ascent to amplify the self-destructive effect. To enable practical\ntraining, we develop an efficient Hessian-free gradient estimate with\ntheoretical error bounds. Extensive evaluation across LLMs and datasets\ndemonstrates that SEAM creates a no-win situation for adversaries: the\nself-destructive models achieve state-of-the-art robustness against\nlow-intensity attacks and undergo catastrophic performance collapse under\nhigh-intensity attacks, rendering them effectively unusable. (warning: this\npaper contains potentially harmful content generated by LLMs.)", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u9632\u5fa1\u65b9\u6cd5SEAM\uff0c\u901a\u8fc7\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6709\u5bb3\u6570\u636e\u5fae\u8c03\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u6765\u589e\u5f3a\u5bf9\u9f50\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u9632\u5fa1\u63aa\u65bd\u867d\u7136\u8bd5\u56fe\u5f3a\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\uff0c\u4f46\u672a\u80fd\u89e3\u51b3\u6a21\u578b\u5728\u6709\u5bb3\u6570\u636e\u4e0a\u56fa\u6709\u7684\u53ef\u8bad\u7ec3\u6027\u95ee\u9898\uff0c\u5bfc\u81f4\u5bf9\u66f4\u5f3a\u653b\u51fb\u7684\u8106\u5f31\u6027\u3002", "method": "\u5f15\u5165SEAM\uff0c\u4e00\u79cd\u65b0\u7684\u5bf9\u9f50\u589e\u5f3a\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7279\u5b9a\u635f\u5931\u51fd\u6570\u548c\u5bf9\u6297\u68af\u5ea6\u4e0a\u5347\u6280\u672f\uff0c\u4f7f\u6a21\u578b\u5728\u5408\u6cd5\u4efb\u52a1\u4e0a\u4fdd\u6301\u80fd\u529b\uff0c\u800c\u5728\u6709\u5bb3\u6570\u636e\u5fae\u8c03\u65f6\u6027\u80fd\u4e0b\u964d\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u65e0Hessian\u68af\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u5b9e\u9645\u8bad\u7ec3\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cSEAM\u521b\u5efa\u4e86\u4e00\u4e2a\u5bf9\u653b\u51fb\u8005\u65e0\u5229\u53ef\u56fe\u7684\u5c40\u9762\uff1a\u5728\u4f4e\u5f3a\u5ea6\u653b\u51fb\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u9c81\u68d2\u6027\uff0c\u5728\u9ad8\u5f3a\u5ea6\u653b\u51fb\u4e0b\u5bfc\u81f4\u707e\u96be\u6027\u7684\u6027\u80fd\u5d29\u6e83\u3002", "conclusion": "SEAM\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u9632\u5fa1\u673a\u5236\uff0c\u4f7f\u5176\u5bf9\u9519\u4f4d\u5c1d\u8bd5\u5177\u6709\u5185\u5728\u7684\u5f39\u6027\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u5bf9\u9f50\u5b89\u5168\u6027\u3002"}}
{"id": "2505.11896", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.11896", "abs": "https://arxiv.org/abs/2505.11896", "authors": ["Chenwei Lou", "Zewei Sun", "Xinnian Liang", "Meng Qu", "Wei Shen", "Wenqi Wang", "Yuntao Li", "Qingping Yang", "Shuangzhi Wu"], "title": "AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities but\noften face challenges with tasks requiring sophisticated reasoning. While\nChain-of-Thought (CoT) prompting significantly enhances reasoning, it\nindiscriminately generates lengthy reasoning steps for all queries, leading to\nsubstantial computational costs and inefficiency, especially for simpler\ninputs. To address this critical issue, we introduce AdaCoT (Adaptive\nChain-of-Thought), a novel framework enabling LLMs to adaptively decide when to\ninvoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem\nthat seeks to balance model performance with the costs associated with CoT\ninvocation (both frequency and computational overhead). We propose a\nreinforcement learning (RL) based method, specifically utilizing Proximal\nPolicy Optimization (PPO), to dynamically control the CoT triggering decision\nboundary by adjusting penalty coefficients, thereby allowing the model to\ndetermine CoT necessity based on implicit query complexity. A key technical\ncontribution is Selective Loss Masking (SLM), designed to counteract decision\nboundary collapse during multi-stage RL training, ensuring robust and stable\nadaptive triggering. Experimental results demonstrate that AdaCoT successfully\nnavigates the Pareto frontier, achieving substantial reductions in CoT usage\nfor queries not requiring elaborate reasoning. For instance, on our production\ntraffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18\\% and\ndecreased average response tokens by 69.06%, while maintaining high performance\non complex tasks.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "AdaCoT\u662f\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u81ea\u9002\u5e94\u5730\u51b3\u5b9a\u4f55\u65f6\u4f7f\u7528Chain-of-Thought\uff08CoT\uff09\u63d0\u793a\uff0c\u4ece\u800c\u5728\u4fdd\u8bc1\u590d\u6742\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5c3d\u7ba1Chain-of-Thought\uff08CoT\uff09\u63d0\u793a\u663e\u8457\u589e\u5f3a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u4e3a\u6240\u6709\u67e5\u8be2\u751f\u6210\u8be6\u7ec6\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u5bfc\u81f4\u4e86\u8f83\u9ad8\u7684\u8ba1\u7b97\u6210\u672c\u548c\u6548\u7387\u4f4e\u4e0b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7b80\u5355\u8f93\u5165\u65f6\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0eCoT\u8c03\u7528\u7684\u6210\u672c\u3002", "method": "AdaCoT\u5c06\u81ea\u9002\u5e94\u63a8\u7406\u95ee\u9898\u5efa\u6a21\u4e3a\u5e15\u7d2f\u6258\u4f18\u5316\u95ee\u9898\uff0c\u76ee\u6807\u662f\u5e73\u8861\u6a21\u578b\u6027\u80fd\u548cCoT\u8c03\u7528\u7684\u6210\u672c\u3002\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\uff0c\u901a\u8fc7\u8c03\u6574\u60e9\u7f5a\u7cfb\u6570\u52a8\u6001\u63a7\u5236CoT\u89e6\u53d1\u51b3\u7b56\u8fb9\u754c\uff0c\u8ba9\u6a21\u578b\u6839\u636e\u9690\u5f0f\u67e5\u8be2\u590d\u6742\u5ea6\u5224\u65ad\u662f\u5426\u9700\u8981CoT\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u9009\u62e9\u6027\u635f\u5931\u5c4f\u853d\uff08SLM\uff09\u6280\u672f\u4ee5\u9632\u6b62\u591a\u9636\u6bb5RL\u8bad\u7ec3\u671f\u95f4\u51b3\u7b56\u8fb9\u754c\u7684\u5d29\u6e83\uff0c\u786e\u4fdd\u81ea\u9002\u5e94\u89e6\u53d1\u7684\u7a33\u5065\u6027\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAdaCoT\u6210\u529f\u5730\u5728\u5e15\u7d2f\u6258\u524d\u6cbf\u8fdb\u884c\u5bfc\u822a\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u4e0d\u9700\u8981\u590d\u6742\u63a8\u7406\u7684\u67e5\u8be2\u4e2d\u7684CoT\u4f7f\u7528\u7387\u3002\u4f8b\u5982\uff0c\u5728\u751f\u4ea7\u6d41\u91cf\u6d4b\u8bd5\u96c6\u4e2d\uff0cAdaCoT\u5c06CoT\u89e6\u53d1\u7387\u964d\u4f4e\u52303.18%\uff0c\u5e76\u51cf\u5c11\u4e8669.06%\u7684\u5e73\u5747\u54cd\u5e94\u6807\u8bb0\uff0c\u540c\u65f6\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "conclusion": "AdaCoT\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u51cf\u5c11\u4e0d\u5fc5\u8981CoT\u4f7f\u7528\u7684\u540c\u65f6\uff0c\u7ef4\u6301\u751a\u81f3\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u7684\u826f\u597d\u6743\u8861\u3002"}}
{"id": "2505.11875", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.11875", "abs": "https://arxiv.org/abs/2505.11875", "authors": ["Chi-Min Chan", "Chunpu Xu", "Jiaming Ji", "Zhen Ye", "Pengcheng Wen", "Chunyang Jiang", "Yaodong Yang", "Wei Xue", "Sirui Han", "Yike Guo"], "title": "J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge", "categories": ["cs.LG", "cs.CL"], "comment": "33 pages, 27 figures", "summary": "The current focus of AI research is shifting from emphasizing model training\ntowards enhancing evaluation quality, a transition that is crucial for driving\nfurther advancements in AI systems. Traditional evaluation methods typically\nrely on reward models assigning scalar preference scores to outputs. Although\neffective, such approaches lack interpretability, leaving users often uncertain\nabout why a reward model rates a particular response as high or low. The advent\nof LLM-as-a-Judge provides a more scalable and interpretable method of\nsupervision, offering insights into the decision-making process. Moreover, with\nthe emergence of large reasoning models, which consume more tokens for deeper\nthinking and answer refinement, scaling test-time computation in the\nLLM-as-a-Judge paradigm presents an avenue for further boosting performance and\nproviding more interpretability through reasoning traces. In this paper, we\nintroduce $\\textbf{J1-7B}$, which is first supervised fine-tuned on\nreflection-enhanced datasets collected via rejection-sampling and subsequently\ntrained using Reinforcement Learning (RL) with verifiable rewards. At inference\ntime, we apply Simple Test-Time Scaling (STTS) strategies for additional\nperformance improvement. Experimental results demonstrate that $\\textbf{J1-7B}$\nsurpasses the previous state-of-the-art LLM-as-a-Judge by $ \\textbf{4.8}$\\% and\nexhibits a $ \\textbf{5.1}$\\% stronger scaling trend under STTS. Additionally,\nwe present three key findings: (1) Existing LLM-as-a-Judge does not inherently\nexhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced\ndatasets continues to demonstrate similarly weak scaling behavior. (3)\nSignificant scaling trend emerges primarily during the RL phase, suggesting\nthat effective STTS capability is acquired predominantly through RL training.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aJ1-7B\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u53ef\u9a8c\u8bc1\u5956\u52b1\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u5e94\u7528\u7b80\u5355\u7684\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u7b56\u7565\uff08STTS\uff09\uff0c\u4ee5\u63d0\u5347\u6027\u80fd\u548c\u89e3\u91ca\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cJ1-7B\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684LLM-as-a-Judge\u6a21\u578b4.8%\uff0c\u5e76\u5728STTS\u4e0b\u8868\u73b0\u51fa5.1%\u66f4\u5f3a\u7684\u6269\u5c55\u8d8b\u52bf\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u663e\u8457\u7684\u6269\u5c55\u8d8b\u52bf\u4e3b\u8981\u5728\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u5f62\u6210\u3002", "motivation": "\u5f53\u524dAI\u7814\u7a76\u7684\u91cd\u70b9\u6b63\u5728\u4ece\u5f3a\u8c03\u6a21\u578b\u8bad\u7ec3\u8f6c\u5411\u63d0\u9ad8\u8bc4\u4f30\u8d28\u91cf\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u5956\u52b1\u6a21\u578b\u7684\u8bc4\u4f30\u65b9\u6cd5\u867d\u7136\u6709\u6548\uff0c\u4f46\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u800c\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u66f4\u591a\u7684token\u8fdb\u884c\u6df1\u5165\u601d\u8003\u548c\u7b54\u6848\u4f18\u5316\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u548c\u89e3\u91ca\u6027\u3002\u56e0\u6b64\uff0c\u5f15\u5165\u66f4\u53ef\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7684\u76d1\u7763\u65b9\u6cd5\u6210\u4e3a\u5fc5\u8981\u3002", "method": "\u9996\u5148\uff0c\u5728\u62d2\u7edd\u91c7\u6837\u6536\u96c6\u7684\u53cd\u601d\u589e\u5f3a\u6570\u636e\u96c6\u4e0a\u5bf9\u6a21\u578b\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff1b\u7136\u540e\u4f7f\u7528\u5e26\u6709\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\u3002\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u91c7\u7528\u7b80\u5355\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\uff08STTS\uff09\u7b56\u7565\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cJ1-7B\u6a21\u578b\u8d85\u8d8a\u4e86\u4e4b\u524d\u6700\u5148\u8fdb\u7684LLM-as-a-Judge\u6a21\u578b4.8%\uff0c\u5e76\u4e14\u5728STTS\u4e0b\u8868\u73b0\u51fa5.1%\u66f4\u5f3a\u7684\u6269\u5c55\u8d8b\u52bf\u3002\u6b64\u5916\uff0c\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u7684LLM-as-a-Judge\u672c\u8eab\u4e0d\u5177\u6709\u8fd9\u79cd\u6269\u5c55\u8d8b\u52bf\uff0c\u4ec5\u5728\u53cd\u601d\u589e\u5f3a\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u8f83\u5f31\u7684\u6269\u5c55\u884c\u4e3a\uff0c\u800c\u663e\u8457\u7684\u6269\u5c55\u8d8b\u52bf\u4e3b\u8981\u5728\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u51fa\u73b0\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684J1-7B\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u8d28\u91cf\u548c\u6027\u80fd\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u6269\u5c55\u8d8b\u52bf\u4e3b\u8981\u5728\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u5f62\u6210\uff0c\u4e3a\u672a\u6765\u8fdb\u4e00\u6b65\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2505.11849", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.11849", "abs": "https://arxiv.org/abs/2505.11849", "authors": ["Yiting Wang", "Guoheng Sun", "Wanghao Ye", "Gang Qu", "Ang Li"], "title": "VeriReason: Reinforcement Learning with Testbench Feedback for Reasoning-Enhanced Verilog Generation", "categories": ["cs.AI", "cs.AR", "cs.LG", "cs.PL"], "comment": "11 pages, 2 figures", "summary": "Automating Register Transfer Level (RTL) code generation using Large Language\nModels (LLMs) offers substantial promise for streamlining digital circuit\ndesign and reducing human effort. However, current LLM-based approaches face\nsignificant challenges with training data scarcity, poor specification-code\nalignment, lack of verification mechanisms, and balancing generalization with\nspecialization. Inspired by DeepSeek-R1, we introduce VeriReason, a framework\nintegrating supervised fine-tuning with Guided Reward Proximal Optimization\n(GRPO) reinforcement learning for RTL generation. Using curated training\nexamples and a feedback-driven reward model, VeriReason combines testbench\nevaluations with structural heuristics while embedding self-checking\ncapabilities for autonomous error correction. On the VerilogEval Benchmark,\nVeriReason delivers significant improvements: achieving 83.1% functional\ncorrectness on the VerilogEval Machine benchmark, substantially outperforming\nboth comparable-sized models and much larger commercial systems like GPT-4\nTurbo. Additionally, our approach demonstrates up to a 2.8X increase in\nfirst-attempt functional correctness compared to baseline methods and exhibits\nrobust generalization to unseen designs. To our knowledge, VeriReason\nrepresents the first system to successfully integrate explicit reasoning\ncapabilities with reinforcement learning for Verilog generation, establishing a\nnew state-of-the-art for automated RTL synthesis. The models and datasets are\navailable at: https://huggingface.co/collections/AI4EDA-CASE Code is Available\nat: https://github.com/NellyW8/VeriReason", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "VeriReason\u662f\u4e00\u4e2a\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8eRTL\u4ee3\u7801\u751f\u6210\u3002\u901a\u8fc7\u6574\u5408\u6d4b\u8bd5\u5e73\u53f0\u8bc4\u4f30\u3001\u7ed3\u6784\u542f\u53d1\u5f0f\u65b9\u6cd5\u4ee5\u53ca\u81ea\u6211\u68c0\u67e5\u80fd\u529b\uff0c\u5b83\u5728VerilogEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8683.1%\u7684\u529f\u80fd\u6b63\u786e\u6027\uff0c\u5e76\u5728\u9996\u6b21\u5c1d\u8bd5\u529f\u80fd\u6b63\u786e\u6027\u65b9\u9762\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e862.8\u500d\u3002\u8fd9\u662f\u9996\u4e2a\u5c06\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u7528\u4e8eVerilog\u751f\u6210\u7684\u7cfb\u7edf\uff0c\u5f00\u521b\u4e86\u81ea\u52a8\u5316RTL\u7efc\u5408\u7684\u65b0\u6807\u51c6\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728RTL\u4ee3\u7801\u751f\u6210\u4e2d\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u5305\u62ec\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u3001\u89c4\u8303\u4e0e\u4ee3\u7801\u5bf9\u9f50\u5dee\u3001\u7f3a\u4e4f\u9a8c\u8bc1\u673a\u5236\u4ee5\u53ca\u96be\u4ee5\u5e73\u8861\u6cdb\u5316\u4e0e\u4e13\u4e1a\u5316\u7b49\u95ee\u9898\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u63d0\u9ad8RTL\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "VeriReason\u6846\u67b6\u91c7\u7528\u76d1\u7763\u5fae\u8c03\u4e0eGuided Reward Proximal Optimization (GRPO)\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u3002\u5229\u7528\u7cbe\u5fc3\u7b56\u5212\u7684\u8bad\u7ec3\u6837\u672c\u548c\u53cd\u9988\u9a71\u52a8\u7684\u5956\u52b1\u6a21\u578b\uff0c\u5c06\u6d4b\u8bd5\u5e73\u53f0\u8bc4\u4f30\u4e0e\u7ed3\u6784\u542f\u53d1\u5f0f\u65b9\u6cd5\u7ed3\u5408\u8d77\u6765\uff0c\u540c\u65f6\u5d4c\u5165\u81ea\u6211\u68c0\u67e5\u80fd\u529b\u4ee5\u5b9e\u73b0\u81ea\u4e3b\u9519\u8bef\u4fee\u6b63\u3002", "result": "\u5728VerilogEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVeriReason\u8fbe\u5230\u4e8683.1%\u7684\u529f\u80fd\u6b63\u786e\u6027\uff0c\u5728\u9996\u6b21\u5c1d\u8bd5\u529f\u80fd\u6b63\u786e\u6027\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa2.8\u500d\uff0c\u5e76\u4e14\u5c55\u73b0\u51fa\u5bf9\u672a\u89c1\u8fc7\u8bbe\u8ba1\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VeriReason\u662f\u9996\u4e2a\u6210\u529f\u5c06\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u7528\u4e8eVerilog\u751f\u6210\u7684\u7cfb\u7edf\uff0c\u4e3a\u81ea\u52a8\u5316RTL\u7efc\u5408\u6811\u7acb\u4e86\u65b0\u7684\u6807\u6746\u3002"}}
