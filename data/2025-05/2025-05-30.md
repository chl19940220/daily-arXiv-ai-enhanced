<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 3]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.CL](#cs.CL) [Total: 5]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [econ.EM](#econ.EM) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: 本文是一篇立场论文，全面调查了人类与AI代理协作的最新经验发展，强调了其技术成就和持续存在的差距。提出了一种新的概念架构（分层探索-利用网络），系统地连接多代理协调、知识管理、控制反馈循环和高级控制机制等技术细节。通过将现有的贡献映射到这一框架上，该方法促进了传统方法的修订，并激励了融合定性和定量范式的新工作。文章结构允许从任何部分开始阅读，既作为技术实现的批判性评论，也为设计或扩展人类与AI共生关系提供了前瞻性的参考。


<details>
  <summary>Details</summary>
Motivation: 当前在处理开放性、复杂任务时，缺乏一个统一的理论框架来连贯地整合各种研究。这促使作者提出一种新的概念架构，以解决这一问题并推动人类与AI代理之间的深度协同进化。

Method: 作者提出了一个新的概念架构：分层探索-利用网络（Hierarchical Exploration-Exploitation Net）。该架构系统地连接了多代理协调、知识管理、控制反馈循环和高级控制机制的技术细节。通过将现有贡献（包括符号AI技术、连接主义LLM基础代理和混合组织实践）映射到此框架上，促进对传统方法的修订并启发新工作。

Result: 通过提出新的概念架构，文章为重新审视传统方法和开发融合定性和定量范式的新方法提供了思路。同时，它为设计或扩展人类与AI代理的共生关系提供了批判性和前瞻性的参考。

Conclusion: 该论文提供了一个新的概念框架，有助于更深层次的人类认知与AI能力的协同进化。它不仅是一个技术实现的批判性审查，还为未来的研究和应用提供了指导方向。

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [2] [Artificial Expert Intelligence through PAC-reasoning](https://arxiv.org/abs/2412.02441)
*Shai Shalev-Shwartz,Amnon Shashua,Gal Beniamini,Yoav Levine,Or Sharir,Noam Wies,Ido Ben-Shaul,Tomer Nussbaum,Shir Granot Peled*

Main category: cs.AI

TL;DR: Artificial Expert Intelligence (AEI)提出了一种超越AGI和窄AI限制的新方法，通过引入``Probably Approximately Correct (PAC) Reasoning''框架，实现复杂问题的可靠分解与精确推理控制，称为System 3精确推理。


<details>
  <summary>Details</summary>
Motivation: 现有的AI系统在预定义任务上表现出色，但在适应性和解决新问题时的精确性上存在不足，因此需要一种新的框架来提高AI系统的适应性和精确性。

Method: AEI提出了一个名为``Probably Approximately Correct (PAC) Reasoning''的框架，该框架提供了可靠的理论保证，用于分解复杂问题，并提供了一种实用机制来控制推理精度。此外，受人类思考方式的启发，AEI将这种新型推理称为System 3（精确推理）。

Result: AEI为误差有界的推理时学习奠定了基础，可以更精确地解决复杂问题，同时保持一定的理论可靠性。

Conclusion: AEI通过结合领域特定的专业知识和顶级人类专家类似的精确推理能力，提供了一个新的方向，即在解决复杂问题时具有更高的适应性和精确性，同时保留了理论上的可靠性。

Abstract: Artificial Expert Intelligence (AEI) seeks to transcend the limitations of
both Artificial General Intelligence (AGI) and narrow AI by integrating
domain-specific expertise with critical, precise reasoning capabilities akin to
those of top human experts. Existing AI systems often excel at predefined tasks
but struggle with adaptability and precision in novel problem-solving. To
overcome this, AEI introduces a framework for ``Probably Approximately Correct
(PAC) Reasoning". This paradigm provides robust theoretical guarantees for
reliably decomposing complex problems, with a practical mechanism for
controlling reasoning precision. In reference to the division of human thought
into System 1 for intuitive thinking and System 2 for reflective
reasoning~\citep{tversky1974judgment}, we refer to this new type of reasoning
as System 3 for precise reasoning, inspired by the rigor of the scientific
method. AEI thus establishes a foundation for error-bounded, inference-time
learning.

</details>


### [3] [Decomposing Elements of Problem Solving: What "Math" Does RL Teach?](https://arxiv.org/abs/2505.22756)
*Tian Qin,Core Francisco Park,Mujin Kwun,Aaron Walsman,Eran Malach,Nikhil Anand,Hidenori Tanaka,David Alvarez-Melis*

Main category: cs.AI

TL;DR: 本文研究了强化学习（RL）方法在提升大语言模型数学推理能力中的作用。通过将问题解决能力分解为计划、执行和验证三个基本方面，作者发现RL主要增强了模型的执行能力，但对新问题的规划能力有限。通过构建一个合成的任务环境，进一步确认了RL在执行稳健性上的提升，并探讨了其在克服规划能力局限性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习方法（如GRPO）在数学推理任务中显著提升了大语言模型的性能，但仅依赖准确率指标无法揭示模型具体掌握了哪些问题解决技能。因此，需要更细致地分析模型的能力组成，以理解其优势与不足。

Method: 作者将数学问题解决能力分解为三个基本部分：计划（将问题映射到解题步骤序列）、执行（正确完成解题步骤）和验证（判断解题结果是否正确）。通过实证研究发现，强化学习主要提升了模型的执行能力，并提出了“温度蒸馏”现象。此外，作者设计了一个最小化的合成任务——解决方案树导航任务，模拟数学问题解决过程，从而深入探索RL的影响。

Result: 实验表明，强化学习显著提升了模型在已知问题上的执行稳健性，但在面对全新问题时，由于规划能力不足，模型遇到了“覆盖范围墙”。合成任务的结果进一步验证了这一发现，并揭示了RL在特定条件下可能通过改进探索和泛化来克服这一障碍。

Conclusion: 本文展示了强化学习在提升大语言模型执行能力方面的有效性，同时暴露了其在规划能力上的局限性。研究结果为未来改进强化学习方法以克服这些障碍提供了方向。代码已开源至 https://github.com/cfpark00/RL-Wall。

Abstract: Mathematical reasoning tasks have become prominent benchmarks for assessing
the reasoning capabilities of LLMs, especially with reinforcement learning (RL)
methods such as GRPO showing significant performance gains. However, accuracy
metrics alone do not support fine-grained assessment of capabilities and fail
to reveal which problem-solving skills have been internalized. To better
understand these capabilities, we propose to decompose problem solving into
fundamental capabilities: Plan (mapping questions to sequences of steps),
Execute (correctly performing solution steps), and Verify (identifying the
correctness of a solution). Empirically, we find that GRPO mainly enhances the
execution skill-improving execution robustness on problems the model already
knows how to solve-a phenomenon we call temperature distillation. More
importantly, we show that RL-trained models struggle with fundamentally new
problems, hitting a 'coverage wall' due to insufficient planning skills. To
explore RL's impact more deeply, we construct a minimal, synthetic
solution-tree navigation task as an analogy for mathematical problem-solving.
This controlled setup replicates our empirical findings, confirming RL
primarily boosts execution robustness. Importantly, in this setting, we
identify conditions under which RL can potentially overcome the coverage wall
through improved exploration and generalization to new solution paths. Our
findings provide insights into the role of RL in enhancing LLM reasoning,
expose key limitations, and suggest a path toward overcoming these barriers.
Code is available at https://github.com/cfpark00/RL-Wall.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [MvDeCor: Multi-view Dense Correspondence Learning for Fine-grained 3D Segmentation](https://arxiv.org/abs/2208.08580)
*Gopal Sharma,Kangxue Yin,Subhransu Maji,Evangelos Kalogerakis,Or Litany,Sanja Fidler*

Main category: cs.CV

TL;DR: 提出了一种结合2D和3D优势的自监督方法，用于细粒度3D形状分割任务，通过多视角渲染和对比学习框架建立密集对应关系，提升在有限标注数据下的泛化能力。实验表明，在纹理和无纹理数据集上均优于现有方法，特别是在训练视角稀疏或形状有纹理时效果更显著。


<details>
  <summary>Details</summary>
Motivation: 受到基于视图的表面表示比基于点云或体素占用的3D表示更能有效建模高分辨率表面细节和纹理的启发，研究者希望利用2D领域的自监督技术来改进细粒度3D形状分割任务。

Method: 给定一个3D形状，首先从多个视角进行渲染，然后在对比学习框架下设置密集对应关系学习任务。通过该方法，所学得的2D表示具有视角不变性和几何一致性。

Result: 在纹理（RenderPeople）和无纹理（PartNet）3D数据集上的实验表明，该方法在细粒度部件分割任务中优于现有最先进方法。尤其是在训练视角稀疏或形状有纹理的情况下，相比基线方法改进更大。

Conclusion: MvDeCor方法通过结合2D处理和3D几何推理的优势，在细粒度3D形状分割任务中表现出色，尤其在训练数据有限或形状具有纹理时效果显著。这证明了将2D自监督技术和3D几何信息相结合的有效性。

Abstract: We propose to utilize self-supervised techniques in the 2D domain for
fine-grained 3D shape segmentation tasks. This is inspired by the observation
that view-based surface representations are more effective at modeling
high-resolution surface details and texture than their 3D counterparts based on
point clouds or voxel occupancy. Specifically, given a 3D shape, we render it
from multiple views, and set up a dense correspondence learning task within the
contrastive learning framework. As a result, the learned 2D representations are
view-invariant and geometrically consistent, leading to better generalization
when trained on a limited number of labeled shapes compared to alternatives
that utilize self-supervision in 2D or 3D alone. Experiments on textured
(RenderPeople) and untextured (PartNet) 3D datasets show that our method
outperforms state-of-the-art alternatives in fine-grained part segmentation.
The improvements over baselines are greater when only a sparse set of views is
available for training or when shapes are textured, indicating that MvDeCor
benefits from both 2D processing and 3D geometric reasoning.

</details>


### [5] [Mix3D: Out-of-Context Data Augmentation for 3D Scenes](https://arxiv.org/abs/2110.02210)
*Alexey Nekrasov,Jonas Schult,Or Litany,Bastian Leibe,Francis Engelmann*

Main category: cs.CV

TL;DR: 本论文提出了一种名为Mix3D的数据增强技术，用于分割大规模3D场景。通过将两个增强场景组合以创建新的训练样本，Mix3D强调平衡全局场景上下文和局部几何结构的重要性，从而让模型不仅仅依赖于场景上下文，还能从局部结构中推断语义。实验表明，使用Mix3D训练的模型在室内（ScanNet、S3DIS）和室外数据集（SemanticKITTI）上性能显著提升。例如，在ScanNet测试基准上，结合Mix3D训练的MinkowskiNet超过了所有先前的最佳方法，达到78.1 mIoU。


<details>
  <summary>Details</summary>
Motivation: 当前针对大规模3D场景分割的研究主要集中在具有大容量和感受野的模型上，这些模型能够充分捕捉输入3D场景的全局上下文。然而，过于依赖强上下文先验可能导致错误分类，例如将过马路的行人误认为是汽车。因此，本文旨在探讨如何平衡全局场景上下文与局部几何结构，以提高模型的泛化能力，使其不仅依赖于训练集中存在的上下文先验。

Method: 作者提出了一种“混合”技术，即Mix3D，它通过将两个增强后的3D场景组合来创建新的训练样本。这种方法将对象实例隐式地放置到新颖且超出常规上下文的环境中，迫使模型不仅依赖于场景上下文，还要从局部结构中推断语义。此外，作者还进行了详细的分析，研究了全局上下文、局部结构以及混合场景对模型性能的影响。

Result: 实验结果表明，使用Mix3D训练的模型在多个数据集上取得了显著的性能提升。具体来说，在室内数据集ScanNet和S3DIS，以及室外数据集SemanticKITTI上，模型表现均有所提高。特别地，结合Mix3D训练的MinkowskiNet在ScanNet测试基准上达到了78.1 mIoU，超越了所有先前的最佳方法。

Conclusion: Mix3D是一种简单而有效的方法，可以轻松应用于任何现有的3D场景分割模型，并带来显著的性能提升。它通过引入超出常规上下文的新环境，增强了模型对局部结构的理解能力，同时减少了对全局上下文的过度依赖。这表明Mix3D在改进3D场景理解任务中的潜力巨大。代码已公开发布，供研究人员进一步探索和应用。

Abstract: We present Mix3D, a data augmentation technique for segmenting large-scale 3D
scenes. Since scene context helps reasoning about object semantics, current
works focus on models with large capacity and receptive fields that can fully
capture the global context of an input 3D scene. However, strong contextual
priors can have detrimental implications like mistaking a pedestrian crossing
the street for a car. In this work, we focus on the importance of balancing
global scene context and local geometry, with the goal of generalizing beyond
the contextual priors in the training set. In particular, we propose a "mixing"
technique which creates new training samples by combining two augmented scenes.
By doing so, object instances are implicitly placed into novel out-of-context
environments and therefore making it harder for models to rely on scene context
alone, and instead infer semantics from local structure as well. We perform
detailed analysis to understand the importance of global context, local
structures and the effect of mixing scenes. In experiments, we show that models
trained with Mix3D profit from a significant performance boost on indoor
(ScanNet, S3DIS) and outdoor datasets (SemanticKITTI). Mix3D can be trivially
used with any existing method, e.g., trained with Mix3D, MinkowskiNet
outperforms all prior state-of-the-art methods by a significant margin on the
ScanNet test benchmark 78.1 mIoU. Code is available at:
https://nekrasov.dev/mix3d/

</details>


### [6] [cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning](https://arxiv.org/abs/2505.22914)
*Maksim Kolodiazhnyi,Denis Tarasov,Dmitrii Zhemchuzhnikov,Alexander Nikulin,Ilya Zisman,Anna Vorontsova,Anton Konushin,Vladislav Kurenkov,Danila Rukhovich*

Main category: cs.CV

TL;DR: 本文提出了一种多模态CAD重建模型，利用视觉-语言模型(VLM)同时处理点云、图像和文本三种输入模态。通过受大语言模型（LLM）启发的两阶段训练流程：监督微调(SFT)和基于在线反馈的强化学习(RL)微调，该模型在DeepCAD基准上超越了现有的单模态方法，并在三个具有挑战性的数据集上取得了新的最佳结果。


<details>
  <summary>Details</summary>
Motivation: 现有的CAD重建方法通常专注于单一输入模态（如点云、图像或文本），这限制了它们的通用性和鲁棒性。因此，需要一种能够同时处理多种输入模态的方法来提高性能和适用范围。

Method: 作者提出了一个多模态CAD重建模型，结合了视觉-语言模型(VLM)的能力，能够同时处理点云、图像和文本三种输入模态。训练过程采用两阶段管道：1) 在大规模程序生成的数据上进行监督微调(SFT)；2) 使用在线反馈（通过程序化方式获取）进行强化学习(RL)微调。此外，作者首次探索了使用在线强化学习算法（如Group Relative Preference Optimization, GRPO）对大语言模型(LLM)进行微调以完成CAD任务。

Result: 在DeepCAD基准测试中，SFT模型在所有三种输入模态上都优于现有的单模态方法。更重要的是，经过RL微调后，该模型在包括一个真实世界数据集在内的三个具有挑战性的数据集上达到了新的最先进水平。

Conclusion: 本文提出的多模态CAD重建模型成功地整合了多种输入模态，并通过两阶段训练流程显著提高了性能。实验结果表明，在线强化学习比离线方法更有效，且该模型在多个数据集上取得了卓越的表现，为未来的CAD重建研究提供了新的方向。

Abstract: Computer-Aided Design (CAD) plays a central role in engineering and
manufacturing, making it possible to create precise and editable 3D models.
Using a variety of sensor or user-provided data as inputs for CAD
reconstruction can democratize access to design applications. However, existing
methods typically focus on a single input modality, such as point clouds,
images, or text, which limits their generalizability and robustness. Leveraging
recent advances in vision-language models (VLM), we propose a multi-modal CAD
reconstruction model that simultaneously processes all three input modalities.
Inspired by large language model (LLM) training paradigms, we adopt a two-stage
pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated
data, followed by reinforcement learning (RL) fine-tuning using online
feedback, obtained programatically. Furthermore, we are the first to explore RL
fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such
as Group Relative Preference Optimization (GRPO) outperform offline
alternatives. In the DeepCAD benchmark, our SFT model outperforms existing
single-modal approaches in all three input modalities simultaneously. More
importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three
challenging datasets, including a real-world one.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model](https://arxiv.org/abs/2505.23579)
*Adibvafa Fallahpour,Andrew Magnuson,Purav Gupta,Shihao Ma,Jack Naimer,Arnav Shah,Haonan Duan,Omar Ibrahim,Hani Goodarzi,Chris J. Maddison,Bo Wang*

Main category: cs.LG

TL;DR: 通过将DNA基础模型与大型语言模型（LLM）深度集成，BioReason在生物推理基准测试中表现出显著性能提升，并提供可解释的逐步生物推理痕迹。


<details>
  <summary>Details</summary>
Motivation: 当前的DNA基础模型虽然具有强大的序列表示能力，但在多步推理方面存在困难，且缺乏透明、直观的生物学解释。为了解决这一问题，研究人员试图开发一种能够整合基因组信息处理和多步推理能力的新型架构。

Method: 研究者引入了BioReason，一种首次将DNA基础模型与大型语言模型（LLM）深度融合的开创性架构。通过监督微调和目标强化学习，使LLM能够直接处理基因组信息作为基本输入，从而促进多模态生物理解并生成逻辑连贯的生物推论。

Result: 在基于KEGG的疾病通路预测等生物推理基准测试中，BioReason的准确率从88%提升到97%，并在变异效应预测任务上平均性能提升了15%。此外，BioReason能够对未见过的生物实体进行推理，并通过可解释的逐步生物痕迹阐明决策过程。

Conclusion: BioReason提供了一种变革性的AI方法，用于从基因组数据中生成更深层次的机制见解和可测试的假设。代码和数据已公开发布，推动了AI在生物学领域的进一步发展。

Abstract: Unlocking deep, interpretable biological reasoning from complex genomic data
is a major AI challenge hindering scientific discovery. Current DNA foundation
models, despite strong sequence representation, struggle with multi-step
reasoning and lack inherent transparent, biologically intuitive explanations.
We introduce BioReason, a pioneering architecture that, for the first time,
deeply integrates a DNA foundation model with a Large Language Model (LLM).
This novel connection enables the LLM to directly process and reason with
genomic information as a fundamental input, fostering a new form of multimodal
biological understanding. BioReason's sophisticated multi-step reasoning is
developed through supervised fine-tuning and targeted reinforcement learning,
guiding the system to generate logical, biologically coherent deductions. On
biological reasoning benchmarks including KEGG-based disease pathway prediction
- where accuracy improves from 88% to 97% - and variant effect prediction,
BioReason demonstrates an average 15% performance gain over strong
single-modality baselines. BioReason reasons over unseen biological entities
and articulates decision-making through interpretable, step-by-step biological
traces, offering a transformative approach for AI in biology that enables
deeper mechanistic insights and accelerates testable hypothesis generation from
genomic data. Data, code, and checkpoints are publicly available at
https://github.com/bowang-lab/BioReason

</details>


### [8] [Diversity-Aware Policy Optimization for Large Language Model Reasoning](https://arxiv.org/abs/2505.23433)
*Jian Yao,Ran Cheng,Xingyu Wu,Jibin Wu,Kay Chen Tan*

Main category: cs.LG

TL;DR: 尽管多样性在强化学习（RL）中起着关键作用，但其对大语言模型（LLM）推理能力的影响尚未被充分研究。本文系统地探讨了RL训练中多样性对LLM推理的影响，并提出了一种新的多样性感知策略优化方法。通过在12个LLMs上的评估，发现高表现模型的解多样性与潜在推理能力之间存在强烈正相关关系。基于这一发现，文章设计了一种token级别的多样性目标，并选择性地应用于正样本，最终在R1-zero训练框架下实现了数学推理基准测试中3.5%的平均提升，同时生成更多样化和稳健的解决方案。


<details>
  <summary>Details</summary>
Motivation: 虽然多样性在强化学习中的重要性已被广泛认可，但在大语言模型推理方面，多样性的具体影响尚未得到深入探索。因此，研究者希望通过分析多样性在RL训练中的作用，揭示其对LLM推理能力的潜在影响，并提出改进方法。

Method: 本文首先在12个LLM上进行了系统性实验，验证了解多样性与模型推理潜力之间的强正相关关系。随后，提出了一种新的多样性感知策略优化方法：设计了一个token级别的多样性目标，并将其重新定义为一个可实践的目标函数。此目标函数仅选择性地应用于正样本，以确保在RL训练过程中显式地促进多样性。最后，将该方法集成到R1-zero训练框架中进行验证。

Result: 实验结果表明，在四个数学推理基准测试中，所提出的方法平均提升了3.5%的表现，同时生成了更多样化和更稳健的解决方案。这证明了在RL训练中显式促进多样性可以有效提高LLM的推理能力。

Conclusion: 本文通过实验证明了解多样性与LLM推理潜力之间的强正相关关系，并提出了一种有效的多样性感知策略优化方法。该方法不仅提高了模型的推理能力，还生成了更多样化和更稳健的解决方案，为未来在RL训练中进一步探索多样性的作用提供了新的方向。

Abstract: The reasoning capabilities of large language models (LLMs) have advanced
rapidly, particularly following the release of DeepSeek R1, which has inspired
a surge of research into data quality and reinforcement learning (RL)
algorithms. Despite the pivotal role diversity plays in RL, its influence on
LLM reasoning remains largely underexplored. To bridge this gap, this work
presents a systematic investigation into the impact of diversity in RL-based
training for LLM reasoning, and proposes a novel diversity-aware policy
optimization method. Across evaluations on 12 LLMs, we observe a strong
positive correlation between the solution diversity and Potential at k (a novel
metric quantifying an LLM's reasoning potential) in high-performing models.
This finding motivates our method to explicitly promote diversity during RL
training. Specifically, we design a token-level diversity and reformulate it
into a practical objective, then we selectively apply it to positive samples.
Integrated into the R1-zero training framework, our method achieves a 3.5
percent average improvement across four mathematical reasoning benchmarks,
while generating more diverse and robust solutions.

</details>


### [9] [Towards Reward Fairness in RLHF: From a Resource Allocation Perspective](https://arxiv.org/abs/2505.23349)
*Sheng Ouyang,Yulan Hu,Ge Chen,Qingyang Li,Fuzheng Zhang,Yong Liu*

Main category: cs.LG

TL;DR: 本文提出了一种无需针对特定偏差的资源分配视角方法，通过公平性正则化和公平性系数两种技术来缓解奖励中的偏差问题，从而改善大型语言模型与人类偏好的对齐方式。实验表明该方法能更公平地使LLMs符合人类偏好。


<details>
  <summary>Details</summary>
Motivation: 在从人类反馈中进行强化学习（RLHF）时，奖励作为人类偏好的代理起着关键作用。然而，如果这些奖励存在各种偏差，则可能对大型语言模型（LLMs）的对齐产生负面影响。因此，需要一种通用的方法来解决奖励中的不公平问题，而不需要为每种偏差单独设计解决方案。

Method: 作者将偏好学习建模为一个资源分配问题，把奖励视为需要分配的资源，并在分配过程中考虑效用和公平性之间的权衡。提出了两种方法：公平性正则化（Fairness Regularization）和公平性系数（Fairness Coefficient），以实现奖励的公平性。这两种方法分别应用于验证场景和强化学习场景，以获得公平奖励模型和策略模型。

Result: 实验结果表明，所提出的方法能够在验证和强化学习场景中更公平地使大型语言模型与人类偏好对齐。

Conclusion: 本文提出了一种新的视角——从资源分配的角度解决奖励不公平问题，并通过公平性正则化和公平性系数两种方法有效缓解了奖励中的偏差，从而提升了大型语言模型与人类偏好的对齐质量。实验结果验证了该方法的有效性。

Abstract: Rewards serve as proxies for human preferences and play a crucial role in
Reinforcement Learning from Human Feedback (RLHF). However, if these rewards
are inherently imperfect, exhibiting various biases, they can adversely affect
the alignment of large language models (LLMs). In this paper, we collectively
define the various biases present in rewards as the problem of reward
unfairness. We propose a bias-agnostic method to address the issue of reward
fairness from a resource allocation perspective, without specifically designing
for each type of bias, yet effectively mitigating them. Specifically, we model
preference learning as a resource allocation problem, treating rewards as
resources to be allocated while considering the trade-off between utility and
fairness in their distribution. We propose two methods, Fairness Regularization
and Fairness Coefficient, to achieve fairness in rewards. We apply our methods
in both verification and reinforcement learning scenarios to obtain a fairness
reward model and a policy model, respectively. Experiments conducted in these
scenarios demonstrate that our approach aligns LLMs with human preferences in a
more fair manner.

</details>


### [10] [Accelerating RLHF Training with Reward Variance Increase](https://arxiv.org/abs/2505.23247)
*Zonglin Yang,Zhexuan Gu,Houduo Qi,Yancheng Yuan*

Main category: cs.LG

TL;DR: 本文提出了一种奖励调整模型，通过增加奖励方差加速基于GRPO的RLHF训练，并设计了高效的算法解决非凸优化问题，最终提升了RLHF训练效率。


<details>
  <summary>Details</summary>
Motivation: 尽管GRPO在许多LLM应用中表现出色，但高效的GRPO-based RLHF训练仍然是一个挑战。研究表明，初始策略模型的高奖励方差可以加速RLHF训练，这为改进方法提供了灵感。

Method: 作者提出了一个奖励调整模型，该模型通过增加奖励方差来加速RLHF训练，同时保留相对偏好和奖励期望。为了解决奖励调整中的非凸优化问题，设计了一个时间复杂度为O(n log n)的算法来找到全局解。此外，将此奖励调整模型与GRPO算法结合，形成了更高效的GRPOVI算法。

Result: 实验结果表明，GRPOVI算法相比原始的GRPO算法显著提高了RLHF训练的效率。

Conclusion: 本文提出的奖励调整模型及其对应的高效算法能够有效加速基于GRPO的RLHF训练。此外，研究还间接解释了规则奖励在GRPO中的经验有效性。

Abstract: Reinforcement learning from human feedback (RLHF) is an essential technique
for ensuring that large language models (LLMs) are aligned with human values
and preferences during the post-training phase. As an effective RLHF approach,
group relative policy optimization (GRPO) has demonstrated success in many
LLM-based applications. However, efficient GRPO-based RLHF training remains a
challenge. Recent studies reveal that a higher reward variance of the initial
policy model leads to faster RLHF training. Inspired by this finding, we
propose a practical reward adjustment model to accelerate RLHF training by
provably increasing the reward variance and preserving the relative preferences
and reward expectation. Our reward adjustment method inherently poses a
nonconvex optimization problem, which is NP-hard to solve in general. To
overcome the computational challenges, we design a novel $O(n \log n)$
algorithm to find a global solution of the nonconvex reward adjustment model by
explicitly characterizing the extreme points of the feasible set. As an
important application, we naturally integrate this reward adjustment model into
the GRPO algorithm, leading to a more efficient GRPO with reward variance
increase (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we
provide an indirect explanation for the empirical effectiveness of GRPO with
rule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment
results demonstrate that the GRPOVI algorithm can significantly improve the
RLHF training efficiency compared to the original GRPO algorithm.

</details>


### [11] [LLM-ODDR: A Large Language Model Framework for Joint Order Dispatching and Driver Repositioning](https://arxiv.org/abs/2505.22695)
*Tengfei Lyu,Siyuan Feng,Hao Liu,Hai Yang*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架LLM-ODDR，利用大语言模型进行网约车订单分配和司机重新定位任务。该框架包含三个关键组件：多目标导向的订单价值评估、公平性感知的订单分配以及时空需求感知的司机重新定位。此外，还开发了针对ODDR任务优化的JointDR-GPT模型。实验表明，该框架在效果、适应异常条件和决策可解释性方面优于传统方法。这是首次将大语言模型作为网约车ODDR任务中的决策代理的研究。


<details>
  <summary>Details</summary>
Motivation: 传统的网约车订单分配和司机重新定位方法（如组合优化、基于规则的启发式算法和强化学习）往往忽视了司机收入的公平性、决策的可解释性和对现实动态的适应性。因此，需要一种新方法来解决这些问题。

Method: LLM-ODDR框架由三个主要部分组成：1) 多目标导向的订单价值评估，通过考虑多个目标来确定订单的整体价值；2) 公平性感知的订单分配，平衡平台收益与司机收入公平性；3) 时空需求感知的司机重新定位，根据历史模式和预测供应优化空闲车辆的位置。同时，开发了JointDR-GPT模型，这是一个经过领域知识微调并针对ODDR任务优化的模型。

Result: 在曼哈顿出租车运营的真实数据集上的广泛实验表明，LLM-ODDR框架在有效性、对异常条件的适应性以及决策的可解释性方面显著优于传统方法。

Conclusion: 这项研究是首次探索大语言模型作为网约车ODDR任务中的决策代理的工作，为在智能交通系统中整合先进的语言模型提供了基础性的见解。

Abstract: Ride-hailing platforms face significant challenges in optimizing order
dispatching and driver repositioning operations in dynamic urban environments.
Traditional approaches based on combinatorial optimization, rule-based
heuristics, and reinforcement learning often overlook driver income fairness,
interpretability, and adaptability to real-world dynamics. To address these
gaps, we propose LLM-ODDR, a novel framework leveraging Large Language Models
(LLMs) for joint Order Dispatching and Driver Repositioning (ODDR) in
ride-hailing services. LLM-ODDR framework comprises three key components: (1)
Multi-objective-guided Order Value Refinement, which evaluates orders by
considering multiple objectives to determine their overall value; (2)
Fairness-aware Order Dispatching, which balances platform revenue with driver
income fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning,
which optimizes idle vehicle placement based on historical patterns and
projected supply. We also develop JointDR-GPT, a fine-tuned model optimized for
ODDR tasks with domain knowledge. Extensive experiments on real-world datasets
from Manhattan taxi operations demonstrate that our framework significantly
outperforms traditional methods in terms of effectiveness, adaptability to
anomalous conditions, and decision interpretability. To our knowledge, this is
the first exploration of LLMs as decision-making agents in ride-hailing ODDR
tasks, establishing foundational insights for integrating advanced language
models within intelligent transportation systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains](https://arxiv.org/abs/2402.00559)
*Alon Jacovi,Yonatan Bitton,Bernd Bohnet,Jonathan Herzig,Or Honovich,Michael Tseng,Michael Collins,Roee Aharoni,Mor Geva*

Main category: cs.CL

TL;DR: 本文介绍了REVEAL数据集，用于评估复杂推理链的自动验证方法。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏细粒度的步骤级数据集来彻底评估推理验证方法，这阻碍了该领域的进展。

Method: 构建了一个名为REVEAL的数据集，包含多种数据集和最先进的语言模型中每个推理步骤的相关性、证据段落归属以及逻辑正确性的全面标签。

Result: 在REVEAL上的评估表明，验证器在验证推理链时遇到困难，特别是在验证逻辑正确性和检测矛盾方面。

Conclusion: REVEAL数据集为开放领域问答设置中的复杂推理链验证提供了基准测试平台。

Abstract: Prompting language models to provide step-by-step answers (e.g.,
"Chain-of-Thought") is the prominent approach for complex reasoning tasks,
where more accurate reasoning chains typically improve downstream task
performance. Recent literature discusses automatic methods to verify reasoning
to evaluate and improve their correctness. However, no fine-grained step-level
datasets are available to enable thorough evaluation of such verification
methods, hindering progress in this direction. We introduce REVEAL: Reasoning
Verification Evaluation, a dataset to benchmark automatic verifiers of complex
Chain-of-Thought reasoning in open-domain question-answering settings. REVEAL
includes comprehensive labels for the relevance, attribution to evidence
passages, and logical correctness of each reasoning step in a language model's
answer, across a variety of datasets and state-of-the-art language models.
Evaluation on REVEAL shows that verifiers struggle at verifying reasoning
chains - in particular, verifying logical correctness and detecting
contradictions. Available at https://reveal-dataset.github.io/ .

</details>


### [13] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 在大规模语言模型（LLMs）中，通过重复采样扩展推理计算可以持续增加覆盖率（解决问题的比例）。实验表明，这种提升部分是由于标准评估基准中的答案分布偏向于少量常见答案。研究者提出了一种基于训练集中答案频率的基线方法，发现它在某些LLM上优于重复采样，而在其他模型上与一种混合策略表现相当。该基线有助于更准确地衡量重复采样在提示无关猜测之外的改进程度。


<details>
  <summary>Details</summary>
Motivation: 研究者观察到，在大语言模型中，随着采样次数的增加，推理问题的覆盖率也会增加。他们推测，这一现象可能部分归因于评估基准中答案分布的偏差，即答案集中在少数常见选项上。为了验证这一假设并更好地理解重复采样的效果，研究者设计了一个基于训练集答案频率的基线方法进行对比分析。

Method: 研究者定义了一个基线方法，该方法根据训练集中答案的频率枚举可能的答案。通过在数学推理和事实知识两个领域进行实验，比较了此基线方法与重复采样以及一种混合策略的表现。混合策略结合了少量模型采样和基于枚举的猜测来生成答案。

Result: 实验结果表明，在某些LLM上，基于训练集答案频率的基线方法比重复采样表现更好；而在其他模型上，其表现与一种混合策略相当，该混合策略使用少量模型采样和枚举猜测相结合的方式生成答案。

Conclusion: 研究者提出的基线方法提供了一种更准确的手段来测量重复采样在提示无关猜测之外对覆盖率的改进程度。这有助于更好地理解大语言模型在不同任务上的性能提升来源。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [14] [ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2505.23723)
*Zexi Liu,Jingyi Chai,Xinyu Zhu,Shuo Tang,Rui Ye,Bo Zhang,Lei Bai,Siheng Chen*

Main category: cs.CL

TL;DR: 本文提出了一种基于学习的智能体ML新范式，通过交互实验使用在线强化学习训练LLM智能体，设计了探索增强微调、分步RL和特定于智能体ML的奖励模块三个关键组件。利用该框架训练出7B参数规模的Qwen-2.5驱动的ML-Agent，在仅9个ML任务上训练后，性能优于671B参数规模的DeepSeek-R1智能体，并展现出持续性能改进和跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大多数自主机器学习工程方法依赖于手动提示工程，缺乏根据多样化实验经验进行适应和优化的能力。为了解决这一问题，文章首次探索了基于学习的智能体ML范式。

Method: 提出了一个包含三个关键组件的智能体ML训练框架：1）探索增强微调，使LLM智能体生成多样动作以增强RL探索；2）分步RL，允许对单个动作步骤进行训练，加速经验收集并提高训练效率；3）特定于智能体ML的奖励模块，将不同的ML反馈信号统一为一致的奖励以优化RL。

Result: 尽管仅在9个ML任务上训练，7B参数规模的ML-Agent在性能上超过了671B参数规模的DeepSeek-R1智能体，同时实现了持续性能改进并展示了卓越的跨任务泛化能力。

Conclusion: 本文提出的基于学习的智能体ML范式及其训练框架显著提升了LLM智能体在自主ML中的表现和泛化能力，证明了小规模模型通过有效训练可以超越大规模模型的潜力。

Abstract: The emergence of large language model (LLM)-based agents has significantly
advanced the development of autonomous machine learning (ML) engineering.
However, most existing approaches rely heavily on manual prompt engineering,
failing to adapt and optimize based on diverse experimental experiences.
Focusing on this, for the first time, we explore the paradigm of learning-based
agentic ML, where an LLM agent learns through interactive experimentation on ML
tasks using online reinforcement learning (RL). To realize this, we propose a
novel agentic ML training framework with three key components: (1)
exploration-enriched fine-tuning, which enables LLM agents to generate diverse
actions for enhanced RL exploration; (2) step-wise RL, which enables training
on a single action step, accelerating experience collection and improving
training efficiency; (3) an agentic ML-specific reward module, which unifies
varied ML feedback signals into consistent rewards for RL optimization.
Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM
for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our
7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it
achieves continuous performance improvements and demonstrates exceptional
cross-task generalization capabilities.

</details>


### [15] [Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation](https://arxiv.org/abs/2505.23657)
*Hongxiang Zhang,Hao Chen,Tianyi Zhang,Muhao Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的解码策略Active Layer-Contrastive Decoding (ActLCD)，通过强化学习策略在生成过程中主动决定何时应用对比层，从而优化大语言模型的事实准确性，减少长上下文中的幻觉现象。实验表明，该方法在五个基准测试中超越了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的解码方法通过改进生成过程中下一个token的选择方式来提升大语言模型（LLMs）的事实准确性，但这些方法主要集中在token级别，并未能有效减少LLMs在长上下文中的幻觉问题。

Method: ActLCD将解码过程视为一个序列决策问题，利用由奖励感知分类器引导的强化学习策略，主动决定在生成过程中何时应用对比层，从而在token级别之外优化事实准确性。

Result: 实验结果表明，ActLCD在五个不同的基准测试中超越了当前最先进的方法，显著减少了不同生成场景中的幻觉现象。

Conclusion: ActLCD作为一种新颖的解码策略，在缓解大语言模型生成过程中的幻觉问题方面表现出了卓越的效果，为提升LLMs的事实准确性提供了一种有效的解决方案。

Abstract: Recent decoding methods improve the factuality of large language
models~(LLMs) by refining how the next token is selected during generation.
These methods typically operate at the token level, leveraging internal
representations to suppress superficial patterns. Nevertheless, LLMs remain
prone to hallucinations, especially over longer contexts. In this paper, we
propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy
that actively decides when to apply contrasting layers during generation. By
casting decoding as a sequential decision-making problem, ActLCD employs a
reinforcement learning policy guided by a reward-aware classifier to optimize
factuality beyond the token level. Our experiments demonstrate that ActLCD
surpasses state-of-the-art methods across five benchmarks, showcasing its
effectiveness in mitigating hallucinations in diverse generation scenarios.

</details>


### [16] [ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind](https://arxiv.org/abs/2505.22961)
*Peixuan Han,Zijia Liu,Jiaxuan You*

Main category: cs.CL

TL;DR: 本文介绍了一种名为ToMAP的方法，通过加入两个理论化心智模块来增强说服者对对手心理状态的感知和分析能力。实验表明，尽管ToMAP模型参数较少，但在多个说服对象模型和不同语料库中表现优于大基线模型（如GPT-4o），且展现出复杂的推理链和减少重复性，生成更多样有效的论点。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在说服方面具有潜力，但目前的研究仍处于初步阶段。特别地，人类擅长主动和动态地建模对手的想法和观点，而当前的大型语言模型在心智理论推理方面存在困难，导致说服时的多样性和对手意识受限。

Method: 提出了一种新的方法——心智理论增强说服者（ToMAP）。该方法通过提示说服者考虑目标中心主张可能存在的异议，并使用文本编码器与训练好的MLP分类器预测对手在这些反论点上的当前立场。同时，设计了精心策划的强化学习方案，使说服者能够学习如何分析与对手相关的信息，并利用这些信息生成更有效的论点。

Result: 实验表明，ToMAP模型（仅包含3B参数）在多个说服对象模型和多样化语料库中，相比更大的基线模型（如GPT-4o）有显著优势，相对增益达39.4%。此外，ToMAP表现出复杂的推理链、减少了训练中的重复性，生成更多样和有效的论点，适合长对话并能采用更逻辑和对手意识强的策略。

Conclusion: 结果证明了ToMAP方法的有效性，强调了其在发展更具说服力的语言代理方面的潜力。

Abstract: Large language models (LLMs) have shown promising potential in persuasion,
but existing works on training LLM persuaders are still preliminary. Notably,
while humans are skilled in modeling their opponent's thoughts and opinions
proactively and dynamically, current LLMs struggle with such Theory of Mind
(ToM) reasoning, resulting in limited diversity and opponent awareness. To
address this limitation, we introduce Theory of Mind Augmented Persuader
(ToMAP), a novel approach for building more flexible persuader agents by
incorporating two theory of mind modules that enhance the persuader's awareness
and analysis of the opponent's mental state. Specifically, we begin by
prompting the persuader to consider possible objections to the target central
claim, and then use a text encoder paired with a trained MLP classifier to
predict the opponent's current stance on these counterclaims. Our carefully
designed reinforcement learning schema enables the persuader learns how to
analyze opponent-related information and utilize it to generate more effective
arguments. Experiments show that the ToMAP persuader, while containing only 3B
parameters, outperforms much larger baselines, like GPT-4o, with a relative
gain of 39.4% across multiple persuadee models and diverse corpora. Notably,
ToMAP exhibits complex reasoning chains and reduced repetition during training,
which leads to more diverse and effective arguments. The opponent-aware feature
of ToMAP also makes it suitable for long conversations and enables it to employ
more logical and opponent-aware strategies. These results underscore our
method's effectiveness and highlight its potential for developing more
persuasive language agents. Code is available at:
https://github.com/ulab-uiuc/ToMAP.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [17] [Accelerating ab initio melting property calculations with machine learning: Application to the high entropy alloy TaVCrW](https://arxiv.org/abs/2408.08654)
*Li-Fang Zhu,Fritz Koermann,Qing Chen,Malin Selleby,Joerg Neugebauer,and Blazej Grabowski*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出了一种高效的基于密度泛函理论（DFT）的方法，通过机器学习势能辅助，大幅降低计算熔化性质的资源消耗，并将其应用于高熵合金TaVCrW。


<details>
  <summary>Details</summary>
Motivation: 实验测量高熔点材料的熔化性质极具挑战性，而传统基于DFT的自由能方法虽然准确但计算成本高昂，难以进行高通量计算。因此，需要一种更高效的方法来预测这些性质。

Method: 设计了一种由特殊机器学习势能辅助的DFT方法，该势能可以精确再现多组分合金的从头算相空间，从而用更高效的自由能微扰计算替代昂贵的热力学积分。

Result: 与现有方法相比，新方法节省了80%的计算资源，并成功应用于高熵合金TaVCrW，计算出其熔点、熔化熵和焓、熔点处的体积变化以及固液两相的热容，结果与calphad外推值合理一致。

Conclusion: 所提出的方法显著降低了计算高熔点材料熔化性质的成本，为高通量计算和新型高性能耐火材料的设计提供了可能。

Abstract: Melting properties are critical for designing novel materials, especially for
discovering high-performance, high-melting refractory materials. Experimental
measurements of these properties are extremely challenging due to their high
melting temperatures. Complementary theoretical predictions are, therefore,
indispensable. The conventional free energy approach using density functional
theory (DFT) has been a gold standard for such purposes because of its high
accuracy. However,it generally involves expensive thermodynamic integration
using ab initio molecular dynamic simulations. The high computational cost
makes high-throughput calculations infeasible. Here, we propose a highly
efficient DFT-based method aided by a specially designed machine learning
potential. As the machine learning potential can closely reproduce the ab
initio phase space, even for multi-component alloys, the costly thermodynamic
integration can be fully substituted with more efficient free energy
perturbation calculations. The method achieves overall savings of computational
resources by 80% compared to current alternatives. We apply the method to the
high-entropy alloy TaVCrW and calculate its melting properties, including
melting temperature, entropy and enthalpy of fusion, and volume change at the
melting point. Additionally, the heat capacities of solid and liquid TaVCrW are
calculated. The results agree reasonably with the calphad extrapolated values.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [18] [Casper DPM: Cascaded Perceptual Dynamic Projection Mapping onto Hands](https://arxiv.org/abs/2409.04397)
*Yotam Erel,Or Kozlovsky-Mordenfeld,Daisuke Iwai,Kosuke Sato,Amit H. Bermano*

Main category: cs.GR

TL;DR: 本论文提出了一种技术，可以动态地将3D内容投影到人手上，并且感知运动到光子的延迟很短。通过结合3D粗略估计和2D校正步骤，该方法提高了投影与手部的对齐度、增加了投影面积并减少了感知延迟。用户研究显示，相较于直接从3D姿态估计投影帧的简单方法，使用本方法的用户对延迟伪影的敏感性降低，任务完成速度更快且更轻松。此外，由于利用了手部的完整3D重建，可以应用任意纹理或效果。


<details>
  <summary>Details</summary>
Motivation: 准确快速地计算出手的姿态和形状是一项具有挑战性的任务，因为手是关节连接且可变形的。为了克服这个问题，需要一种新的技术来实现更精确和实时的手部投影。

Method: 该方法结合了较慢的3D粗略手部姿态估计和高速2D校正步骤。这种组合改善了投影与手部的对齐，增加了投影区域，并减少了感知延迟。同时，由于采用完整的3D手部重建，可以应用任意纹理或合理性能的效果。

Result: 通过两次用户研究，结果表明使用该方法时，用户对延迟伪影的敏感性降低，执行相关任务的速度更快且更轻松。相较之下，简单地从3D姿态估计直接投影帧的方法表现较差。

Conclusion: 本研究提出了一种有效的方法，能够动态地将3D内容投影到人手上，同时减少感知延迟和提高用户体验。这种方法不仅在技术上有创新，还展示了多种新颖的应用场景和用途。

Abstract: We present a technique for dynamically projecting 3D content onto human hands
with short perceived motion-to-photon latency. Computing the pose and shape of
human hands accurately and quickly is a challenging task due to their
articulated and deformable nature. We combine a slower 3D coarse estimation of
the hand pose with high speed 2D correction steps which improve the alignment
of the projection to the hands, increase the projected surface area, and reduce
perceived latency. Since our approach leverages a full 3D reconstruction of the
hands, any arbitrary texture or reasonably performant effect can be applied,
which was not possible before. We conducted two user studies to assess the
benefits of using our method. The results show subjects are less sensitive to
latency artifacts and perform faster and with more ease a given associated task
over the naive approach of directly projecting rendered frames from the 3D pose
estimation. We demonstrate several novel use cases and applications.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [19] [Model-Based AI planning and Execution Systems for Robotics](https://arxiv.org/abs/2505.04493)
*Or Wertheim,Ronen I. Brafman*

Main category: cs.RO

TL;DR: 基于模型的规划和执行系统为构建能够自动组合多种基本技能以完成多样化任务的灵活自主机器人提供了一种原则性的方法。本文回顾了现有的设计选择、问题解决方案，并提出了未来发展的方向。


<details>
  <summary>Details</summary>
Motivation: 尽管基于模型的规划和执行系统的概念几乎与现代机器人学一样古老，但直到ROSPlan系统的出现，才真正将通用推理架构与现代机器人平台集成起来。这促使了更多面向机器人任务级控制的基于模型的系统的发展。因此，有必要对这些系统的设计选择、问题及解决方案进行总结和分析。

Method: 本文通过回顾和比较现有基于模型的机器人任务级控制系统，探讨了它们在设计中的多样选择以及所尝试解决的问题，并对目前提出的各种解决方案进行了归纳。

Result: 文章总结了当前基于模型的机器人任务级控制系统的现状，明确了不同系统的设计理念和解决问题的方法，并为未来该领域的发展提供了建议。

Conclusion: 基于模型的规划和执行系统对于实现灵活的自主机器人至关重要。随着技术的进步，未来需要进一步探索如何更好地整合这些系统，提高其适应性和通用性，从而推动机器人技术的发展。

Abstract: Model-based planning and execution systems offer a principled approach to
building flexible autonomous robots that can perform diverse tasks by
automatically combining a host of basic skills. This idea is almost as old as
modern robotics. Yet, while diverse general-purpose reasoning architectures
have been proposed since, general-purpose systems that are integrated with
modern robotic platforms have emerged only recently, starting with the
influential ROSPlan system. Since then, a growing number of model-based systems
for robot task-level control have emerged. In this paper, we consider the
diverse design choices and issues existing systems attempt to address, the
different solutions proposed so far, and suggest avenues for future
development.

</details>


### [20] [Hardware Design and Learning-Based Software Architecture of Musculoskeletal Wheeled Robot Musashi-W for Real-World Applications](https://arxiv.org/abs/2403.11729)
*Kento Kawaharazuka,Akihiro Miki,Masahiro Bando,Temma Suzuki,Yoshimoto Ribayashi,Yasunori Toshimitsu,Yuya Nagamatsu,Kei Okada,and Masayuki Inaba*

Main category: cs.RO

TL;DR: 本文介绍了一种结合轮式底盘和肌肉骨骼上肢的机器人Musashi-W，旨在克服传统人形机器人在实际任务中的应用困难。通过硬件设计与软件系统的结合，展示了其在清洁、搬运重物和动态布料操作等任务中的优势。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的肌肉骨骼人形机器人能够模仿人类身体的灵活性和冗余性，但它们在实际任务中的应用仍然受到限制，特别是由于柔性身体的双足行走难度较大。因此，研究者尝试开发一种新的机器人设计，以更好地利用肌肉骨骼结构的优势并解决实际问题。

Method: 研究者设计了Musashi-W，这是一种将轮式底盘与肌肉骨骼上肢相结合的机器人。同时，他们构建了一个软件系统，该系统集成了静态和动态身体模式学习、反射控制以及视觉识别技术。这些技术共同支持机器人完成各种任务。

Result: Musashi-W成功展示了其在多个实际任务中的能力，包括通过人类教学进行清洁、考虑肌肉增加的重物搬运，以及通过可变刚度的动态布料操作来布置桌子。这些结果证明了该机器人设计的有效性。

Conclusion: Musashi-W的设计及其软硬件系统的结合充分利用了肌肉骨骼上肢的优势，为肌肉骨骼机器人在实际任务中的应用提供了新思路。未来的研究可以进一步探索这种设计在更多复杂场景中的潜力。

Abstract: Various musculoskeletal humanoids have been developed so far. While these
humanoids have the advantage of their flexible and redundant bodies that mimic
the human body, they are still far from being applied to real-world tasks. One
of the reasons for this is the difficulty of bipedal walking in a flexible
body. Thus, we developed a musculoskeletal wheeled robot, Musashi-W, by
combining a wheeled base and musculoskeletal upper limbs for real-world
applications. Also, we constructed its software system by combining static and
dynamic body schema learning, reflex control, and visual recognition. We show
that the hardware and software of Musashi-W can make the most of the advantages
of the musculoskeletal upper limbs, through several tasks of cleaning by human
teaching, carrying a heavy object considering muscle addition, and setting a
table through dynamic cloth manipulation with variable stiffness.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [21] [Prices and preferences in the electric vehicle market](https://arxiv.org/abs/2403.00458)
*Chung Yi See,Vasco Rato Santos,Lucas Woodley,Megan Yeo,Daniel Palmer,Shuheng Zhang,and Ashley Nunes*

Main category: econ.EM

TL;DR: 尽管电动车比汽油车污染少，但其较高的采购价格阻碍了普及。现有讨论主要将价格差异归因于电池成本，但本文通过分析2011年至2023年的电动车属性和市场条件数据，发现电动车价格主要受标准配置的豪华设施、附加功能以及经销商安装的配件影响，而电池容量对价格的影响次之。此外，电动车续航里程与价格呈负相关，而电池容量与价格正相关。消费者偏好功能丰富且动力强劲的车辆，这导致电动车燃料经济性降低，减少了生命周期排放优势。这些结果提醒我们在脱碳努力中需重新审视电动车的推广策略。


<details>
  <summary>Details</summary>
Motivation: 当前关于电动车高价格的主要讨论集中在电池成本上，认为电池成本下降是推动电动车广泛采用的关键。然而，这种观点是否全面尚待验证。作者希望通过分析电动车属性和市场条件数据，探讨电动车价格的实际驱动因素，并评估消费者偏好对电动车生命周期排放优势的影响。

Method: 研究收集并分析了2011年至2023年间电动车的属性和市场条件数据。具体来说，作者考察了电动车的价格与其豪华设施、附加功能、经销商配件、马力、续航里程和电池容量之间的关系。通过统计分析，揭示了这些因素如何共同影响电动车价格及消费者的购买决策。

Result: 研究发现：1) 电动车价格主要受到豪华设施、附加功能和经销商配件的影响；2) 续航里程与价格呈负相关，表明续航焦虑可能不如预期严重；3) 电池容量与价格正相关，因为更大容量通常意味着更高性能；4) 消费者偏好导致电动车燃料经济性下降，减少了至少3.26%的生命周期排放优势。

Conclusion: 电动车高价的主要原因并非单纯电池成本，而是消费者对功能丰富和高性能车辆的偏好。这种偏好在一定程度上削弱了电动车的环保优势。因此，在推动电动车作为脱碳路径时，需要重新考虑如何平衡消费者需求与环境目标。

Abstract: Although electric vehicles are less polluting than gasoline powered vehicles,
adoption is challenged by higher procurement prices. Existing discourse
emphasizes EV battery costs as being principally responsible for this price
differential and widespread adoption is routinely conditioned upon battery
costs declining. We scrutinize such reasoning by sourcing data on EV attributes
and market conditions between 2011 and 2023. Our findings are fourfold. First,
EV prices are influenced principally by the number of amenities, additional
features, and dealer-installed accessories sold as standard on an EV, and to a
lesser extent, by EV horsepower. Second, EV range is negatively correlated with
EV price implying that range anxiety concerns may be less consequential than
existing discourse suggests. Third, battery capacity is positively correlated
with EV price, due to more capacity being synonymous with the delivery of more
horsepower. Collectively, this suggests that higher procurement prices for EVs
reflects consumer preference for vehicles that are feature dense and more
powerful. Fourth and finally, accommodating these preferences have produced
vehicles with lower fuel economy, a shift that reduces envisioned lifecycle
emissions benefits by at least 3.26 percent, subject to the battery pack
chemistry leveraged and the carbon intensity of the electrical grid. These
findings warrant attention as decarbonization efforts increasingly emphasize
electrification as a pathway for complying with domestic and international
climate agreements.

</details>
