<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.CL](#cs.CL) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: 这篇文章是一篇立场论文，批判性地综述了人类与AI代理协作的广泛经验发展，强调了其技术成就和持续存在的差距。提出了一种新的概念架构，系统地将多代理协调、知识管理、控制论反馈回路和高层控制机制相互关联，为现有研究提供了映射框架。文章结构灵活，可从任何部分阅读，既是对技术实现的批判性回顾，也是设计或扩展人类-AI共生关系的前瞻性参考。


<details>
  <summary>Details</summary>
Motivation: 当前关于人类与AI代理协作的研究虽然取得了许多技术成就，但缺乏一个统一的理论框架来整合这些多样化的研究，特别是在处理开放性、复杂任务时。这促使作者提出一种新的概念架构以填补这一空白。

Method: 作者提出了一个名为分层探索-利用网络（Hierarchical Exploration-Exploitation Net）的新概念架构。该架构系统地连接了多代理协调、知识管理、控制论反馈回路和高层控制机制等技术细节。通过将现有的贡献，包括符号AI技术、基于连接主义的LLM代理以及混合组织实践，映射到这个框架上，从而促进了对传统方法的修订，并激发了融合定性和定量范式的新工作。

Result: 该新概念架构提供了一个统一的视角，可以更好地理解现有的人类与AI代理协作研究，并为未来的研究指明了方向，有助于推动人类认知和AI能力的共同进化。

Conclusion: 文章总结了当前人类与AI代理协作研究的技术成就和不足，并通过提出一个新的概念架构，为未来的深入研究奠定了基础。这一架构不仅有助于修订传统方法，还为设计或扩展人类-AI共生关系提供了前瞻性的参考，推动了人类认知和AI能力的共同进化。

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning](https://arxiv.org/abs/2505.14625)
*Zhangchen Xu,Yuetai Li,Fengqing Jiang,Bhaskar Ramasubramanian,Luyao Niu,Bill Yuchen Lin,Radha Poovendran*

Main category: cs.LG

TL;DR: 本文研究了强化学习中验证器错误拒绝正确模型输出（false negatives）的问题，并提出了一种轻量级的LLM验证器tinyV来缓解该问题，从而提高了数学推理任务的通过率和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 强化学习的成功依赖于奖励信号的可靠性，然而现有的验证器存在大量错误拒绝正确模型输出的现象，这会严重影响RL训练的效果。因此，需要解决验证器中的false negatives问题以提升RL训练的质量。

Method: 通过对Big-Math-RL-Verified数据集的深入分析，发现超过38%的模型生成响应受到false negatives的影响。为了解决这一问题，提出了一个轻量级的LLM验证器tinyV，它能够动态识别潜在的false negatives并恢复有效的响应，从而提供更准确的奖励估计。

Result: 在多个数学推理基准测试中，集成tinyV后，通过率提高了高达10%，并且相对于基线加速了收敛。

Conclusion: 本文揭示了验证器false negatives对RL训练的严重危害，并通过提出tinyV提供了一个实际可行的解决方案，从而改善了基于RL的LLM微调效果。

Abstract: Reinforcement Learning (RL) has become a powerful tool for enhancing the
reasoning abilities of large language models (LLMs) by optimizing their
policies with reward signals. Yet, RL's success relies on the reliability of
rewards, which are provided by verifiers. In this paper, we expose and analyze
a widespread problem--false negatives--where verifiers wrongly reject correct
model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals
that over 38% of model-generated responses suffer from false negatives, where
the verifier fails to recognize correct answers. We show, both empirically and
theoretically, that these false negatives severely impair RL training by
depriving the model of informative gradient signals and slowing convergence. To
mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments
existing rule-based methods, which dynamically identifies potential false
negatives and recovers valid responses to produce more accurate reward
estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts
pass rates by up to 10% and accelerates convergence relative to the baseline.
Our findings highlight the critical importance of addressing verifier false
negatives and offer a practical approach to improve RL-based fine-tuning of
LLMs. Our code is available at https://github.com/uw-nsl/TinyV.

</details>


### [3] [AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum](https://arxiv.org/abs/2505.14264)
*Jian Xiong,Jingbo Zhou,Jingyong Ye,Dejing Dou*

Main category: cs.LG

TL;DR: 提出了一种新的强化学习算法AAPO，用于优化大语言模型的推理能力，尤其在链式思维数据有限时表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的基于群体相对优势估计的强化学习方法在训练效率上存在不足，特别是在估计的优势接近零时。为了提高训练效率并简化训练过程，需要一种改进的方法来解决这个问题。

Method: 提出了Advantage-Augmented Policy Optimization (AAPO)，这是一种通过动量估计方案增强优势来优化交叉熵损失的新型强化学习算法。该方法可以有效缓解与群体相对优势估计相关的效率问题。

Result: 在多个数学推理基准上的实验结果表明，AAPO的表现优于其他方法。

Conclusion: AAPO作为一种新型的强化学习算法，有效地解决了现有方法中的训练效率问题，并在提升大语言模型推理能力方面展现了优越性能。

Abstract: Reinforcement learning (RL) has emerged as an effective approach for
enhancing the reasoning capabilities of large language models (LLMs),
especially in scenarios where supervised fine-tuning (SFT) falls short due to
limited chain-of-thought (CoT) data. Among RL-based post-training methods,
group relative advantage estimation, as exemplified by Group Relative Policy
Optimization (GRPO), has attracted considerable attention for eliminating the
dependency on the value model, thereby simplifying training compared to
traditional approaches like Proximal Policy Optimization (PPO). However, we
observe that exsiting group relative advantage estimation method still suffers
from training inefficiencies, particularly when the estimated advantage
approaches zero. To address this limitation, we propose Advantage-Augmented
Policy Optimization (AAPO), a novel RL algorithm that optimizes the
cross-entropy (CE) loss using advantages enhanced through a momentum-based
estimation scheme. This approach effectively mitigates the inefficiencies
associated with group relative advantage estimation. Experimental results on
multiple mathematical reasoning benchmarks demonstrate the superior performance
of AAPO.

</details>


### [4] [Safety Subspaces are Not Distinct: A Fine-Tuning Case Study](https://arxiv.org/abs/2505.14185)
*Kaustubh Ponkshe,Shaan Shah,Raghav Singhal,Praneeth Vepakomma*

Main category: cs.LG

TL;DR: 本研究探讨了大语言模型（LLMs）中与安全性相关的行为是否集中在特定的子空间内，并发现安全性行为和不安全性行为在同一子空间内被放大，且不同安全含义的提示激活了重叠的表示。这表明对齐并非几何上局部化，而是由模型更广泛学习动态中的纠缠、高影响组件所产生。因此，基于子空间的防御可能面临根本限制，需要探索替代策略来在持续训练中保持对齐。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的安全性对齐通常通过指令微调和基于人类反馈的强化学习实现，但这种对齐容易受到进一步微调的影响，甚至可能导致有害行为的重新出现。已有研究表明，对齐可能对应于权重空间中的可识别几何方向，形成可以隔离或保护以防止错位的子空间。本文旨在从几何视角全面研究这些假设的有效性。

Method: 研究者通过多个实验分析了参数空间和激活空间中的子空间特性，测试了安全性相关行为是否集中于特定子空间，以及是否有可区分的内部表示模式导致有害行为。实验涉及五种开源大语言模型。

Result: 实验结果表明，放大安全行为的子空间同时也放大了不安全行为，且具有不同安全含义的提示激活了重叠的表示。没有发现选择性控制安全性的子空间。

Conclusion: 研究结果挑战了几何局部化对齐的假设，指出安全性是由模型更广泛学习动态中的纠缠组件产生的。基于子空间的防御可能面临基本限制，需要探索其他方法来在持续训练中保持对齐。

Abstract: Large Language Models (LLMs) rely on safety alignment to produce socially
acceptable responses. This is typically achieved through instruction tuning and
reinforcement learning from human feedback. However, this alignment is known to
be brittle: further fine-tuning, even on benign or lightly contaminated data,
can degrade safety and reintroduce harmful behaviors. A growing body of work
suggests that alignment may correspond to identifiable geometric directions in
weight space, forming subspaces that could, in principle, be isolated or
preserved to defend against misalignment. In this work, we conduct a
comprehensive empirical study of this geometric perspective. We examine whether
safety-relevant behavior is concentrated in specific subspaces, whether it can
be separated from general-purpose learning, and whether harmfulness arises from
distinguishable patterns in internal representations. Across both parameter and
activation space, our findings are consistent: subspaces that amplify safe
behaviors also amplify unsafe ones, and prompts with different safety
implications activate overlapping representations. We find no evidence of a
subspace that selectively governs safety. These results challenge the
assumption that alignment is geometrically localized. Rather than residing in
distinct directions, safety appears to emerge from entangled, high-impact
components of the model's broader learning dynamics. This suggests that
subspace-based defenses may face fundamental limitations and underscores the
need for alternative strategies to preserve alignment under continued training.
We corroborate these findings through multiple experiments on five open-source
LLMs. Our code is publicly available at:
https://github.com/CERT-Lab/safety-subspaces.

</details>


### [5] [RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs](https://arxiv.org/abs/2505.13697)
*Soumya Rani Samineni,Durgesh Kalwar,Karthik Valmeekam,Kaya Stechly,Subbarao Kambhampati*

Main category: cs.LG

TL;DR: 近期基于强化学习的大型语言模型（LLM）后训练方法受到关注，本文通过分析RL/GRPO方法背后的假设和公式化问题，指出其在特定假设下等价于监督学习，并通过实验表明迭代监督微调可达到与GRPO类似的效果。虽然RL可能对提升LLM推理能力有用，但当前流行的建模方式存在缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着DeepSeek R1等应用了GRPO方法的LLM发布，强化学习后训练方法因被认为能显著提升LLM的推理能力而备受关注。然而，这些方法在将LLM训练建模为马尔可夫决策过程（MDP）时所作的简化假设尚未被充分审视。因此，本文旨在深入探讨这些假设的影响及其合理性。

Method: 文章首先指出了当前流行的两种关键结构假设：(1) 将MDP状态定义为动作的简单拼接，即状态对应上下文窗口，动作为LLM中的token；(2) 将轨迹奖励均匀分配到每个步骤。随后，作者通过理论分析证明，在这些假设下，强化学习方法实际上退化为一种结果驱动的监督学习。接着，作者通过在GSM8K和Countdown等基准数据集上使用Qwen-2.5基础模型进行实验，比较了迭代监督微调（结合正负样本）与GRPO方法的表现。

Result: 实验结果表明，迭代监督微调可以实现与GRPO方法相当的性能。此外，作者指出这些结构假设间接鼓励生成更长的中间token序列，从而支持了“RL生成更长思考痕迹”的叙述。这表明当前的RL框架可能存在解释上的偏差。

Conclusion: 尽管强化学习可能是提升LLM推理能力的有效工具，但本文的研究表明，当前流行的简化结构假设使得RL方法的实际效果与监督学习相似，且其解释可能存在误导性。因此，未来研究需要重新审视这些假设并探索更合理的建模方式。

Abstract: Reinforcement learning-based post-training of large language models (LLMs)
has recently gained attention, particularly following the release of DeepSeek
R1, which applied GRPO for fine-tuning. Amid the growing hype around improved
reasoning abilities attributed to RL post-training, we critically examine the
formulation and assumptions underlying these methods. We start by highlighting
the popular structural assumptions made in modeling LLM training as a Markov
Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't
quite need the RL/GRPO apparatus. The two critical structural assumptions
include (1) making the MDP states be just a concatenation of the actions-with
states becoming the context window and the actions becoming the tokens in LLMs
and (2) splitting the reward of a state-action trajectory uniformly across the
trajectory. Through a comprehensive analysis, we demonstrate that these
simplifying assumptions make the approach effectively equivalent to an
outcome-driven supervised learning. Our experiments on benchmarks including
GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised
fine-tuning, incorporating both positive and negative samples, achieves
performance comparable to GRPO-based training. We will also argue that the
structural assumptions indirectly incentivize the RL to generate longer
sequences of intermediate tokens-which in turn feeds into the narrative of "RL
generating longer thinking traces." While RL may well be a very useful
technique for improving the reasoning abilities of LLMs, our analysis shows
that the simplistic structural assumptions made in modeling the underlying MDP
render the popular LLM RL frameworks and their interpretations questionable.

</details>


### [6] [4Hammer: a board-game reinforcement learning environment for the hour long time frame](https://arxiv.org/abs/2505.13638)
*Massimo Fioravanti,Giovanni Agosta*

Main category: cs.LG

TL;DR: 提出了一种名为4Hammer的强化学习环境，用于评估大语言模型在复杂棋盘游戏中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在短期任务中表现出色，但在需要较长时间的任务上表现不佳。目前缺乏针对复杂棋盘游戏设计的强化学习环境，以评估LLMs的能力。

Method: 通过创建4Hammer这一基于Warhammer 40,000子集的数字孪生模拟环境，该环境具有复杂的规则和零和博弈特性，要求玩家理解详细规则、掌握游戏单位间的互动并跟踪游戏状态。

Result: 尚未明确给出具体实验结果，但此环境的设计为未来评估LLMs和强化学习算法提供了新平台。

Conclusion: 4Hammer作为一款专为强化学习与LLM评估设计的复杂棋盘游戏环境，填补了现有研究中的空白，并为长期任务的性能评估提供了可能。

Abstract: Large Language Models (LLMs) have demonstrated strong performance on tasks
with short time frames, but struggle with tasks requiring longer durations.
While datasets covering extended-duration tasks, such as software engineering
tasks or video games, do exist, there are currently few implementations of
complex board games specifically designed for reinforcement learning and LLM
evaluation. To address this gap, we propose the 4Hammer reinforcement learning
environment, a digital twin simulation of a subset of Warhammer 40,000-a
complex, zero-sum board game. Warhammer 40,000 features intricate rules,
requiring human players to thoroughly read and understand over 50 pages of
detailed natural language rules, grasp the interactions between their game
pieces and those of their opponents, and independently track and communicate
the evolving game state.

</details>


### [7] [Optimizing Anytime Reasoning via Budget Relative Policy Optimization](https://arxiv.org/abs/2505.13438)
*Penghui Qi,Zichen Liu,Tianyu Pang,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: 在数学推理任务中，AnytimeReasoner通过优化任意时刻推理性能和引入新的方差缩减技术BRPO，在不同先验分布下所有推理预算范围内均优于GRPO，提升了训练和标记效率。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要使用强化学习来最大化推理过程结束时可验证的奖励，但这些方法仅针对大且固定的标记预算下的最终性能进行优化，导致训练和部署效率低下。因此，需要一种能够提升标记效率并适应不同标记预算约束的新方法。

Method: 提出了一种名为AnytimeReasoner的新框架，该框架通过截断完整的思考过程以适应从先验分布采样的标记预算，并促使模型为每个截断的思考生成最优答案摘要进行验证，从而引入密集的可验证奖励。此外，还提出了一个新颖的方差缩减技术Budget Relative Policy Optimization (BRPO)，用于增强学习过程中加强思考策略时的鲁棒性和效率。

Result: 实证结果表明，在各种先验分布和所有思考预算范围内，所提出的方法在数学推理任务中始终优于GRPO，同时提高了训练和标记效率。

Conclusion: AnytimeReasoner框架通过优化任意时刻推理性能和引入BRPO技术，成功提升了标记效率和训练效率，为更高效地利用大型语言模型在不同预算约束下的推理能力提供了新思路。

Abstract: Scaling test-time compute is crucial for enhancing the reasoning capabilities
of large language models (LLMs). Existing approaches typically employ
reinforcement learning (RL) to maximize a verifiable reward obtained at the end
of reasoning traces. However, such methods optimize only the final performance
under a large and fixed token budget, which hinders efficiency in both training
and deployment. In this work, we present a novel framework, AnytimeReasoner, to
optimize anytime reasoning performance, which aims to improve token efficiency
and the flexibility of reasoning under varying token budget constraints. To
achieve this, we truncate the complete thinking process to fit within sampled
token budgets from a prior distribution, compelling the model to summarize the
optimal answer for each truncated thinking for verification. This introduces
verifiable dense rewards into the reasoning process, facilitating more
effective credit assignment in RL optimization. We then optimize the thinking
and summary policies in a decoupled manner to maximize the cumulative reward.
Additionally, we introduce a novel variance reduction technique, Budget
Relative Policy Optimization (BRPO), to enhance the robustness and efficiency
of the learning process when reinforcing the thinking policy. Empirical results
in mathematical reasoning tasks demonstrate that our method consistently
outperforms GRPO across all thinking budgets under various prior distributions,
enhancing both training and token efficiency.

</details>


### [8] [Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs](https://arxiv.org/abs/2505.13026)
*Jack Chen,Fazhong Liu,Naruto Liu,Yuhan Luo,Erqu Qin,Harry Zheng,Tian Dong,Haojin Zhu,Yan Meng,Xiao Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为SASR的逐步自适应混合训练框架，该框架结合了监督微调(SFT)和强化学习(RL)，解决了单独使用SFT或RL时面临的过拟合和模式坍塌问题。SASR通过梯度范数和分布偏离的自适应动态调整算法，将SFT与在线RL方法GRPO无缝集成，从而在不同任务间实现良好的泛化能力，并减少对数据质量的依赖。实验结果表明，SASR在数学推理和逻辑问题解决方面优于SFT、RL和静态混合训练方法。


<details>
  <summary>Details</summary>
Motivation: 当前的大规模语言模型主要通过监督微调(SFT)和强化学习(RL)来提升推理能力，但单独使用这两种方法都存在缺陷：SFT容易过拟合，而RL容易出现模式坍塌。现有的静态混合训练方案虽然有所改进，但在不同任务间的泛化能力和对数据质量的依赖性上仍面临挑战。因此，需要一种能够动态平衡SFT和RL的训练框架，以克服这些限制并进一步提升模型性能。

Method: SASR框架首先利用SFT进行预热，建立基本的推理技能，然后通过基于梯度范数和分布偏离的自适应动态调整算法，将SFT与在线RL方法GRPO无缝结合。具体而言，SASR会根据训练状态监测结果，动态调整SFT和RL之间的权重，确保训练过程中的平滑过渡，同时维持核心推理能力并探索不同的优化路径。

Result: 实验结果表明，SASR在数学推理和逻辑问题解决任务上的表现显著优于单独使用SFT、RL以及静态混合训练方法。SASR不仅提升了模型的泛化能力，还减少了对高质量数据的依赖，为大规模语言模型的训练提供了一种更高效、更稳定的方法。

Conclusion: SASR作为一种逐步自适应混合训练框架，成功地统一了SFT和RL的理论基础，并通过动态平衡两种方法在整个优化过程中实现了更好的性能。相比现有的静态混合训练方案，SASR在不同任务间表现出更强的泛化能力，并降低了对数据质量的敏感性，为未来大规模语言模型的训练提供了新的方向和思路。

Abstract: Large language models (LLMs) excel at mathematical reasoning and logical
problem-solving. The current popular training paradigms primarily use
supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the
models' reasoning abilities. However, when using SFT or RL alone, there are
respective challenges: SFT may suffer from overfitting, while RL is prone to
mode collapse. The state-of-the-art methods have proposed hybrid training
schemes. However, static switching faces challenges such as poor generalization
across different tasks and high dependence on data quality. In response to
these challenges, inspired by the curriculum learning-quiz mechanism in human
reasoning cultivation, We propose SASR, a step-wise adaptive hybrid training
framework that theoretically unifies SFT and RL and dynamically balances the
two throughout optimization. SASR uses SFT for initial warm-up to establish
basic reasoning skills, and then uses an adaptive dynamic adjustment algorithm
based on gradient norm and divergence relative to the original distribution to
seamlessly integrate SFT with the online RL method GRPO. By monitoring the
training status of LLMs and adjusting the training process in sequence, SASR
ensures a smooth transition between training schemes, maintaining core
reasoning abilities while exploring different paths. Experimental results
demonstrate that SASR outperforms SFT, RL, and static hybrid training methods.

</details>


### [9] [DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management](https://arxiv.org/abs/2505.12951)
*Xuerui Su,Liya Guo,Yue Wang,Yi Zhu,Zhiming Ma,Zun Wang,Yuting Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习算法Decoupled Group Reward Optimization (DGRO)，用于大型语言模型的推理任务。通过解耦传统的正则化系数，DGRO可以精确控制探索与利用之间的平衡，并在逻辑数据集上取得了96.9%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 当前大多数推理方法依赖于手工设计的基于规则的奖励函数，但这种方法在强化学习中的探索与利用权衡方面存在复杂性，且理论和经验影响尚未充分研究。因此，需要一种更有效的强化学习算法来优化大型语言模型的推理能力。

Method: DGRO算法将传统的正则化系数分解为两个独立的超参数：一个用于缩放策略梯度项，另一个用于调节采样策略的距离。此外，该算法还研究了奖励方差对收敛速度和最终模型性能的影响，并通过理论分析和广泛的实证验证进行了评估。

Result: 实验结果表明，DGRO在逻辑数据集上达到了96.9%的平均准确率，并在数学基准测试中表现出强大的泛化能力。

Conclusion: DGRO算法不仅能够精确控制探索与利用之间的平衡，还可以无缝扩展到在线策略镜像下降算法和直接奖励优化。这使得DGRO成为一种适用于大型语言模型推理任务的通用强化学习算法。

Abstract: Inference scaling further accelerates Large Language Models (LLMs) toward
Artificial General Intelligence (AGI), with large-scale Reinforcement Learning
(RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning
approaches usually rely on handcrafted rule-based reward functions. However,
the tarde-offs of exploration and exploitation in RL algorithms involves
multiple complex considerations, and the theoretical and empirical impacts of
manually designed reward functions remain insufficiently explored. In this
paper, we propose Decoupled Group Reward Optimization (DGRO), a general RL
algorithm for LLM reasoning. On the one hand, DGRO decouples the traditional
regularization coefficient into two independent hyperparameters: one scales the
policy gradient term, and the other regulates the distance from the sampling
policy. This decoupling not only enables precise control over balancing
exploration and exploitation, but also can be seamlessly extended to Online
Policy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward
Optimization. On the other hand, we observe that reward variance significantly
affects both convergence speed and final model performance. We conduct both
theoretical analysis and extensive empirical validation to assess DGRO,
including a detailed ablation study that investigates its performance and
optimization dynamics. Experimental results show that DGRO achieves
state-of-the-art performance on the Logic dataset with an average accuracy of
96.9\%, and demonstrates strong generalization across mathematical benchmarks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [10] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 在大规模语言模型（LLMs）中，通过重复采样来扩展推理计算可以提高覆盖范围（解决问题的比例）。然而，这种改进部分可能是因为标准评估基准的答案分布偏向于少量常见答案。研究通过定义一个基于训练集答案频率的基线，发现该基线在某些LLMs中优于重复采样，而在其他LLMs中与混合策略相当。这为更准确地衡量重复采样的效果提供了一种方法。


<details>
  <summary>Details</summary>
Motivation: 研究者观察到，在大语言模型中，随着样本数量的增加，通过重复采样持续扩展推理计算会增加覆盖范围。他们推测这种改进部分是由于标准评估基准中的答案分布偏向于少量常见答案，因此希望验证这一猜想，并探索重复采样的实际效果。

Method: 研究者定义了一个基线方法，该方法根据训练集中答案的出现频率枚举答案。然后在两个领域（数学推理和事实知识）进行了实验，比较了该基线与重复模型采样以及一种混合策略的表现，其中混合策略通过仅使用10个模型样本获取答案，并通过枚举猜测剩余的尝试。

Result: 实验结果显示，在某些LLMs中，基于训练集答案频率的基线方法优于重复采样；而在其他LLMs中，其表现与混合策略相当。这表明重复采样的效果可能受到答案分布的影响。

Conclusion: 通过引入基于训练集答案频率的基线方法，研究为更准确地测量重复采样在提示无关猜测之外对覆盖范围的提升提供了一种新方法。这有助于更好地理解重复采样在大语言模型中的作用。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [11] [KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation](https://arxiv.org/abs/2505.14552)
*Jiajun Shi,Jian Yang,Jiaheng Liu,Xingyuan Bu,Jiangjie Chen,Junting Zhou,Kaijing Ma,Zhoufutu Wen,Bingli Wang,Yancheng He,Liang Song,Hualei Zhu,Shilong Li,Xingjian Wang,Wei Zhang,Ruibin Yuan,Yifan Yao,Wenjun Yang,Yunli Wang,Siyuan Fang,Siyu Yuan,Qianyu He,Xiangru Tang,Yingshui Tan,Wangchunshu Zhou,Zhaoxiang Zhang,Zhoujun Li,Wenhao Huang,Ge Zhang*

Main category: cs.CL

TL;DR: 最近大语言模型（LLMs）的发展表明，需要更全面的评估方法来准确评估其推理能力。现有的基准测试通常是领域特定的，因此无法完全捕捉LLM的一般推理潜力。为了解决这一限制，我们引入了知识正交推理体育馆（KORGym），这是一个受KOR-Bench和Gymnasium启发的动态评估平台。KORGym提供了超过五十种文本或视觉格式的游戏，并支持带有强化学习场景的交互式、多轮评估。通过KORGym，我们对19个LLMs和8个VLMs进行了广泛的实验，揭示了模型家族内部一致的推理模式，并展示了闭源模型的优越性能。进一步分析研究了模态、推理策略、强化学习技术和响应长度对模型性能的影响。我们期望KORGym成为推动LLM推理研究和开发适合复杂交互环境的评估方法的宝贵资源。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型在不同领域的应用越来越广泛，但现有的评估基准往往局限于特定领域，无法全面反映模型的通用推理能力。这促使研究者开发一个更加综合和灵活的评估平台，以适应不断发展的模型需求。

Method: 研究者设计并实现了一个名为KORGym的动态评估平台，该平台包括超过五十种游戏，涵盖文本和视觉形式。KORGym支持交互式、多轮次评估，并结合强化学习技术进行更深入的模型分析。通过对19个大语言模型和8个视觉语言模型进行实验，验证了该平台的有效性。

Result: 实验结果表明，KORGym能够有效揭示不同模型家族之间的推理模式差异，并证明了闭源模型在推理能力上的优势。此外，研究还探讨了模态、推理策略、强化学习技术及响应长度等因素对模型性能的具体影响。

Conclusion: KORGym作为一个新颖且全面的评估工具，将有助于推动大语言模型推理能力的研究进展，同时为复杂交互环境下的模型评估提供新的思路和方法。

Abstract: Recent advancements in large language models (LLMs) underscore the need for
more comprehensive evaluation methods to accurately assess their reasoning
capabilities. Existing benchmarks are often domain-specific and thus cannot
fully capture an LLM's general reasoning potential. To address this limitation,
we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic
evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over
fifty games in either textual or visual formats and supports interactive,
multi-turn assessments with reinforcement learning scenarios. Using KORGym, we
conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent
reasoning patterns within model families and demonstrating the superior
performance of closed-source models. Further analysis examines the effects of
modality, reasoning strategies, reinforcement learning techniques, and response
length on model performance. We expect KORGym to become a valuable resource for
advancing LLM reasoning research and developing evaluation methodologies suited
to complex, interactive environments.

</details>


### [12] [Time-R1: Towards Comprehensive Temporal Reasoning in LLMs](https://arxiv.org/abs/2505.13508)
*Zijia Liu,Peixuan Han,Haofei Yu,Haoru Li,Jiaxuan You*

Main category: cs.CL

TL;DR: 本文介绍了一种名为Time-R1的框架，通过强化学习方法赋予中等规模（3B参数）的语言模型全面的时间处理能力，包括理解、预测和创造性生成。该框架分三个阶段开发，前两个阶段基于动态规则奖励系统构建时间理解和未来事件预测能力，第三阶段实现对未来场景的创造性生成。实验表明，Time-R1在复杂未来事件预测和创造性场景生成任务上超越了200倍大的模型，如671B参数的DeepSeek-R1。此外，作者还发布了大规模多任务时间推理数据集Time-Bench及Time-R1的检查点，以推动进一步研究。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型虽然功能强大，但在时间智能方面存在不足，难以将对过去的推理与未来的预测和合理生成结合起来。现有的方法通常只针对孤立的时间技能，例如回答过去事件的问题或基本预测，且在处理超出知识截止日期的事件或需要创造性预见的任务时表现不佳。因此，需要一种能够全面提升语言模型时间处理能力的方法。

Method: Time-R1框架采用三阶段开发路径：
1. 第一阶段通过历史数据构建基础的时间理解能力和逻辑事件-时间映射。
2. 第二阶段利用强化学习课程和精心设计的动态规则奖励系统，训练模型对未来事件的预测能力，特别是超出其知识截止日期的事件。
3. 第三阶段无需微调即可实现对未来场景的创造性生成。
整个过程由强化学习驱动，逐步提升模型的时间处理能力。

Result: 实验结果表明，Time-R1在高度复杂的未来事件预测和创造性场景生成基准测试中，显著优于超过200倍大的模型，如671B参数的DeepSeek-R1。这证明了经过精心设计的渐进式强化学习微调可以使较小、高效的模型达到优越的时间性能。

Conclusion: 本研究表明，通过精心设计的渐进式强化学习微调，较小规模的语言模型可以实现比大规模模型更优越的时间处理能力。Time-R1为真正具备时间感知能力的AI提供了一条实用且可扩展的路径。为了促进进一步研究，作者发布了大规模多任务时间推理数据集Time-Bench以及Time-R1的一系列检查点。

Abstract: Large Language Models (LLMs) demonstrate impressive capabilities but lack
robust temporal intelligence, struggling to integrate reasoning about the past
with predictions and plausible generations of the future. Meanwhile, existing
methods typically target isolated temporal skills, such as question answering
about past events or basic forecasting, and exhibit poor generalization,
particularly when dealing with events beyond their knowledge cutoff or
requiring creative foresight. To address these limitations, we introduce
\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter)
LLM with comprehensive temporal abilities: understanding, prediction, and
creative generation. Our approach features a novel three-stage development
path; the first two constitute a \textit{reinforcement learning (RL)
curriculum} driven by a meticulously designed dynamic rule-based reward system.
This framework progressively builds (1) foundational temporal understanding and
logical event-time mappings from historical data, (2) future event prediction
skills for events beyond its knowledge cutoff, and finally (3) enables
remarkable generalization to creative future scenario generation without any
fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms
models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,
on highly challenging future event prediction and creative scenario generation
benchmarks. This work provides strong evidence that thoughtfully engineered,
progressive RL fine-tuning allows smaller, efficient models to achieve superior
temporal performance, offering a practical and scalable path towards truly
time-aware AI. To foster further research, we also release \textit{Time-Bench},
a large-scale multi-task temporal reasoning dataset derived from 10 years of
news data, and our series of \textit{Time-R1} checkpoints.

</details>
