{"id": "2505.00018", "keyword": "LLM Agent", "pdf": "https://arxiv.org/pdf/2505.00018", "abs": "https://arxiv.org/abs/2505.00018", "authors": ["Ju Wu", "Calvin K. L. Or"], "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability.", "keywords": ["LLM Agent"], "AI": {"tldr": "This paper surveys recent developments in human-AI collaboration, pointing out achievements and gaps. It proposes a new conceptual architecture (Hierarchical Exploration-Exploitation Net) to integrate varied studies and facilitate new work fusing qualitative and quantitative paradigms.", "motivation": "There is a lack of a unifying theoretical framework that can coherently integrate various studies on human-AI agents collaboration, particularly for open-ended, complex tasks.", "method": "The paper proposes a novel conceptual architecture called Hierarchical Exploration-Exploitation Net that interlinks multi-agent coordination, knowledge management, cybernetic feedback loops, and higher-level control mechanisms.", "result": "This approach allows mapping existing contributions onto the proposed framework, facilitating revision of legacy methods and inspiring new work that combines different paradigms.", "conclusion": "The insights provided offer a stepping stone toward deeper co-evolution of human cognition and AI capability."}}
{"id": "2410.15466", "keyword": "LLM reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing.", "keywords": ["LLM reasoning"], "AI": {"tldr": "\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\uff0c\u901a\u8fc7\u91cd\u590d\u91c7\u6837\u6765\u6269\u5c55\u63a8\u7406\u8ba1\u7b97\uff0c\u968f\u7740\u6837\u672c\u6570\u91cf\u7684\u589e\u52a0\uff0c\u8986\u76d6\u7387\uff08\u89e3\u51b3\u95ee\u9898\u7684\u6bd4\u4f8b\uff09\u4f1a\u6301\u7eed\u63d0\u9ad8\u3002\u672c\u6587\u7814\u7a76\u4e86\u8fd9\u4e00\u73b0\u8c61\u90e8\u5206\u662f\u7531\u4e8e\u6807\u51c6\u8bc4\u4f30\u57fa\u51c6\u4e2d\u7684\u7b54\u6848\u5206\u5e03\u504f\u5411\u4e8e\u8f83\u5c0f\u7684\u5e38\u89c1\u7b54\u6848\u96c6\u3002\u901a\u8fc7\u5b9a\u4e49\u4e00\u4e2a\u57fa\u4e8e\u8bad\u7ec3\u96c6\u4e2d\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\uff0c\u53d1\u73b0\u5bf9\u4e8e\u67d0\u4e9bLLMs\uff0c\u8be5\u57fa\u7ebf\u6bd4\u91cd\u590d\u6a21\u578b\u91c7\u6837\u8868\u73b0\u66f4\u597d\uff1b\u800c\u5bf9\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5176\u8986\u76d6\u7387\u4e0e\u4e00\u79cd\u6df7\u5408\u7b56\u7565\u76f8\u5f53\u3002\u6b64\u57fa\u7ebf\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u6d4b\u91cf\u91cd\u590d\u91c7\u6837\u5728\u63d0\u793a\u65e0\u5173\u731c\u6d4b\u4e4b\u5916\u7684\u8986\u76d6\u7387\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u91cd\u590d\u91c7\u6837\u5728\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u6548\u679c\uff0c\u5e76\u5206\u6790\u8fd9\u79cd\u6548\u679c\u662f\u5426\u4e0e\u8bc4\u4f30\u57fa\u51c6\u4e2d\u7b54\u6848\u5206\u5e03\u7684\u504f\u659c\u6709\u5173\u3002", "method": "\u5b9a\u4e49\u4e86\u4e00\u4e2a\u6839\u636e\u8bad\u7ec3\u96c6\u4e2d\u7b54\u6848\u9891\u7387\u679a\u4e3e\u7b54\u6848\u7684\u57fa\u7ebf\uff0c\u5e76\u5728\u6570\u5b66\u63a8\u7406\u548c\u4e8b\u5b9e\u77e5\u8bc6\u4e24\u4e2a\u9886\u57df\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u6bd4\u8f83\u4e86\u8be5\u57fa\u7ebf\u3001\u91cd\u590d\u6a21\u578b\u91c7\u6837\u4ee5\u53ca\u6df7\u5408\u7b56\u7565\u7684\u8868\u73b0\u3002", "result": "\u5bf9\u4e8e\u67d0\u4e9bLLMs\uff0c\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u4f18\u4e8e\u91cd\u590d\u6a21\u578b\u91c7\u6837\uff1b\u800c\u5bf9\u5176\u4ed6\u6a21\u578b\uff0c\u5176\u8868\u73b0\u4e0e\u6df7\u5408\u7b56\u7565\u76f8\u5f53\u3002", "conclusion": "\u91cd\u590d\u91c7\u6837\u7684\u6027\u80fd\u63d0\u5347\u90e8\u5206\u5f52\u56e0\u4e8e\u8bc4\u4f30\u57fa\u51c6\u4e2d\u7b54\u6848\u5206\u5e03\u7684\u504f\u659c\u3002\u4f7f\u7528\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u8861\u91cf\u91cd\u590d\u91c7\u6837\u5728\u63d0\u793a\u65e0\u5173\u731c\u6d4b\u4e4b\u5916\u7684\u5b9e\u9645\u6539\u8fdb\u3002"}}
{"id": "2505.08189", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.08189", "abs": "https://arxiv.org/abs/2505.08189", "authors": ["Alex Zhihao Dou", "Dongfei Cui", "Jun Yan", "Weida Wang", "Benteng Chen", "Haoming Wang", "Zeke Xie", "Shufei Zhang"], "title": "DSADF: Thinking Fast and Slow for Decision Making", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Although Reinforcement Learning (RL) agents are effective in well-defined\nenvironments, they often struggle to generalize their learned policies to\ndynamic settings due to their reliance on trial-and-error interactions. Recent\nwork has explored applying Large Language Models (LLMs) or Vision Language\nModels (VLMs) to boost the generalization of RL agents through policy\noptimization guidance or prior knowledge. However, these approaches often lack\nseamless coordination between the RL agent and the foundation model, leading to\nunreasonable decision-making in unfamiliar environments and efficiency\nbottlenecks. Making full use of the inferential capabilities of foundation\nmodels and the rapid response capabilities of RL agents and enhancing the\ninteraction between the two to form a dual system is still a lingering\nscientific question. To address this problem, we draw inspiration from\nKahneman's theory of fast thinking (System 1) and slow thinking (System 2),\ndemonstrating that balancing intuition and deep reasoning can achieve nimble\ndecision-making in a complex world. In this study, we propose a Dual-System\nAdaptive Decision Framework (DSADF), integrating two complementary modules:\nSystem 1, comprising an RL agent and a memory space for fast and intuitive\ndecision making, and System 2, driven by a VLM for deep and analytical\nreasoning. DSADF facilitates efficient and adaptive decision-making by\ncombining the strengths of both systems. The empirical study in the video game\nenvironment: Crafter and Housekeep demonstrates the effectiveness of our\nproposed method, showing significant improvements in decision abilities for\nboth unseen and known tasks.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "To solve the generalization problem of RL agents in dynamic settings, this paper proposes DSADF which integrates fast intuitive decision making (System 1) and deep analytical reasoning (System 2). Empirical results show significant improvements in decision-making abilities.", "motivation": "RL agents struggle to generalize their learned policies to dynamic settings. Using LLMs or VLMs has been explored but lacks seamless coordination between RL agents and foundation models.", "method": "Propose DSADF integrating two complementary modules: System 1 with an RL agent and memory space for fast decisions, and System 2 driven by a VLM for deep reasoning.", "result": "Empirical study in Crafter and Housekeep environments demonstrates significant improvements in decision-making abilities for both unseen and known tasks.", "conclusion": "DSADF facilitates efficient and adaptive decision-making by combining the strengths of fast intuitive decision making and deep analytical reasoning."}}
{"id": "2505.07782", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.07782", "abs": "https://arxiv.org/abs/2505.07782", "authors": ["Rushi Qiang", "Yuchen Zhuang", "Yinghao Li", "Dingu Sagar V K", "Rongzhi Zhang", "Changhao Li", "Ian Shu-Hei Wong", "Sherry Yang", "Percy Liang", "Chao Zhang", "Bo Dai"], "title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering", "categories": ["cs.LG"], "comment": null, "summary": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement\nlearning, evaluating, and improving autonomous large language model (LLM)\nagents in iterative machine learning engineering (MLE) workflows. Unlike\nexisting benchmarks that primarily rely on static datasets or single-attempt\nevaluations, MLE-Dojo provides an interactive environment enabling agents to\niteratively experiment, debug, and refine solutions through structured feedback\nloops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,\nopen-ended MLE tasks carefully curated to reflect realistic engineering\nscenarios such as data processing, architecture search, hyperparameter tuning,\nand code debugging. Its fully executable environment supports comprehensive\nagent training via both supervised fine-tuning and reinforcement learning,\nfacilitating iterative experimentation, realistic data sampling, and real-time\noutcome verification. Extensive evaluations of eight frontier LLMs reveal that\nwhile current models achieve meaningful iterative improvements, they still\nexhibit significant limitations in autonomously generating long-horizon\nsolutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's\nflexible and extensible architecture seamlessly integrates diverse data\nsources, tools, and evaluation protocols, uniquely enabling model-based agent\ntuning and promoting interoperability, scalability, and reproducibility. We\nopen-source our framework and benchmarks to foster community-driven innovation\ntowards next-generation MLE agents.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "The paper presents MLE-Dojo, a framework for reinforcement learning and evaluating autonomous LLM agents in MLE workflows. It provides an interactive environment based on real-world Kaggle challenges, supporting comprehensive agent training and revealing limitations of current LLMs.", "motivation": "Existing benchmarks primarily rely on static datasets or single-attempt evaluations, lacking the ability to provide structured feedback loops for iterative improvement.", "method": "MLE-Dojo is built upon 200+ real-world Kaggle challenges, offering an interactive environment that supports both supervised fine-tuning and reinforcement learning for agent training.", "result": "Evaluations show that while current LLMs can achieve meaningful iterative improvements, they still have significant limitations in generating long-term solutions and resolving complex errors efficiently.", "conclusion": "MLE-Dojo's flexible architecture promotes interoperability, scalability, and reproducibility, and it is open-sourced to foster community-driven innovation."}}
{"id": "2505.07274", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.07274", "abs": "https://arxiv.org/abs/2505.07274", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains", "categories": ["cs.LG"], "comment": null, "summary": "Integrating large language models (LLMs) as priors in reinforcement learning\n(RL) offers significant advantages but comes with substantial computational\ncosts. We present a principled cache-efficient framework for posterior sampling\nwith LLM-derived priors that dramatically reduces these costs while maintaining\nhigh performance. At the core of our approach is an adaptive caching mechanism,\nwhere cache parameters are meta-optimized using surrogate gradients derived\nfrom policy performance. This design enables efficient inference across both\ndiscrete text environments (e.g., TextWorld, ALFWorld) and continuous control\ndomains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries\nand 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU)\nwhile retaining 96--98\\% of uncached performance. Our theoretical analysis\nprovides KL divergence bounds on approximation quality, validated empirically.\nThe framework extends to offline RL, where our CQL-Prior variant improves\nperformance by 14--29\\% and reduces training time by 38--40\\%. Extensive\nevaluations across a diverse suite of eight tasks demonstrate the\ngeneralizability and practical viability of LLM-guided RL in\nresource-constrained settings.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u7684\u5148\u9a8c\u77e5\u8bc6\u8fdb\u884c\u6574\u5408\uff0c\u867d\u7136\u6709\u663e\u8457\u4f18\u52bf\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u5148\u9a8c\u77e5\u8bc6\u7684\u540e\u9a8c\u91c7\u6837\u9ad8\u6548\u7f13\u5b58\u6846\u67b6\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002\u901a\u8fc7\u9002\u5e94\u6027\u7f13\u5b58\u673a\u5236\u548c\u4ee3\u7406\u68af\u5ea6\u5143\u4f18\u5316\uff0c\u8be5\u65b9\u6cd5\u5728\u79bb\u6563\u6587\u672c\u73af\u5883\u548c\u8fde\u7eed\u63a7\u5236\u9886\u57df\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u51cf\u5c11\u4e86LLM\u67e5\u8be2\u6b21\u6570\u548c\u5ef6\u8fdf\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5927\u90e8\u5206\u672a\u7f13\u5b58\u6027\u80fd\u3002\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8fd1\u4f3c\u8d28\u91cf\uff0c\u5e76\u4e14\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u5230\u79bb\u7ebfRL\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u5c3d\u7ba1\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u7684\u5148\u9a8c\u77e5\u8bc6\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u5176\u5e26\u6765\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u4e9f\u5f85\u89e3\u51b3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u7ef4\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u5148\u9a8c\u77e5\u8bc6\u7684\u540e\u9a8c\u91c7\u6837\u9ad8\u6548\u7f13\u5b58\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u9002\u5e94\u6027\u7f13\u5b58\u673a\u5236\u3002\u7f13\u5b58\u53c2\u6570\u901a\u8fc7\u6765\u81ea\u7b56\u7565\u6027\u80fd\u7684\u4ee3\u7406\u68af\u5ea6\u8fdb\u884c\u5143\u4f18\u5316\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u79bb\u6563\u6587\u672c\u73af\u5883\u548c\u8fde\u7eed\u63a7\u5236\u9886\u57df\uff0c\u5e76\u4e14\u53ef\u4ee5\u6269\u5c55\u5230\u79bb\u7ebfRL\u3002", "result": "\u5728\u591a\u4e2a\u73af\u5883\u4e2d\uff0c\u4e0e\u672a\u4f7f\u7528\u7f13\u5b58\u7684\u60c5\u51b5\u76f8\u6bd4\uff0cLLM\u67e5\u8be2\u6b21\u6570\u51cf\u5c11\u4e863.8-4.7\u500d\uff0c\u4e2d\u4f4d\u5ef6\u8fdf\u964d\u4f4e\u4e864.0-12.0\u500d\uff0c\u540c\u65f6\u4fdd\u7559\u4e8696-98%\u7684\u6027\u80fd\u3002\u5728\u79bb\u7ebfRL\u4e2d\uff0cCQL-Prior\u53d8\u4f53\u63d0\u9ad8\u4e8614-29%\u7684\u6027\u80fd\u5e76\u51cf\u5c11\u4e8638-40%\u7684\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "\u63d0\u51fa\u7684\u7f13\u5b58\u9ad8\u6548\u6846\u67b6\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u73af\u5883\uff0c\u5e76\u4e14\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u5c55\u793a\u4e86\u901a\u7528\u6027\u548c\u5b9e\u9645\u53ef\u884c\u6027\u3002"}}
{"id": "2505.06335", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.06335", "abs": "https://arxiv.org/abs/2505.06335", "authors": ["Jinsheng Yuan", "Yuhang Hao", "Weisi Guo", "Yun Wu", "Chongyan Gu"], "title": "Remote Rowhammer Attack using Adversarial Observations on Federated Learning Clients", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Federated Learning (FL) has the potential for simultaneous global learning\namongst a large number of parallel agents, enabling emerging AI such as LLMs to\nbe trained across demographically diverse data. Central to this being efficient\nis the ability for FL to perform sparse gradient updates and remote direct\nmemory access at the central server. Most of the research in FL security\nfocuses on protecting data privacy at the edge client or in the communication\nchannels between the client and server. Client-facing attacks on the server are\nless well investigated as the assumption is that a large collective of clients\noffer resilience.\n  Here, we show that by attacking certain clients that lead to a high frequency\nrepetitive memory update in the server, we can remote initiate a rowhammer\nattack on the server memory. For the first time, we do not need backdoor access\nto the server, and a reinforcement learning (RL) attacker can learn how to\nmaximize server repetitive memory updates by manipulating the client's sensor\nobservation. The consequence of the remote rowhammer attack is that we are able\nto achieve bit flips, which can corrupt the server memory. We demonstrate the\nfeasibility of our attack using a large-scale FL automatic speech recognition\n(ASR) systems with sparse updates, our adversarial attacking agent can achieve\naround 70\\% repeated update rate (RUR) in the targeted server model,\neffectively inducing bit flips on server DRAM. The security implications are\nthat can cause disruptions to learning or may inadvertently cause elevated\nprivilege. This paves the way for further research on practical mitigation\nstrategies in FL and hardware design.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u901a\u8fc7\u653b\u51fb\u7279\u5b9a\u5ba2\u6237\u7aef\uff0c\u53ef\u4ee5\u8fdc\u7a0b\u53d1\u8d77\u5bf9\u670d\u52a1\u5668\u5185\u5b58\u7684Rowhammer\u653b\u51fb\uff0c\u4ece\u800c\u5bfc\u81f4\u670d\u52a1\u5668\u5185\u5b58\u4f4d\u7ffb\u8f6c\uff0c\u5f71\u54cd\u8054\u90a6\u5b66\u4e60\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524d\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u7684\u5b89\u5168\u6027\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u4fdd\u62a4\u8fb9\u7f18\u5ba2\u6237\u7aef\u7684\u6570\u636e\u9690\u79c1\u6216\u5ba2\u6237\u7aef\u4e0e\u670d\u52a1\u5668\u4e4b\u95f4\u7684\u901a\u4fe1\u6e20\u9053\u5b89\u5168\uff0c\u800c\u9488\u5bf9\u670d\u52a1\u5668\u7aef\u7684\u5ba2\u6237\u7aef\u653b\u51fb\u7814\u7a76\u8f83\u5c11\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u901a\u8fc7\u653b\u51fb\u67d0\u4e9b\u7279\u5b9a\u5ba2\u6237\u7aef\uff0c\u662f\u5426\u53ef\u4ee5\u5728\u670d\u52a1\u5668\u4e0a\u8fdc\u7a0b\u5f15\u53d1Rowhammer\u653b\u51fb\uff0c\u4ece\u800c\u7834\u574f\u670d\u52a1\u5668\u5185\u5b58\u3002", "method": "\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u653b\u51fb\u8005\uff0c\u901a\u8fc7\u64cd\u63a7\u5ba2\u6237\u7aef\u7684\u4f20\u611f\u5668\u89c2\u5bdf\u503c\uff0c\u6700\u5927\u5316\u670d\u52a1\u5668\u7684\u91cd\u590d\u5185\u5b58\u66f4\u65b0\u7387\uff08RUR\uff09\u3002\u5728\u5927\u89c4\u6a21\u8054\u90a6\u5b66\u4e60\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u8be5\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u7a00\u758f\u66f4\u65b0\u6765\u589e\u52a0\u653b\u51fb\u6548\u679c\u3002", "result": "\u5728\u76ee\u6807\u670d\u52a1\u5668\u6a21\u578b\u4e2d\uff0c\u5bf9\u6297\u653b\u51fb\u4ee3\u7406\u80fd\u591f\u8fbe\u5230\u7ea670%\u7684\u91cd\u590d\u66f4\u65b0\u7387\uff08RUR\uff09\uff0c\u6210\u529f\u8bf1\u5bfc\u670d\u52a1\u5668DRAM\u4e0a\u7684\u4f4d\u7ffb\u8f6c\u3002", "conclusion": "\u6b64\u653b\u51fb\u65b9\u5f0f\u53ef\u80fd\u5bfc\u81f4\u5b66\u4e60\u4e2d\u65ad\u6216\u610f\u5916\u63d0\u5347\u6743\u9650\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u548c\u786c\u4ef6\u8bbe\u8ba1\u7684\u5b9e\u9645\u7f13\u89e3\u7b56\u7565\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2505.06284", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.06284", "abs": "https://arxiv.org/abs/2505.06284", "authors": ["Zhiqiang Wang", "Ruoxi Cheng"], "title": "DMRL: Data- and Model-aware Reward Learning for Data Extraction", "categories": ["cs.LG", "cs.CR"], "comment": "Data- and Model-aware Reward Learning for Data Extraction. arXiv\n  admin note: substantial text overlap with arXiv:2503.18991", "summary": "Large language models (LLMs) are inherently vulnerable to unintended privacy\nbreaches. Consequently, systematic red-teaming research is essential for\ndeveloping robust defense mechanisms. However, current data extraction methods\nsuffer from several limitations: (1) rely on dataset duplicates (addressable\nvia deduplication), (2) depend on prompt engineering (now countered by\ndetection and defense), and (3) rely on random-search adversarial generation.\nTo address these challenges, we propose DMRL, a Data- and Model-aware Reward\nLearning approach for data extraction. This technique leverages inverse\nreinforcement learning to extract sensitive data from LLMs. Our method consists\nof two main components: (1) constructing an introspective reasoning dataset\nthat captures leakage mindsets to guide model behavior, and (2) training reward\nmodels with Group Relative Policy Optimization (GRPO), dynamically tuning\noptimization based on task difficulty at both the data and model levels.\nComprehensive experiments across various LLMs demonstrate that DMRL outperforms\nall baseline methods in data extraction performance.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "The paper proposes DMRL, a Data- and Model-aware Reward Learning approach to extract sensitive data from LLMs using inverse reinforcement learning. It outperforms baselines in data extraction.", "motivation": "To address the vulnerabilities of LLMs to unintended privacy breaches and overcome limitations in current data extraction methods.", "method": "DMRL consists of constructing an introspective reasoning dataset capturing leakage mindsets and training reward models with Group Relative Policy Optimization (GRPO), dynamically tuning optimization based on task difficulty at both data and model levels.", "result": "Comprehensive experiments across various LLMs show that DMRL outperforms all baseline methods in data extraction performance.", "conclusion": "DMRL is an effective approach for extracting sensitive data from LLMs."}}
{"id": "2505.04842", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.04842", "abs": "https://arxiv.org/abs/2505.04842", "authors": ["Kusha Sareen", "Morgane M Moss", "Alessandro Sordoni", "Rishabh Agarwal", "Arian Hosseini"], "title": "Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners,\nsuch as GRPO or Leave-one-out PPO, abandon the learned value function in favor\nof empirically estimated returns. This hinders test-time compute scaling that\nrelies on using the value-function for verification. In this work, we propose\nRL$^V$ that augments any ``value-free'' RL method by jointly training the LLM\nas both a reasoner and a generative verifier using RL-generated data, adding\nverification capabilities without significant overhead. Empirically, RL$^V$\nboosts MATH accuracy by over 20\\% with parallel sampling and enables\n$8-32\\times$ efficient test-time compute scaling compared to the base RL\nmethod. RL$^V$ also exhibits strong generalization capabilities for both\neasy-to-hard and out-of-domain tasks. Furthermore, RL$^V$ achieves\n$1.2-1.6\\times$ higher performance when jointly scaling parallel and sequential\ntest-time compute with a long reasoning R1 model.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "RL$^V$ is an augmented reinforcement learning method that adds verification capabilities to value-free RL methods, boosting MATH accuracy and enabling efficient test-time compute scaling.", "motivation": "Prevalent RL methods for fine-tuning LLM reasoners abandon the learned value function in favor of empirically estimated returns, which hinders test-time compute scaling.", "method": "RL$^V$ jointly trains the LLM as both a reasoner and a generative verifier using RL-generated data.", "result": "Empirically, RL$^V$ boosts MATH accuracy by over 20\\% with parallel sampling and enables $8-32\\times$ efficient test-time compute scaling. It also exhibits strong generalization capabilities and achieves $1.2-1.6\\times$ higher performance when jointly scaling parallel and sequential test-time compute.", "conclusion": "RL$^V$ successfully augments value-free RL methods by adding verification capabilities without significant overhead."}}
{"id": "2505.03439", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.03439", "abs": "https://arxiv.org/abs/2505.03439", "authors": ["Artem Karpov", "Tinuade Adeleke", "Seong Hah Cho", "Natalia Perez-Campanero"], "title": "The Steganographic Potentials of Language Models", "categories": ["cs.AI", "cs.CR", "cs.LG"], "comment": "Published at Building Trust Workshop at ICLR 2025", "summary": "The potential for large language models (LLMs) to hide messages within plain\ntext (steganography) poses a challenge to detection and thwarting of unaligned\nAI agents, and undermines faithfulness of LLMs reasoning. We explore the\nsteganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)\nto: (1) develop covert encoding schemes, (2) engage in steganography when\nprompted, and (3) utilize steganography in realistic scenarios where hidden\nreasoning is likely, but not prompted. In these scenarios, we detect the\nintention of LLMs to hide their reasoning as well as their steganography\nperformance. Our findings in the fine-tuning experiments as well as in\nbehavioral non fine-tuning evaluations reveal that while current models exhibit\nrudimentary steganographic abilities in terms of security and capacity,\nexplicit algorithmic guidance markedly enhances their capacity for information\nconcealment.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "Large Language Models (LLMs) can potentially hide messages within texts through steganography, which is a challenge to detection and faithful reasoning. This study explores the steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL). The findings reveal that current models have basic steganographic abilities, but explicit algorithmic guidance can significantly enhance their information concealment capacity.", "motivation": "To understand and address the potential risk of LLMs using steganography to hide messages, which could undermine the faithfulness of their reasoning and pose challenges to detecting unaligned AI agents.", "method": "The research involves fine-tuning LLMs via reinforcement learning to explore three aspects: developing covert encoding schemes, engaging in steganography when prompted, and utilizing steganography in realistic scenarios without being prompted. Evaluations are conducted both in fine-tuning experiments and behavioral non fine-tuning evaluations.", "result": "Current LLMs show basic steganographic abilities in terms of security and capacity. However, with explicit algorithmic guidance, there is a significant improvement in their capacity for information concealment.", "conclusion": "While current LLMs have rudimentary steganographic abilities, they can be greatly enhanced with proper algorithmic guidance. This implies the need for further research and development in ensuring the faithfulness and alignment of LLMs."}}
{"id": "2505.03209", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.03209", "abs": "https://arxiv.org/abs/2505.03209", "authors": ["Borui Wang", "Kathleen McKeown", "Rex Ying"], "title": "DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning from expert demonstrations has long remained a\nchallenging research problem, and existing state-of-the-art methods using\nbehavioral cloning plus further RL training often suffer from poor\ngeneralization, low sample efficiency, and poor model interpretability.\nInspired by the strong reasoning abilities of large language models (LLMs), we\npropose a novel strategy-based reinforcement learning framework integrated with\nLLMs called DYnamic STrategy Induction with Llms for reinforcement learning\n(DYSTIL) to overcome these limitations. DYSTIL dynamically queries a\nstrategy-generating LLM to induce textual strategies based on advantage\nestimations and expert demonstrations, and gradually internalizes induced\nstrategies into the RL agent through policy optimization to improve its\nperformance through boosting policy generalization and enhancing sample\nefficiency. It also provides a direct textual channel to observe and interpret\nthe evolution of the policy's underlying strategies during training. We test\nDYSTIL over challenging RL environments from Minigrid and BabyAI, and\nempirically demonstrate that DYSTIL significantly outperforms state-of-the-art\nbaseline methods by 17.75% in average success rate while also enjoying higher\nsample efficiency during the learning process.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "Reinforcement learning from expert demonstrations is challenging. Current methods suffer poor generalization, low sample efficiency, and poor model interpretability. This paper proposes DYSTIL, a novel framework integrated with LLMs to overcome these limitations. DYSTIL dynamically queries a strategy-generating LLM based on advantage estimations and expert demonstrations, improving policy generalization and sample efficiency while providing interpretability.", "motivation": "The motivation of this paper is to address the challenges in reinforcement learning from expert demonstrations, particularly the issues of poor generalization, low sample efficiency, and poor model interpretability that current state-of-the-art methods face.", "method": "The method proposed in this paper is called DYnamic STrategy Induction with Llms for reinforcement learning (DYSTIL). It involves dynamically querying a strategy-generating LLM to induce textual strategies based on advantage estimations and expert demonstrations. These strategies are then internalized into the RL agent through policy optimization.", "result": "The results show that DYSTIL significantly outperforms state-of-the-art baseline methods by 17.75% in average success rate and also enjoys higher sample efficiency during the learning process. These findings were empirically demonstrated through testing over challenging RL environments from Minigrid and BabyAI.", "conclusion": "The conclusion is that DYSTIL, a novel strategy-based reinforcement learning framework integrated with LLMs, successfully overcomes the limitations of poor generalization, low sample efficiency, and poor model interpretability in reinforcement learning from expert demonstrations."}}
{"id": "2505.03181", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.03181", "abs": "https://arxiv.org/abs/2505.03181", "authors": ["Jake Grigsby", "Yuke Zhu", "Michael Ryoo", "Juan Carlos Niebles"], "title": "VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making", "categories": ["cs.LG"], "comment": "SSI-FM Workshop ICLR 2025", "summary": "Recent research looks to harness the general knowledge and reasoning of large\nlanguage models (LLMs) into agents that accomplish user-specified goals in\ninteractive environments. Vision-language models (VLMs) extend LLMs to\nmulti-modal data and provide agents with the visual reasoning necessary for new\napplications in areas such as computer automation. However, agent tasks\nemphasize skills where accessible open-weight VLMs lag behind their LLM\nequivalents. For example, VLMs are less capable of following an environment's\nstrict output syntax requirements and are more focused on open-ended question\nanswering. Overcoming these limitations requires supervised fine-tuning (SFT)\non task-specific expert demonstrations. Our work approaches these challenges\nfrom an offline-to-online reinforcement learning (RL) perspective. RL lets us\nfine-tune VLMs to agent tasks while learning from the unsuccessful decisions of\nour own model or more capable (larger) models. We explore an off-policy RL\nsolution that retains the stability and simplicity of the widely used SFT\nworkflow while allowing our agent to self-improve and learn from low-quality\ndatasets. We demonstrate this technique with two open-weight VLMs across three\nmulti-modal agent domains.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "This paper explores an off-policy reinforcement learning solution to fine-tune vision-language models (VLMs) for agent tasks, enabling them to learn from unsuccessful decisions and low-quality datasets.", "motivation": "Agent tasks require skills where open-weight VLMs lag behind LLMs, such as following strict output syntax requirements. Supervised fine-tuning on task-specific expert demonstrations is required to overcome these limitations.", "method": "The work approaches the challenges of enhancing VLM capabilities for agent tasks from an offline-to-online reinforcement learning perspective, allowing the model to learn from its own unsuccessful decisions or those of more capable models.", "result": "The technique was demonstrated using two open-weight VLMs across three multi-modal agent domains, showing potential for agents to self-improve and learn effectively from low-quality datasets.", "conclusion": "An off-policy RL solution retains the stability and simplicity of supervised fine-tuning while enabling VLMs used in agents to improve through learning from a broader range of experiences."}}
{"id": "2505.02722", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.02722", "abs": "https://arxiv.org/abs/2505.02722", "authors": ["Junu Kim", "Chaeeun Shim", "Sungjin Park", "Su Yeon Lee", "Gee Young Suh", "Chae-Man Lim", "Seong Jin Choi", "Song Mi Moon", "Kyoung-Ho Song", "Eu Suk Kim", "Hong Bin Kim", "Sejoong Kim", "Chami Im", "Dong-Wan Kang", "Yong Soo Kim", "Hee-Joon Bae", "Sung Yoon Lim", "Han-Gil Jeong", "Edward Choi"], "title": "Enhancing LLMs' Clinical Reasoning with Real-World Data from a Nationwide Sepsis Registry", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Although large language models (LLMs) have demonstrated impressive reasoning\ncapabilities across general domains, their effectiveness in real-world clinical\npractice remains limited. This is likely due to their insufficient exposure to\nreal-world clinical data during training, as such data is typically not\nincluded due to privacy concerns. To address this, we propose enhancing the\nclinical reasoning capabilities of LLMs by leveraging real-world clinical data.\nWe constructed reasoning-intensive questions from a nationwide sepsis registry\nand fine-tuned Phi-4 on these questions using reinforcement learning, resulting\nin C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the\nin-domain test set, as evidenced by both quantitative metrics and expert\nevaluations. Furthermore, its enhanced reasoning capabilities generalized to a\nsepsis dataset involving different tasks and patient cohorts, an open-ended\nconsultations on antibiotics use task, and other diseases. Future research\nshould focus on training LLMs with large-scale, multi-disease clinical datasets\nto develop more powerful, general-purpose clinical reasoning models.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "Large Language Models (LLMs) show great potential but lack effectiveness in clinical practice due to limited exposure to real-world clinical data. This study proposes enhancing LLMs' clinical reasoning through real-world clinical data, specifically by constructing reasoning-intensive questions from a sepsis registry and fine-tuning Phi-4 using reinforcement learning, leading to the creation of C-Reason. C-Reason demonstrates strong clinical reasoning capabilities and generalizes well across different datasets and tasks.", "motivation": "To improve the clinical reasoning capabilities of LLMs by addressing their insufficient exposure to real-world clinical data during training.", "method": "Constructed reasoning-intensive questions from a nationwide sepsis registry and fine-tuned Phi-4 on these questions using reinforcement learning, resulting in C-Reason.", "result": "C-Reason exhibited strong clinical reasoning capabilities not only on the in-domain test set but also generalized to other datasets involving different tasks and patient cohorts.", "conclusion": "Future research should focus on training LLMs with large-scale, multi-disease clinical datasets to develop more powerful, general-purpose clinical reasoning models."}}
