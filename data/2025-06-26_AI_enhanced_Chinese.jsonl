{"id": "2506.19939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19939", "abs": "https://arxiv.org/abs/2506.19939", "authors": ["Aryan Singh Dalal", "Sidharth Rai", "Rahul Singh", "Treman Singh Kaloya", "Rahul Harsha Cheppally", "Ajay Sharda"], "title": "Computer Vision based Automated Quantification of Agricultural Sprayers Boom Displacement", "comment": "Under publication process for COMPAG", "summary": "Application rate errors when using self-propelled agricultural sprayers for\nagricultural production remain a concern. Among other factors, spray boom\ninstability is one of the major contributors to application errors. Spray\nbooms' width of 38m, combined with 30 kph driving speeds, varying terrain, and\nmachine dynamics when maneuvering complex field boundaries, make controls of\nthese booms very complex. However, there is no quantitative knowledge on the\nextent of boom movement to systematically develop a solution that might include\nboom designs and responsive boom control systems. Therefore, this study was\nconducted to develop an automated computer vision system to quantify the boom\nmovement of various agricultural sprayers. A computer vision system was\ndeveloped to track a target on the edge of the sprayer boom in real time. YOLO\nV7, V8, and V11 neural network models were trained to track the boom's\nmovements in field operations to quantify effective displacement in the\nvertical and transverse directions. An inclinometer sensor was mounted on the\nboom to capture boom angles and validate the neural network model output. The\nresults showed that the model could detect the target with more than 90 percent\naccuracy, and distance estimates of the target on the boom were within 0.026 m\nof the inclinometer sensor data. This system can quantify the boom movement on\nthe current sprayer and potentially on any other sprayer with minor\nmodifications. The data can be used to make design improvements to make sprayer\nbooms more stable and achieve greater application accuracy.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u91cf\u5316\u519c\u4e1a\u55b7\u96fe\u5668\u55b7\u6746\u7684\u8fd0\u52a8\uff0c\u4ee5\u63d0\u9ad8\u55b7\u96fe\u5e94\u7528\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u55b7\u96fe\u6746\u7684\u4e0d\u7a33\u5b9a\u6027\u662f\u519c\u4e1a\u55b7\u96fe\u5668\u5e94\u7528\u4e2d\u8bef\u5dee\u7684\u4e3b\u8981\u6765\u6e90\u4e4b\u4e00\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5b9a\u91cf\u6570\u636e\u6765\u6539\u8fdb\u8bbe\u8ba1\u548c\u63a7\u5236\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528YOLO V7\u3001V8\u548cV11\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5b9e\u65f6\u8ddf\u8e2a\u55b7\u6746\u4e0a\u7684\u76ee\u6807\uff0c\u5e76\u7ed3\u5408\u503e\u89d2\u4f20\u611f\u5668\u9a8c\u8bc1\u6a21\u578b\u8f93\u51fa\u3002", "result": "\u6a21\u578b\u68c0\u6d4b\u76ee\u6807\u7684\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u8ddd\u79bb\u4f30\u8ba1\u4e0e\u4f20\u611f\u5668\u6570\u636e\u8bef\u5dee\u57280.026\u7c73\u4ee5\u5185\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u6709\u6548\u91cf\u5316\u55b7\u6746\u8fd0\u52a8\uff0c\u4e3a\u6539\u8fdb\u55b7\u96fe\u5668\u8bbe\u8ba1\u548c\u63d0\u9ad8\u5e94\u7528\u7cbe\u5ea6\u63d0\u4f9b\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2506.19955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19955", "abs": "https://arxiv.org/abs/2506.19955", "authors": ["Yiming Ma", "Victor Sanchez", "Tanaya Guha"], "title": "EBC-ZIP: Improving Blockwise Crowd Counting with Zero-Inflated Poisson Regression", "comment": null, "summary": "Density map estimation has become the mainstream paradigm in crowd counting.\nHowever, most existing methods overlook the extreme sparsity of ground-truth\ndensity maps. In real-world crowd scenes, the vast majority of spatial regions\n(often over 95%) contain no people, leading to heavily imbalanced count\ndistributions. Ignoring this imbalance can bias models toward overestimating\ndense regions and underperforming in sparse areas. Furthermore, most loss\nfunctions used in density estimation are majorly based on MSE and implicitly\nassume Gaussian distributions, which are ill-suited for modeling discrete,\nnon-negative count data. In this paper, we propose EBC-ZIP, a crowd counting\nframework that models the spatial distribution of counts using a Zero-Inflated\nPoisson (ZIP) regression formulation. Our approach replaces the traditional\nregression loss with the negative log-likelihood of the ZIP distribution,\nenabling better handling of zero-heavy distributions while preserving count\naccuracy. Built upon the recently proposed Enhanced Block Classification (EBC)\nframework, EBC-ZIP inherits EBC's advantages in preserving the discreteness of\ntargets and ensuring training stability, while further improving performance\nthrough a more principled probabilistic loss. We also evaluate EBC-ZIP with\nbackbones of varying computational complexity to assess its scalability.\nExtensive experiments on four crowd counting benchmarks demonstrate that\nEBC-ZIP consistently outperforms EBC and achieves state-of-the-art results.", "AI": {"tldr": "EBC-ZIP\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96f6\u81a8\u80c0\u6cca\u677e\u56de\u5f52\u7684\u4eba\u7fa4\u8ba1\u6570\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u5bc6\u5ea6\u56fe\u7a00\u758f\u6027\u53ca\u635f\u5931\u51fd\u6570\u4e0d\u9002\u5408\u79bb\u6563\u8ba1\u6570\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u5730\u9762\u771f\u5b9e\u5bc6\u5ea6\u56fe\u7684\u6781\u7aef\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u7a00\u758f\u533a\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u4f20\u7edf\u635f\u5931\u51fd\u6570\u5047\u8bbe\u9ad8\u65af\u5206\u5e03\uff0c\u4e0d\u9002\u5408\u79bb\u6563\u8ba1\u6570\u6570\u636e\u3002", "method": "\u63d0\u51faEBC-ZIP\u6846\u67b6\uff0c\u91c7\u7528\u96f6\u81a8\u80c0\u6cca\u677e\u56de\u5f52\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u589e\u5f3a\u5757\u5206\u7c7b\u6846\u67b6\u7684\u4f18\u52bf\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEBC-ZIP\u8868\u73b0\u4f18\u4e8eEBC\uff0c\u5e76\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "EBC-ZIP\u901a\u8fc7\u66f4\u5408\u7406\u7684\u6982\u7387\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u7fa4\u8ba1\u6570\u7684\u6027\u80fd\u3002"}}
{"id": "2506.20066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20066", "abs": "https://arxiv.org/abs/2506.20066", "authors": ["Hsiang-Wei Huang", "Wenhao Chai", "Kuang-Ming Chen", "Cheng-Yen Yang", "Jenq-Neng Hwang"], "title": "ToSA: Token Merging with Spatial Awareness", "comment": "Accepted by IROS 2025", "summary": "Token merging has emerged as an effective strategy to accelerate Vision\nTransformers (ViT) by reducing computational costs. However, existing methods\nprimarily rely on the visual token's feature similarity for token merging,\noverlooking the potential of integrating spatial information, which can serve\nas a reliable criterion for token merging in the early layers of ViT, where the\nvisual tokens only possess weak visual information. In this paper, we propose\nToSA, a novel token merging method that combines both semantic and spatial\nawareness to guide the token merging process. ToSA leverages the depth image as\ninput to generate pseudo spatial tokens, which serve as auxiliary spatial\ninformation for the visual token merging process. With the introduced spatial\nawareness, ToSA achieves a more informed merging strategy that better preserves\ncritical scene structure. Experimental results demonstrate that ToSA\noutperforms previous token merging methods across multiple benchmarks on visual\nand embodied question answering while largely reducing the runtime of the ViT,\nmaking it an efficient solution for ViT acceleration. The code will be\navailable at: https://github.com/hsiangwei0903/ToSA", "AI": {"tldr": "ToSA\u662f\u4e00\u79cd\u65b0\u7684\u4ee4\u724c\u5408\u5e76\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bed\u4e49\u548c\u7a7a\u95f4\u4fe1\u606f\uff0c\u901a\u8fc7\u6df1\u5ea6\u56fe\u50cf\u751f\u6210\u4f2a\u7a7a\u95f4\u4ee4\u724c\uff0c\u4f18\u5316ViT\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u89c6\u89c9\u4ee4\u724c\u7684\u7279\u5f81\u76f8\u4f3c\u6027\u8fdb\u884c\u5408\u5e76\uff0c\u5ffd\u7565\u4e86\u65e9\u671f\u5c42\u4e2d\u7a7a\u95f4\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u56fe\u50cf\u751f\u6210\u4f2a\u7a7a\u95f4\u4ee4\u724c\uff0c\u7ed3\u5408\u8bed\u4e49\u548c\u7a7a\u95f4\u4fe1\u606f\u6307\u5bfc\u4ee4\u724c\u5408\u5e76\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11ViT\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "ToSA\u662f\u4e00\u79cd\u9ad8\u6548\u7684ViT\u52a0\u901f\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u573a\u666f\u7ed3\u6784\u3002"}}
{"id": "2506.20202", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2506.20202", "abs": "https://arxiv.org/abs/2506.20202", "authors": ["Da Li", "Donggang Jia", "Yousef Rajeh", "Dominik Engel", "Ivan Viola"], "title": "RaRa Clipper: A Clipper for Gaussian Splatting Based on Ray Tracer and Rasterizer", "comment": null, "summary": "With the advancement of Gaussian Splatting techniques, a growing number of\ndatasets based on this representation have been developed. However, performing\naccurate and efficient clipping for Gaussian Splatting remains a challenging\nand unresolved problem, primarily due to the volumetric nature of Gaussian\nprimitives, which makes hard clipping incapable of precisely localizing their\npixel-level contributions. In this paper, we propose a hybrid rendering\nframework that combines rasterization and ray tracing to achieve efficient and\nhigh-fidelity clipping of Gaussian Splatting data. At the core of our method is\nthe RaRa strategy, which first leverages rasterization to quickly identify\nGaussians intersected by the clipping plane, followed by ray tracing to compute\nattenuation weights based on their partial occlusion. These weights are then\nused to accurately estimate each Gaussian's contribution to the final image,\nenabling smooth and continuous clipping effects. We validate our approach on\ndiverse datasets, including general Gaussians, hair strand Gaussians, and\nmulti-layer Gaussians, and conduct user studies to evaluate both perceptual\nquality and quantitative performance. Experimental results demonstrate that our\nmethod delivers visually superior results while maintaining real-time rendering\nperformance and preserving high fidelity in the unclipped regions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5149\u6805\u5316\u548c\u5149\u7ebf\u8ffd\u8e2a\u7684\u6df7\u5408\u6e32\u67d3\u6846\u67b6\uff08RaRa\u7b56\u7565\uff09\uff0c\u7528\u4e8e\u9ad8\u6548\u4e14\u9ad8\u4fdd\u771f\u5730\u5b9e\u73b0\u9ad8\u65af\u6cfc\u6e85\u6570\u636e\u7684\u88c1\u526a\u3002", "motivation": "\u9ad8\u65af\u6cfc\u6e85\u6570\u636e\u7684\u7cbe\u786e\u548c\u9ad8\u6548\u88c1\u526a\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u9ad8\u65af\u57fa\u5143\u7684\u4f53\u79ef\u7279\u6027\u4f7f\u5f97\u786c\u88c1\u526a\u96be\u4ee5\u7cbe\u786e\u5b9a\u4f4d\u5176\u50cf\u7d20\u7ea7\u8d21\u732e\u3002", "method": "\u91c7\u7528RaRa\u7b56\u7565\uff0c\u5148\u901a\u8fc7\u5149\u6805\u5316\u5feb\u901f\u8bc6\u522b\u88ab\u88c1\u526a\u5e73\u9762\u76f8\u4ea4\u7684\u9ad8\u65af\u57fa\u5143\uff0c\u518d\u901a\u8fc7\u5149\u7ebf\u8ffd\u8e2a\u8ba1\u7b97\u57fa\u4e8e\u90e8\u5206\u906e\u6321\u7684\u8870\u51cf\u6743\u91cd\uff0c\u4ee5\u51c6\u786e\u4f30\u8ba1\u6bcf\u4e2a\u9ad8\u65af\u57fa\u5143\u5bf9\u6700\u7ec8\u56fe\u50cf\u7684\u8d21\u732e\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\uff08\u5305\u62ec\u666e\u901a\u9ad8\u65af\u3001\u53d1\u4e1d\u9ad8\u65af\u548c\u591a\u5c42\u9ad8\u65af\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7528\u6237\u7814\u7a76\u8868\u660e\u5176\u5177\u6709\u4f18\u8d8a\u7684\u611f\u77e5\u8d28\u91cf\u548c\u5b9a\u91cf\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5b9e\u65f6\u6e32\u67d3\u6027\u80fd\u548c\u9ad8\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9\u4e0a\u66f4\u4f18\u7684\u88c1\u526a\u6548\u679c\u3002"}}
{"id": "2506.20103", "categories": ["cs.CV", "cs.AI", "I.4"], "pdf": "https://arxiv.org/pdf/2506.20103", "abs": "https://arxiv.org/abs/2506.20103", "authors": ["Jiahao Lin", "Weixuan Peng", "Bojia Zi", "Yifeng Gao", "Xianbiao Qi", "Xingjun Ma", "Yu-Gang Jiang"], "title": "BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos", "comment": "7 page,4 figures,2 tables", "summary": "Recent advances in deep generative models have led to significant progress in\nvideo generation, yet the fidelity of AI-generated videos remains limited.\nSynthesized content often exhibits visual artifacts such as temporally\ninconsistent motion, physically implausible trajectories, unnatural object\ndeformations, and local blurring that undermine realism and user trust.\nAccurate detection and spatial localization of these artifacts are crucial for\nboth automated quality control and for guiding the development of improved\ngenerative models. However, the research community currently lacks a\ncomprehensive benchmark specifically designed for artifact localization in AI\ngenerated videos. Existing datasets either restrict themselves to video or\nframe level detection or lack the fine-grained spatial annotations necessary\nfor evaluating localization methods. To address this gap, we introduce\nBrokenVideos, a benchmark dataset of 3,254 AI-generated videos with\nmeticulously annotated, pixel-level masks highlighting regions of visual\ncorruption. Each annotation is validated through detailed human inspection to\nensure high quality ground truth. Our experiments show that training state of\nthe art artifact detection models and multi modal large language models (MLLMs)\non BrokenVideos significantly improves their ability to localize corrupted\nregions. Through extensive evaluation, we demonstrate that BrokenVideos\nestablishes a critical foundation for benchmarking and advancing research on\nartifact localization in generative video models. The dataset is available at:\nhttps://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86BrokenVideos\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5b9a\u4f4dAI\u751f\u6210\u89c6\u9891\u4e2d\u7684\u89c6\u89c9\u4f2a\u5f71\uff0c\u63d0\u5347\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "AI\u751f\u6210\u89c6\u9891\u4e2d\u5b58\u5728\u89c6\u89c9\u4f2a\u5f71\uff0c\u5f71\u54cd\u771f\u5b9e\u6027\u548c\u7528\u6237\u4fe1\u4efb\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u7528\u4e8e\u4f2a\u5f71\u5b9a\u4f4d\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faBrokenVideos\u6570\u636e\u96c6\uff0c\u5305\u542b3,254\u4e2aAI\u751f\u6210\u89c6\u9891\uff0c\u5e26\u6709\u50cf\u7d20\u7ea7\u6807\u6ce8\u7684\u4f2a\u5f71\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528BrokenVideos\u8bad\u7ec3\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u4f2a\u5f71\u5b9a\u4f4d\u80fd\u529b\u3002", "conclusion": "BrokenVideos\u4e3a\u751f\u6210\u89c6\u9891\u6a21\u578b\u7684\u4f2a\u5f71\u5b9a\u4f4d\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2506.20267", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20267", "abs": "https://arxiv.org/abs/2506.20267", "authors": ["Fabian Bongratz", "Tom Nuno Wolf", "Jaume Gual Ramon", "Christian Wachinger"], "title": "X-SiT: Inherently Interpretable Surface Vision Transformers for Dementia Diagnosis", "comment": "MICCAI 2025", "summary": "Interpretable models are crucial for supporting clinical decision-making,\ndriving advances in their development and application for medical images.\nHowever, the nature of 3D volumetric data makes it inherently challenging to\nvisualize and interpret intricate and complex structures like the cerebral\ncortex. Cortical surface renderings, on the other hand, provide a more\naccessible and understandable 3D representation of brain anatomy, facilitating\nvisualization and interactive exploration. Motivated by this advantage and the\nwidespread use of surface data for studying neurological disorders, we present\nthe eXplainable Surface Vision Transformer (X-SiT). This is the first\ninherently interpretable neural network that offers human-understandable\npredictions based on interpretable cortical features. As part of X-SiT, we\nintroduce a prototypical surface patch decoder for classifying surface patch\nembeddings, incorporating case-based reasoning with spatially corresponding\ncortical prototypes. The results demonstrate state-of-the-art performance in\ndetecting Alzheimer's disease and frontotemporal dementia while additionally\nproviding informative prototypes that align with known disease patterns and\nreveal classification errors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u8868\u9762\u89c6\u89c9\u53d8\u6362\u5668\uff08X-SiT\uff09\uff0c\u7528\u4e8e\u57fa\u4e8e\u53ef\u89e3\u91ca\u7684\u76ae\u5c42\u7279\u5f81\u8fdb\u884c\u533b\u5b66\u56fe\u50cf\u5206\u6790\uff0c\u5e76\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u548c\u989d\u989e\u53f6\u75f4\u5446\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "3D\u4f53\u79ef\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u96be\u4ee5\u53ef\u89c6\u5316\u7279\u6027\u4fc3\u4f7f\u7814\u7a76\u8005\u5f00\u53d1\u66f4\u6613\u7406\u89e3\u7684\u76ae\u5c42\u8868\u9762\u6e32\u67d3\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u63d0\u51faX-SiT\uff0c\u4e00\u79cd\u57fa\u4e8e\u539f\u578b\u8868\u9762\u5757\u89e3\u7801\u5668\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u6848\u4f8b\u63a8\u7406\u548c\u7a7a\u95f4\u5bf9\u5e94\u7684\u76ae\u5c42\u539f\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "X-SiT\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u548c\u989d\u989e\u53f6\u75f4\u5446\u68c0\u6d4b\u4e2d\u8fbe\u5230\u6700\u65b0\u6280\u672f\u6c34\u5e73\uff0c\u5e76\u63d0\u4f9b\u4e0e\u5df2\u77e5\u75be\u75c5\u6a21\u5f0f\u4e00\u81f4\u7684\u539f\u578b\u3002", "conclusion": "X-SiT\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\uff0c\u8fd8\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\uff0c\u6709\u52a9\u4e8e\u4e34\u5e8a\u51b3\u7b56\u548c\u75be\u75c5\u7814\u7a76\u3002"}}
{"id": "2506.19923", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19923", "abs": "https://arxiv.org/abs/2506.19923", "authors": ["Kaito Baba", "Chaoran Liu", "Shuhei Kurita", "Akiyoshi Sannai"], "title": "Prover Agent: An Agent-based Framework for Formal Mathematical Proofs", "comment": "22 pages, 2 figures", "summary": "We present Prover Agent, a novel AI agent for automated theorem proving that\nintegrates large language models (LLMs) with a formal proof assistant, Lean.\nProver Agent coordinates an informal reasoning LLM, a formal prover model, and\nfeedback from Lean while also generating auxiliary lemmas to assist in\ndiscovering the overall proof strategy. It achieves an 86.1% success rate on\nthe MiniF2F benchmark, establishing a new state-of-the-art among methods using\nsmall language models (SLMs) with a much lower sample budget than previous\napproaches. We also present case studies illustrating how these generated\nlemmas contribute to solving challenging problems.", "AI": {"tldr": "Prover Agent\u662f\u4e00\u79cd\u65b0\u578bAI\u4ee3\u7406\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u5f62\u5f0f\u5316\u8bc1\u660e\u52a9\u624bLean\uff0c\u5b9e\u73b0\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\uff0c\u6210\u529f\u7387\u8fbe86.1%\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u4e2dLLMs\u4e0e\u5f62\u5f0f\u5316\u5de5\u5177\u7684\u534f\u540c\u95ee\u9898\uff0c\u63d0\u9ad8\u8bc1\u660e\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "method": "\u6574\u5408\u975e\u6b63\u5f0f\u63a8\u7406LLM\u3001\u5f62\u5f0f\u5316\u8bc1\u660e\u6a21\u578b\u548cLean\u53cd\u9988\uff0c\u751f\u6210\u8f85\u52a9\u5f15\u7406\u4ee5\u53d1\u73b0\u6574\u4f53\u8bc1\u660e\u7b56\u7565\u3002", "result": "\u5728MiniF2F\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523086.1%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u5176\u4ed6\u5c0f\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u3002", "conclusion": "Prover Agent\u901a\u8fc7\u534f\u540cLLMs\u548cLean\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u7684\u6027\u80fd\u3002"}}
{"id": "2506.19952", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19952", "abs": "https://arxiv.org/abs/2506.19952", "authors": ["Deepon Halder", "Thanmay Jayakumar", "Raj Dabre"], "title": "CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation", "comment": null, "summary": "Large language models (LLMs), despite their ability to perform few-shot\nmachine translation (MT), often lag behind dedicated MT systems trained on\nparallel corpora, which are crucial for high quality machine translation (MT).\nHowever, parallel corpora are often scarce or non-existent for low-resource\nlanguages. In this paper, we propose CycleDistill, a bootstrapping approach\nleveraging LLMs and few-shot translation to obtain high-quality MT systems.\nCycleDistill involves iteratively generating synthetic parallel corpora from\nmonolingual corpora via zero- or few-shot MT, which is then used to fine-tune\nthe model that was used for generating said data for MT. CycleDistill does not\nneed parallel corpora beyond 1 to 4 few-shot examples, and in our experiments\nfocusing on three Indian languages, by relying solely on monolingual corpora,\nit can achieve high-quality machine translation, improving upon a few-shot\nbaseline model by over 20-30 chrF points on average in the first iteration. We\nalso study the effect of leveraging softmax activations during the distillation\nprocess and observe mild improvements in translation quality.", "AI": {"tldr": "CycleDistill\u5229\u7528LLMs\u548c\u5c11\u6837\u672c\u7ffb\u8bd1\u751f\u6210\u5408\u6210\u5e73\u884c\u8bed\u6599\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u5e73\u884c\u8bed\u6599\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u96f6\u6837\u672c\u6216\u5c11\u6837\u672c\u7ffb\u8bd1\u751f\u6210\u5408\u6210\u5e73\u884c\u8bed\u6599\uff0c\u8fed\u4ee3\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5728\u4e09\u79cd\u5370\u5ea6\u8bed\u8a00\u4e0a\uff0c\u9996\u8f6e\u8fed\u4ee3\u5e73\u5747\u63d0\u534720-30 chrF\u70b9\u3002", "conclusion": "CycleDistill\u65e0\u9700\u5927\u91cf\u5e73\u884c\u8bed\u6599\u5373\u53ef\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u3002"}}
{"id": "2506.20134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20134", "abs": "https://arxiv.org/abs/2506.20134", "authors": ["Ningwei Xie", "Zizi Tian", "Lei Yang", "Xiao-Ping Zhang", "Meng Guo", "Jie Li"], "title": "From 2D to 3D Cognition: A Brief Survey of General World Models", "comment": null, "summary": "World models have garnered increasing attention in the development of\nartificial general intelligence (AGI), serving as computational frameworks for\nlearning representations of the external world and forecasting future states.\nWhile early efforts focused on 2D visual perception and simulation, recent\n3D-aware generative world models have demonstrated the ability to synthesize\ngeometrically consistent, interactive 3D environments, marking a shift toward\n3D spatial cognition. Despite rapid progress, the field lacks systematic\nanalysis to categorize emerging techniques and clarify their roles in advancing\n3D cognitive world models. This survey addresses this need by introducing a\nconceptual framework, providing a structured and forward-looking review of\nworld models transitioning from 2D perception to 3D cognition. Within this\nframework, we highlight two key technological drivers, particularly advances in\n3D representations and the incorporation of world knowledge, as fundamental\npillars. Building on these, we dissect three core cognitive capabilities that\nunderpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning,\nand 3D spatial interaction. We further examine the deployment of these\ncapabilities in real-world applications, including embodied AI, autonomous\ndriving, digital twin, and gaming/VR. Finally, we identify challenges across\ndata, modeling, and deployment, and outline future directions for advancing\nmore robust and generalizable 3D world models.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4ece2D\u611f\u77e5\u52303D\u8ba4\u77e5\u7684\u4e16\u754c\u6a21\u578b\u53d1\u5c55\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u6846\u67b6\uff0c\u5e76\u5206\u6790\u4e863D\u8868\u793a\u548c\u4e16\u754c\u77e5\u8bc6\u4e24\u5927\u6280\u672f\u9a71\u52a8\u529b\uff0c\u4ee5\u53ca3D\u4e16\u754c\u5efa\u6a21\u7684\u4e09\u5927\u6838\u5fc3\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u9886\u57df\u7f3a\u4e4f\u5bf93D\u8ba4\u77e5\u4e16\u754c\u6a21\u578b\u65b0\u5174\u6280\u672f\u7684\u7cfb\u7edf\u5206\u7c7b\u548c\u5206\u6790\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u6982\u5ff5\u6846\u67b6\uff0c\u7cfb\u7edf\u56de\u987e\u4e86\u4ece2D\u52303D\u7684\u4e16\u754c\u6a21\u578b\u6280\u672f\uff0c\u91cd\u70b9\u5206\u6790\u4e863D\u8868\u793a\u548c\u4e16\u754c\u77e5\u8bc6\u7684\u4f5c\u7528\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e09\u5927\u6838\u5fc3\u80fd\u529b\u3002", "result": "\u603b\u7ed3\u4e863D\u4e16\u754c\u6a21\u578b\u5728\u7269\u7406\u573a\u666f\u751f\u6210\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u4ea4\u4e92\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5728\u5177\u4f53\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u3002", "conclusion": "\u6307\u51fa\u4e86\u6570\u636e\u3001\u5efa\u6a21\u548c\u90e8\u7f72\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u63a8\u52a8\u66f4\u9c81\u68d2\u548c\u901a\u7528\u76843D\u4e16\u754c\u6a21\u578b\u53d1\u5c55\u3002"}}
{"id": "2506.20367", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20367", "abs": "https://arxiv.org/abs/2506.20367", "authors": ["Edoardo Alberto Dominici", "Jozef Hladky", "Floor Verhoeven", "Lukas Radl", "Thomas Deixelberger", "Stefan Ainetter", "Philipp Drescher", "Stefan Hauswiesner", "Arno Coomans", "Giacomo Nazzaro", "Konstantinos Vardis", "Markus Steinberger"], "title": "DreamAnywhere: Object-Centric Panoramic 3D Scene Generation", "comment": null, "summary": "Recent advances in text-to-3D scene generation have demonstrated significant\npotential to transform content creation across multiple industries. Although\nthe research community has made impressive progress in addressing the\nchallenges of this complex task, existing methods often generate environments\nthat are only front-facing, lack visual fidelity, exhibit limited scene\nunderstanding, and are typically fine-tuned for either indoor or outdoor\nsettings. In this work, we address these issues and propose DreamAnywhere, a\nmodular system for the fast generation and prototyping of 3D scenes. Our system\nsynthesizes a 360{\\deg} panoramic image from text, decomposes it into\nbackground and objects, constructs a complete 3D representation through hybrid\ninpainting, and lifts object masks to detailed 3D objects that are placed in\nthe virtual environment. DreamAnywhere supports immersive navigation and\nintuitive object-level editing, making it ideal for scene exploration, visual\nmock-ups, and rapid prototyping -- all with minimal manual modeling. These\nfeatures make our system particularly suitable for low-budget movie production,\nenabling quick iteration on scene layout and visual tone without the overhead\nof traditional 3D workflows. Our modular pipeline is highly customizable as it\nallows components to be replaced independently. Compared to current\nstate-of-the-art text and image-based 3D scene generation approaches,\nDreamAnywhere shows significant improvements in coherence in novel view\nsynthesis and achieves competitive image quality, demonstrating its\neffectiveness across diverse and challenging scenarios. A comprehensive user\nstudy demonstrates a clear preference for our method over existing approaches,\nvalidating both its technical robustness and practical usefulness.", "AI": {"tldr": "DreamAnywhere\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u5feb\u901f\u751f\u6210\u548c\u539f\u578b\u53163D\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u3001\u573a\u666f\u7406\u89e3\u548c\u591a\u73af\u5883\u9002\u5e94\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u52303D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u4ec5\u9762\u5411\u6b63\u9762\u89c6\u89d2\uff0c\u89c6\u89c9\u4fdd\u771f\u5ea6\u4f4e\uff0c\u573a\u666f\u7406\u89e3\u6709\u9650\uff0c\u4e14\u4ec5\u9002\u7528\u4e8e\u5ba4\u5185\u6216\u5ba4\u5916\u73af\u5883\u3002DreamAnywhere\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u4ece\u6587\u672c\u751f\u6210360\u5ea6\u5168\u666f\u56fe\u50cf\uff0c\u5206\u89e3\u4e3a\u80cc\u666f\u548c\u5bf9\u8c61\uff0c\u901a\u8fc7\u6df7\u5408\u4fee\u590d\u6784\u5efa\u5b8c\u65743D\u8868\u793a\uff0c\u5e76\u5c06\u5bf9\u8c61\u63a9\u7801\u63d0\u5347\u4e3a\u8be6\u7ec63D\u5bf9\u8c61\u3002\u652f\u6301\u6c89\u6d78\u5f0f\u5bfc\u822a\u548c\u5bf9\u8c61\u7ea7\u7f16\u8f91\u3002", "result": "DreamAnywhere\u5728\u65b0\u89c6\u89d2\u5408\u6210\u4e00\u81f4\u6027\u548c\u56fe\u50cf\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u5176\u6280\u672f\u7a33\u5065\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "DreamAnywhere\u9002\u7528\u4e8e\u4f4e\u6210\u672c\u7535\u5f71\u5236\u4f5c\uff0c\u652f\u6301\u5feb\u901f\u573a\u666f\u5e03\u5c40\u548c\u89c6\u89c9\u8272\u8c03\u8fed\u4ee3\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u4f7f\u5176\u9ad8\u5ea6\u53ef\u5b9a\u5236\u3002"}}
{"id": "2506.19977", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19977", "abs": "https://arxiv.org/abs/2506.19977", "authors": ["Deng Pan", "Keerthiram Murugesan", "Nuno Moniz", "Nitesh Chawla"], "title": "Context Attribution with Multi-Armed Bandit Optimization", "comment": null, "summary": "Understanding which parts of the retrieved context contribute to a large\nlanguage model's generated answer is essential for building interpretable and\ntrustworthy generative QA systems. We propose a novel framework that formulates\ncontext attribution as a combinatorial multi-armed bandit (CMAB) problem. Each\ncontext segment is treated as a bandit arm, and we employ Combinatorial\nThompson Sampling (CTS) to efficiently explore the exponentially large space of\ncontext subsets under a limited query budget. Our method defines a reward\nfunction based on normalized token likelihoods, capturing how well a subset of\nsegments supports the original model response. Unlike traditional\nperturbation-based attribution methods such as SHAP, which sample subsets\nuniformly and incur high computational costs, our approach adaptively balances\nexploration and exploitation by leveraging posterior estimates of segment\nrelevance. This leads to substantially improved query efficiency while\nmaintaining high attribution fidelity. Extensive experiments on diverse\ndatasets and LLMs demonstrate that our method achieves competitive attribution\nquality with fewer model queries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\uff08CMAB\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u8bc6\u522b\u68c0\u7d22\u4e0a\u4e0b\u6587\u4e2d\u5bf9\u751f\u6210\u7b54\u6848\u8d21\u732e\u6700\u5927\u7684\u90e8\u5206\uff0c\u63d0\u5347\u751f\u6210\u5f0fQA\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u7406\u89e3\u68c0\u7d22\u4e0a\u4e0b\u6587\u4e2d\u54ea\u4e9b\u90e8\u5206\u5bf9\u751f\u6210\u7b54\u6848\u6709\u8d21\u732e\uff0c\u5bf9\u6784\u5efa\u53ef\u89e3\u91ca\u4e14\u53ef\u4fe1\u7684\u751f\u6210\u5f0fQA\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5c06\u4e0a\u4e0b\u6587\u5f52\u56e0\u95ee\u9898\u5efa\u6a21\u4e3a\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u4f7f\u7528\u7ec4\u5408\u6c64\u666e\u68ee\u91c7\u6837\uff08CTS\uff09\u5728\u6709\u9650\u67e5\u8be2\u9884\u7b97\u4e0b\u9ad8\u6548\u63a2\u7d22\u4e0a\u4e0b\u6587\u5b50\u96c6\u7a7a\u95f4\uff0c\u5b9a\u4e49\u57fa\u4e8e\u5f52\u4e00\u5316\u6807\u8bb0\u4f3c\u7136\u7684\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u548cLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ee5\u66f4\u5c11\u7684\u6a21\u578b\u67e5\u8be2\u5b9e\u73b0\u7ade\u4e89\u6027\u7684\u5f52\u56e0\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u67e5\u8be2\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u5f52\u56e0\u4fdd\u771f\u5ea6\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u6270\u52a8\u5f52\u56e0\u65b9\u6cd5\u3002"}}
{"id": "2506.19967", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19967", "abs": "https://arxiv.org/abs/2506.19967", "authors": ["Travis Thompson", "Seung-Hwan Lim", "Paul Liu", "Ruoying He", "Dongkuan Xu"], "title": "Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs", "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive capabilities in\nlanguage understanding and generation, yet they continue to underperform on\nknowledge-intensive reasoning tasks due to limited access to structured context\nand multi-hop information. Retrieval-Augmented Generation (RAG) partially\nmitigates this by grounding generation in retrieved context, but conventional\nRAG and GraphRAG methods often fail to capture relational structure across\nnodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel\nframework that enhances LLM-based graph reasoning by applying inference-time\ncompute scaling. Our method combines sequential scaling with deep\nchain-of-thought graph traversal, and parallel scaling with majority voting\nover sampled trajectories within an interleaved reasoning-execution loop.\nExperiments on the GRBench benchmark demonstrate that our approach\nsignificantly improves multi-hop question answering performance, achieving\nsubstantial gains over both traditional GraphRAG and prior graph traversal\nbaselines. These findings suggest that inference-time scaling is a practical\nand architecture-agnostic solution for structured knowledge reasoning with LLMs", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInference-Scaled GraphRAG\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u7406\u65f6\u8ba1\u7b97\u6269\u5c55\u63d0\u5347LLM\u5728\u56fe\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u663e\u8457\u6539\u5584\u4e86\u591a\u8df3\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u5173\u7cfb\u7ed3\u6784\u3002", "method": "\u7ed3\u5408\u987a\u5e8f\u6269\u5c55\uff08\u6df1\u5ea6\u94fe\u5f0f\u56fe\u904d\u5386\uff09\u548c\u5e76\u884c\u6269\u5c55\uff08\u591a\u6570\u6295\u7968\u91c7\u6837\u8f68\u8ff9\uff09\uff0c\u5728\u63a8\u7406-\u6267\u884c\u5faa\u73af\u4e2d\u5b9e\u73b0\u56fe\u63a8\u7406\u3002", "result": "\u5728GRBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edfGraphRAG\u548c\u5176\u4ed6\u56fe\u904d\u5386\u57fa\u7ebf\u3002", "conclusion": "\u63a8\u7406\u65f6\u6269\u5c55\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u4e0e\u67b6\u6784\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u63d0\u5347LLMs\u5728\u7ed3\u6784\u5316\u77e5\u8bc6\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2506.20151", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20151", "abs": "https://arxiv.org/abs/2506.20151", "authors": ["Haipeng Fan", "Shiyuan Zhang", "Baohunesitu", "Zihang Guo", "Huaiwen Zhang"], "title": "EAR: Erasing Concepts from Unified Autoregressive Models", "comment": "11 pages, 7 figures, 1 tables", "summary": "Autoregressive (AR) models have achieved unified and strong performance\nacross both visual understanding and image generation tasks. However, removing\nundesired concepts from AR models while maintaining overall generation quality\nremains an open challenge. In this paper, we propose Erasure Autoregressive\nModel (EAR), a fine-tuning method for effective and utility-preserving concept\nerasure in AR models. Specifically, we introduce Windowed Gradient Accumulation\n(WGA) strategy to align patch-level decoding with erasure objectives, and\nThresholded Loss Masking (TLM) strategy to protect content unrelated to the\ntarget concept during fine-tuning. Furthermore, we propose a novel benchmark,\nErase Concept Generator and Visual Filter (ECGVF), aim at provide a more\nrigorous and comprehensive foundation for evaluating concept erasure in AR\nmodels. Specifically, we first employ structured templates across diverse large\nlanguage models (LLMs) to pre-generate a large-scale corpus of\ntarget-replacement concept prompt pairs. Subsequently, we generate images from\nthese prompts and subject them to rigorous filtering via a visual classifier to\nensure concept fidelity and alignment. Extensive experimental results conducted\non the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR\nachieves marked improvements in both erasure effectiveness and model utility\npreservation. Code is available at: https://github.com/immc-lab/ear/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEAR\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u5b9e\u73b0\u9ad8\u6548\u4e14\u4fdd\u7559\u6548\u7528\u7684\u6982\u5ff5\u64e6\u9664\uff0c\u5e76\u5f15\u5165\u4e86WGA\u548cTLM\u7b56\u7565\u4ee5\u53caECGVF\u57fa\u51c6\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u548c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5982\u4f55\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u53bb\u9664\u4e0d\u671f\u671b\u7684\u6982\u5ff5\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51faEAR\u65b9\u6cd5\uff0c\u7ed3\u5408WGA\u7b56\u7565\u548cTLM\u7b56\u7565\uff0c\u5e76\u5f15\u5165ECGVF\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEAR\u5728\u6982\u5ff5\u64e6\u9664\u6548\u679c\u548c\u6a21\u578b\u6548\u7528\u4fdd\u7559\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "EAR\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u6982\u5ff5\u64e6\u9664\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.20652", "categories": ["cs.GR", "cs.CV", "68U05 (Primary), 68T45 (Secondary)", "I.3.7; I.3.8; I.4.9"], "pdf": "https://arxiv.org/pdf/2506.20652", "abs": "https://arxiv.org/abs/2506.20652", "authors": ["Roi Bar-On", "Dana Cohen-Bar", "Daniel Cohen-Or"], "title": "EditP23: 3D Editing via Propagation of Image Prompts to Multi-View", "comment": "Code, supplementary videos, interactive 3D visualizations, and\n  additional results are available at https://editp23.github.io/", "summary": "We present EditP23, a method for mask-free 3D editing that propagates 2D\nimage edits to multi-view representations in a 3D-consistent manner. In\ncontrast to traditional approaches that rely on text-based prompting or\nexplicit spatial masks, EditP23 enables intuitive edits by conditioning on a\npair of images: an original view and its user-edited counterpart. These image\nprompts are used to guide an edit-aware flow in the latent space of a\npre-trained multi-view diffusion model, allowing the edit to be coherently\npropagated across views. Our method operates in a feed-forward manner, without\noptimization, and preserves the identity of the original object, in both\nstructure and appearance. We demonstrate its effectiveness across a range of\nobject categories and editing scenarios, achieving high fidelity to the source\nwhile requiring no manual masks.", "AI": {"tldr": "EditP23\u662f\u4e00\u79cd\u65e0\u9700\u63a9\u7801\u76843D\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc72D\u56fe\u50cf\u7f16\u8f91\u4f20\u64ad\u5230\u591a\u89c6\u56fe\u8868\u793a\uff0c\u5b9e\u73b03D\u4e00\u81f4\u6027\u7f16\u8f91\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u6216\u663e\u5f0f\u7a7a\u95f4\u63a9\u7801\uff0cEditP23\u901a\u8fc7\u56fe\u50cf\u5bf9\uff08\u539f\u59cb\u89c6\u56fe\u548c\u7528\u6237\u7f16\u8f91\u540e\u7684\u89c6\u56fe\uff09\u5b9e\u73b0\u66f4\u76f4\u89c2\u7684\u7f16\u8f91\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u7f16\u8f91\u611f\u77e5\u6d41\u4f20\u64ad\u7f16\u8f91\uff0c\u65e0\u9700\u4f18\u5316\uff0c\u4fdd\u6301\u539f\u59cb\u5bf9\u8c61\u7684\u7ed3\u6784\u548c\u5916\u89c2\u3002", "result": "\u5728\u591a\u79cd\u5bf9\u8c61\u7c7b\u522b\u548c\u7f16\u8f91\u573a\u666f\u4e2d\u8868\u73b0\u9ad8\u6548\uff0c\u65e0\u9700\u624b\u52a8\u63a9\u7801\u5373\u53ef\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u3002", "conclusion": "EditP23\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u76f4\u89c2\u76843D\u7f16\u8f91\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.20008", "categories": ["cs.AI", "cs.PL", "cs.SE", "68T50, 81P68, 68T07, 68T20", "I.2.7; I.2.2"], "pdf": "https://arxiv.org/pdf/2506.20008", "abs": "https://arxiv.org/abs/2506.20008", "authors": ["Abdul Basit", "Minghao Shao", "Haider Asif", "Nouhaila Innan", "Muhammad Kashif", "Alberto Marchisio", "Muhammad Shafique"], "title": "QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges", "comment": "8 pages, 6 figures, 3 tables, submitted to QAI 2025", "summary": "Recent advances in Large Language Models (LLMs) have demonstrated strong\npotential in code generation, yet their effectiveness in quantum computing\nremains underexplored. This paper benchmarks LLMs for PennyLane-based quantum\ncode generation using real-world challenges from the Quantum Hackathon (QHack).\nWe introduce QHackBench, a novel benchmark dataset derived from QHack\ncompetitions, and evaluate model performance under vanilla prompting and\nRetrieval-Augmented Generation (RAG). Our structured evaluation framework\nassesses functional correctness, syntactic validity, and execution success\nacross varying challenge difficulties. Results indicate that RAG-enhanced\nmodels, supplemented with an augmented PennyLane dataset, approximately\ngenerate similar results as the standard prompting, particularly in complex\nquantum algorithms. Additionally, we introduce a multi-agent evaluation\npipeline that iteratively refines incorrect solutions, further enhancing\nexecution success rates. To foster further research, we commit to publicly\nreleasing QHackBench, along with our evaluation framework and experimental\nresults, enabling continued advancements in AI-assisted quantum programming.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728PennyLane\u91cf\u5b50\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u8868\u73b0\uff0c\u5f15\u5165QHackBench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u6bd4\u8f83\u4e86\u6807\u51c6\u63d0\u793a\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u6548\u679c\u3002\u7ed3\u679c\u8868\u660eRAG\u5728\u590d\u6742\u91cf\u5b50\u7b97\u6cd5\u4e2d\u8868\u73b0\u63a5\u8fd1\u6807\u51c6\u63d0\u793a\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u591a\u4ee3\u7406\u8bc4\u4f30\u7ba1\u9053\u4ee5\u63d0\u5347\u6210\u529f\u7387\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u91cf\u5b50\u8ba1\u7b97\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528QHackBench\u6570\u636e\u96c6\uff0c\u8bc4\u4f30LLMs\u5728\u6807\u51c6\u63d0\u793a\u548cRAG\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5f15\u5165\u591a\u4ee3\u7406\u8bc4\u4f30\u7ba1\u9053\u4f18\u5316\u7ed3\u679c\u3002", "result": "RAG\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u63a5\u8fd1\u6807\u51c6\u63d0\u793a\uff0c\u591a\u4ee3\u7406\u7ba1\u9053\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u6210\u529f\u7387\u3002", "conclusion": "\u8bba\u6587\u4e3aAI\u8f85\u52a9\u91cf\u5b50\u7f16\u7a0b\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u548c\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2506.19998", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19998", "abs": "https://arxiv.org/abs/2506.19998", "authors": ["Xinyi Ni", "Haonan Jian", "Qiuyang Wang", "Vedanshi Chetan Shah", "Pengyu Hong"], "title": "Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation", "comment": null, "summary": "REST APIs play important roles in enriching the action space of web agents,\nyet most API-based agents rely on curated and uniform toolsets that do not\nreflect the complexity of real-world APIs. Building tool-using agents for\narbitrary domains remains a major challenge, as it requires reading\nunstructured API documentation, testing APIs and inferring correct parameters.\nWe propose Doc2Agent, a scalable pipeline to build agents that can call\nPython-based tools generated from API documentation. Doc2Agent generates\nexecutable tools from API documentations and iteratively refines them using a\ncode agent. We evaluate our approach on real-world APIs, WebArena APIs, and\nresearch APIs, producing validated tools. We achieved a 55\\% relative\nperformance improvement with 90\\% lower cost compared to direct API calling on\nWebArena benchmark. A domain-specific agent built for glycomaterial science\nfurther demonstrates the pipeline's adaptability to complex, knowledge-rich\ntasks. Doc2Agent offers a generalizable solution for building tool agents from\nunstructured API documentation at scale.", "AI": {"tldr": "Doc2Agent\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u4eceAPI\u6587\u6863\u751f\u6210\u53ef\u6267\u884c\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u4ee3\u7801\u4ee3\u7406\u8fed\u4ee3\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6210\u672c\u6548\u7387\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684API\u590d\u6742\u4e14\u591a\u6837\u5316\uff0c\u73b0\u6709\u5de5\u5177\u96c6\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u4ece\u975e\u7ed3\u6784\u5316API\u6587\u6863\u4e2d\u6784\u5efa\u5de5\u5177\u4ee3\u7406\u3002", "method": "Doc2Agent\u4eceAPI\u6587\u6863\u751f\u6210Python\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u4ee3\u7801\u4ee3\u7406\u8fed\u4ee3\u4f18\u5316\u8fd9\u4e9b\u5de5\u5177\u3002", "result": "\u5728WebArena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u63d0\u5347\u4e8655%\uff0c\u6210\u672c\u964d\u4f4e\u4e8690%\uff0c\u5e76\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982\u7cd6\u6750\u6599\u79d1\u5b66\uff09\u5c55\u793a\u4e86\u9002\u5e94\u6027\u3002", "conclusion": "Doc2Agent\u4e3a\u4ece\u975e\u7ed3\u6784\u5316API\u6587\u6863\u5927\u89c4\u6a21\u6784\u5efa\u5de5\u5177\u4ee3\u7406\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.20152", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.20152", "abs": "https://arxiv.org/abs/2506.20152", "authors": ["Deepak Ghimire", "Kilho Lee", "Seong-heum Kim"], "title": "Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration", "comment": null, "summary": "Structured pruning is a well-established technique for compressing neural\nnetworks, making it suitable for deployment in resource-limited edge devices.\nThis paper presents an efficient Loss-Aware Automatic Selection of Structured\nPruning Criteria (LAASP) for slimming and accelerating deep neural networks.\nThe majority of pruning methodologies employ a sequential process consisting of\nthree stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed\npruning technique adopts a pruning-while-training approach that eliminates the\nfirst stage and integrates the second and third stages into a single cycle. The\nautomatic selection of magnitude or similarity-based filter pruning criteria\nfrom a specified pool of criteria and the specific pruning layer at each\npruning iteration is guided by the network's overall loss on a small subset of\nthe training data. To mitigate the abrupt accuracy drop due to pruning, the\nnetwork is retrained briefly after each reduction of a predefined number of\nfloating-point operations (FLOPs). The optimal pruning rates for each layer in\nthe network are automatically determined, eliminating the need for manual\nallocation of fixed or variable pruning rates for each layer. Experiments on\nthe VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets\ndemonstrate the effectiveness of the proposed method. In particular, the\nResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the\ntop-1 accuracy compared to state-of-the-art methods while reducing the network\nFLOPs by 52\\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces\nFLOPs by more than 42\\% with a negligible 0.33\\% drop in top-5 accuracy. The\nsource code of this paper is publicly available online -\nhttps://github.com/ghimiredhikura/laasp.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u635f\u5931\u611f\u77e5\u81ea\u52a8\u9009\u62e9\u7ed3\u6784\u5316\u526a\u679d\u6807\u51c6\uff08LAASP\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u538b\u7f29\u548c\u52a0\u901f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u8fb9\u8bad\u7ec3\u8fb9\u526a\u679d\u7684\u7b56\u7565\uff0c\u81ea\u52a8\u9009\u62e9\u526a\u679d\u6807\u51c6\u548c\u5c42\uff0c\u5e76\u901a\u8fc7\u77ed\u6682\u91cd\u8bad\u7ec3\u51cf\u5c11\u526a\u679d\u5e26\u6765\u7684\u7cbe\u5ea6\u635f\u5931\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728CIFAR-10\u548cImageNet\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u526a\u679d\u65b9\u6cd5\u9700\u8981\u5206\u9636\u6bb5\u8fdb\u884c\uff08\u8bad\u7ec3\u3001\u526a\u679d\u3001\u5fae\u8c03\uff09\uff0c\u6548\u7387\u8f83\u4f4e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8fb9\u8bad\u7ec3\u8fb9\u526a\u679d\u7684\u65b9\u5f0f\uff0c\u7b80\u5316\u6d41\u7a0b\u5e76\u63d0\u9ad8\u526a\u679d\u6548\u679c\u3002", "method": "\u63d0\u51faLAASP\u65b9\u6cd5\uff0c\u81ea\u52a8\u9009\u62e9\u526a\u679d\u6807\u51c6\u548c\u5c42\uff0c\u7ed3\u5408\u8fb9\u8bad\u7ec3\u8fb9\u526a\u679d\u7684\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u77ed\u6682\u91cd\u8bad\u7ec3\u51cf\u5c11\u7cbe\u5ea6\u635f\u5931\u3002", "result": "\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\uff0cResNet56\u548cResNet110\u6a21\u578b\u7684FLOPs\u51cf\u5c1152%\uff0c\u7cbe\u5ea6\u63d0\u5347\uff1b\u5728ImageNet\u6570\u636e\u96c6\u4e0a\uff0cResNet50\u6a21\u578b\u7684FLOPs\u51cf\u5c1142%\uff0c\u7cbe\u5ea6\u4ec5\u4e0b\u964d0.33%\u3002", "conclusion": "LAASP\u65b9\u6cd5\u5728\u538b\u7f29\u795e\u7ecf\u7f51\u7edc\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u3002"}}
{"id": "2506.20009", "categories": ["cs.AI", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.20009", "abs": "https://arxiv.org/abs/2506.20009", "authors": ["Konstantinos Vrettos", "Michail E. Klontzas"], "title": "Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks", "comment": "18 pages, 3 Figures", "summary": "Background The increasing adoption of Artificial Intelligence (AI) in\nhealthcare has sparked growing concerns about its environmental and ethical\nimplications. Commercial Large Language Models (LLMs), such as ChatGPT and\nDeepSeek, require substantial resources, while the utilization of these systems\nfor medical purposes raises critical issues regarding patient privacy and\nsafety. Methods We developed a customizable Retrieval-Augmented Generation\n(RAG) framework for medical tasks, which monitors its energy usage and CO2\nemissions. This system was then used to create RAGs based on various\nopen-source LLMs. The tested models included both general purpose models like\nllama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs\nperformance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs\no4-mini model. A dataset of medical questions was used for the evaluation.\nResults Custom RAG models outperformed commercial models in accuracy and energy\nconsumption. The RAG model built on llama3.1:8B achieved the highest accuracy\n(58.5%) and was significantly better than other models, including o4-mini and\nDeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption\nand CO2 footprint among all models, with a Performance per kWh of 0.52 and a\ntotal CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x\ntimes more accuracy points per kWh and 172% less electricity usage while\nmaintaining higher accuracy. Conclusion Our study demonstrates that local LLMs\ncan be leveraged to develop RAGs that outperform commercial, online LLMs in\nmedical tasks, while having a smaller environmental impact. Our modular\nframework promotes sustainable AI development, reducing electricity usage and\naligning with the UNs Sustainable Development Goals.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u5b9a\u5236\u7684RAG\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u7597\u4efb\u52a1\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u5546\u4e1aLLM\uff0c\u540c\u65f6\u80fd\u8017\u66f4\u4f4e\u3002", "motivation": "\u89e3\u51b3AI\u5728\u533b\u7597\u9886\u57df\u7684\u73af\u5883\u548c\u4f26\u7406\u95ee\u9898\uff0c\u5982\u8d44\u6e90\u6d88\u8017\u548c\u60a3\u8005\u9690\u79c1\u3002", "method": "\u5f00\u53d1\u4e86\u76d1\u63a7\u80fd\u8017\u548cCO2\u6392\u653e\u7684RAG\u6846\u67b6\uff0c\u5e76\u57fa\u4e8e\u5f00\u6e90LLM\u6784\u5efaRAG\u6a21\u578b\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u81ea\u5b9a\u4e49RAG\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u80fd\u8017\u4e0a\u5747\u4f18\u4e8e\u5546\u4e1a\u6a21\u578b\uff0cllama3.1:8B\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u672c\u5730LLM\u5f00\u53d1\u7684RAG\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5546\u4e1aLLM\uff0c\u4e14\u66f4\u73af\u4fdd\uff0c\u7b26\u5408\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u3002"}}
{"id": "2506.19999", "categories": ["cs.LG", "cs.CL", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2506.19999", "abs": "https://arxiv.org/abs/2506.19999", "authors": ["Francesco Ignazio Re", "Andreas Opedal", "Glib Manaiev", "Mario Giulianelli", "Ryan Cotterell"], "title": "A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior", "comment": "ACL 2025", "summary": "Reading is a process that unfolds across space and time, alternating between\nfixations where a reader focuses on a specific point in space, and saccades\nwhere a reader rapidly shifts their focus to a new point. An ansatz of\npsycholinguistics is that modeling a reader's fixations and saccades yields\ninsight into their online sentence processing. However, standard approaches to\nsuch modeling rely on aggregated eye-tracking measurements and models that\nimpose strong assumptions, ignoring much of the spatio-temporal dynamics that\noccur during reading. In this paper, we propose a more general probabilistic\nmodel of reading behavior, based on a marked spatio-temporal point process,\nthat captures not only how long fixations last, but also where they land in\nspace and when they take place in time. The saccades are modeled using a Hawkes\nprocess, which captures how each fixation excites the probability of a new\nfixation occurring near it in time and space. The duration time of fixation\nevents is modeled as a function of fixation-specific predictors convolved\nacross time, thus capturing spillover effects. Empirically, our Hawkes process\nmodel exhibits a better fit to human saccades than baselines. With respect to\nfixation durations, we observe that incorporating contextual surprisal as a\npredictor results in only a marginal improvement in the model's predictive\naccuracy. This finding suggests that surprisal theory struggles to explain\nfine-grained eye movements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u70b9\u8fc7\u7a0b\u7684\u6982\u7387\u6a21\u578b\uff0c\u7528\u4e8e\u66f4\u5168\u9762\u5730\u6a21\u62df\u9605\u8bfb\u884c\u4e3a\uff0c\u5305\u62ec\u6ce8\u89c6\u70b9\u7684\u4f4d\u7f6e\u3001\u65f6\u95f4\u548c\u6301\u7eed\u65f6\u95f4\uff0c\u4ee5\u53ca\u773c\u8df3\u7684\u52a8\u6001\u7279\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u4e8e\u805a\u5408\u7684\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\u548c\u5f3a\u5047\u8bbe\uff0c\u5ffd\u7565\u4e86\u9605\u8bfb\u8fc7\u7a0b\u4e2d\u7684\u65f6\u7a7a\u52a8\u6001\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u901a\u7528\u7684\u6a21\u578b\u6765\u6355\u6349\u8fd9\u4e9b\u7ec6\u8282\u3002", "method": "\u4f7f\u7528\u6807\u8bb0\u7684\u65f6\u7a7a\u70b9\u8fc7\u7a0b\u6a21\u578b\uff0c\u5176\u4e2d\u773c\u8df3\u901a\u8fc7Hawkes\u8fc7\u7a0b\u5efa\u6a21\uff0c\u6ce8\u89c6\u6301\u7eed\u65f6\u95f4\u901a\u8fc7\u65f6\u95f4\u5377\u79ef\u7684\u9884\u6d4b\u56e0\u5b50\u5efa\u6a21\u3002", "result": "Hawkes\u8fc7\u7a0b\u6a21\u578b\u6bd4\u57fa\u7ebf\u6a21\u578b\u66f4\u597d\u5730\u62df\u5408\u4e86\u4eba\u7c7b\u7684\u773c\u8df3\u884c\u4e3a\uff0c\u4f46\u4e0a\u4e0b\u6587\u610f\u5916\u6027\u5bf9\u6ce8\u89c6\u6301\u7eed\u65f6\u95f4\u7684\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u5347\u6709\u9650\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u610f\u5916\u6027\u7406\u8bba\u5728\u89e3\u91ca\u7cbe\u7ec6\u773c\u52a8\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u63d0\u51fa\u7684\u6a21\u578b\u4e3a\u9605\u8bfb\u884c\u4e3a\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u63cf\u8ff0\u3002"}}
{"id": "2506.20155", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20155", "abs": "https://arxiv.org/abs/2506.20155", "authors": ["Avadhoot Jadhav", "Ashutosh Srivastava", "Abhinav Java", "Silky Singh", "Tarun Ram Menta", "Surgan Jandial", "Balaji Krishnamurthy"], "title": "Towards Efficient Exemplar Based Image Editing with Multimodal VLMs", "comment": "Accepted at ECCV 2024 (AI4VA Workshop)", "summary": "Text-to-Image Diffusion models have enabled a wide array of image editing\napplications. However, capturing all types of edits through text alone can be\nchallenging and cumbersome. The ambiguous nature of certain image edits is\nbetter expressed through an exemplar pair, i.e., a pair of images depicting an\nimage before and after an edit respectively. In this work, we tackle\nexemplar-based image editing -- the task of transferring an edit from an\nexemplar pair to a content image(s), by leveraging pretrained text-to-image\ndiffusion models and multimodal VLMs. Even though our end-to-end pipeline is\noptimization-free, our experiments demonstrate that it still outperforms\nbaselines on multiple types of edits while being ~4x faster.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u793a\u4f8b\u5bf9\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u548c\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u4f18\u5316\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u7684\u7f16\u8f91\u6548\u679c\u3002", "motivation": "\u7531\u4e8e\u4ec5\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\u96be\u4ee5\u6355\u6349\u6240\u6709\u7c7b\u578b\u7684\u56fe\u50cf\u7f16\u8f91\u9700\u6c42\uff0c\u793a\u4f8b\u5bf9\uff08\u7f16\u8f91\u524d\u540e\u7684\u56fe\u50cf\uff09\u80fd\u66f4\u76f4\u89c2\u5730\u8868\u8fbe\u7f16\u8f91\u610f\u56fe\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u548c\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u6784\u5efa\u7aef\u5230\u7aef\u7684\u4f18\u5316\u81ea\u7531\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u7f16\u8f91\u7c7b\u578b\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u901f\u5ea6\u63d0\u5347\u7ea64\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u793a\u4f8b\u5bf9\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u7f16\u8f91\uff0c\u5c55\u793a\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u7f16\u8f91\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.20018", "categories": ["cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2506.20018", "abs": "https://arxiv.org/abs/2506.20018", "authors": ["Zechun Deng", "Ziwei Liu", "Ziqian Bi", "Junhao Song", "Chia Xin Liang", "Joe Yeong", "Junfeng Hao"], "title": "Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models", "comment": null, "summary": "This paper investigates real-time decision support systems that leverage\nlow-latency AI models, bringing together recent progress in holistic AI-driven\ndecision tools, integration with Edge-IoT technologies, and approaches for\neffective human-AI teamwork. It looks into how large language models can assist\ndecision-making, especially when resources are limited. The research also\nexamines the effects of technical developments such as DeLLMa, methods for\ncompressing models, and improvements for analytics on edge devices, while also\naddressing issues like limited resources and the need for adaptable frameworks.\nThrough a detailed review, the paper offers practical perspectives on\ndevelopment strategies and areas of application, adding to the field by\npointing out opportunities for more efficient and flexible AI-supported\nsystems. The conclusions set the stage for future breakthroughs in this\nfast-changing area, highlighting how AI can reshape real-time decision support.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u7ed3\u5408\u4f4e\u5ef6\u8fdfAI\u6a21\u578b\u3001Edge-IoT\u6280\u672f\u548c\u4eba\u673a\u534f\u4f5c\u65b9\u6cd5\uff0c\u7814\u7a76\u5982\u4f55\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u8d44\u6e90\u53d7\u9650\u7684\u51b3\u7b56\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6574\u5408AI\u9a71\u52a8\u7684\u51b3\u7b56\u5de5\u5177\u4e0e\u8fb9\u7f18\u8ba1\u7b97\u6280\u672f\uff0c\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u548c\u9002\u5e94\u6027\u6846\u67b6\u9700\u6c42\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8be6\u7ec6\u7efc\u8ff0\uff0c\u5206\u6790DeLLMa\u7b49\u6280\u672f\u8fdb\u5c55\u3001\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u548c\u8fb9\u7f18\u8bbe\u5907\u5206\u6790\u6539\u8fdb\u3002", "result": "\u63d0\u4f9b\u4e86\u5f00\u53d1\u7b56\u7565\u548c\u5e94\u7528\u9886\u57df\u7684\u5b9e\u7528\u89c6\u89d2\uff0c\u6307\u51fa\u9ad8\u6548\u7075\u6d3bAI\u7cfb\u7edf\u7684\u673a\u4f1a\u3002", "conclusion": "\u4e3a\u672a\u6765\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u9886\u57df\u7684\u7a81\u7834\u5960\u5b9a\u57fa\u7840\uff0c\u5f3a\u8c03AI\u91cd\u5851\u51b3\u7b56\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.20073", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20073", "abs": "https://arxiv.org/abs/2506.20073", "authors": ["Kethmi Hirushini Hettige", "Jiahao Ji", "Cheng Long", "Shili Xiang", "Gao Cong", "Jingyuan Wang"], "title": "A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs", "comment": null, "summary": "Spatio-temporal data mining plays a pivotal role in informed decision making\nacross diverse domains. However, existing models are often restricted to narrow\ntasks, lacking the capacity for multi-task inference and complex long-form\nreasoning that require generation of in-depth, explanatory outputs. These\nlimitations restrict their applicability to real-world, multi-faceted decision\nscenarios. In this work, we introduce STReason, a novel framework that\nintegrates the reasoning strengths of large language models (LLMs) with the\nanalytical capabilities of spatio-temporal models for multi-task inference and\nexecution. Without requiring task-specific finetuning, STReason leverages\nin-context learning to decompose complex natural language queries into modular,\ninterpretable programs, which are then systematically executed to generate both\nsolutions and detailed rationales. To facilitate rigorous evaluation, we\nconstruct a new benchmark dataset and propose a unified evaluation framework\nwith metrics specifically designed for long-form spatio-temporal reasoning.\nExperimental results show that STReason significantly outperforms advanced LLM\nbaselines across all metrics, particularly excelling in complex,\nreasoning-intensive spatio-temporal scenarios. Human evaluations further\nvalidate STReason's credibility and practical utility, demonstrating its\npotential to reduce expert workload and broaden the applicability to real-world\nspatio-temporal tasks. We believe STReason provides a promising direction for\ndeveloping more capable and generalizable spatio-temporal reasoning systems.", "AI": {"tldr": "STReason\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u65f6\u7a7a\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u652f\u6301\u591a\u4efb\u52a1\u63a8\u7406\u548c\u590d\u6742\u957f\u5f62\u5f0f\u63a8\u7406\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65f6\u7a7a\u6570\u636e\u6316\u6398\u6a21\u578b\u5c40\u9650\u4e8e\u5355\u4e00\u4efb\u52a1\uff0c\u7f3a\u4e4f\u591a\u4efb\u52a1\u63a8\u7406\u548c\u6df1\u5165\u89e3\u91ca\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "STReason\u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u5c06\u590d\u6742\u67e5\u8be2\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u7a0b\u5e8f\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff0c\u751f\u6210\u89e3\u51b3\u65b9\u6848\u548c\u8be6\u7ec6\u89e3\u91ca\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSTReason\u5728\u6240\u6709\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u590d\u6742\u63a8\u7406\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "STReason\u4e3a\u5f00\u53d1\u66f4\u901a\u7528\u548c\u5f3a\u5927\u7684\u65f6\u7a7a\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u5e76\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.20168", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20168", "abs": "https://arxiv.org/abs/2506.20168", "authors": ["Zhentao He", "Can Zhang", "Ziheng Wu", "Zhenghao Chen", "Yufei Zhan", "Yifan Li", "Zhao Zhang", "Xian Wang", "Minghui Qiu"], "title": "Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models", "comment": null, "summary": "Recent advancements in multimodal large language models have enhanced\ndocument understanding by integrating textual and visual information. However,\nexisting models exhibit incompleteness within their paradigm in real-world\nscenarios, particularly under visual degradation. In such conditions, the\ncurrent response paradigm often fails to adequately perceive visual degradation\nand ambiguity, leading to overreliance on linguistic priors or misaligned\nvisual-textual reasoning. This difficulty in recognizing uncertainty frequently\nresults in the generation of hallucinatory content, especially when a precise\nanswer is not feasible. To better demonstrate and analyze this phenomenon and\nproblem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR\nhallucination in degraded document understanding. This dataset includes test\nsamples spanning identity cards and invoices, with simulated real-world\ndegradations for OCR reliability. This setup allows for evaluating models'\ncapacity, under degraded input, to distinguish reliable visual information and\nanswer accordingly, thereby highlighting the challenge of avoiding\nhallucination on uncertain data. To achieve vision-faithful reasoning and\nthereby avoid the aforementioned issues, we further introduce a GRPO-based\nframework featuring a novel reward mechanism. By incorporating a self-awareness\nof visual uncertainty and an analysis method that initiates refusal to answer\nto increase task difficulty within our supervised fine-tuning and reinforcement\nlearning framework, we successfully mitigated hallucinations in ambiguous\nregions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model\nachieves a 22\\% absolute improvement in hallucination-free accuracy over GPT-4o\non KIE-HVQA and there is no significant performance drop in standard tasks,\nhighlighting both effectiveness and robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faKIE-HVQA\u57fa\u51c6\uff0c\u8bc4\u4f30OCR\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u5f15\u5165GRPO\u6846\u67b6\u4ee5\u51cf\u5c11\u5e7b\u89c9\uff0c\u5b9e\u9a8c\u663e\u793a\u51767B\u6a21\u578b\u5728\u5e7b\u89c9\u51cf\u5c11\u4e0a\u4f18\u4e8eGPT-4o\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u9000\u5316\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u6613\u4ea7\u751f\u5e7b\u89c9\u5185\u5bb9\uff0c\u9700\u6539\u8fdb\u89c6\u89c9-\u6587\u672c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faKIE-HVQA\u57fa\u51c6\u6a21\u62df\u771f\u5b9e\u9000\u5316\u6587\u6863\uff0c\u5e76\u8bbe\u8ba1GRPO\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u4e0d\u786e\u5b9a\u6027\u548c\u62d2\u7edd\u56de\u7b54\u673a\u5236\u3002", "result": "7B\u6a21\u578b\u5728KIE-HVQA\u4e0a\u5e7b\u89c9\u51cf\u5c1122%\uff0c\u4e14\u6807\u51c6\u4efb\u52a1\u6027\u80fd\u65e0\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "GRPO\u6846\u67b6\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\uff0c\u63d0\u5347\u6a21\u578b\u5728\u9000\u5316\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.20020", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20020", "abs": "https://arxiv.org/abs/2506.20020", "authors": ["Saloni Dash", "Am\u00e9lie Reymond", "Emma S. Spiro", "Aylin Caliskan"], "title": "Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning", "comment": null, "summary": "Reasoning in humans is prone to biases due to underlying motivations like\nidentity protection, that undermine rational decision-making and judgment. This\nmotivated reasoning at a collective level can be detrimental to society when\ndebating critical issues such as human-driven climate change or vaccine safety,\nand can further aggravate political polarization. Prior studies have reported\nthat large language models (LLMs) are also susceptible to human-like cognitive\nbiases, however, the extent to which LLMs selectively reason toward\nidentity-congruent conclusions remains largely unexplored. Here, we investigate\nwhether assigning 8 personas across 4 political and socio-demographic\nattributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and\nproprietary) across two reasoning tasks from human-subject studies -- veracity\ndiscernment of misinformation headlines and evaluation of numeric scientific\nevidence -- we find that persona-assigned LLMs have up to 9% reduced veracity\ndiscernment relative to models without personas. Political personas\nspecifically, are up to 90% more likely to correctly evaluate scientific\nevidence on gun control when the ground truth is congruent with their induced\npolitical identity. Prompt-based debiasing methods are largely ineffective at\nmitigating these effects. Taken together, our empirical findings are the first\nto suggest that persona-assigned LLMs exhibit human-like motivated reasoning\nthat is hard to mitigate through conventional debiasing prompts -- raising\nconcerns of exacerbating identity-congruent reasoning in both LLMs and humans.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5206\u914d\u7279\u5b9a\u8eab\u4efd\u89d2\u8272\u540e\uff0c\u4f1a\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u52a8\u673a\u6027\u63a8\u7406\uff0c\u4e14\u96be\u4ee5\u901a\u8fc7\u5e38\u89c4\u65b9\u6cd5\u6d88\u9664\u504f\u89c1\u3002", "motivation": "\u63a2\u8ba8LLMs\u662f\u5426\u4f1a\u5728\u8eab\u4efd\u89d2\u8272\u5206\u914d\u540e\u8868\u73b0\u51fa\u52a8\u673a\u6027\u63a8\u7406\uff0c\u4ee5\u53ca\u8fd9\u79cd\u504f\u89c1\u5bf9\u793e\u4f1a\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u4e3a8\u4e2aLLMs\u5206\u914d8\u79cd\u4e0d\u540c\u653f\u6cbb\u548c\u793e\u4f1a\u4eba\u53e3\u5c5e\u6027\u89d2\u8272\uff0c\u6d4b\u8bd5\u5176\u5728\u4fe1\u606f\u771f\u5b9e\u6027\u8fa8\u522b\u548c\u79d1\u5b66\u8bc1\u636e\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u8eab\u4efd\u89d2\u8272\u5206\u914d\u7684LLMs\u5728\u4fe1\u606f\u8fa8\u522b\u80fd\u529b\u4e0a\u964d\u4f4e9%\uff0c\u653f\u6cbb\u89d2\u8272\u5728\u79d1\u5b66\u8bc1\u636e\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa90%\u7684\u504f\u89c1\u503e\u5411\u3002\u53bb\u504f\u89c1\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "conclusion": "LLMs\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u52a8\u673a\u6027\u63a8\u7406\uff0c\u4e14\u96be\u4ee5\u6d88\u9664\uff0c\u53ef\u80fd\u52a0\u5267\u793e\u4f1a\u4e2d\u7684\u8eab\u4efd\u504f\u89c1\u95ee\u9898\u3002"}}
{"id": "2506.20081", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20081", "abs": "https://arxiv.org/abs/2506.20081", "authors": ["Dhruv Gupta", "Gayathri Ganesh Lakshmy", "Yiqing Xie"], "title": "SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization", "comment": null, "summary": "Retrieval-Augmented Code Generation (RACG) is a critical technique for\nenhancing code generation by retrieving relevant information. In this work, we\nconduct an in-depth analysis of code retrieval by systematically masking\nspecific features while preserving code functionality. Our discoveries include:\n(1) although trained on code, current retrievers heavily rely on surface-level\ntextual features (e.g., docstrings, identifier names), and (2) they exhibit a\nstrong bias towards well-documented code, even if the documentation is\nirrelevant.Based on our discoveries, we propose SACL, a framework that enriches\ntextual information and reduces bias by augmenting code or structural knowledge\nwith semantic information. Extensive experiments show that SACL substantially\nimproves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval /\nMBPP / SWE-Bench-Lite), which also leads to better code generation performance\n(e.g., by 4.88% Pass@1 on HumanEval).", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u4ee3\u7801\u68c0\u7d22\u7684\u4f9d\u8d56\u7279\u5f81\uff0c\u53d1\u73b0\u5f53\u524d\u68c0\u7d22\u5668\u8fc7\u4e8e\u4f9d\u8d56\u8868\u9762\u6587\u672c\u7279\u5f81\u548c\u6587\u6863\u504f\u89c1\uff0c\u63d0\u51fa\u4e86SACL\u6846\u67b6\u4ee5\u589e\u5f3a\u8bed\u4e49\u4fe1\u606f\u5e76\u51cf\u5c11\u504f\u89c1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u68c0\u7d22\u548c\u751f\u6210\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u68c0\u7d22\u6280\u672f\u8fc7\u4e8e\u4f9d\u8d56\u8868\u9762\u6587\u672c\u7279\u5f81\uff08\u5982\u6587\u6863\u5b57\u7b26\u4e32\u3001\u6807\u8bc6\u7b26\u540d\u79f0\uff09\u548c\u5b58\u5728\u5bf9\u6587\u6863\u7684\u504f\u89c1\uff0c\u5f71\u54cd\u4e86\u4ee3\u7801\u751f\u6210\u7684\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u5c4f\u853d\u7279\u5b9a\u7279\u5f81\u4f46\u4fdd\u7559\u4ee3\u7801\u529f\u80fd\uff0c\u5206\u6790\u4ee3\u7801\u68c0\u7d22\u7684\u4f9d\u8d56\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u4e86SACL\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u8bed\u4e49\u4fe1\u606f\u6765\u51cf\u5c11\u504f\u89c1\u3002", "result": "SACL\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u68c0\u7d22\u6027\u80fd\uff08\u5982HumanEval\u4e0aRecall@1\u63d0\u534712.8%\uff09\uff0c\u5e76\u6539\u5584\u4e86\u4ee3\u7801\u751f\u6210\uff08\u5982HumanEval\u4e0aPass@1\u63d0\u53474.88%\uff09\u3002", "conclusion": "SACL\u901a\u8fc7\u589e\u5f3a\u8bed\u4e49\u4fe1\u606f\u548c\u51cf\u5c11\u504f\u89c1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u68c0\u7d22\u548c\u751f\u6210\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7684\u4ee3\u7801\u751f\u6210\u6280\u672f\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2506.20174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20174", "abs": "https://arxiv.org/abs/2506.20174", "authors": ["Man Duc Chuc"], "title": "Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition", "comment": null, "summary": "Foundation models are rapidly transforming Earth Observation data mining by\nenabling generalizable and scalable solutions for key tasks such as scene\nclassification and semantic segmentation. While most efforts in the geospatial\ndomain have focused on developing large models trained from scratch using\nmassive Earth Observation datasets, an alternative strategy that remains\nunderexplored is the reuse and combination of existing pretrained models. In\nthis study, we investigate whether foundation models pretrained on remote\nsensing and general vision datasets can be effectively combined to improve\nperformance across a diverse set of key Earth Observation tasks. Using the\nGEO-Bench benchmark, we evaluate several prominent models, including Prithvi,\nHiera, and DOFA, on eleven datasets covering a range of spatial resolutions,\nsensor modalities, and task types. The results show that feature-level\nensembling of smaller pretrained models can match or exceed the performance of\nmuch larger models, while requiring less training time and computational\nresources. Moreover, the study highlights the potential of applying knowledge\ndistillation to transfer the strengths of ensembles into more compact models,\noffering a practical path for deploying foundation models in real-world Earth\nObservation applications.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u9065\u611f\u4e0e\u901a\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u63d0\u5347\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u7279\u5f81\u7ea7\u96c6\u6210\u5c0f\u6a21\u578b\u53ef\u5ab2\u7f8e\u6216\u8d85\u8d8a\u5927\u6a21\u578b\uff0c\u4e14\u66f4\u9ad8\u6548\u3002", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u590d\u7528\u4e0e\u7ec4\u5408\uff0c\u4ee5\u66ff\u4ee3\u4ece\u5934\u8bad\u7ec3\u5927\u578b\u6a21\u578b\u7684\u4f20\u7edf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u5730\u7403\u89c2\u6d4b\u6570\u636e\u6316\u6398\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u5229\u7528GEO-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5305\u62ecPrithvi\u3001Hiera\u548cDOFA\u5728\u5185\u7684\u591a\u4e2a\u6a21\u578b\u572811\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u91c7\u7528\u7279\u5f81\u7ea7\u96c6\u6210\u65b9\u6cd5\u3002", "result": "\u7279\u5f81\u7ea7\u96c6\u6210\u7684\u5c0f\u6a21\u578b\u6027\u80fd\u53ef\u5ab2\u7f8e\u6216\u8d85\u8d8a\u5927\u6a21\u578b\uff0c\u4e14\u8bad\u7ec3\u65f6\u95f4\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u66f4\u4f4e\uff1b\u77e5\u8bc6\u84b8\u998f\u53ef\u8fdb\u4e00\u6b65\u538b\u7f29\u6a21\u578b\u3002", "conclusion": "\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u7ec4\u5408\u4e0e\u77e5\u8bc6\u84b8\u998f\u4e3a\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.20059", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20059", "abs": "https://arxiv.org/abs/2506.20059", "authors": ["Weijieying Ren", "Tianxiang Zhao", "Lei Wang", "Tianchun Wang", "Vasant Honavar"], "title": "DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have led to remarkable\nprogresses in medical consultation. However, existing medical LLMs overlook the\nessential role of Electronic Health Records (EHR) and focus primarily on\ndiagnosis recommendation, limiting their clinical applicability. We propose\nDiaLLM, the first medical LLM that integrates heterogeneous EHR data into\nclinically grounded dialogues, enabling clinical test recommendation, result\ninterpretation, and diagnosis prediction to better align with real-world\nmedical practice. To construct clinically grounded dialogues from EHR, we\ndesign a Clinical Test Reference (CTR) strategy that maps each clinical code to\nits corresponding description and classifies test results as \"normal\" or\n\"abnormal\". Additionally, DiaLLM employs a reinforcement learning framework for\nevidence acquisition and automated diagnosis. To handle the large action space,\nwe introduce a reject sampling strategy to reduce redundancy and improve\nexploration efficiency. Furthermore, a confirmation reward and a\nclass-sensitive diagnosis reward are designed to guide accurate diagnosis\nprediction. Extensive experimental results demonstrate that DiaLLM outperforms\nbaselines in clinical test recommendation and diagnosis prediction.", "AI": {"tldr": "DiaLLM\u662f\u4e00\u79cd\u65b0\u578b\u533b\u7597LLM\uff0c\u6574\u5408\u4e86\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\uff0c\u652f\u6301\u4e34\u5e8a\u6d4b\u8bd5\u63a8\u8350\u3001\u7ed3\u679c\u89e3\u91ca\u548c\u8bca\u65ad\u9884\u6d4b\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u533b\u7597LLM\u5ffd\u89c6EHR\u7684\u4f5c\u7528\u4e14\u4ec5\u5173\u6ce8\u8bca\u65ad\u63a8\u8350\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5b9e\u7528\u6027\u3002DiaLLM\u65e8\u5728\u901a\u8fc7\u6574\u5408EHR\u6570\u636e\u63d0\u5347\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faClinical Test Reference\uff08CTR\uff09\u7b56\u7565\uff0c\u5c06\u4e34\u5e8a\u4ee3\u7801\u6620\u5c04\u4e3a\u63cf\u8ff0\u5e76\u5206\u7c7b\u6d4b\u8bd5\u7ed3\u679c\uff1b\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5f15\u5165\u62d2\u7edd\u91c7\u6837\u7b56\u7565\u4ee5\u51cf\u5c11\u5197\u4f59\uff0c\u5e76\u8bbe\u8ba1\u786e\u8ba4\u5956\u52b1\u548c\u7c7b\u522b\u654f\u611f\u8bca\u65ad\u5956\u52b1\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDiaLLM\u5728\u4e34\u5e8a\u6d4b\u8bd5\u63a8\u8350\u548c\u8bca\u65ad\u9884\u6d4b\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DiaLLM\u901a\u8fc7\u6574\u5408EHR\u6570\u636e\u548c\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u7597LLM\u7684\u4e34\u5e8a\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.20083", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20083", "abs": "https://arxiv.org/abs/2506.20083", "authors": ["Yingji Zhang", "Danilo S. Carvalho", "Andr\u00e9 Freitas"], "title": "Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder", "comment": "In progress", "summary": "Integrating compositional and symbolic properties into current distributional\nsemantic spaces can enhance the interpretability, controllability,\ncompositionality, and generalisation capabilities of Transformer-based\nauto-regressive language models (LMs). In this survey, we offer a novel\nperspective on latent space geometry through the lens of compositional\nsemantics, a direction we refer to as \\textit{semantic representation\nlearning}. This direction enables a bridge between symbolic and distributional\nsemantics, helping to mitigate the gap between them. We review and compare\nthree mainstream autoencoder architectures-Variational AutoEncoder (VAE),\nVector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the\ndistinctive latent geometries they induce in relation to semantic structure and\ninterpretability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u7ed3\u5408\u7ec4\u5408\u548c\u7b26\u53f7\u8bed\u4e49\u7279\u6027\u63d0\u5347Transformer\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u63a7\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u51fa\u8bed\u4e49\u8868\u793a\u5b66\u4e60\u65b9\u5411\uff0c\u5e76\u6bd4\u8f83\u4e09\u79cd\u81ea\u7f16\u7801\u5668\u67b6\u6784\u7684\u6f5c\u5728\u51e0\u4f55\u7279\u6027\u3002", "motivation": "\u901a\u8fc7\u6574\u5408\u7ec4\u5408\u548c\u7b26\u53f7\u8bed\u4e49\u7279\u6027\uff0c\u5f25\u8865\u5206\u5e03\u8bed\u4e49\u4e0e\u7b26\u53f7\u8bed\u4e49\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u7efc\u8ff0\u5e76\u6bd4\u8f83\u4e86\u4e09\u79cd\u81ea\u7f16\u7801\u5668\u67b6\u6784\uff08VAE\u3001VQVAE\u3001SAE\uff09\u53ca\u5176\u5728\u8bed\u4e49\u7ed3\u6784\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u7684\u6f5c\u5728\u51e0\u4f55\u7279\u6027\u3002", "result": "\u8bed\u4e49\u8868\u793a\u5b66\u4e60\u65b9\u5411\u4e3a\u8fde\u63a5\u7b26\u53f7\u4e0e\u5206\u5e03\u8bed\u4e49\u63d0\u4f9b\u4e86\u6865\u6881\uff0c\u4e0d\u540c\u81ea\u7f16\u7801\u5668\u67b6\u6784\u5728\u8bed\u4e49\u8868\u793a\u4e0a\u5404\u6709\u7279\u70b9\u3002", "conclusion": "\u7ed3\u5408\u7ec4\u5408\u548c\u7b26\u53f7\u8bed\u4e49\u7279\u6027\u662f\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u6709\u6548\u9014\u5f84\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u8bed\u4e49\u8868\u793a\u5b66\u4e60\u7684\u5e94\u7528\u3002"}}
{"id": "2506.20179", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.20179", "abs": "https://arxiv.org/abs/2506.20179", "authors": ["Enzhe Zhao", "Zhichang Guo", "Yao Li", "Fanghui Song", "Boying Wu"], "title": "Progressive Alignment Degradation Learning for Pansharpening", "comment": "13 pages, 9 figures", "summary": "Deep learning-based pansharpening has been shown to effectively generate\nhigh-resolution multispectral (HRMS) images. To create supervised ground-truth\nHRMS images, synthetic data generated using the Wald protocol is commonly\nemployed. This protocol assumes that networks trained on artificial\nlow-resolution data will perform equally well on high-resolution data. However,\nwell-trained models typically exhibit a trade-off in performance between\nreduced-resolution and full-resolution datasets. In this paper, we delve into\nthe Wald protocol and find that its inaccurate approximation of real-world\ndegradation patterns limits the generalization of deep pansharpening models. To\naddress this issue, we propose the Progressive Alignment Degradation Module\n(PADM), which uses mutual iteration between two sub-networks, PAlignNet and\nPDegradeNet, to adaptively learn accurate degradation processes without relying\non predefined operators. Building on this, we introduce HFreqdiff, which embeds\nhigh-frequency details into a diffusion framework and incorporates CFB and BACM\nmodules for frequency-selective detail extraction and precise reverse process\nlearning. These innovations enable effective integration of high-resolution\npanchromatic and multispectral images, significantly enhancing spatial\nsharpness and quality. Experiments and ablation studies demonstrate the\nproposed method's superior performance compared to state-of-the-art techniques.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08PADM\u548cHFreqdiff\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u57fa\u4e8eWald\u534f\u8bae\u7684\u56fe\u50cf\u878d\u5408\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u9000\u5316\u6a21\u5f0f\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfWald\u534f\u8bae\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u65e0\u6cd5\u51c6\u786e\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7684\u9000\u5316\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u6df1\u5ea6\u56fe\u50cf\u878d\u5408\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u6e10\u8fdb\u5bf9\u9f50\u9000\u5316\u6a21\u5757\uff08PADM\uff09\u548cHFreqdiff\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5b66\u4e60\u9000\u5316\u8fc7\u7a0b\u548c\u9ad8\u9891\u7ec6\u8282\u6269\u6563\uff0c\u63d0\u5347\u56fe\u50cf\u878d\u5408\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7a7a\u95f4\u6e05\u6670\u5ea6\u548c\u56fe\u50cf\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "PADM\u548cHFreqdiff\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.20130", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20130", "abs": "https://arxiv.org/abs/2506.20130", "authors": ["Adrien Bibal", "Steven N. Minton", "Deborah Khider", "Yolanda Gil"], "title": "AI Copilots for Reproducibility in Science: A Case Study", "comment": null, "summary": "Open science initiatives seek to make research outputs more transparent,\naccessible, and reusable, but ensuring that published findings can be\nindependently reproduced remains a persistent challenge. This paper introduces\nOpenPub, an AI-powered platform that supports researchers, reviewers, and\nreaders through a suite of modular copilots focused on key open science tasks.\nIn this work, we present the Reproducibility Copilot, which analyzes\nmanuscripts, code, and supplementary materials to generate structured Jupyter\nNotebooks and recommendations aimed at facilitating computational, or \"rote\",\nreproducibility. We conducted feasibility tests using previously studied\nresearch papers with known reproducibility benchmarks. Results indicate that\nOpenPub can substantially reduce reproduction time - from over 30 hours to\nabout 1 hour - while achieving high coverage of figures, tables, and results\nsuitable for computational reproduction. The system systematically detects\nbarriers to reproducibility, including missing hyperparameters, undocumented\npreprocessing steps, and incomplete or inaccessible datasets. These findings\nsuggest that AI-driven tools can meaningfully reduce the burden of\nreproducibility efforts and contribute to more transparent and verifiable\nscientific communication. The modular copilot architecture also provides a\nfoundation for extending AI assistance to additional open science objectives\nbeyond reproducibility.", "AI": {"tldr": "OpenPub\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u5e73\u53f0\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u52a9\u624b\u652f\u6301\u5f00\u653e\u79d1\u5b66\u4efb\u52a1\uff0c\u7279\u522b\u662f\u53ef\u590d\u73b0\u6027\u3002\u5176Reproducibility Copilot\u80fd\u5206\u6790\u8bba\u6587\u3001\u4ee3\u7801\u548c\u8865\u5145\u6750\u6599\uff0c\u751f\u6210\u7ed3\u6784\u5316Jupyter Notebook\u548c\u5efa\u8bae\uff0c\u663e\u8457\u51cf\u5c11\u590d\u73b0\u65f6\u95f4\u3002", "motivation": "\u5f00\u653e\u79d1\u5b66\u5021\u8bae\u65e8\u5728\u63d0\u9ad8\u7814\u7a76\u8f93\u51fa\u7684\u900f\u660e\u5ea6\u548c\u53ef\u590d\u7528\u6027\uff0c\u4f46\u72ec\u7acb\u590d\u73b0\u7814\u7a76\u6210\u679c\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "OpenPub\u5e73\u53f0\u901a\u8fc7Reproducibility Copilot\u5206\u6790\u7814\u7a76\u6750\u6599\uff0c\u751f\u6210\u7ed3\u6784\u5316Notebook\u548c\u5efa\u8bae\uff0c\u5e76\u8fdb\u884c\u53ef\u884c\u6027\u6d4b\u8bd5\u3002", "result": "\u6d4b\u8bd5\u663e\u793a\uff0cOpenPub\u80fd\u5c06\u590d\u73b0\u65f6\u95f4\u4ece30\u591a\u5c0f\u65f6\u7f29\u77ed\u81f3\u7ea61\u5c0f\u65f6\uff0c\u5e76\u9ad8\u8986\u76d6\u7387\u5730\u590d\u73b0\u56fe\u8868\u548c\u7ed3\u679c\u3002", "conclusion": "AI\u5de5\u5177\u53ef\u663e\u8457\u51cf\u8f7b\u590d\u73b0\u8d1f\u62c5\uff0c\u4fc3\u8fdb\u66f4\u900f\u660e\u7684\u79d1\u5b66\u4ea4\u6d41\uff0c\u6a21\u5757\u5316\u67b6\u6784\u8fd8\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u5f00\u653e\u79d1\u5b66\u76ee\u6807\u3002"}}
{"id": "2506.20093", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20093", "abs": "https://arxiv.org/abs/2506.20093", "authors": ["Yilin Wang", "Peixuan Lei", "Jie Song", "Yuzhe Hao", "Tao Chen", "Yuxuan Zhang", "Lei Jia", "Yuanxiang Li", "Zhongyu Wei"], "title": "ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset", "comment": null, "summary": "Time-series data are critical in diverse applications, such as industrial\nmonitoring, medical diagnostics, and climate research. However, effectively\nintegrating these high-dimensional temporal signals with natural language for\ndynamic, interactive tasks remains a significant challenge. To address this, we\nintroduce the Time-Series Question Answering (Time-Series QA) task and release\nEngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset\ndesigned to capture complex interactions between time-series signals and\nnatural language. Building on this resource, we propose the Instruct Time\nTransformer (ITFormer), a novel framework that bridges time-series encoders\nwith frozen large language models (LLMs). ITFormer effectively extracts,\naligns, and fuses temporal and textual features, achieving a strong improvement\nin QA accuracy over strong baselines with fewer than 1\\% additional trainable\nparameters. By combining computational efficiency with robust cross-modal\nmodeling, our work establishes a adaptable paradigm for integrating temporal\ndata with natural language, paving the way for new research and applications in\nmulti-modal AI. More details about the project, including datasets and code,\nare available at: https://pandalin98.github.io/itformer_site/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Time-Series QA\u4efb\u52a1\u548cEngineMT-QA\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86Instruct Time Transformer (ITFormer)\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u878d\u5408\u65f6\u95f4\u5e8f\u5217\u4e0e\u81ea\u7136\u8bed\u8a00\uff0c\u663e\u8457\u63d0\u5347\u95ee\u7b54\u51c6\u786e\u6027\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5728\u591a\u4e2a\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5982\u4f55\u5c06\u5176\u4e0e\u81ea\u7136\u8bed\u8a00\u9ad8\u6548\u7ed3\u5408\u4ee5\u652f\u6301\u52a8\u6001\u4ea4\u4e92\u4efb\u52a1\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faITFormer\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u5668\u4e0e\u51bb\u7ed3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u53d6\u3001\u5bf9\u9f50\u5e76\u878d\u5408\u65f6\u5e8f\u4e0e\u6587\u672c\u7279\u5f81\u3002", "result": "ITFormer\u5728\u95ee\u7b54\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14\u4ec5\u589e\u52a0\u4e0d\u52301%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "conclusion": "\u7814\u7a76\u4e3a\u591a\u6a21\u6001AI\u4e2d\u65f6\u5e8f\u6570\u636e\u4e0e\u81ea\u7136\u8bed\u8a00\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u9ad8\u6548\u8303\u5f0f\uff0c\u63a8\u52a8\u4e86\u76f8\u5173\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2506.20214", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.20214", "abs": "https://arxiv.org/abs/2506.20214", "authors": ["Yanzhe Chen", "Huasong Zhong", "Yan Li", "Zhenheng Yang"], "title": "UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation", "comment": "19 pages, 5 figures", "summary": "Unified multimodal large language models (MLLMs) have shown promise in\njointly advancing multimodal understanding and generation, with visual\ncodebooks discretizing images into tokens for autoregressive modeling. Existing\ncodebook-based methods either rely on small vocabularies (~16K entries) that\nlack fine-grained semantics or naively scale up, resulting in low token\nutilization and unstable training. We propose UniCode$^2$, a cascaded codebook\nframework enabling large-scale, semantically aligned, and stable visual\ntokenization. By clustering millions of SigLIP sequence embeddings, we build a\n500K-entry codebook that preserves vision-language alignment while expanding\ncapacity. Stability is ensured via a cascaded design: a frozen codebook anchors\nthe embedding space, and a trainable codebook refines task-specific semantics.\nThis decoupling promotes high utilization and robust learning. Moreover, the\nalignment of our visual tokens with textual semantics enables seamless\nintegration with pretrained diffusion decoders, supporting high-quality visual\nsynthesis with minimal adaptation. UniCode^2 delivers strong performance across\ndiverse benchmarks, demonstrating the viability of scaling visual token spaces\nwithout sacrificing stability, semantics, or modularity.", "AI": {"tldr": "UniCode^2\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea7\u8054\u7801\u672c\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u3001\u8bed\u4e49\u5bf9\u9f50\u4e14\u7a33\u5b9a\u7684\u89c6\u89c9\u6807\u8bb0\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bcd\u6c47\u91cf\u5c0f\u6216\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7801\u672c\u7684\u65b9\u6cd5\u8981\u4e48\u8bcd\u6c47\u91cf\u5c0f\uff08\u7ea616K\u6761\u76ee\uff09\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8bed\u4e49\uff0c\u8981\u4e48\u76f2\u76ee\u6269\u5c55\u5bfc\u81f4\u6807\u8bb0\u5229\u7528\u7387\u4f4e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u901a\u8fc7\u805a\u7c7b\u6570\u767e\u4e07SigLIP\u5e8f\u5217\u5d4c\u5165\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a50\u4e07\u6761\u76ee\u7684\u7801\u672c\uff0c\u91c7\u7528\u7ea7\u8054\u8bbe\u8ba1\uff1a\u51bb\u7ed3\u7801\u672c\u951a\u5b9a\u5d4c\u5165\u7a7a\u95f4\uff0c\u53ef\u8bad\u7ec3\u7801\u672c\u7ec6\u5316\u4efb\u52a1\u7279\u5b9a\u8bed\u4e49\u3002", "result": "UniCode^2\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u9ad8\u8d28\u91cf\u89c6\u89c9\u5408\u6210\uff0c\u4e14\u65e0\u9700\u727a\u7272\u7a33\u5b9a\u6027\u3001\u8bed\u4e49\u6216\u6a21\u5757\u6027\u3002", "conclusion": "UniCode^2\u8bc1\u660e\u4e86\u5728\u4e0d\u727a\u7272\u7a33\u5b9a\u6027\u3001\u8bed\u4e49\u6216\u6a21\u5757\u6027\u7684\u524d\u63d0\u4e0b\u6269\u5c55\u89c6\u89c9\u6807\u8bb0\u7a7a\u95f4\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2506.20249", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.20249", "abs": "https://arxiv.org/abs/2506.20249", "authors": ["Junyan Cheng", "Peter Clark", "Kyle Richardson"], "title": "Language Modeling by Language Models", "comment": null, "summary": "Can we leverage LLMs to model the process of discovering novel language model\n(LM) architectures? Inspired by real research, we propose a multi-agent LLM\napproach that simulates the conventional stages of research, from ideation and\nliterature search (proposal stage) to design implementation (code generation),\ngenerative pre-training, and downstream evaluation (verification). Using ideas\nfrom scaling laws, our system, Genesys, employs a Ladder of Scales approach;\nnew designs are proposed, adversarially reviewed, implemented, and selectively\nverified at increasingly larger model scales (14M$\\sim$350M parameters) with a\nnarrowing budget (the number of models we can train at each scale). To help\nmake discovery efficient and factorizable, Genesys uses a novel genetic\nprogramming backbone, which we show has empirical advantages over commonly used\ndirect prompt generation workflows (e.g., $\\sim$86\\% percentage point\nimprovement in successful design generation, a key bottleneck). We report\nexperiments involving 1,162 newly discovered designs (1,062 fully verified\nthrough pre-training) and find the best designs to be highly competitive with\nknown architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common\nbenchmarks). We couple these results with comprehensive system-level ablations\nand formal results, which give broader insights into the design of effective\nautonomous discovery systems.", "AI": {"tldr": "\u5229\u7528\u591a\u667a\u80fd\u4f53LLM\u6a21\u62df\u7814\u7a76\u8fc7\u7a0b\uff0c\u63d0\u51faGenesys\u7cfb\u7edf\uff0c\u901a\u8fc7\u9057\u4f20\u7f16\u7a0b\u751f\u6210\u65b0\u67b6\u6784\u8bbe\u8ba1\uff0c\u5728\u591a\u4e2a\u89c4\u6a21\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u5df2\u77e5\u67b6\u6784\u3002", "motivation": "\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u5229\u7528LLM\u6a21\u62df\u53d1\u73b0\u65b0\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u7684\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u7814\u7a76\u6548\u7387\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53LLM\u65b9\u6cd5\uff0c\u7ed3\u5408\u9057\u4f20\u7f16\u7a0b\u548cLadder of Scales\u7b56\u7565\uff0c\u5206\u9636\u6bb5\u751f\u6210\u3001\u9a8c\u8bc1\u8bbe\u8ba1\u3002", "result": "\u751f\u62101,162\u4e2a\u65b0\u8bbe\u8ba1\uff0c\u5176\u4e2d1,062\u4e2a\u901a\u8fc7\u9884\u8bad\u7ec3\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8eGPT2\u548cMamba2\u7b49\u67b6\u6784\u3002", "conclusion": "Genesys\u7cfb\u7edf\u5728\u81ea\u4e3b\u53d1\u73b0\u65b0\u67b6\u6784\u65b9\u9762\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\uff0c\u4e3a\u81ea\u52a8\u5316\u7814\u7a76\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.20100", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20100", "abs": "https://arxiv.org/abs/2506.20100", "authors": ["Vardhan Dongre", "Chi Gui", "Shubham Garg", "Hooshang Nayyeri", "Gokhan Tur", "Dilek Hakkani-T\u00fcr", "Vikram S. Adve"], "title": "MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations", "comment": "66 pages, 32 figures, 23 tables", "summary": "We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning\nand decision-making in consultative interaction settings. Designed for the\nagriculture domain, MIRAGE captures the full complexity of expert consultations\nby combining natural user queries, expert-authored responses, and image-based\ncontext, offering a high-fidelity benchmark for evaluating models on grounded\nreasoning, clarification strategies, and long-form generation in a real-world,\nknowledge-intensive domain. Grounded in over 35,000 real user-expert\ninteractions and curated through a carefully designed multi-step pipeline,\nMIRAGE spans diverse crop health, pest diagnosis, and crop management\nscenarios. The benchmark includes more than 7,000 unique biological entities,\ncovering plant species, pests, and diseases, making it one of the most\ntaxonomically diverse benchmarks available for vision-language models, grounded\nin the real world. Unlike existing benchmarks that rely on well-specified user\ninputs and closed-set taxonomies, MIRAGE features underspecified, context-rich\nscenarios with open-world settings, requiring models to infer latent knowledge\ngaps, handle rare entities, and either proactively guide the interaction or\nrespond. Project Page: https://mirage-benchmark.github.io", "AI": {"tldr": "MIRAGE\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u519c\u4e1a\u9886\u57df\u7684\u4e13\u5bb6\u7ea7\u63a8\u7406\u548c\u51b3\u7b56\uff0c\u7ed3\u5408\u81ea\u7136\u7528\u6237\u67e5\u8be2\u3001\u4e13\u5bb6\u56de\u7b54\u548c\u56fe\u50cf\u4e0a\u4e0b\u6587\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u4f9d\u8d56\u660e\u786e\u8f93\u5165\u548c\u5c01\u95ed\u5206\u7c7b\uff0c\u800cMIRAGE\u65e8\u5728\u6355\u6349\u771f\u5b9e\u54a8\u8be2\u573a\u666f\u7684\u590d\u6742\u6027\uff0c\u5305\u62ec\u672a\u660e\u786e\u8f93\u5165\u548c\u5f00\u653e\u4e16\u754c\u8bbe\u7f6e\uff0c\u4ee5\u63a8\u52a8\u6a21\u578b\u5728\u63a8\u7406\u548c\u751f\u6210\u65b9\u9762\u7684\u8fdb\u6b65\u3002", "method": "\u57fa\u4e8e35,000\u591a\u4e2a\u771f\u5b9e\u7528\u6237-\u4e13\u5bb6\u4ea4\u4e92\u6570\u636e\uff0c\u901a\u8fc7\u591a\u6b65\u9aa4\u6d41\u7a0b\u6784\u5efa\uff0c\u6db5\u76d6\u4f5c\u7269\u5065\u5eb7\u3001\u5bb3\u866b\u8bca\u65ad\u548c\u7ba1\u7406\u573a\u666f\uff0c\u5305\u542b7,000\u591a\u4e2a\u751f\u7269\u5b9e\u4f53\u3002", "result": "MIRAGE\u6210\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5206\u7c7b\u6700\u591a\u6837\u5316\u7684\u57fa\u51c6\u4e4b\u4e00\uff0c\u652f\u6301\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e0b\u7684\u63a8\u7406\u548c\u751f\u6210\u4efb\u52a1\u3002", "conclusion": "MIRAGE\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u4fdd\u771f\u57fa\u51c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\u3002"}}
{"id": "2506.20222", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20222", "abs": "https://arxiv.org/abs/2506.20222", "authors": ["Pujing Yang", "Guangyi Zhang", "Yunlong Cai", "Lei Yu", "Guanding Yu"], "title": "Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission", "comment": null, "summary": "Event cameras asynchronously capture pixel-level intensity changes with\nextremely low latency. They are increasingly used in conjunction with RGB\ncameras for a wide range of vision-related applications. However, a major\nchallenge in these hybrid systems lies in the transmission of the large volume\nof triggered events and RGB images. To address this, we propose a transmission\nscheme that retains efficient reconstruction performance of both sources while\naccomplishing real-time deblurring in parallel. Conventional RGB cameras and\nevent cameras typically capture the same scene in different ways, often\nresulting in significant redundant information across their outputs. To address\nthis, we develop a joint event and image (E-I) transmission framework to\neliminate redundancy and thereby optimize channel bandwidth utilization. Our\napproach employs Bayesian modeling and the information bottleneck method to\ndisentangle the shared and domain-specific information within the E-I inputs.\nThis disentangled information bottleneck framework ensures both the compactness\nand informativeness of extracted shared and domain-specific information.\nMoreover, it adaptively allocates transmission bandwidth based on scene\ndynamics, i.e., more symbols are allocated to events for dynamic details or to\nimages for static information. Simulation results demonstrate that the proposed\nscheme not only achieves superior reconstruction quality compared to\nconventional systems but also delivers enhanced deblurring performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4e8b\u4ef6\u548c\u56fe\u50cf\uff08E-I\uff09\u4f20\u8f93\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u5efa\u6a21\u548c\u4fe1\u606f\u74f6\u9888\u65b9\u6cd5\u6d88\u9664\u5197\u4f59\uff0c\u4f18\u5316\u5e26\u5bbd\u5229\u7528\uff0c\u5e76\u5b9e\u73b0\u5b9e\u65f6\u53bb\u6a21\u7cca\u3002", "motivation": "\u6df7\u5408\u7cfb\u7edf\u4e2d\u4e8b\u4ef6\u76f8\u673a\u548cRGB\u76f8\u673a\u4f20\u8f93\u5927\u91cf\u6570\u636e\u5b58\u5728\u6311\u6218\uff0c\u4e14\u4e24\u8005\u8f93\u51fa\u5b58\u5728\u5197\u4f59\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u8d1d\u53f6\u65af\u5efa\u6a21\u548c\u4fe1\u606f\u74f6\u9888\u65b9\u6cd5\u5206\u79bb\u5171\u4eab\u548c\u9886\u57df\u7279\u5b9a\u4fe1\u606f\uff0c\u52a8\u6001\u5206\u914d\u4f20\u8f93\u5e26\u5bbd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u53bb\u6a21\u7cca\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7cfb\u7edf\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u4f18\u5316\u4e86\u5e26\u5bbd\u5229\u7528\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u91cd\u5efa\u548c\u53bb\u6a21\u7cca\u6548\u679c\u3002"}}
{"id": "2506.20274", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20274", "abs": "https://arxiv.org/abs/2506.20274", "authors": ["Liya Wang", "David Yi", "Damien Jose", "John Passarelli", "James Gao", "Jordan Leventis", "Kang Li"], "title": "Enterprise Large Language Model Evaluation Benchmark", "comment": "Submitted to MLNLP 2025 at https://csity2025.org/mlnlp/index", "summary": "Large Language Models (LLMs) ) have demonstrated promise in boosting\nproductivity across AI-powered tools, yet existing benchmarks like Massive\nMultitask Language Understanding (MMLU) inadequately assess enterprise-specific\ntask complexities. We propose a 14-task framework grounded in Bloom's Taxonomy\nto holistically evaluate LLM capabilities in enterprise contexts. To address\nchallenges of noisy data and costly annotation, we develop a scalable pipeline\ncombining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented\ngeneration (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six\nleading models shows open-source contenders like DeepSeek R1 rival proprietary\nmodels in reasoning tasks but lag in judgment-based scenarios, likely due to\noverthinking. Our benchmark reveals critical enterprise performance gaps and\noffers actionable insights for model optimization. This work provides\nenterprises a blueprint for tailored evaluations and advances practical LLM\ndeployment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eBloom\u5206\u7c7b\u6cd5\u768414\u4efb\u52a1\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6807\u6ce8\u4e0e\u8bc4\u4f30\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\uff08\u5982MMLU\uff09\u672a\u80fd\u5145\u5206\u8bc4\u4f30\u4f01\u4e1a\u7279\u5b9a\u4efb\u52a1\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408LLM-as-a-Labeler\u3001LLM-as-a-Judge\u548cCRAG\u6280\u672f\uff0c\u6784\u5efa\u4e869,700\u6837\u672c\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u5f00\u6e90\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u63a5\u8fd1\u4e13\u6709\u6a21\u578b\uff0c\u4f46\u5728\u5224\u65ad\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u53ef\u80fd\u56e0\u8fc7\u5ea6\u601d\u8003\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4f01\u4e1a\u63d0\u4f9b\u4e86\u5b9a\u5236\u5316\u8bc4\u4f30\u7684\u84dd\u56fe\uff0c\u5e76\u63a8\u52a8\u4e86LLM\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2506.20112", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.20112", "abs": "https://arxiv.org/abs/2506.20112", "authors": ["Songsoo Kim", "Seungtae Lee", "See Young Lee", "Joonho Kim", "Keechan Kan", "Dukyong Yoon"], "title": "A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection", "comment": "29 pages, 5 figures, 4 tables. Code available at\n  https://github.com/radssk/mp-rred", "summary": "Background: The positive predictive value (PPV) of large language model\n(LLM)-based proofreading for radiology reports is limited due to the low error\nprevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV\nand reduces operational costs compared with baseline approaches. Materials and\nMethods: A retrospective analysis was performed on 1,000 consecutive radiology\nreports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III\ndatabase. Two external datasets (CheXpert and Open-i) were validation sets.\nThree LLM frameworks were tested: (1) single-prompt detector; (2) extractor\nplus detector; and (3) extractor, detector, and false-positive verifier.\nPrecision was measured by PPV and absolute true positive rate (aTPR).\nEfficiency was calculated from model inference charges and reviewer\nremuneration. Statistical significance was tested using cluster bootstrap,\nexact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV\nincreased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,\nFramework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.\nbaselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per\n1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and\nUSD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.\nHuman-reviewed reports decreased from 192 to 88. External validation supported\nFramework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR\n(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and\nreduced operational costs, maintaining detection performance, providing an\neffective strategy for AI-assisted radiology report quality assurance.", "AI": {"tldr": "\u4e09\u9636\u6bb5LLM\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u653e\u5c04\u5b66\u62a5\u544a\u7684\u6b63\u9884\u6d4b\u503c\uff08PPV\uff09\u5e76\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u3002", "motivation": "\u7531\u4e8e\u9519\u8bef\u7387\u4f4e\uff0c\u57fa\u4e8eLLM\u7684\u653e\u5c04\u5b66\u62a5\u544a\u6821\u5bf9\u7684\u6b63\u9884\u6d4b\u503c\u6709\u9650\uff0c\u9700\u6539\u8fdb\u6846\u67b6\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u4e09\u9636\u6bb5LLM\u6846\u67b6\uff08\u68c0\u6d4b\u5668\u3001\u63d0\u53d6\u5668\u52a0\u68c0\u6d4b\u5668\u3001\u63d0\u53d6\u5668\u52a0\u68c0\u6d4b\u5668\u52a0\u5047\u9633\u6027\u9a8c\u8bc1\u5668\uff09\u5206\u67901,000\u4efd\u653e\u5c04\u5b66\u62a5\u544a\uff0c\u5e76\u9a8c\u8bc1\u4e8e\u5916\u90e8\u6570\u636e\u96c6\u3002", "result": "\u4e09\u9636\u6bb5\u6846\u67b6PPV\u663e\u8457\u63d0\u5347\u81f30.159\uff0c\u8fd0\u8425\u6210\u672c\u964d\u4f4e42.6%\uff0c\u68c0\u6d4b\u6027\u80fd\u7a33\u5b9a\u3002", "conclusion": "\u4e09\u9636\u6bb5LLM\u6846\u67b6\u4e3aAI\u8f85\u52a9\u653e\u5c04\u5b66\u62a5\u544a\u8d28\u91cf\u4fdd\u8bc1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7b56\u7565\u3002"}}
{"id": "2506.20254", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20254", "abs": "https://arxiv.org/abs/2506.20254", "authors": ["Kun Yuan", "Tingxuan Chen", "Shi Li", "Joel L. Lavanchy", "Christian Heiliger", "Ege \u00d6zsoy", "Yiming Huang", "Long Bai", "Nassir Navab", "Vinkle Srivastav", "Hongliang Ren", "Nicolas Padoy"], "title": "Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and Task-graph Guided Refinement", "comment": "Accepted by MICCAI 2025", "summary": "The complexity and diversity of surgical workflows, driven by heterogeneous\noperating room settings, institutional protocols, and anatomical variability,\npresent a significant challenge in developing generalizable models for\ncross-institutional and cross-procedural surgical understanding. While recent\nsurgical foundation models pretrained on large-scale vision-language data offer\npromising transferability, their zero-shot performance remains constrained by\ndomain shifts, limiting their utility in unseen surgical environments. To\naddress this, we introduce Surgical Phase Anywhere (SPA), a lightweight\nframework for versatile surgical workflow understanding that adapts foundation\nmodels to institutional settings with minimal annotation. SPA leverages\nfew-shot spatial adaptation to align multi-modal embeddings with\ninstitution-specific surgical scenes and phases. It also ensures temporal\nconsistency through diffusion modeling, which encodes task-graph priors derived\nfrom institutional procedure protocols. Finally, SPA employs dynamic test-time\nadaptation, exploiting the mutual agreement between multi-modal phase\nprediction streams to adapt the model to a given test video in a\nself-supervised manner, enhancing the reliability under test-time distribution\nshifts. SPA is a lightweight adaptation framework, allowing hospitals to\nrapidly customize phase recognition models by defining phases in natural\nlanguage text, annotating a few images with the phase labels, and providing a\ntask graph defining phase transitions. The experimental results show that the\nSPA framework achieves state-of-the-art performance in few-shot surgical phase\nrecognition across multiple institutions and procedures, even outperforming\nfull-shot models with 32-shot labeled data. Code is available at\nhttps://github.com/CAMMA-public/SPA", "AI": {"tldr": "SPA\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u5c11\u91cf\u6807\u6ce8\u548c\u591a\u6a21\u6001\u9002\u5e94\uff0c\u63d0\u5347\u624b\u672f\u5de5\u4f5c\u6d41\u7406\u89e3\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u624b\u672f\u5de5\u4f5c\u6d41\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u5bfc\u81f4\u8de8\u673a\u6784\u548c\u8de8\u624b\u672f\u7684\u901a\u7528\u6a21\u578b\u5f00\u53d1\u56f0\u96be\uff0c\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5728\u96f6\u6837\u672c\u6027\u80fd\u4e0a\u53d7\u9650\u4e8e\u9886\u57df\u504f\u79fb\u3002", "method": "SPA\u7ed3\u5408\u5c11\u91cf\u7a7a\u95f4\u9002\u5e94\u3001\u6269\u6563\u5efa\u6a21\u548c\u52a8\u6001\u6d4b\u8bd5\u65f6\u9002\u5e94\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5b9a\u4e49\u9636\u6bb5\u3001\u5c11\u91cf\u6807\u6ce8\u548c\u4efb\u52a1\u56fe\u5feb\u901f\u5b9a\u5236\u6a21\u578b\u3002", "result": "SPA\u5728\u5c11\u91cf\u6807\u6ce8\u4e0b\u5b9e\u73b0\u4e86\u8de8\u673a\u6784\u548c\u624b\u672f\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u751a\u81f3\u4f18\u4e8e\u5168\u6807\u6ce8\u6a21\u578b\u3002", "conclusion": "SPA\u4e3a\u533b\u9662\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u65b9\u6cd5\uff0c\u5feb\u901f\u9002\u5e94\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u6a21\u578b\u3002"}}
{"id": "2506.20332", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20332", "abs": "https://arxiv.org/abs/2506.20332", "authors": ["Jihao Gu", "Qihang Ai", "Yingyao Wang", "Pi Bu", "Jingxuan Xing", "Zekun Zhu", "Wei Jiang", "Ziming Wang", "Yingxiu Zhao", "Ming-Liang Zhang", "Jun Song", "Yuning Jiang", "Bo Zheng"], "title": "Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards", "comment": "14 pages, 12 figures", "summary": "Vision-language model-based mobile agents have gained the ability to not only\nunderstand complex instructions and mobile screenshots, but also optimize their\naction outputs via thinking and reasoning, benefiting from reinforcement\nlearning, such as Group Relative Policy Optimization (GRPO). However, existing\nresearch centers on offline reinforcement learning training or online\noptimization using action-level rewards, which limits the agent's dynamic\ninteraction with the environment. This often results in agents settling into\nlocal optima, thereby weakening their ability for exploration and error action\ncorrection. To address these challenges, we introduce an approach called\nMobile-R1, which employs interactive multi-turn reinforcement learning with\ntask-level rewards for mobile agents. Our training framework consists of three\nstages: initial format finetuning, single-step online training via action-level\nreward, followed by online training via task-level reward based on multi-turn\ntrajectories. This strategy is designed to enhance the exploration and error\ncorrection capabilities of Mobile-R1, leading to significant performance\nimprovements. Moreover, we have collected a dataset covering 28 Chinese\napplications with 24,521 high-quality manual annotations and established a new\nbenchmark with 500 trajectories. We will open source all resources, including\nthe dataset, benchmark, model weight, and codes:\nhttps://mobile-r1.github.io/Mobile-R1/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMobile-R1\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u5f0f\u5f3a\u5316\u5b66\u4e60\u4e0e\u4efb\u52a1\u7ea7\u5956\u52b1\u63d0\u5347\u79fb\u52a8\u4ee3\u7406\u7684\u63a2\u7d22\u4e0e\u7ea0\u9519\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c40\u9650\u4e8e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6216\u52a8\u4f5c\u7ea7\u5956\u52b1\u7684\u5728\u7ebf\u4f18\u5316\uff0c\u5bfc\u81f4\u4ee3\u7406\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u63a2\u7d22\u4e0e\u7ea0\u9519\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u521d\u59cb\u683c\u5f0f\u5fae\u8c03\u3001\u52a8\u4f5c\u7ea7\u5956\u52b1\u5355\u6b65\u5728\u7ebf\u8bad\u7ec3\u3001\u4efb\u52a1\u7ea7\u5956\u52b1\u591a\u8f6e\u8f68\u8ff9\u5728\u7ebf\u8bad\u7ec3\u3002", "result": "\u663e\u8457\u63d0\u5347\u4ee3\u7406\u6027\u80fd\uff0c\u5e76\u6784\u5efa\u5305\u542b28\u4e2a\u4e2d\u6587\u5e94\u7528\u7684\u6570\u636e\u96c6\u4e0e500\u8f68\u8ff9\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "Mobile-R1\u901a\u8fc7\u4efb\u52a1\u7ea7\u5956\u52b1\u548c\u591a\u8f6e\u8bad\u7ec3\u6709\u6548\u589e\u5f3a\u4ee3\u7406\u80fd\u529b\uff0c\u76f8\u5173\u8d44\u6e90\u5c06\u5f00\u6e90\u3002"}}
{"id": "2506.20119", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20119", "abs": "https://arxiv.org/abs/2506.20119", "authors": ["Masaki Uto", "Yuma Ito"], "title": "Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests", "comment": "Accepted to EvalLAC'25: 2nd Workshop on Automatic Evaluation of\n  Learning and Assessment Content, held at AIED 2025, Palermo, Italy. This is\n  the camera-ready version submitted to CEUR Workshop Proceedings", "summary": "Evaluating the abilities of learners is a fundamental objective in the field\nof education. In particular, there is an increasing need to assess higher-order\nabilities such as expressive skills and logical thinking. Constructed-response\ntests such as short-answer and essay-based questions have become widely used as\na method to meet this demand. Although these tests are effective, they require\nsubstantial manual grading, making them both labor-intensive and costly. Item\nresponse theory (IRT) provides a promising solution by enabling the estimation\nof ability from incomplete score data, where human raters grade only a subset\nof answers provided by learners across multiple test items. However, the\naccuracy of ability estimation declines as the proportion of missing scores\nincreases. Although data augmentation techniques for imputing missing scores\nhave been explored in order to address this limitation, they often struggle\nwith inaccuracy for sparse or heterogeneous data. To overcome these challenges,\nthis study proposes a novel method for imputing missing scores by leveraging\nautomated scoring technologies for accurate IRT-based ability estimation. The\nproposed method achieves high accuracy in ability estimation while markedly\nreducing manual grading workload.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u81ea\u52a8\u8bc4\u5206\u6280\u672f\u586b\u8865\u7f3a\u5931\u5206\u6570\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8IRT\u80fd\u529b\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u8bc4\u5206\u5de5\u4f5c\u91cf\u3002", "motivation": "\u8bc4\u4f30\u5b66\u4e60\u8005\u7684\u9ad8\u9636\u80fd\u529b\uff08\u5982\u8868\u8fbe\u80fd\u529b\u548c\u903b\u8f91\u601d\u7ef4\uff09\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u4f20\u7edf\u6784\u5efa\u53cd\u5e94\u6d4b\u8bd5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u8bc4\u5206\uff0c\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002IRT\u867d\u80fd\u4ece\u4e0d\u5b8c\u6574\u5206\u6570\u6570\u636e\u4f30\u8ba1\u80fd\u529b\uff0c\u4f46\u7f3a\u5931\u5206\u6570\u6bd4\u4f8b\u589e\u52a0\u65f6\u51c6\u786e\u6027\u4e0b\u964d\u3002", "method": "\u5229\u7528\u81ea\u52a8\u8bc4\u5206\u6280\u672f\u586b\u8865\u7f3a\u5931\u5206\u6570\uff0c\u7ed3\u5408IRT\u8fdb\u884c\u80fd\u529b\u4f30\u8ba1\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u80fd\u529b\u4f30\u8ba1\u4e2d\u5b9e\u73b0\u9ad8\u51c6\u786e\u6027\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u8bc4\u5206\u5de5\u4f5c\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89e3\u51b3IRT\u5728\u7a00\u758f\u6216\u5f02\u6784\u6570\u636e\u4e2d\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u540c\u65f6\u4f18\u5316\u4e86\u8bc4\u5206\u6548\u7387\u3002"}}
{"id": "2506.20255", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20255", "abs": "https://arxiv.org/abs/2506.20255", "authors": ["Ayush Lodh", "Ritabrata Chakraborty", "Shivakumara Palaiahnakote", "Umapada Pal"], "title": "A Transformer Based Handwriting Recognition System Jointly Using Online and Offline Features", "comment": "15 pages, 7 figures", "summary": "We posit that handwriting recognition benefits from complementary cues\ncarried by the rasterized complex glyph and the pen's trajectory, yet most\nsystems exploit only one modality. We introduce an end-to-end network that\nperforms early fusion of offline images and online stroke data within a shared\nlatent space. A patch encoder converts the grayscale crop into fixed-length\nvisual tokens, while a lightweight transformer embeds the $(x, y, \\text{pen})$\nsequence. Learnable latent queries attend jointly to both token streams,\nyielding context-enhanced stroke embeddings that are pooled and decoded under a\ncross-entropy loss objective. Because integration occurs before any high-level\nclassification, temporal cues reinforce each other during representation\nlearning, producing stronger writer independence. Comprehensive experiments on\nIAMOn-DB and VNOn-DB demonstrate that our approach achieves state-of-the-art\naccuracy, exceeding previous bests by up to 1\\%. Our study also shows\nadaptation of this pipeline with gesturification on the ISI-Air dataset. Our\ncode can be found here.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u79bb\u7ebf\u56fe\u50cf\u548c\u5728\u7ebf\u7b14\u753b\u6570\u636e\u7684\u7aef\u5230\u7aef\u7f51\u7edc\uff0c\u901a\u8fc7\u65e9\u671f\u878d\u5408\u63d0\u5347\u624b\u5199\u8bc6\u522b\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u624b\u5199\u8bc6\u522b\u901a\u5e38\u4ec5\u5229\u7528\u5355\u4e00\u6a21\u6001\uff08\u5982\u79bb\u7ebf\u56fe\u50cf\u6216\u5728\u7ebf\u7b14\u753b\u6570\u636e\uff09\uff0c\u800c\u8bba\u6587\u8ba4\u4e3a\u7ed3\u5408\u4e24\u79cd\u6a21\u6001\u7684\u4e92\u8865\u7ebf\u7d22\u80fd\u63d0\u5347\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7f51\u7edc\uff0c\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u65e9\u671f\u878d\u5408\u79bb\u7ebf\u56fe\u50cf\u548c\u5728\u7ebf\u7b14\u753b\u6570\u636e\uff0c\u4f7f\u7528\u8865\u4e01\u7f16\u7801\u5668\u548c\u8f7b\u91cf\u7ea7Transformer\u5904\u7406\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6f5c\u5728\u67e5\u8be2\u589e\u5f3a\u4e0a\u4e0b\u6587\u3002", "result": "\u5728IAMOn-DB\u548cVNOn-DB\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u65b9\u6cd5\u63d0\u53471%\u3002", "conclusion": "\u65e9\u671f\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u80fd\u589e\u5f3a\u8868\u793a\u5b66\u4e60\uff0c\u63d0\u5347\u624b\u5199\u8bc6\u522b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.20357", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20357", "abs": "https://arxiv.org/abs/2506.20357", "authors": ["Sungwon Han", "Sungkyu Park", "Seungeon Lee"], "title": "Tabular Feature Discovery With Reasoning Type Exploration", "comment": null, "summary": "Feature engineering for tabular data remains a critical yet challenging step\nin machine learning. Recently, large language models (LLMs) have been used to\nautomatically generate new features by leveraging their vast knowledge.\nHowever, existing LLM-based approaches often produce overly simple or\nrepetitive features, partly due to inherent biases in the transformations the\nLLM chooses and the lack of structured reasoning guidance during generation. In\nthis paper, we propose a novel method REFeat, which guides an LLM to discover\ndiverse and informative features by leveraging multiple types of reasoning to\nsteer the feature generation process. Experiments on 59 benchmark datasets\ndemonstrate that our approach not only achieves higher predictive accuracy on\naverage, but also discovers more diverse and meaningful features. These results\nhighlight the promise of incorporating rich reasoning paradigms and adaptive\nstrategy selection into LLM-driven feature discovery for tabular data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aREFeat\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7c7b\u578b\u63a8\u7406\u5f15\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u591a\u6837\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u65b9\u6cd5\u751f\u6210\u7279\u5f81\u8fc7\u4e8e\u7b80\u5355\u6216\u91cd\u590d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u7279\u5f81\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u751f\u6210\u7279\u5f81\u8fc7\u4e8e\u7b80\u5355\u6216\u91cd\u590d\u7684\u95ee\u9898\uff0c\u90e8\u5206\u539f\u56e0\u662fLLM\u7684\u56fa\u6709\u504f\u89c1\u548c\u7f3a\u4e4f\u7ed3\u6784\u5316\u63a8\u7406\u6307\u5bfc\u3002", "method": "\u63d0\u51faREFeat\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u7c7b\u578b\u63a8\u7406\u5f15\u5bfcLLM\u751f\u6210\u7279\u5f81\uff0c\u5b9e\u9a8c\u572859\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cREFeat\u4e0d\u4ec5\u5e73\u5747\u9884\u6d4b\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u8fd8\u80fd\u751f\u6210\u66f4\u591a\u6837\u4e14\u6709\u610f\u4e49\u7684\u7279\u5f81\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u5728LLM\u9a71\u52a8\u7684\u8868\u683c\u6570\u636e\u7279\u5f81\u53d1\u73b0\u4e2d\u7ed3\u5408\u4e30\u5bcc\u63a8\u7406\u8303\u5f0f\u548c\u81ea\u9002\u5e94\u7b56\u7565\u9009\u62e9\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.20128", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20128", "abs": "https://arxiv.org/abs/2506.20128", "authors": ["Aashiq Muhamed"], "title": "CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation", "comment": "Accepted at LLM4Eval @ SIGIR 2025", "summary": "RAG systems enhance LLMs by incorporating external knowledge, which is\ncrucial for domains that demand factual accuracy and up-to-date information.\nHowever, evaluating the multifaceted quality of RAG outputs, spanning aspects\nsuch as contextual coherence, query relevance, factual correctness, and\ninformational completeness, poses significant challenges. Existing evaluation\nmethods often rely on simple lexical overlap metrics, which are inadequate for\ncapturing these nuances, or involve complex multi-stage pipelines with\nintermediate steps like claim extraction or require finetuning specialized\njudge models, hindering practical efficiency. To address these limitations, we\npropose CCRS (Contextual Coherence and Relevance Score), a novel suite of five\nmetrics that utilizes a single, powerful, pretrained LLM as a zero-shot,\nend-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance\n(QR), Information Density (ID), Answer Correctness (AC), and Information Recall\n(IR). We apply CCRS to evaluate six diverse RAG system configurations on the\nchallenging BioASQ dataset. Our analysis demonstrates that CCRS effectively\ndiscriminates between system performances, confirming, for instance, that the\nMistral-7B reader outperforms Llama variants. We provide a detailed analysis of\nCCRS metric properties, including score distributions, convergent/discriminant\nvalidity, tie rates, population statistics, and discriminative power. Compared\nto the complex RAGChecker framework, CCRS offers comparable or superior\ndiscriminative power for key aspects like recall and faithfulness, while being\nsignificantly more computationally efficient. CCRS thus provides a practical,\ncomprehensive, and efficient framework for evaluating and iteratively improving\nRAG systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCCRS\u7684\u65b0\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30RAG\u7cfb\u7edf\u7684\u591a\u7ef4\u5ea6\u8d28\u91cf\uff0c\u5305\u62ec\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\u3001\u95ee\u9898\u76f8\u5173\u6027\u7b49\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u5168\u9762\u6355\u6349RAG\u8f93\u51fa\u7684\u591a\u7ef4\u8d28\u91cf\uff08\u5982\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u4fe1\u606f\u5b8c\u6574\u6027\u7b49\uff09\uff0c\u4e14\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faCCRS\uff0c\u57fa\u4e8e\u9884\u8bad\u7ec3LLM\u7684\u96f6\u6837\u672c\u7aef\u5230\u7aef\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u4e94\u4e2a\u6307\u6807\uff1a\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\u3001\u95ee\u9898\u76f8\u5173\u6027\u7b49\u3002", "result": "\u5728BioASQ\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cCCRS\u80fd\u6709\u6548\u533a\u5206\u4e0d\u540cRAG\u7cfb\u7edf\u6027\u80fd\uff0c\u4e14\u6bd4RAGChecker\u66f4\u9ad8\u6548\u3002", "conclusion": "CCRS\u4e3aRAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u5168\u9762\u4e14\u9ad8\u6548\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2506.20263", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20263", "abs": "https://arxiv.org/abs/2506.20263", "authors": ["Ning Luo", "Meiyin Hu", "Huan Wan", "Yanyan Yang", "Zhuohang Jiang", "Xin Wei"], "title": "Hierarchical Mask-Enhanced Dual Reconstruction Network for Few-Shot Fine-Grained Image Classification", "comment": null, "summary": "Few-shot fine-grained image classification (FS-FGIC) presents a significant\nchallenge, requiring models to distinguish visually similar subclasses with\nlimited labeled examples. Existing methods have critical limitations:\nmetric-based methods lose spatial information and misalign local features,\nwhile reconstruction-based methods fail to utilize hierarchical feature\ninformation and lack mechanisms to focus on discriminative regions. We propose\nthe Hierarchical Mask-enhanced Dual Reconstruction Network (HMDRN), which\nintegrates dual-layer feature reconstruction with mask-enhanced feature\nprocessing to improve fine-grained classification. HMDRN incorporates a\ndual-layer feature reconstruction and fusion module that leverages\ncomplementary visual information from different network hierarchies. Through\nlearnable fusion weights, the model balances high-level semantic\nrepresentations from the last layer with mid-level structural details from the\npenultimate layer. Additionally, we design a spatial binary mask-enhanced\ntransformer self-reconstruction module that processes query features through\nadaptive thresholding while maintaining complete support features, enhancing\nfocus on discriminative regions while filtering background noise. Extensive\nexperiments on three challenging fine-grained datasets demonstrate that HMDRN\nconsistently outperforms state-of-the-art methods across Conv-4 and ResNet-12\nbackbone architectures. Comprehensive ablation studies validate the\neffectiveness of each proposed component, revealing that dual-layer\nreconstruction enhances inter-class discrimination while mask-enhanced\ntransformation reduces intra-class variations. Visualization results provide\nevidence of HMDRN's superior feature reconstruction capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHMDRN\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5c42\u6b21\u7279\u5f81\u91cd\u5efa\u548c\u63a9\u7801\u589e\u5f3a\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u6837\u672c\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4e2d\u5b58\u5728\u7a7a\u95f4\u4fe1\u606f\u4e22\u5931\u3001\u5c40\u90e8\u7279\u5f81\u9519\u4f4d\u3001\u7f3a\u4e4f\u5c42\u6b21\u7279\u5f81\u5229\u7528\u548c\u5224\u522b\u533a\u57df\u5173\u6ce8\u673a\u5236\u7b49\u95ee\u9898\u3002", "method": "HMDRN\u7ed3\u5408\u53cc\u5c42\u6b21\u7279\u5f81\u91cd\u5efa\u4e0e\u63a9\u7801\u589e\u5f3a\u5904\u7406\uff0c\u5229\u7528\u4e92\u8865\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u5904\u7406\u589e\u5f3a\u5224\u522b\u533a\u57df\u3002", "result": "\u5728\u4e09\u4e2a\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\uff0cHMDRN\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u7684\u6709\u6548\u6027\u3002", "conclusion": "HMDRN\u901a\u8fc7\u53cc\u5c42\u6b21\u91cd\u5efa\u548c\u63a9\u7801\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u7c7b\u5185\u5dee\u5f02\u3002"}}
{"id": "2506.20384", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20384", "abs": "https://arxiv.org/abs/2506.20384", "authors": ["Dror Ivry", "Oran Nahum"], "title": "Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios", "comment": "6 pages, 2 figures", "summary": "This paper introduces two significant contributions to address the issue of\ngrounding claims in a given context. Grounding means that given a context\n(document) and a claim, there's at least one supportive evidence for the claim\nin the document. We will introduce Paladin-mini, a compact (3.8B parameters)\nopen-source classifier model (used for labeling data as grounded or ungrounded)\nengineered for robust performance in real-world scenarios, and the\ngrounding-benchmark, a new evaluation dataset designed to assess performance on\ncritical reasoning tasks. We'll also demonstrate the results of Paladin-mini\nwith benchmarks against the current State-of-the-art and share clear and\nreproducible results.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Paladin-mini\u6a21\u578b\u548cgrounding-benchmark\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u4e0a\u4e0b\u6587\u4e2d\u7684\u58f0\u660e\u9a8c\u8bc1\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5728\u7ed9\u5b9a\u4e0a\u4e0b\u6587\u4e2d\u9a8c\u8bc1\u58f0\u660e\u662f\u5426\u6709\u652f\u6301\u8bc1\u636e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faPaladin-mini\uff083.8B\u53c2\u6570\u7684\u5f00\u6e90\u5206\u7c7b\u6a21\u578b\uff09\u548cgrounding-benchmark\u8bc4\u4f30\u6570\u636e\u96c6\u3002", "result": "\u5c55\u793a\u4e86Paladin-mini\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u6280\u672f\u7684\u5bf9\u6bd4\u7ed3\u679c\uff0c\u6027\u80fd\u7a33\u5065\u4e14\u53ef\u590d\u73b0\u3002", "conclusion": "Paladin-mini\u548cgrounding-benchmark\u4e3a\u58f0\u660e\u9a8c\u8bc1\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.20160", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20160", "abs": "https://arxiv.org/abs/2506.20160", "authors": ["Ruosen Li", "Ziming Luo", "Quan Zhang", "Ruochen Li", "Ben Zhou", "Ali Payani", "Xinya Du"], "title": "AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control", "comment": null, "summary": "Large reasoning models (LRMs) achieve impressive reasoning capabilities by\ngenerating lengthy chain-of-thoughts, but this \"overthinking\" incurs high\nlatency and cost without commensurate accuracy gains. In this work, we\nintroduce AALC, a lightweight, accuracy-aware length reward integrated into\nreinforcement learning that dynamically balances correctness and brevity during\ntraining. By incorporating validation accuracy into the reward and employing a\nsmooth, dynamically scheduled length penalty, AALC delays length penalty until\ntarget performance is met. Through extensive experiments across standard and\nout-of-distribution math benchmarks, we show that our approach reduces response\nlength by over 50% while maintaining or even improving the original accuracy.\nFurthermore, qualitative analysis reveals that our method curbs redundant\nreasoning patterns such as excessive subgoal setting and verification, leading\nto structurally refined outputs rather than naive truncation. We also identify\nthat efficiency gains are accompanied by reduced interpretability: models\ntrained with AALC omit some narrative framing and explanatory context. These\nfindings highlight the potential of reward-based strategies to guide LRMs\ntoward more efficient, generalizable reasoning paths.", "AI": {"tldr": "AALC\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u51c6\u786e\u6027\u611f\u77e5\u957f\u5ea6\u5956\u52b1\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5e73\u8861\u6b63\u786e\u6027\u548c\u7b80\u6d01\u6027\uff0c\u663e\u8457\u51cf\u5c11\u63a8\u7406\u6a21\u578b\u7684\u54cd\u5e94\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u901a\u8fc7\u751f\u6210\u5197\u957f\u7684\u601d\u7ef4\u94fe\u83b7\u5f97\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8fd9\u79cd\u65b9\u5f0f\u5e26\u6765\u4e86\u9ad8\u5ef6\u8fdf\u548c\u6210\u672c\uff0c\u800c\u51c6\u786e\u6027\u63d0\u5347\u6709\u9650\u3002", "method": "AALC\u5c06\u9a8c\u8bc1\u51c6\u786e\u6027\u7eb3\u5165\u5956\u52b1\u673a\u5236\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u8c03\u5ea6\u7684\u957f\u5ea6\u60e9\u7f5a\uff0c\u5ef6\u8fdf\u957f\u5ea6\u60e9\u7f5a\u76f4\u81f3\u8fbe\u5230\u76ee\u6807\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAALC\u5c06\u54cd\u5e94\u957f\u5ea6\u51cf\u5c1150%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u6027\uff0c\u5e76\u51cf\u5c11\u5197\u4f59\u63a8\u7406\u6a21\u5f0f\u3002", "conclusion": "AALC\u5c55\u793a\u4e86\u57fa\u4e8e\u5956\u52b1\u7684\u7b56\u7565\u5728\u5f15\u5bfcLRMs\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u901a\u7528\u63a8\u7406\u8def\u5f84\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4f46\u4e5f\u53ef\u80fd\u964d\u4f4e\u6a21\u578b\u7684\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.20272", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20272", "abs": "https://arxiv.org/abs/2506.20272", "authors": ["Juan Jos\u00e9 Murillo-Fuentes", "Pablo M. Olmos", "Laura Alba-Carcel\u00e9n"], "title": "Forensic Study of Paintings Through the Comparison of Fabrics", "comment": null, "summary": "The study of canvas fabrics in works of art is a crucial tool for\nauthentication, attribution and conservation. Traditional methods are based on\nthread density map matching, which cannot be applied when canvases do not come\nfrom contiguous positions on a roll. This paper presents a novel approach based\non deep learning to assess the similarity of textiles. We introduce an\nautomatic tool that evaluates the similarity between canvases without relying\non thread density maps. A Siamese deep learning model is designed and trained\nto compare pairs of images by exploiting the feature representations learned\nfrom the scans. In addition, a similarity estimation method is proposed,\naggregating predictions from multiple pairs of cloth samples to provide a\nrobust similarity score. Our approach is applied to canvases from the Museo\nNacional del Prado, corroborating the hypothesis that plain weave canvases,\nwidely used in painting, can be effectively compared even when their thread\ndensities are similar. The results demonstrate the feasibility and accuracy of\nthe proposed method, opening new avenues for the analysis of masterpieces.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7eba\u7ec7\u54c1\u76f8\u4f3c\u6027\u8bc4\u4f30\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u827a\u672f\u54c1\u7684\u9274\u5b9a\u4e0e\u4fdd\u62a4\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7ebf\u5bc6\u5ea6\u56fe\u5339\u914d\u7684\u65b9\u6cd5\u65e0\u6cd5\u9002\u7528\u4e8e\u975e\u8fde\u7eed\u4f4d\u7f6e\u7684\u753b\u5e03\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86Siamese\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u56fe\u50cf\u5bf9\u6bd4\u8f83\u5b66\u4e60\u7279\u5f81\u8868\u793a\uff0c\u5e76\u63d0\u51fa\u76f8\u4f3c\u6027\u4f30\u8ba1\u65b9\u6cd5\u3002", "result": "\u5728Museo Nacional del Prado\u7684\u753b\u5e03\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5373\u4f7f\u7ebf\u5bc6\u5ea6\u76f8\u4f3c\u4e5f\u80fd\u6709\u6548\u6bd4\u8f83\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u827a\u672f\u54c1\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
