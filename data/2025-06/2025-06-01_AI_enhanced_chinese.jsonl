{"id": "2505.00018", "keyword": "LLM Agent", "pdf": "https://arxiv.org/pdf/2505.00018", "abs": "https://arxiv.org/abs/2505.00018", "authors": ["Ju Wu", "Calvin K. L. Or"], "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability.", "keywords": ["LLM Agent"], "AI": {"tldr": "This position paper critically surveys recent empirical developments on human-AI collaboration, proposes a new conceptual architecture to integrate these studies, and serves as both a review and a reference for future research.", "motivation": "We observe a lack of a unifying theoretical framework that can coherently integrate these varied studies, especially when tackling open-ended, complex tasks. To address this, we propose a novel conceptual architecture.", "method": "The paper's structure allows it to be read from any section, serving equally as a critical review of technical implementations and as a forward-looking reference for designing or extending human-AI symbioses.", "result": "By mapping existing contributions, from symbolic AI techniques and connectionist LLM-based agents to hybrid organizational practices, onto this proposed framework (Hierarchical Exploration-Exploitation Net), our approach facilitates revision of legacy methods and inspires new work that fuses qualitative and quantitative paradigms.", "conclusion": "Together, these insights offer a stepping stone toward deeper co-evolution of human cognition and AI capability."}}
{"id": "2410.15466", "keyword": "LLM reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing.", "keywords": ["LLM reasoning", "Reasoning"], "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u901a\u8fc7\u91cd\u590d\u62bd\u6837\u6765\u63d0\u9ad8\u8986\u76d6\u7387\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u51c6\u6765\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u8fd9\u79cd\u65b9\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6548\u679c\u3002", "motivation": "\u4f5c\u8005\u63a8\u6d4b\uff0c\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u901a\u8fc7\u91cd\u590d\u62bd\u6837\u589e\u52a0\u8986\u76d6\u8303\u56f4\u7684\u90e8\u5206\u539f\u56e0\u662f\u7531\u4e8e\u6807\u51c6\u8bc4\u4f30\u57fa\u51c6\u7684\u7b54\u6848\u5206\u5e03\u504f\u5411\u4e8e\u4e00\u5c0f\u90e8\u5206\u5e38\u89c1\u7b54\u6848\u3002", "method": "\u5b9a\u4e49\u4e86\u4e00\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6839\u636e\u8bad\u7ec3\u96c6\u4e2d\u7b54\u6848\u7684\u666e\u904d\u6027\u5217\u4e3e\u7b54\u6848\uff0c\u5e76\u8fdb\u884c\u4e86\u6db5\u76d6\u6570\u5b66\u63a8\u7406\u548c\u4e8b\u5b9e\u77e5\u8bc6\u4e24\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8fd9\u79cd\u57fa\u51c6\u65b9\u6cd5\u5728\u67d0\u4e9bLLMs\u4e0a\u4f18\u4e8e\u91cd\u590d\u6a21\u578b\u62bd\u6837\uff0c\u800c\u5bf9\u4e8e\u5176\u4ed6LLMs\uff0c\u5219\u4e0e\u6df7\u5408\u7b56\u7565\u7684\u6548\u679c\u76f8\u5f53\uff0c\u540e\u8005\u901a\u8fc7\u4f7f\u7528\u4ec510\u4e2a\u6a21\u578b\u6837\u672c\u6765\u83b7\u53d6k\u4e2a\u7b54\u6848\uff0c\u5e76\u901a\u8fc7\u679a\u4e3e\u731c\u6d4b\u5269\u4f59\u7684k-10\u6b21\u5c1d\u8bd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u51c6\u65b9\u6cd5\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u5728\u91cd\u590d\u62bd\u6837\u4e2d\u8986\u76d6\u8303\u56f4\u7684\u6539\u8fdb\u7a0b\u5ea6\uff0c\u8d85\u51fa\u4e86\u4e0e\u63d0\u793a\u65e0\u5173\u7684\u731c\u6d4b\u3002"}}
{"id": "2402.00559", "keyword": "Chain of Thoughts", "pdf": "https://arxiv.org/pdf/2402.00559", "abs": "https://arxiv.org/abs/2402.00559", "authors": ["Alon Jacovi", "Yonatan Bitton", "Bernd Bohnet", "Jonathan Herzig", "Or Honovich", "Michael Tseng", "Michael Collins", "Roee Aharoni", "Mor Geva"], "title": "A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains", "categories": ["cs.CL"], "comment": "Accepted to ACL 2024", "summary": "Prompting language models to provide step-by-step answers (e.g.,\n\"Chain-of-Thought\") is the prominent approach for complex reasoning tasks,\nwhere more accurate reasoning chains typically improve downstream task\nperformance. Recent literature discusses automatic methods to verify reasoning\nto evaluate and improve their correctness. However, no fine-grained step-level\ndatasets are available to enable thorough evaluation of such verification\nmethods, hindering progress in this direction. We introduce REVEAL: Reasoning\nVerification Evaluation, a dataset to benchmark automatic verifiers of complex\nChain-of-Thought reasoning in open-domain question-answering settings. REVEAL\nincludes comprehensive labels for the relevance, attribution to evidence\npassages, and logical correctness of each reasoning step in a language model's\nanswer, across a variety of datasets and state-of-the-art language models.\nEvaluation on REVEAL shows that verifiers struggle at verifying reasoning\nchains - in particular, verifying logical correctness and detecting\ncontradictions. Available at https://reveal-dataset.github.io/ .", "keywords": ["Chain of Thoughts", "Reasoning"], "AI": {"tldr": "\u8be5\u8bba\u6587\u63a8\u51fa\u4e86REVEAL\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u9a8c\u8bc1\u590d\u6742\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u6548\u679c\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u9a8c\u8bc1\u5668\u5728\u9a8c\u8bc1\u903b\u8f91\u6b63\u786e\u6027\u548c\u68c0\u6d4b\u77db\u76fe\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6b65\u9aa4\u7ea7\u6807\u6ce8\u4ee5\u63a8\u52a8\u7814\u7a76\u8fdb\u5c55\u3002", "motivation": "\u5c3d\u7ba1\u63d0\u793a\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u9010\u6b65\u7b54\u6848\uff08\u5982\u201c\u601d\u7ef4\u94fe\u201d\uff09\u5df2\u6210\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u5e76\u4e14\u66f4\u51c6\u786e\u7684\u63a8\u7406\u94fe\u901a\u5e38\u80fd\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u6b65\u9aa4\u7ea7\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u81ea\u52a8\u9a8c\u8bc1\u65b9\u6cd5\u3002\u8fd9\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u96c6REVEAL\uff0c\u5305\u542b\u591a\u65b9\u9762\u7684\u8be6\u7ec6\u6807\u6ce8\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u9a8c\u8bc1\u6a21\u578b\u5728\u5f00\u653e\u9886\u57df\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u8fd9\u4e9b\u6807\u6ce8\u8986\u76d6\u4e86\u8bed\u8a00\u6a21\u578b\u7b54\u6848\u4e2d\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u7684\u76f8\u5173\u6027\u3001\u8bc1\u636e\u5f52\u5c5e\u548c\u903b\u8f91\u6b63\u786e\u6027\u3002", "result": "\u901a\u8fc7\u5bf9REVEAL\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u53d1\u73b0\uff0c\u73b0\u6709\u7684\u9a8c\u8bc1\u5668\u5728\u9a8c\u8bc1\u590d\u6742\u7684\u63a8\u7406\u94fe\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u5224\u65ad\u903b\u8f91\u6b63\u786e\u6027\u548c\u8bc6\u522b\u77db\u76fe\u65b9\u9762\u3002\u8fd9\u8868\u660e\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u81ea\u52a8\u9a8c\u8bc1\u6280\u672f\u3002", "conclusion": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86REVEAL\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u9a8c\u8bc1\u590d\u6742\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002\u901a\u8fc7\u5f15\u5165\u5168\u9762\u7684\u6b65\u9aa4\u7ea7\u6807\u7b7e\uff0c\u5305\u62ec\u76f8\u5173\u6027\u3001\u8bc1\u636e\u6bb5\u843d\u7684\u5f52\u5c5e\u548c\u903b\u8f91\u6b63\u786e\u6027\uff0cREVEAL\u65e8\u5728\u63a8\u52a8\u81ea\u52a8\u9a8c\u8bc1\u65b9\u6cd5\u7684\u53d1\u5c55\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7684\u9a8c\u8bc1\u5668\u5728\u9a8c\u8bc1\u63a8\u7406\u94fe\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9a8c\u8bc1\u903b\u8f91\u6b63\u786e\u6027\u548c\u68c0\u6d4b\u77db\u76fe\u65b9\u9762\u3002"}}
{"id": "2505.04493", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2505.04493", "abs": "https://arxiv.org/abs/2505.04493", "authors": ["Or Wertheim", "Ronen I. Brafman"], "title": "Model-Based AI planning and Execution Systems for Robotics", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Model-based planning and execution systems offer a principled approach to\nbuilding flexible autonomous robots that can perform diverse tasks by\nautomatically combining a host of basic skills. This idea is almost as old as\nmodern robotics. Yet, while diverse general-purpose reasoning architectures\nhave been proposed since, general-purpose systems that are integrated with\nmodern robotic platforms have emerged only recently, starting with the\ninfluential ROSPlan system. Since then, a growing number of model-based systems\nfor robot task-level control have emerged. In this paper, we consider the\ndiverse design choices and issues existing systems attempt to address, the\ndifferent solutions proposed so far, and suggest avenues for future\ndevelopment.", "keywords": ["Reasoning"], "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7efc\u8ff0\u4e86\u73b0\u4ee3\u673a\u5668\u4eba\u4e2d\u57fa\u4e8e\u6a21\u578b\u7684\u4efb\u52a1\u7ea7\u63a7\u5236\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u8ba8\u8bba\u4e86\u8bbe\u8ba1\u9009\u62e9\u3001\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5efa\u8bae\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u4e0e\u6267\u884c\u7cfb\u7edf\u7684\u7406\u5ff5\u5386\u53f2\u60a0\u4e45\uff0c\u4f46\u76f4\u5230\u6700\u8fd1\u624d\u51fa\u73b0\u80fd\u591f\u4e0e\u73b0\u4ee3\u673a\u5668\u4eba\u5e73\u53f0\u6709\u6548\u96c6\u6210\u7684\u901a\u7528\u7cfb\u7edf\uff0c\u5982ROSPlan\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5bf9\u73b0\u6709\u7cfb\u7edf\u7684\u591a\u6837\u6027\u8fdb\u884c\u603b\u7ed3\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u6587\u7ae0\u901a\u8fc7\u56de\u987e\u73b0\u4ee3\u673a\u5668\u4eba\u5b66\u4e2d\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u4e0e\u6267\u884c\u7cfb\u7edf\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u7cfb\u7edf\u7684\u8bbe\u8ba1\u9009\u62e9\u3001\u89e3\u51b3\u7684\u95ee\u9898\u53ca\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u53ef\u80fd\u7684\u7814\u7a76\u65b9\u5411\u3002", "result": "\u6587\u7ae0\u63d0\u4f9b\u4e86\u5bf9\u73b0\u6709\u57fa\u4e8e\u6a21\u578b\u7684\u673a\u5668\u4eba\u4efb\u52a1\u63a7\u5236\u7cfb\u7edf\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u5305\u62ec\u5176\u8bbe\u8ba1\u9009\u62e9\u3001\u89e3\u51b3\u7684\u5173\u952e\u95ee\u9898\u4ee5\u53ca\u6240\u91c7\u7528\u7684\u4e0d\u540c\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u7684\u6f5c\u5728\u65b9\u5411\u3002", "conclusion": "\u8fd9\u7bc7\u6587\u7ae0\u603b\u7ed3\u4e86\u57fa\u4e8e\u6a21\u578b\u7684\u673a\u5668\u4eba\u4efb\u52a1\u7ea7\u63a7\u5236\u7cfb\u7edf\u7684\u53d1\u5c55\u73b0\u72b6\uff0c\u5e76\u63a2\u8ba8\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u8bbe\u8ba1\u9009\u62e9\u548c\u89e3\u51b3\u65b9\u6848\u65b9\u9762\u7684\u591a\u6837\u6027\u3002\u4f5c\u8005\u6307\u51fa\u672a\u6765\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316\u89c4\u5212\u4e0e\u6267\u884c\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u901a\u7528\u6027\u4e0e\u96c6\u6210\u80fd\u529b\u3002"}}
{"id": "2412.02441", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2412.02441", "abs": "https://arxiv.org/abs/2412.02441", "authors": ["Shai Shalev-Shwartz", "Amnon Shashua", "Gal Beniamini", "Yoav Levine", "Or Sharir", "Noam Wies", "Ido Ben-Shaul", "Tomer Nussbaum", "Shir Granot Peled"], "title": "Artificial Expert Intelligence through PAC-reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Artificial Expert Intelligence (AEI) seeks to transcend the limitations of\nboth Artificial General Intelligence (AGI) and narrow AI by integrating\ndomain-specific expertise with critical, precise reasoning capabilities akin to\nthose of top human experts. Existing AI systems often excel at predefined tasks\nbut struggle with adaptability and precision in novel problem-solving. To\novercome this, AEI introduces a framework for ``Probably Approximately Correct\n(PAC) Reasoning\". This paradigm provides robust theoretical guarantees for\nreliably decomposing complex problems, with a practical mechanism for\ncontrolling reasoning precision. In reference to the division of human thought\ninto System 1 for intuitive thinking and System 2 for reflective\nreasoning~\\citep{tversky1974judgment}, we refer to this new type of reasoning\nas System 3 for precise reasoning, inspired by the rigor of the scientific\nmethod. AEI thus establishes a foundation for error-bounded, inference-time\nlearning.", "keywords": ["Reasoning"], "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578b\u4eba\u5de5\u667a\u80fd\u6846\u67b6Artificial Expert Intelligence (AEI)\uff0c\u5b83\u7ed3\u5408\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4e0e\u7cbe\u786e\u63a8\u7406\u80fd\u529b\uff0c\u4ee5\u63d0\u5347\u73b0\u6709AI\u5728\u590d\u6742\u95ee\u9898\u4e0a\u7684\u9002\u5e94\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684AI\u7cfb\u7edf\u5728\u9884\u5b9a\u4e49\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9002\u5e94\u6027\u548c\u65b0\u9896\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800cAEI\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u7f3a\u9677\uff0c\u5b9e\u73b0\u66f4\u63a5\u8fd1\u9876\u7ea7\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73\u7684\u667a\u80fd\u3002", "method": "AEI \u5f15\u5165\u4e86\u201c\u53ef\u80fd\u8fd1\u4f3c\u6b63\u786e\uff08PAC\uff09\u63a8\u7406\u201d\u8303\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u79f0\u4e3aSystem 3\u7684\u7cbe\u786e\u63a8\u7406\u7c7b\u578b\uff0c\u4ee5\u63a7\u5236\u63a8\u7406\u7cbe\u5ea6\u5e76\u5b9e\u73b0\u8bef\u5dee\u6709\u754c\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "AEI \u6210\u529f\u5730\u5c06\u79d1\u5b66\u65b9\u6cd5\u7684\u4e25\u8c28\u6027\u878d\u5165\u5230\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u7406\u8bba\u652f\u6301\u7684\u6846\u67b6\uff0c\u53ef\u5728\u63a8\u7406\u65f6\u5b66\u4e60\u5e76\u63a7\u5236\u7ed3\u679c\u7684\u7cbe\u5ea6\u3002", "conclusion": "AEI \u63d0\u51fa\u4e86\u4e00\u4e2a\u8d85\u8d8aAGI\u548c\u72ed\u4e49AI\u5c40\u9650\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4e0e\u7cbe\u786e\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u95ee\u9898\u6c42\u89e3\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8df5\u673a\u5236\u3002"}}
{"id": "2409.04397", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2409.04397", "abs": "https://arxiv.org/abs/2409.04397", "authors": ["Yotam Erel", "Or Kozlovsky-Mordenfeld", "Daisuke Iwai", "Kosuke Sato", "Amit H. Bermano"], "title": "Casper DPM: Cascaded Perceptual Dynamic Projection Mapping onto Hands", "categories": ["cs.GR"], "comment": "Project page: https://yoterel.github.io/casper-project-page/", "summary": "We present a technique for dynamically projecting 3D content onto human hands\nwith short perceived motion-to-photon latency. Computing the pose and shape of\nhuman hands accurately and quickly is a challenging task due to their\narticulated and deformable nature. We combine a slower 3D coarse estimation of\nthe hand pose with high speed 2D correction steps which improve the alignment\nof the projection to the hands, increase the projected surface area, and reduce\nperceived latency. Since our approach leverages a full 3D reconstruction of the\nhands, any arbitrary texture or reasonably performant effect can be applied,\nwhich was not possible before. We conducted two user studies to assess the\nbenefits of using our method. The results show subjects are less sensitive to\nlatency artifacts and perform faster and with more ease a given associated task\nover the naive approach of directly projecting rendered frames from the 3D pose\nestimation. We demonstrate several novel use cases and applications.", "keywords": ["Reasoning"], "AI": {"tldr": "\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5c063D\u5185\u5bb9\u52a8\u6001\u6295\u5c04\u5230\u4eba\u624b\u7684\u65b0\u6280\u672f\uff0c\u901a\u8fc7\u7ed3\u54083D\u624b\u90e8\u59ff\u6001\u7c97\u7565\u4f30\u8ba1\u4e0e\u9ad8\u901f2D\u6821\u6b63\u6b65\u9aa4\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u611f\u77e5\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u4e86\u6295\u5f71\u7684\u5bf9\u9f50\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u7531\u4e8e\u4eba\u624b\u5177\u6709\u9ad8\u5ea6\u5173\u8282\u5316\u548c\u53ef\u53d8\u5f62\u7684\u7279\u70b9\uff0c\u51c6\u786e\u4e14\u5feb\u901f\u5730\u8ba1\u7b97\u5176\u59ff\u6001\u548c\u5f62\u72b6\u662f\u4e00\u9879\u6311\u6218\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6280\u672f\u6765\u6539\u8fdb3D\u5185\u5bb9\u52a8\u6001\u6295\u5f71\u5230\u4eba\u624b\u4e0a\u7684\u4f53\u9a8c\u3002", "method": "\u6587\u7ae0\u7684\u65b9\u6cd5\u662f\u5c063D\u624b\u90e8\u59ff\u6001\u7684\u7c97\u7565\u4f30\u8ba1\u4e0e\u9ad8\u901f2D\u6821\u6b63\u6b65\u9aa4\u76f8\u7ed3\u5408\uff0c\u4ece\u800c\u4f18\u5316\u6295\u5f71\u6548\u679c\u5e76\u51cf\u5c11\u5ef6\u8fdf\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5b8c\u6574\u76843D\u624b\u90e8\u91cd\u5efa\uff0c\u53ef\u4ee5\u5e94\u7528\u4efb\u610f\u7eb9\u7406\u6216\u6027\u80fd\u5408\u7406\u7684\u7279\u6548\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528\u8be5\u65b9\u6cd5\u540e\uff0c\u53d7\u8bd5\u8005\u5bf9\u5ef6\u8fdf\u4f2a\u5f71\u7684\u654f\u611f\u5ea6\u964d\u4f4e\uff0c\u5728\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u5f97\u66f4\u5feb\u3001\u66f4\u8f7b\u677e\u3002\u6b64\u5916\uff0c\u8fd8\u6f14\u793a\u4e86\u591a\u79cd\u521b\u65b0\u7528\u4f8b\u548c\u5e94\u7528\u3002", "conclusion": "\u6587\u7ae0\u7684\u7ed3\u8bba\u662f\uff0c\u901a\u8fc7\u7ed3\u5408\u8f83\u6162\u76843D\u624b\u90e8\u59ff\u6001\u7c97\u7565\u4f30\u8ba1\u4e0e\u9ad8\u901f2D\u6821\u6b63\u6b65\u9aa4\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6295\u5f71\u5bf9\u624b\u90e8\u7684\u5bf9\u9f50\u6027\u3001\u589e\u52a0\u6295\u5f71\u8868\u9762\u79ef\uff0c\u5e76\u51cf\u5c11\u611f\u77e5\u5ef6\u8fdf\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u5f97\u7528\u6237\u5728\u6267\u884c\u4efb\u52a1\u65f6\u66f4\u52a0\u6d41\u7545\u548c\u5feb\u901f\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u591a\u4e2a\u65b0\u9896\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2408.08654", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2408.08654", "abs": "https://arxiv.org/abs/2408.08654", "authors": ["Li-Fang Zhu", "Fritz Koermann", "Qing Chen", "Malin Selleby", "Joerg Neugebauer", "and Blazej Grabowski"], "title": "Accelerating ab initio melting property calculations with machine learning: Application to the high entropy alloy TaVCrW", "categories": ["cond-mat.mtrl-sci"], "comment": "14 pages, 6 figures", "summary": "Melting properties are critical for designing novel materials, especially for\ndiscovering high-performance, high-melting refractory materials. Experimental\nmeasurements of these properties are extremely challenging due to their high\nmelting temperatures. Complementary theoretical predictions are, therefore,\nindispensable. The conventional free energy approach using density functional\ntheory (DFT) has been a gold standard for such purposes because of its high\naccuracy. However,it generally involves expensive thermodynamic integration\nusing ab initio molecular dynamic simulations. The high computational cost\nmakes high-throughput calculations infeasible. Here, we propose a highly\nefficient DFT-based method aided by a specially designed machine learning\npotential. As the machine learning potential can closely reproduce the ab\ninitio phase space, even for multi-component alloys, the costly thermodynamic\nintegration can be fully substituted with more efficient free energy\nperturbation calculations. The method achieves overall savings of computational\nresources by 80% compared to current alternatives. We apply the method to the\nhigh-entropy alloy TaVCrW and calculate its melting properties, including\nmelting temperature, entropy and enthalpy of fusion, and volume change at the\nmelting point. Additionally, the heat capacities of solid and liquid TaVCrW are\ncalculated. The results agree reasonably with the calphad extrapolated values.", "keywords": ["Reasoning"], "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7ed3\u5408\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u548c\u673a\u5668\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u51c6\u786e\u5730\u9884\u6d4b\u9ad8\u71b5\u5408\u91d1\u7b49\u6750\u6599\u7684\u7194\u5316\u6027\u8d28\uff0c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u51cf\u5c11\u4e8680%\u3002", "motivation": "\u7194\u5316\u6027\u8d28\u5bf9\u4e8e\u8bbe\u8ba1\u65b0\u578b\u9ad8\u6027\u80fd\u8010\u706b\u6750\u6599\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9a8c\u6d4b\u91cf\u56e0\u5176\u9ad8\u6e29\u7279\u6027\u800c\u6781\u5177\u6311\u6218\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u7406\u8bba\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u673a\u5668\u5b66\u4e60\u52bf\u51fd\u6570\u66ff\u4ee3\u4f20\u7edf\u7684\u70ed\u529b\u5b66\u79ef\u5206\u65b9\u6cd5\uff0c\u5229\u7528\u81ea\u7531\u80fd\u5fae\u6270\u8ba1\u7b97\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u9ad8\u71b5\u5408\u91d1\u7684\u7194\u5316\u6027\u8d28\u9884\u6d4b\u3002", "result": "\u4f5c\u8005\u5e94\u7528\u65b0\u65b9\u6cd5\u5bf9\u9ad8\u71b5\u5408\u91d1TaVCrW\u8fdb\u884c\u4e86\u7194\u5316\u6e29\u5ea6\u3001\u7194\u878d\u71b5\u548c\u7113\u53d8\u4ee5\u53ca\u4f53\u79ef\u53d8\u5316\u7684\u8ba1\u7b97\uff0c\u5e76\u5f97\u5230\u4e86\u56fa\u6001\u548c\u6db2\u6001\u7684\u70ed\u5bb9\u6570\u636e\uff0c\u8fd9\u4e9b\u7ed3\u679c\u4e0e\u73b0\u6709\u7684calphad\u5916\u63a8\u503c\u76f8\u7b26\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u52bf\u51fd\u6570\u6765\u9884\u6d4b\u6750\u6599\u7684\u7194\u5316\u6027\u8d28\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u8d44\u6e90\u6d88\u8017\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u8282\u7701\u4e8680%\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u9ad8\u71b5\u5408\u91d1TaVCrW\u7684\u7194\u5316\u6027\u8d28\u8ba1\u7b97\uff0c\u7ed3\u679c\u4e0ecalphad\u5916\u63a8\u503c\u57fa\u672c\u4e00\u81f4\u3002"}}
{"id": "2403.11729", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2403.11729", "abs": "https://arxiv.org/abs/2403.11729", "authors": ["Kento Kawaharazuka", "Akihiro Miki", "Masahiro Bando", "Temma Suzuki", "Yoshimoto Ribayashi", "Yasunori Toshimitsu", "Yuya Nagamatsu", "Kei Okada", "and Masayuki Inaba"], "title": "Hardware Design and Learning-Based Software Architecture of Musculoskeletal Wheeled Robot Musashi-W for Real-World Applications", "categories": ["cs.RO"], "comment": "Accepted at Humanoids2022", "summary": "Various musculoskeletal humanoids have been developed so far. While these\nhumanoids have the advantage of their flexible and redundant bodies that mimic\nthe human body, they are still far from being applied to real-world tasks. One\nof the reasons for this is the difficulty of bipedal walking in a flexible\nbody. Thus, we developed a musculoskeletal wheeled robot, Musashi-W, by\ncombining a wheeled base and musculoskeletal upper limbs for real-world\napplications. Also, we constructed its software system by combining static and\ndynamic body schema learning, reflex control, and visual recognition. We show\nthat the hardware and software of Musashi-W can make the most of the advantages\nof the musculoskeletal upper limbs, through several tasks of cleaning by human\nteaching, carrying a heavy object considering muscle addition, and setting a\ntable through dynamic cloth manipulation with variable stiffness.", "keywords": ["Reasoning"], "AI": {"tldr": "Researchers developed Musashi-W, a wheeled musculoskeletal robot with advanced software integration, enabling it to perform practical tasks like cleaning, carrying heavy loads, and manipulating cloth.", "motivation": "Despite the flexibility and redundancy of musculoskeletal humanoids, they struggle with real-world applications, especially bipedal walking. This research aims to overcome these challenges by developing a more applicable design using a wheeled platform combined with musculoskeletal arms.", "method": "The researchers developed Musashi-W, a musculoskeletal wheeled robot integrating a wheeled base with musculoskeletal upper limbs. Its software system combines static and dynamic body schema learning, reflex control, and visual recognition to optimize performance.", "result": "Musashi-W demonstrated its effectiveness through various tasks like cleaning via human teaching, carrying heavy objects with added muscle support, and dynamically manipulating cloth for table setting, showcasing its versatility and adaptability.", "conclusion": "Musashi-W is a successful combination of hardware and software advancements that enable practical applications of musculoskeletal robotics, particularly in tasks requiring flexibility, strength, and interaction with complex environments."}}
{"id": "2403.00458", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2403.00458", "abs": "https://arxiv.org/abs/2403.00458", "authors": ["Chung Yi See", "Vasco Rato Santos", "Lucas Woodley", "Megan Yeo", "Daniel Palmer", "Shuheng Zhang", "and Ashley Nunes"], "title": "Prices and preferences in the electric vehicle market", "categories": ["econ.EM"], "comment": "Main paper: 5 tables, 2 figures", "summary": "Although electric vehicles are less polluting than gasoline powered vehicles,\nadoption is challenged by higher procurement prices. Existing discourse\nemphasizes EV battery costs as being principally responsible for this price\ndifferential and widespread adoption is routinely conditioned upon battery\ncosts declining. We scrutinize such reasoning by sourcing data on EV attributes\nand market conditions between 2011 and 2023. Our findings are fourfold. First,\nEV prices are influenced principally by the number of amenities, additional\nfeatures, and dealer-installed accessories sold as standard on an EV, and to a\nlesser extent, by EV horsepower. Second, EV range is negatively correlated with\nEV price implying that range anxiety concerns may be less consequential than\nexisting discourse suggests. Third, battery capacity is positively correlated\nwith EV price, due to more capacity being synonymous with the delivery of more\nhorsepower. Collectively, this suggests that higher procurement prices for EVs\nreflects consumer preference for vehicles that are feature dense and more\npowerful. Fourth and finally, accommodating these preferences have produced\nvehicles with lower fuel economy, a shift that reduces envisioned lifecycle\nemissions benefits by at least 3.26 percent, subject to the battery pack\nchemistry leveraged and the carbon intensity of the electrical grid. These\nfindings warrant attention as decarbonization efforts increasingly emphasize\nelectrification as a pathway for complying with domestic and international\nclimate agreements.", "keywords": ["Reasoning"], "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\u7535\u52a8\u8f66\u7684\u9ad8\u4ef7\u683c\u4e3b\u8981\u53cd\u6620\u6d88\u8d39\u8005\u5bf9\u529f\u80fd\u4e30\u5bcc\u548c\u66f4\u5f3a\u5927\u8f66\u8f86\u7684\u504f\u597d\uff0c\u800c\u975e\u4ec5\u7531\u7535\u6c60\u6210\u672c\u51b3\u5b9a\uff0c\u800c\u8fd9\u79cd\u504f\u597d\u53ef\u80fd\u964d\u4f4e\u71c3\u6cb9\u7ecf\u6d4e\u6027\u548c\u51cf\u6392\u6548\u76ca\u3002", "motivation": "\u7535\u52a8\u8f66\u867d\u7136\u6bd4\u6c7d\u6cb9\u8f66\u6c61\u67d3\u5c11\uff0c\u4f46\u7531\u4e8e\u8f83\u9ad8\u7684\u91c7\u8d2d\u4ef7\u683c\uff0c\u5176\u666e\u53ca\u53d7\u5230\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u5ba1\u89c6\u73b0\u6709\u8ba8\u8bba\u4e2d\u5173\u4e8e\u7535\u6c60\u6210\u672c\u662f\u4ef7\u683c\u5dee\u5f02\u4e3b\u8981\u539f\u56e0\u7684\u89c2\u70b9\uff0c\u5e76\u8fdb\u4e00\u6b65\u63a2\u7a76\u5f71\u54cd\u7535\u52a8\u8f66\u4ef7\u683c\u7684\u5176\u4ed6\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u5206\u67902011\u5e74\u81f32023\u5e74\u95f4\u7684\u7535\u52a8\u8f66\u5c5e\u6027\u548c\u5e02\u573a\u72b6\u51b5\u6570\u636e\uff0c\u63a2\u8ba8\u5f71\u54cd\u7535\u52a8\u8f66\u4ef7\u683c\u7684\u56e0\u7d20\u53ca\u5176\u4e0e\u7535\u6c60\u6210\u672c\u3001\u8f66\u8f86\u7279\u5f81\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u9996\u5148\uff0c\u7535\u52a8\u8f66\u7684\u4ef7\u683c\u4e3b\u8981\u53d7\u6807\u51c6\u914d\u7f6e\u7684\u8bbe\u65bd\u3001\u9644\u52a0\u529f\u80fd\u548c\u7ecf\u9500\u5546\u5b89\u88c5\u7684\u914d\u4ef6\u6570\u91cf\u5f71\u54cd\uff1b\u5176\u6b21\uff0c\u7eed\u822a\u91cc\u7a0b\u4e0e\u4ef7\u683c\u5448\u8d1f\u76f8\u5173\uff1b\u7b2c\u4e09\uff0c\u7535\u6c60\u5bb9\u91cf\u4e0e\u4ef7\u683c\u6b63\u76f8\u5173\uff0c\u56e0\u4e3a\u66f4\u5927\u7684\u5bb9\u91cf\u610f\u5473\u7740\u66f4\u591a\u7684\u9a6c\u529b\u8f93\u51fa\uff1b\u6700\u540e\uff0c\u8fd9\u79cd\u6d88\u8d39\u504f\u597d\u5bfc\u81f4\u4e86\u8f83\u4f4e\u7684\u71c3\u6cb9\u7ecf\u6d4e\u6027\uff0c\u51cf\u5c11\u4e86\u9884\u671f\u7684\u51cf\u6392\u6548\u76ca\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u7535\u52a8\u8f66\u7684\u8f83\u9ad8\u4ef7\u683c\u4e3b\u8981\u53cd\u6620\u4e86\u6d88\u8d39\u8005\u5bf9\u529f\u80fd\u4e30\u5bcc\u548c\u66f4\u5f3a\u5927\u8f66\u8f86\u7684\u504f\u597d\uff0c\u4f46\u8fd9\u79cd\u504f\u597d\u5bfc\u81f4\u4e86\u8f83\u4f4e\u7684\u71c3\u6cb9\u7ecf\u6d4e\u6027\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u9884\u671f\u7684\u751f\u547d\u5468\u671f\u6392\u653e\u6548\u76ca\u3002\u8fd9\u63d0\u793a\u5728\u5f3a\u8c03\u7535\u6c14\u5316\u4f5c\u4e3a\u8131\u78b3\u8def\u5f84\u7684\u540c\u65f6\uff0c\u9700\u8981\u5173\u6ce8\u5176\u5bf9\u5b9e\u9645\u51cf\u6392\u76ee\u6807\u7684\u5f71\u54cd\u3002"}}
{"id": "2505.23723", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.23723", "abs": "https://arxiv.org/abs/2505.23723", "authors": ["Zexi Liu", "Jingyi Chai", "Xinyu Zhu", "Shuo Tang", "Rui Ye", "Bo Zhang", "Lei Bai", "Siheng Chen"], "title": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The emergence of large language model (LLM)-based agents has significantly\nadvanced the development of autonomous machine learning (ML) engineering.\nHowever, most existing approaches rely heavily on manual prompt engineering,\nfailing to adapt and optimize based on diverse experimental experiences.\nFocusing on this, for the first time, we explore the paradigm of learning-based\nagentic ML, where an LLM agent learns through interactive experimentation on ML\ntasks using online reinforcement learning (RL). To realize this, we propose a\nnovel agentic ML training framework with three key components: (1)\nexploration-enriched fine-tuning, which enables LLM agents to generate diverse\nactions for enhanced RL exploration; (2) step-wise RL, which enables training\non a single action step, accelerating experience collection and improving\ntraining efficiency; (3) an agentic ML-specific reward module, which unifies\nvaried ML feedback signals into consistent rewards for RL optimization.\nLeveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM\nfor autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our\n7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it\nachieves continuous performance improvements and demonstrates exceptional\ncross-task generalization capabilities.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u81ea\u4e3b\u5b66\u4e60\u3002\u63d0\u51fa\u7684\u6846\u67b6\u5305\u62ec\u63a2\u7d22\u589e\u5f3a\u5fae\u8c03\u3001\u9010\u6b65\u5f3a\u5316\u5b66\u4e60\u548c\u4ee3\u7406\u4e13\u7528\u5956\u52b1\u6a21\u5757\uff0c\u6700\u7ec8\u8bad\u7ec3\u51fa\u7684\u5c0f\u578bML-Agent\u5728\u6027\u80fd\u4e0a\u8d85\u8fc7\u4e86\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u5e76\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6301\u7eed\u4f18\u5316\u6f5c\u529b\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u4f9d\u8d56\u624b\u52a8\u63d0\u793a\u5de5\u7a0b\uff0c\u7f3a\u4e4f\u6839\u636e\u591a\u6837\u5316\u5b9e\u9a8c\u7ecf\u9a8c\u8fdb\u884c\u9002\u5e94\u548c\u4f18\u5316\u7684\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e00\u4e2a\u80fd\u591f\u901a\u8fc7\u4ea4\u4e92\u5f0f\u5b9e\u9a8c\u8fdb\u884c\u81ea\u4e3b\u5b66\u4e60\u7684LLM\u4ee3\u7406\uff0c\u4ece\u800c\u63a8\u52a8\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60\u7684\u53d1\u5c55\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ee3\u7406\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff1a\u589e\u5f3a\u63a2\u7d22\u7684\u5fae\u8c03\u3001\u9010\u6b65\u5f3a\u5316\u5b66\u4e60\u4ee5\u53ca\u7279\u5b9a\u4e8e\u4ee3\u7406\u673a\u5668\u5b66\u4e60\u7684\u5956\u52b1\u6a21\u5757\u3002\u8fd9\u4e9b\u65b9\u6cd5\u65e8\u5728\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u7684\u63a2\u7d22\u6027\u3001\u52a0\u901f\u7ecf\u9a8c\u6536\u96c6\u5e76\u4f18\u5316\u8bad\u7ec3\u6548\u7387\uff0c\u540c\u65f6\u7edf\u4e00\u591a\u6837\u7684\u673a\u5668\u5b66\u4e60\u53cd\u9988\u4fe1\u53f7\u4ee5\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u3002", "result": "\u5229\u7528\u63d0\u51fa\u7684\u6846\u67b6\uff0c\u7814\u7a76\u4eba\u5458\u6210\u529f\u8bad\u7ec3\u4e86\u4e00\u4e2a7B\u89c4\u6a21\u7684Qwen-2.5 LLM\u9a71\u52a8\u7684ML-Agent\u3002\u5c3d\u7ba1\u4ec5\u57289\u4e2a\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86671B\u89c4\u6a21\u7684DeepSeek-R1\u4ee3\u7406\uff0c\u5e76\u4e14\u5c55\u793a\u4e86\u51fa\u8272\u7684\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u548c\u6301\u7eed\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u63a2\u7d22\u4e86\u57fa\u4e8e\u5b66\u4e60\u7684\u4ee3\u7406\u673a\u5668\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u5b9e\u9a8c\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4f7fLLM\u4ee3\u7406\u80fd\u591f\u81ea\u4e3b\u5b66\u4e60\u3002\u63d0\u51fa\u7684\u6846\u67b6\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u8fd8\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002\u5c3d\u7ba1\u89c4\u6a21\u8f83\u5c0f\uff0c\u4f46ML-Agent\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u6301\u7eed\u6027\u80fd\u6539\u8fdb\u7684\u80fd\u529b\u3002"}}
{"id": "2208.08580", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2208.08580", "abs": "https://arxiv.org/abs/2208.08580", "authors": ["Gopal Sharma", "Kangxue Yin", "Subhransu Maji", "Evangelos Kalogerakis", "Or Litany", "Sanja Fidler"], "title": "MvDeCor: Multi-view Dense Correspondence Learning for Fine-grained 3D Segmentation", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "project page: https://nv-tlabs.github.io/MvDeCor/", "summary": "We propose to utilize self-supervised techniques in the 2D domain for\nfine-grained 3D shape segmentation tasks. This is inspired by the observation\nthat view-based surface representations are more effective at modeling\nhigh-resolution surface details and texture than their 3D counterparts based on\npoint clouds or voxel occupancy. Specifically, given a 3D shape, we render it\nfrom multiple views, and set up a dense correspondence learning task within the\ncontrastive learning framework. As a result, the learned 2D representations are\nview-invariant and geometrically consistent, leading to better generalization\nwhen trained on a limited number of labeled shapes compared to alternatives\nthat utilize self-supervision in 2D or 3D alone. Experiments on textured\n(RenderPeople) and untextured (PartNet) 3D datasets show that our method\noutperforms state-of-the-art alternatives in fine-grained part segmentation.\nThe improvements over baselines are greater when only a sparse set of views is\navailable for training or when shapes are textured, indicating that MvDeCor\nbenefits from both 2D processing and 3D geometric reasoning.", "keywords": ["Reasoning"], "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.23657", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.23657", "abs": "https://arxiv.org/abs/2505.23657", "authors": ["Hongxiang Zhang", "Hao Chen", "Tianyi Zhang", "Muhao Chen"], "title": "Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent decoding methods improve the factuality of large language\nmodels~(LLMs) by refining how the next token is selected during generation.\nThese methods typically operate at the token level, leveraging internal\nrepresentations to suppress superficial patterns. Nevertheless, LLMs remain\nprone to hallucinations, especially over longer contexts. In this paper, we\npropose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy\nthat actively decides when to apply contrasting layers during generation. By\ncasting decoding as a sequential decision-making problem, ActLCD employs a\nreinforcement learning policy guided by a reward-aware classifier to optimize\nfactuality beyond the token level. Our experiments demonstrate that ActLCD\nsurpasses state-of-the-art methods across five benchmarks, showcasing its\neffectiveness in mitigating hallucinations in diverse generation scenarios.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u7801\u7b56\u7565 ActLCD\uff0c\u5b83\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u5956\u52b1\u611f\u77e5\u5206\u7c7b\u5668\u4f18\u5316\u4e8b\u5b9e\u6027\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u7684\u89e3\u7801\u65b9\u6cd5\u5df2\u7ecf\u6539\u8fdb\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u6027\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u5c24\u5176\u662f\u5728\u8f83\u957f\u7684\u4e0a\u4e0b\u6587\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Active Layer-Contrastive Decoding (ActLCD) \u7684\u89e3\u7801\u7b56\u7565\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u51b3\u5b9a\u4f55\u65f6\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5e94\u7528\u5bf9\u6bd4\u5c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cActLCD \u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u663e\u793a\u4e86\u5176\u5728\u4e0d\u540c\u751f\u6210\u573a\u666f\u4e2d\u51cf\u5c11\u5e7b\u89c9\u7684\u6709\u6548\u6027\u3002", "conclusion": "ActLCD \u662f\u4e00\u79cd\u65b0\u7684\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u89e3\u7801\u89c6\u4e3a\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u548c\u5956\u52b1\u611f\u77e5\u5206\u7c7b\u5668\u6765\u4f18\u5316\u4e8b\u5b9e\u6027\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u679c\u3002"}}
{"id": "2110.02210", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2110.02210", "abs": "https://arxiv.org/abs/2110.02210", "authors": ["Alexey Nekrasov", "Jonas Schult", "Or Litany", "Bastian Leibe", "Francis Engelmann"], "title": "Mix3D: Out-of-Context Data Augmentation for 3D Scenes", "categories": ["cs.CV"], "comment": "Accepted for publication at 3DV 2021. Camera-ready submission. Link\n  to code: https://github.com/kumuji/mix3d - Project page:\n  https://nekrasov.dev/mix3d/", "summary": "We present Mix3D, a data augmentation technique for segmenting large-scale 3D\nscenes. Since scene context helps reasoning about object semantics, current\nworks focus on models with large capacity and receptive fields that can fully\ncapture the global context of an input 3D scene. However, strong contextual\npriors can have detrimental implications like mistaking a pedestrian crossing\nthe street for a car. In this work, we focus on the importance of balancing\nglobal scene context and local geometry, with the goal of generalizing beyond\nthe contextual priors in the training set. In particular, we propose a \"mixing\"\ntechnique which creates new training samples by combining two augmented scenes.\nBy doing so, object instances are implicitly placed into novel out-of-context\nenvironments and therefore making it harder for models to rely on scene context\nalone, and instead infer semantics from local structure as well. We perform\ndetailed analysis to understand the importance of global context, local\nstructures and the effect of mixing scenes. In experiments, we show that models\ntrained with Mix3D profit from a significant performance boost on indoor\n(ScanNet, S3DIS) and outdoor datasets (SemanticKITTI). Mix3D can be trivially\nused with any existing method, e.g., trained with Mix3D, MinkowskiNet\noutperforms all prior state-of-the-art methods by a significant margin on the\nScanNet test benchmark 78.1 mIoU. Code is available at:\nhttps://nekrasov.dev/mix3d/", "keywords": ["Reasoning"], "AI": {"tldr": "Mix3D is a novel data augmentation method that improves 3D scene segmentation by balancing global context and local geometry, achieving strong results on multiple datasets.", "motivation": "Current models for 3D scene segmentation focus on capturing global context, which can lead to errors when contextual priors mislead the model (e.g., mistaking a pedestrian for a car). The authors aim to balance global context with local geometric details to improve generalization.", "method": "The paper proposes a 'mixing' technique where new training samples are created by combining two augmented scenes. This approach forces models to rely less on global context and more on local structures to infer semantics.", "result": "Models trained with Mix3D achieve significant performance improvements, such as MinkowskiNet outperforming prior state-of-the-art methods by a large margin on the ScanNet test benchmark with 78.1 mIoU.", "conclusion": "Mix3D is an effective data augmentation technique that balances global scene context and local geometry, leading to improved performance in 3D scene segmentation tasks across both indoor and outdoor datasets."}}
{"id": "2505.23579", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.23579", "abs": "https://arxiv.org/abs/2505.23579", "authors": ["Adibvafa Fallahpour", "Andrew Magnuson", "Purav Gupta", "Shihao Ma", "Jack Naimer", "Arnav Shah", "Haonan Duan", "Omar Ibrahim", "Hani Goodarzi", "Chris J. Maddison", "Bo Wang"], "title": "BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model", "categories": ["cs.LG"], "comment": "16 pages, 3 figures, 2 tables", "summary": "Unlocking deep, interpretable biological reasoning from complex genomic data\nis a major AI challenge hindering scientific discovery. Current DNA foundation\nmodels, despite strong sequence representation, struggle with multi-step\nreasoning and lack inherent transparent, biologically intuitive explanations.\nWe introduce BioReason, a pioneering architecture that, for the first time,\ndeeply integrates a DNA foundation model with a Large Language Model (LLM).\nThis novel connection enables the LLM to directly process and reason with\ngenomic information as a fundamental input, fostering a new form of multimodal\nbiological understanding. BioReason's sophisticated multi-step reasoning is\ndeveloped through supervised fine-tuning and targeted reinforcement learning,\nguiding the system to generate logical, biologically coherent deductions. On\nbiological reasoning benchmarks including KEGG-based disease pathway prediction\n- where accuracy improves from 88% to 97% - and variant effect prediction,\nBioReason demonstrates an average 15% performance gain over strong\nsingle-modality baselines. BioReason reasons over unseen biological entities\nand articulates decision-making through interpretable, step-by-step biological\ntraces, offering a transformative approach for AI in biology that enables\ndeeper mechanistic insights and accelerates testable hypothesis generation from\ngenomic data. Data, code, and checkpoints are publicly available at\nhttps://github.com/bowang-lab/BioReason", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "BioReason is a pioneering architecture that integrates a DNA foundation model with a Large Language Model to enable multi-step, interpretable biological reasoning from genomic data.", "motivation": "Unlocking deep, interpretable biological reasoning from complex genomic data is a major AI challenge hindering scientific discovery. Current DNA foundation models struggle with multi-step reasoning and lack transparent explanations.", "method": "BioReason integrates a DNA foundation model with a Large Language Model (LLM) through supervised fine-tuning and targeted reinforcement learning.", "result": "BioReason demonstrates an average 15% performance gain over strong single-modality baselines on biological reasoning benchmarks including KEGG-based disease pathway prediction where accuracy improves from 88% to 97%, and variant effect prediction.", "conclusion": "BioReason offers a transformative approach for AI in biology by enabling deeper mechanistic insights and accelerating testable hypothesis generation from genomic data."}}
{"id": "2505.23433", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.23433", "abs": "https://arxiv.org/abs/2505.23433", "authors": ["Jian Yao", "Ran Cheng", "Xingyu Wu", "Jibin Wu", "Kay Chen Tan"], "title": "Diversity-Aware Policy Optimization for Large Language Model Reasoning", "categories": ["cs.LG"], "comment": null, "summary": "The reasoning capabilities of large language models (LLMs) have advanced\nrapidly, particularly following the release of DeepSeek R1, which has inspired\na surge of research into data quality and reinforcement learning (RL)\nalgorithms. Despite the pivotal role diversity plays in RL, its influence on\nLLM reasoning remains largely underexplored. To bridge this gap, this work\npresents a systematic investigation into the impact of diversity in RL-based\ntraining for LLM reasoning, and proposes a novel diversity-aware policy\noptimization method. Across evaluations on 12 LLMs, we observe a strong\npositive correlation between the solution diversity and Potential at k (a novel\nmetric quantifying an LLM's reasoning potential) in high-performing models.\nThis finding motivates our method to explicitly promote diversity during RL\ntraining. Specifically, we design a token-level diversity and reformulate it\ninto a practical objective, then we selectively apply it to positive samples.\nIntegrated into the R1-zero training framework, our method achieves a 3.5\npercent average improvement across four mathematical reasoning benchmarks,\nwhile generating more diverse and robust solutions.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u591a\u6837\u6027\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6837\u6027\u611f\u77e5\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u663e\u5f0f\u4fc3\u8fdb\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u591a\u6837\u6027\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6837\u6027\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u5176\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u7684\u5f71\u54cd\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6587\u7ae0\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u8c03\u67e5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u6765\u589e\u5f3aLLM\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6837\u6027\u611f\u77e5\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8bcd\u5143\u7ea7\u522b\u7684\u591a\u6837\u6027\u6307\u6807\uff0c\u5e76\u5c06\u5176\u91cd\u65b0\u5236\u5b9a\u4e3a\u4e00\u4e2a\u5b9e\u9645\u7684\u76ee\u6807\u51fd\u6570\uff0c\u7136\u540e\u9009\u62e9\u6027\u5730\u5c06\u5176\u5e94\u7528\u4e8e\u6b63\u6837\u672c\u3002\u8fd9\u79cd\u65b9\u6cd5\u88ab\u6574\u5408\u8fdbR1-zero\u8bad\u7ec3\u6846\u67b6\u4e2d\u3002", "result": "\u901a\u8fc7\u5bf912\u4e2aLLM\u7684\u8bc4\u4f30\uff0c\u89c2\u5bdf\u5230\u89e3\u51b3\u65b9\u6848\u591a\u6837\u6027\u548cPotential at k\uff08\u4e00\u4e2a\u91cf\u5316LLM\u63a8\u7406\u6f5c\u529b\u7684\u65b0\u6307\u6807\uff09\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6b63\u76f8\u5173\u3002\u6240\u63d0\u51fa\u7684\u591a\u6837\u6027\u611f\u77e5\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u5728\u56db\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u5347\u4e863.5%\u7684\u8868\u73b0\u3002", "conclusion": "\u6587\u7ae0\u603b\u7ed3\u6307\u51fa\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u591a\u6837\u6027\u611f\u77e5\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6210\u6548\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u751f\u6210\u4e86\u66f4\u591a\u6837\u5316\u548c\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23349", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.23349", "abs": "https://arxiv.org/abs/2505.23349", "authors": ["Sheng Ouyang", "Yulan Hu", "Ge Chen", "Qingyang Li", "Fuzheng Zhang", "Yong Liu"], "title": "Towards Reward Fairness in RLHF: From a Resource Allocation Perspective", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ACL 2025", "summary": "Rewards serve as proxies for human preferences and play a crucial role in\nReinforcement Learning from Human Feedback (RLHF). However, if these rewards\nare inherently imperfect, exhibiting various biases, they can adversely affect\nthe alignment of large language models (LLMs). In this paper, we collectively\ndefine the various biases present in rewards as the problem of reward\nunfairness. We propose a bias-agnostic method to address the issue of reward\nfairness from a resource allocation perspective, without specifically designing\nfor each type of bias, yet effectively mitigating them. Specifically, we model\npreference learning as a resource allocation problem, treating rewards as\nresources to be allocated while considering the trade-off between utility and\nfairness in their distribution. We propose two methods, Fairness Regularization\nand Fairness Coefficient, to achieve fairness in rewards. We apply our methods\nin both verification and reinforcement learning scenarios to obtain a fairness\nreward model and a policy model, respectively. Experiments conducted in these\nscenarios demonstrate that our approach aligns LLMs with human preferences in a\nmore fair manner.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u8fd9\u7bc7\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u7f13\u89e3\u5956\u52b1\u4fe1\u53f7\u4e2d\u7684\u4e0d\u516c\u5e73\u6027\u95ee\u9898\uff0c\u800c\u65e0\u9700\u660e\u786e\u5904\u7406\u6bcf\u79cd\u5177\u4f53\u7684\u504f\u5dee\u7c7b\u578b\u3002\u901a\u8fc7\u5c06\u504f\u597d\u5b66\u4e60\u89c6\u4e3a\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u516c\u5e73\u6027\u6b63\u5219\u5316\u548c\u516c\u5e73\u6027\u7cfb\u6570\u7b49\u6280\u672f\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u4e86\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u516c\u5e73\u6027\u7684\u80fd\u529b\u3002", "motivation": "\u5956\u52b1\u901a\u5e38\u88ab\u89c6\u4e3a\u4eba\u7c7b\u504f\u597d\u7684\u4ee3\u7406\uff0c\u5728\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002\u7136\u800c\uff0c\u5982\u679c\u8fd9\u4e9b\u5956\u52b1\u672c\u8eab\u5b58\u5728\u5404\u79cd\u504f\u5dee\uff0c\u5219\u53ef\u80fd\u5bfc\u81f4\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5bf9\u9f50\u6548\u679c\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u51cf\u8f7b\u5956\u52b1\u4e2d\u7684\u504f\u5dee\u3001\u63d0\u9ad8\u5176\u516c\u5e73\u6027\u6210\u4e3a\u7814\u7a76\u7684\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4ece\u8d44\u6e90\u5206\u914d\u7684\u89d2\u5ea6\u89e3\u51b3\u5956\u52b1\u516c\u5e73\u6027\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u800c\u65e0\u9700\u4e13\u95e8\u9488\u5bf9\u6bcf\u79cd\u7c7b\u578b\u7684\u504f\u5dee\u8fdb\u884c\u8bbe\u8ba1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4f5c\u8005\u5f15\u5165\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u516c\u5e73\u6027\u6b63\u5219\u5316\uff08Fairness Regularization\uff09\u548c\u516c\u5e73\u6027\u7cfb\u6570\uff08Fairness Coefficient\uff09\uff0c\u5206\u522b\u5e94\u7528\u4e8e\u9a8c\u8bc1\u548c\u5f3a\u5316\u5b66\u4e60\u573a\u666f\uff0c\u4ee5\u83b7\u5f97\u516c\u5e73\u7684\u5956\u52b1\u6a21\u578b\u548c\u7b56\u7565\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u591a\u4e2a\u573a\u666f\u4e0b\u6709\u6548\u7f13\u89e3\u5956\u52b1\u7684\u4e0d\u516c\u5e73\u95ee\u9898\uff0c\u4ece\u800c\u66f4\u516c\u5e73\u5730\u5bf9\u9f50\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5c06\u504f\u597d\u5b66\u4e60\u5efa\u6a21\u4e3a\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u516c\u5e73\u6027\u6b63\u5219\u5316\u548c\u516c\u5e73\u6027\u7cfb\u6570\u4e24\u79cd\u65b9\u6cd5\uff0c\u6210\u529f\u5730\u5728\u4e0d\u9488\u5bf9\u7279\u5b9a\u504f\u5dee\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u5956\u52b1\u7684\u516c\u5e73\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u4ee5\u66f4\u52a0\u516c\u5e73\u7684\u65b9\u5f0f\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u3002"}}
{"id": "2505.23247", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.23247", "abs": "https://arxiv.org/abs/2505.23247", "authors": ["Zonglin Yang", "Zhexuan Gu", "Houduo Qi", "Yancheng Yuan"], "title": "Accelerating RLHF Training with Reward Variance Increase", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) is an essential technique\nfor ensuring that large language models (LLMs) are aligned with human values\nand preferences during the post-training phase. As an effective RLHF approach,\ngroup relative policy optimization (GRPO) has demonstrated success in many\nLLM-based applications. However, efficient GRPO-based RLHF training remains a\nchallenge. Recent studies reveal that a higher reward variance of the initial\npolicy model leads to faster RLHF training. Inspired by this finding, we\npropose a practical reward adjustment model to accelerate RLHF training by\nprovably increasing the reward variance and preserving the relative preferences\nand reward expectation. Our reward adjustment method inherently poses a\nnonconvex optimization problem, which is NP-hard to solve in general. To\novercome the computational challenges, we design a novel $O(n \\log n)$\nalgorithm to find a global solution of the nonconvex reward adjustment model by\nexplicitly characterizing the extreme points of the feasible set. As an\nimportant application, we naturally integrate this reward adjustment model into\nthe GRPO algorithm, leading to a more efficient GRPO with reward variance\nincrease (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we\nprovide an indirect explanation for the empirical effectiveness of GRPO with\nrule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment\nresults demonstrate that the GRPOVI algorithm can significantly improve the\nRLHF training efficiency compared to the original GRPO algorithm.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5956\u52b1\u8c03\u6574\u65b9\u6cd5\u53ca\u5176\u9ad8\u6548\u6c42\u89e3\u7b97\u6cd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eGRPO\u7b97\u6cd5\u4e2d\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u4e86\u5f3a\u5316\u5b66\u4e60\u4ece\u4eba\u7c7b\u53cd\u9988\u8bad\u7ec3\u7684\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684GRPO\u65b9\u6cd5\u867d\u7136\u5728\u8bb8\u591a\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5176\u8bad\u7ec3\u6548\u7387\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002\u7814\u7a76\u8868\u660e\u521d\u59cb\u7b56\u7565\u6a21\u578b\u7684\u5956\u52b1\u65b9\u5dee\u8d8a\u5927\uff0cRLHF\u8bad\u7ec3\u8d8a\u5feb\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u4f5c\u8005\u8bd5\u56fe\u901a\u8fc7\u63d0\u9ad8\u5956\u52b1\u65b9\u5dee\u6765\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u5956\u52b1\u8c03\u6574\u6a21\u578b\uff0c\u65e8\u5728\u589e\u52a0\u5956\u52b1\u65b9\u5dee\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5bf9\u504f\u597d\u548c\u5956\u52b1\u671f\u671b\u4e0d\u53d8\u3002\u4e3a\u4e86\u89e3\u51b3\u8be5\u6a21\u578b\u5e26\u6765\u7684\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u4ed6\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2aO(n log n)\u7684\u9ad8\u6548\u7b97\u6cd5\u6765\u5bfb\u627e\u5168\u5c40\u89e3\uff0c\u5e76\u5c06\u8fd9\u4e00\u65b9\u6cd5\u96c6\u6210\u5230GRPO\u7b97\u6cd5\u4e2d\uff0c\u5f62\u6210\u4e86GRPOVI\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGRPOVI\u7b97\u6cd5\u76f8\u6bd4\u539f\u59cbGRPO\u7b97\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8RLHF\u8bad\u7ec3\u7684\u6548\u7387\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u95f4\u63a5\u89e3\u91ca\uff0c\u8bf4\u660e\u4e3a\u4f55\u5728DeepSeek-R1\u4e2d\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u673a\u5236\u5728\u5b9e\u8df5\u4e2d\u662f\u6709\u6548\u7684\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5956\u52b1\u8c03\u6574\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u52a0\u901f\u57fa\u4e8e\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u4ece\u4eba\u7c7b\u53cd\u9988\uff08RLHF\uff09\u8bad\u7ec3\u8fc7\u7a0b\u3002\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u4ed6\u4eec\u5f00\u53d1\u4e86\u66f4\u9ad8\u6548\u7684GRPOVI\u7b97\u6cd5\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5176\u76f8\u5bf9\u4e8e\u539f\u59cbGRPO\u7b97\u6cd5\u5728RLHF\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u7684\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2505.22961", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.22961", "abs": "https://arxiv.org/abs/2505.22961", "authors": ["Peixuan Han", "Zijia Liu", "Jiaxuan You"], "title": "ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have shown promising potential in persuasion,\nbut existing works on training LLM persuaders are still preliminary. Notably,\nwhile humans are skilled in modeling their opponent's thoughts and opinions\nproactively and dynamically, current LLMs struggle with such Theory of Mind\n(ToM) reasoning, resulting in limited diversity and opponent awareness. To\naddress this limitation, we introduce Theory of Mind Augmented Persuader\n(ToMAP), a novel approach for building more flexible persuader agents by\nincorporating two theory of mind modules that enhance the persuader's awareness\nand analysis of the opponent's mental state. Specifically, we begin by\nprompting the persuader to consider possible objections to the target central\nclaim, and then use a text encoder paired with a trained MLP classifier to\npredict the opponent's current stance on these counterclaims. Our carefully\ndesigned reinforcement learning schema enables the persuader learns how to\nanalyze opponent-related information and utilize it to generate more effective\narguments. Experiments show that the ToMAP persuader, while containing only 3B\nparameters, outperforms much larger baselines, like GPT-4o, with a relative\ngain of 39.4% across multiple persuadee models and diverse corpora. Notably,\nToMAP exhibits complex reasoning chains and reduced repetition during training,\nwhich leads to more diverse and effective arguments. The opponent-aware feature\nof ToMAP also makes it suitable for long conversations and enables it to employ\nmore logical and opponent-aware strategies. These results underscore our\nmethod's effectiveness and highlight its potential for developing more\npersuasive language agents. Code is available at:\nhttps://github.com/ulab-uiuc/ToMAP.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bf4\u670d\u4ee3\u7406ToMAP\uff0c\u901a\u8fc7\u7ed3\u5408\u7406\u8bba\u5fc3\u667a\u6a21\u5757\u548c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bf4\u670d\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6a21\u62df\u5bf9\u624b\u601d\u7ef4\u548c\u751f\u6210\u591a\u6837\u5316\u8bf4\u670d\u7b56\u7565\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u4eba\u7c7b\u5728\u8fd9\u65b9\u9762\u5177\u6709\u8f83\u5f3a\u7684\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u589e\u5f3a\u8bf4\u670d\u8005\u4ee3\u7406\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u9884\u6d4b\u5bf9\u624b\u7acb\u573a\u7684\u6a21\u5757\u548c\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u5229\u7528\u5bf9\u624b\u76f8\u5173\u4fe1\u606f\u751f\u6210\u66f4\u6709\u6548\u7684\u8bba\u70b9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c3d\u7ba1\u53c2\u6570\u91cf\u4ec5\u4e3a3B\uff0cToMAP\u7684\u8868\u73b0\u4f18\u4e8eGPT-4o\u7b49\u66f4\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u8bf4\u670d\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e8639.4%\u7684\u76f8\u5bf9\u63d0\u5347\u3002", "conclusion": "ToMAP\u901a\u8fc7\u5f15\u5165\u4e24\u4e2a\u7406\u8bba\u5fc3\u667a\u6a21\u5757\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bf4\u670d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u5177\u903b\u8f91\u6027\u548c\u5bf9\u624b\u610f\u8bc6\u7684\u8bf4\u670d\u80fd\u529b\u3002"}}
{"id": "2505.22914", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.22914", "abs": "https://arxiv.org/abs/2505.22914", "authors": ["Maksim Kolodiazhnyi", "Denis Tarasov", "Dmitrii Zhemchuzhnikov", "Alexander Nikulin", "Ilya Zisman", "Anna Vorontsova", "Anton Konushin", "Vladislav Kurenkov", "Danila Rukhovich"], "title": "cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Computer-Aided Design (CAD) plays a central role in engineering and\nmanufacturing, making it possible to create precise and editable 3D models.\nUsing a variety of sensor or user-provided data as inputs for CAD\nreconstruction can democratize access to design applications. However, existing\nmethods typically focus on a single input modality, such as point clouds,\nimages, or text, which limits their generalizability and robustness. Leveraging\nrecent advances in vision-language models (VLM), we propose a multi-modal CAD\nreconstruction model that simultaneously processes all three input modalities.\nInspired by large language model (LLM) training paradigms, we adopt a two-stage\npipeline: supervised fine-tuning (SFT) on large-scale procedurally generated\ndata, followed by reinforcement learning (RL) fine-tuning using online\nfeedback, obtained programatically. Furthermore, we are the first to explore RL\nfine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such\nas Group Relative Preference Optimization (GRPO) outperform offline\nalternatives. In the DeepCAD benchmark, our SFT model outperforms existing\nsingle-modal approaches in all three input modalities simultaneously. More\nimportantly, after RL fine-tuning, cadrille sets new state-of-the-art on three\nchallenging datasets, including a real-world one.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u6a21\u6001CAD\u91cd\u5efa\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684CAD\u91cd\u5efa\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u5355\u4e00\u8f93\u5165\u6a21\u6001\uff0c\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5904\u7406\u591a\u79cd\u8f93\u5165\u6a21\u6001\u7684CAD\u91cd\u5efa\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u8bbe\u8ba1\u5e94\u7528\u7684\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u53d7\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u8303\u5f0f\u7684\u542f\u53d1\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u9996\u5148\u5728\u5927\u89c4\u6a21\u7a0b\u5e8f\u751f\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03(SFT)\uff0c\u7136\u540e\u4f7f\u7528\u5728\u7ebf\u53cd\u9988\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u3002\u9996\u6b21\u63a2\u7d22\u4e86\u9488\u5bf9CAD\u4efb\u52a1\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5982Group Relative Preference Optimization (GRPO) \u8d85\u8fc7\u79bb\u7ebf\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "result": "\u5728DeepCAD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u63d0\u51fa\u7684SFT\u6a21\u578b\u5728\u6240\u6709\u4e09\u79cd\u8f93\u5165\u6a21\u6001\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u5355\u6a21\u6001\u65b9\u6cd5\u3002\u7ecf\u8fc7RL\u5fae\u8c03\u540e\uff0c\u8be5\u6a21\u578b\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5305\u62ec\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001CAD\u91cd\u5efa\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u70b9\u4e91\u3001\u56fe\u50cf\u548c\u6587\u672c\u4e09\u79cd\u8f93\u5165\u6a21\u6001\uff0c\u5e76\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u8868\u73b0\u3002\u6700\u7ec8\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u4f18\u7ed3\u679c\uff0c\u5c24\u5176\u662f\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002"}}
{"id": "2505.22756", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.22756", "abs": "https://arxiv.org/abs/2505.22756", "authors": ["Tian Qin", "Core Francisco Park", "Mujin Kwun", "Aaron Walsman", "Eran Malach", "Nikhil Anand", "Hidenori Tanaka", "David Alvarez-Melis"], "title": "Decomposing Elements of Problem Solving: What \"Math\" Does RL Teach?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Mathematical reasoning tasks have become prominent benchmarks for assessing\nthe reasoning capabilities of LLMs, especially with reinforcement learning (RL)\nmethods such as GRPO showing significant performance gains. However, accuracy\nmetrics alone do not support fine-grained assessment of capabilities and fail\nto reveal which problem-solving skills have been internalized. To better\nunderstand these capabilities, we propose to decompose problem solving into\nfundamental capabilities: Plan (mapping questions to sequences of steps),\nExecute (correctly performing solution steps), and Verify (identifying the\ncorrectness of a solution). Empirically, we find that GRPO mainly enhances the\nexecution skill-improving execution robustness on problems the model already\nknows how to solve-a phenomenon we call temperature distillation. More\nimportantly, we show that RL-trained models struggle with fundamentally new\nproblems, hitting a 'coverage wall' due to insufficient planning skills. To\nexplore RL's impact more deeply, we construct a minimal, synthetic\nsolution-tree navigation task as an analogy for mathematical problem-solving.\nThis controlled setup replicates our empirical findings, confirming RL\nprimarily boosts execution robustness. Importantly, in this setting, we\nidentify conditions under which RL can potentially overcome the coverage wall\nthrough improved exploration and generalization to new solution paths. Our\nfindings provide insights into the role of RL in enhancing LLM reasoning,\nexpose key limitations, and suggest a path toward overcoming these barriers.\nCode is available at https://github.com/cfpark00/RL-Wall.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5982\u4f55\u5f71\u54cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0RL\u4e3b\u8981\u63d0\u5347\u4e86\u5df2\u77e5\u95ee\u9898\u7684\u6267\u884c\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u65b0\u95ee\u9898\u4e0a\u53d7\u9650\u4e8e\u89c4\u5212\u80fd\u529b\u4e0d\u8db3\u3002", "motivation": "\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u5df2\u6210\u4e3a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u57fa\u51c6\uff0c\u5c24\u5176\u662f\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u5982GRPO\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u7136\u800c\uff0c\u4ec5\u51ed\u51c6\u786e\u7387\u6307\u6807\u65e0\u6cd5\u5bf9\u8fd9\u4e9b\u80fd\u529b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff0c\u4e5f\u65e0\u6cd5\u63ed\u793a\u54ea\u4e9b\u95ee\u9898\u89e3\u51b3\u6280\u80fd\u5df2\u7ecf\u88ab\u5185\u5316\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u66f4\u597d\u5730\u7406\u89e3RL\u8bad\u7ec3\u5982\u4f55\u5f71\u54cdLLM\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u89e3\u95ee\u9898\u6c42\u89e3\u4e3a\u8ba1\u5212\u3001\u6267\u884c\u548c\u9a8c\u8bc1\u4e09\u4e2a\u57fa\u672c\u80fd\u529b\u6765\u8bc4\u4f30LLM\u7684\u80fd\u529b\u3002\u4f5c\u8005\u4f7f\u7528GRPO\u7b49\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u6700\u5c0f\u7684\u5408\u6210\u89e3\u51b3\u65b9\u6848\u6811\u5bfc\u822a\u4efb\u52a1\u4f5c\u4e3a\u6570\u5b66\u95ee\u9898\u6c42\u89e3\u7684\u7c7b\u6bd4\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cGRPO\u4e3b\u8981\u589e\u5f3a\u4e86\u6267\u884c\u6280\u80fd\uff0c\u6539\u5584\u4e86\u6a21\u578b\u5df2\u7ecf\u77e5\u9053\u5982\u4f55\u89e3\u51b3\u7684\u95ee\u9898\u7684\u6267\u884c\u9c81\u68d2\u6027\u3002\u4f46\u540c\u65f6\u53d1\u73b0\uff0cRL\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5904\u7406\u5168\u65b0\u7684\u95ee\u9898\u65f6\u9047\u5230\u4e86\u56f0\u96be\uff0c\u8fd9\u662f\u7531\u4e8e\u89c4\u5212\u6280\u80fd\u7684\u4e0d\u8db3\u5bfc\u81f4\u7684\u201c\u8986\u76d6\u5899\u201d\u3002\u901a\u8fc7\u5408\u6210\u7684\u4efb\u52a1\u8bbe\u7f6e\uff0c\u7814\u7a76\u8005\u786e\u8ba4\u4e86RL\u4e3b\u8981\u63d0\u5347\u7684\u662f\u6267\u884c\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u627e\u5230\u4e86RL\u53ef\u80fd\u901a\u8fc7\u66f4\u597d\u7684\u63a2\u7d22\u548c\u6cdb\u5316\u6765\u514b\u670d\u8986\u76d6\u5899\u7684\u6761\u4ef6\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u603b\u7ed3\u4e86RL\u5728\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u63ed\u793a\u4e86\u4e00\u4e9b\u5173\u952e\u7684\u9650\u5236\u56e0\u7d20\u3002\u7814\u7a76\u53d1\u73b0\uff0cRL\u4e3b\u8981\u63d0\u9ad8\u4e86\u6267\u884c\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u9762\u5bf9\u5168\u65b0\u7684\u95ee\u9898\u65f6\uff0c\u7531\u4e8e\u89c4\u5212\u80fd\u529b\u4e0d\u8db3\u800c\u9047\u5230\u4e86\u201c\u8986\u76d6\u5899\u201d\u3002\u901a\u8fc7\u6539\u8fdb\u63a2\u7d22\u548c\u6cdb\u5316\u5230\u65b0\u7684\u89e3\u51b3\u8def\u5f84\uff0cRL\u6709\u53ef\u80fd\u514b\u670d\u8fd9\u4e00\u969c\u788d\u3002"}}
{"id": "2505.22695", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.22695", "abs": "https://arxiv.org/abs/2505.22695", "authors": ["Tengfei Lyu", "Siyuan Feng", "Hao Liu", "Hai Yang"], "title": "LLM-ODDR: A Large Language Model Framework for Joint Order Dispatching and Driver Repositioning", "categories": ["cs.LG"], "comment": null, "summary": "Ride-hailing platforms face significant challenges in optimizing order\ndispatching and driver repositioning operations in dynamic urban environments.\nTraditional approaches based on combinatorial optimization, rule-based\nheuristics, and reinforcement learning often overlook driver income fairness,\ninterpretability, and adaptability to real-world dynamics. To address these\ngaps, we propose LLM-ODDR, a novel framework leveraging Large Language Models\n(LLMs) for joint Order Dispatching and Driver Repositioning (ODDR) in\nride-hailing services. LLM-ODDR framework comprises three key components: (1)\nMulti-objective-guided Order Value Refinement, which evaluates orders by\nconsidering multiple objectives to determine their overall value; (2)\nFairness-aware Order Dispatching, which balances platform revenue with driver\nincome fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning,\nwhich optimizes idle vehicle placement based on historical patterns and\nprojected supply. We also develop JointDR-GPT, a fine-tuned model optimized for\nODDR tasks with domain knowledge. Extensive experiments on real-world datasets\nfrom Manhattan taxi operations demonstrate that our framework significantly\noutperforms traditional methods in terms of effectiveness, adaptability to\nanomalous conditions, and decision interpretability. To our knowledge, this is\nthe first exploration of LLMs as decision-making agents in ride-hailing ODDR\ntasks, establishing foundational insights for integrating advanced language\nmodels within intelligent transportation systems.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "LLM-ODDR is a new ride-hailing decision-making framework using Large Language Models to improve order dispatching and driver repositioning by addressing fairness, adaptability, and interpretability shortcomings of traditional methods.", "motivation": "Traditional approaches to ride-hailing order dispatching and driver repositioning often neglect driver income fairness, interpretability, and adaptability to real-world dynamics. The authors aim to address these gaps by introducing a novel framework that leverages Large Language Models (LLMs) for more effective and interpretable decision-making.", "method": "The LLM-ODDR framework includes three components: (1) Multi-objective-guided Order Value Refinement to evaluate orders based on multiple objectives; (2) Fairness-aware Order Dispatching to balance platform revenue and driver income fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning to optimize idle vehicle placement using historical patterns and projected supply. A fine-tuned model, JointDR-GPT, is also developed with domain-specific knowledge.", "result": "Experiments on real-world Manhattan taxi datasets show that the LLM-ODDR framework significantly outperforms traditional combinatorial optimization, rule-based heuristics, and reinforcement learning methods in terms of effectiveness, adaptability under anomalous conditions, and interpretability of decisions.", "conclusion": "LLM-ODDR represents the first application of Large Language Models as decision-making agents in ride-hailing order dispatching and driver repositioning tasks. It outperforms traditional methods in effectiveness, adaptability, and interpretability, providing foundational insights for integrating advanced language models into intelligent transportation systems."}}
