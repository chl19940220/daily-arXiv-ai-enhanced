<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 2]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: This position paper critically surveys recent empirical developments in human-AI agent collaboration and proposes a new conceptual architecture to bridge persistent gaps.


<details>
  <summary>Details</summary>
Motivation: We observe a lack of a unifying theoretical framework that can coherently integrate these varied studies, especially when tackling open-ended, complex tasks.

Method: We propose a novel conceptual architecture: one that systematically interlinks the technical details of multi-agent coordination, knowledge management, cybernetic feedback loops, and higher-level control mechanisms.

Result: The paper's structure allows it to be read from any section, serving equally as a critical review of technical implementations and as a forward-looking reference for designing or extending human-AI symbioses.

Conclusion: Together, these insights offer a stepping stone toward deeper co-evolution of human cognition and AI capability.

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [2] [Artificial Expert Intelligence through PAC-reasoning](https://arxiv.org/abs/2412.02441)
*Shai Shalev-Shwartz,Amnon Shashua,Gal Beniamini,Yoav Levine,Or Sharir,Noam Wies,Ido Ben-Shaul,Tomer Nussbaum,Shir Granot Peled*

Main category: cs.AI

TL;DR: 本文介绍了Artificial Expert Intelligence (AEI)，它结合了领域专业知识与类似顶级人类专家的精确推理能力，提出了一种新的推理框架System 3，用于解决现有AI在适应性和精确性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的AI系统虽然在预定义任务上表现出色，但在适应性和解决新颖问题的精确性方面仍存在不足。因此，需要一种超越AGI和狭义AI限制的新方法，即Artificial Expert Intelligence（AEI）。

Method: AEI引入了一种名为“Probably Approximately Correct (PAC) Reasoning”的新范式，该范式参考了人类思维中System 1和System 2的区分，强调精确推理类似于科学方法的严谨性。

Result: 提出了一个全新的推理框架——System 3，其基于严格的科学方法，能够实现可靠的问题分解并控制推理精度，从而推动误差有界的推理时间学习的发展。

Conclusion: AEI通过结合领域专业知识与精确推理能力，为复杂问题求解提供了新的框架，并且在理论上保证了分解问题的可靠性以及控制推理精度的机制。这种系统被称作System 3，它为误差有界的推理时间学习奠定了基础。

Abstract: Artificial Expert Intelligence (AEI) seeks to transcend the limitations of
both Artificial General Intelligence (AGI) and narrow AI by integrating
domain-specific expertise with critical, precise reasoning capabilities akin to
those of top human experts. Existing AI systems often excel at predefined tasks
but struggle with adaptability and precision in novel problem-solving. To
overcome this, AEI introduces a framework for ``Probably Approximately Correct
(PAC) Reasoning". This paradigm provides robust theoretical guarantees for
reliably decomposing complex problems, with a practical mechanism for
controlling reasoning precision. In reference to the division of human thought
into System 1 for intuitive thinking and System 2 for reflective
reasoning~\citep{tversky1974judgment}, we refer to this new type of reasoning
as System 3 for precise reasoning, inspired by the rigor of the scientific
method. AEI thus establishes a foundation for error-bounded, inference-time
learning.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning](https://arxiv.org/abs/2505.24871)
*Yiqing Liang,Jielin Qiu,Wenhao Ding,Zuxin Liu,James Tompkin,Mengdi Xu,Mengzhou Xia,Zhengzhong Tu,Laixi Shi,Jiacheng Zhu*

Main category: cs.CV

TL;DR: 本文介绍了一种适用于多模态大语言模型的强化学习框架，通过优化数据混合策略，在多任务学习中实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 由于多模态任务的多样性和复杂性，需要一种有效的数据混合策略来解决不同数据集之间的冲突目标并提升泛化能力。

Method: 开发了一个多模态RLVR框架，并提出了一个能够预测和优化数据混合效果的策略。

Result: 实验表明，结合数据混合预测策略的多领域RLVR训练显著增强了多模态模型的推理能力，平均准确率提升了5.24%至20.74%。

Conclusion: 论文提出了一种用于多模态大语言模型的强化学习框架，通过系统化的数据混合策略显著提升了模型在分布外基准测试中的准确性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for post-training large language models (LLMs), achieving
state-of-the-art performance on tasks with structured, verifiable answers.
Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but
is complicated by the broader, heterogeneous nature of vision-language tasks
that demand nuanced visual, logical, and spatial capabilities. As such,
training MLLMs using RLVR on multiple datasets could be beneficial but creates
challenges with conflicting objectives from interaction among diverse datasets,
highlighting the need for optimal dataset mixture strategies to improve
generalization and reasoning. We introduce a systematic post-training framework
for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation
and benchmark implementation. Specifically, (1) We developed a multimodal RLVR
framework for multi-dataset post-training by curating a dataset that contains
different verifiable vision-language problems and enabling multi-domain online
RL learning with different verifiable rewards; (2) We proposed a data mixture
strategy that learns to predict the RL fine-tuning outcome from the data
mixture distribution, and consequently optimizes the best mixture.
Comprehensive experiments showcase that multi-domain RLVR training, when
combined with mixture prediction strategies, can significantly boost MLLM
general reasoning capacities. Our best mixture improves the post-trained
model's accuracy on out-of-distribution benchmarks by an average of 5.24%
compared to the same model post-trained with uniform data mixture, and by a
total of 20.74% compared to the pre-finetuning baseline.

</details>


### [4] [MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM](https://arxiv.org/abs/2505.24238)
*Bowen Dong,Minheng Ni,Zitong Huang,Guanglei Yang,Wangmeng Zuo,Lei Zhang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Multimodal hallucination in multimodal large language models (MLLMs)
restricts the correctness of MLLMs. However, multimodal hallucinations are
multi-sourced and arise from diverse causes. Existing benchmarks fail to
adequately distinguish between perception-induced hallucinations and
reasoning-induced hallucinations. This failure constitutes a significant issue
and hinders the diagnosis of multimodal reasoning failures within MLLMs. To
address this, we propose the {\dataset} benchmark, which isolates reasoning
hallucinations by constructing questions where input images are correctly
perceived by MLLMs yet reasoning errors persist. {\dataset} introduces
multi-granular evaluation metrics: accuracy, factuality, and LLMs hallucination
score for hallucination quantification. Our analysis reveals that (1) the model
scale, data scale, and training stages significantly affect the degree of
logical, fabrication, and factual hallucinations; (2) current MLLMs show no
effective improvement on spatial hallucinations caused by misinterpreted
spatial relationships, indicating their limited visual reasoning capabilities;
and (3) question types correlate with distinct hallucination patterns,
highlighting targeted challenges and potential mitigation strategies. To
address these challenges, we propose {\method}, a method that combines
curriculum reinforcement fine-tuning to encourage models to generate
logic-consistent reasoning chains by stepwise reducing learning difficulty, and
collaborative hint inference to reduce reasoning complexity. {\method}
establishes a baseline on {\dataset}, and reduces the logical hallucinations in
original base models.

</details>


### [5] [MvDeCor: Multi-view Dense Correspondence Learning for Fine-grained 3D Segmentation](https://arxiv.org/abs/2208.08580)
*Gopal Sharma,Kangxue Yin,Subhransu Maji,Evangelos Kalogerakis,Or Litany,Sanja Fidler*

Main category: cs.CV

TL;DR: 这篇论文提出了MvDeCor，一种结合2D自监督技术和3D几何推理的细粒度3D形状分割方法，其在多种条件下均表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于基于视图的表面表示在建模高分辨率表面细节和纹理方面比基于点云或体素的3D表示更有效，因此作者希望通过利用2D领域的自监督技术来提升3D形状分割的效果。

Method: 该论文提出了一种基于多视角渲染和对比学习的2D自监督学习方法，用于3D形状分割任务。具体来说，作者通过多个视角渲染3D形状，并在对比学习框架内设置密集对应关系学习任务，从而获得视图不变且几何一致的2D表示。

Result: 实验显示，与仅使用2D或3D自监督的方法相比，MvDeCor在RenderPeople（带纹理）和PartNet（不带纹理）数据集上的表现更好，尤其是在训练视图稀疏或形状带有纹理时效果更显著。

Conclusion: 实验结果表明，MvDeCor在细粒度3D形状分割任务上优于现有方法，特别是在训练数据有限或形状具有纹理的情况下。

Abstract: We propose to utilize self-supervised techniques in the 2D domain for
fine-grained 3D shape segmentation tasks. This is inspired by the observation
that view-based surface representations are more effective at modeling
high-resolution surface details and texture than their 3D counterparts based on
point clouds or voxel occupancy. Specifically, given a 3D shape, we render it
from multiple views, and set up a dense correspondence learning task within the
contrastive learning framework. As a result, the learned 2D representations are
view-invariant and geometrically consistent, leading to better generalization
when trained on a limited number of labeled shapes compared to alternatives
that utilize self-supervision in 2D or 3D alone. Experiments on textured
(RenderPeople) and untextured (PartNet) 3D datasets show that our method
outperforms state-of-the-art alternatives in fine-grained part segmentation.
The improvements over baselines are greater when only a sparse set of views is
available for training or when shapes are textured, indicating that MvDeCor
benefits from both 2D processing and 3D geometric reasoning.

</details>


### [6] [Mix3D: Out-of-Context Data Augmentation for 3D Scenes](https://arxiv.org/abs/2110.02210)
*Alexey Nekrasov,Jonas Schult,Or Litany,Bastian Leibe,Francis Engelmann*

Main category: cs.CV

TL;DR: Mix3D是一种通过混合两个增强场景来创建新训练样本的数据增强技术，旨在平衡全局上下文和局部几何特征，从而提高3D场景分割模型的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 当前的3D场景分割工作过于依赖于全局上下文信息，这可能导致一些错误，如将行人误认为汽车。因此，需要一种平衡全局场景上下文和局部几何特征的方法，以提高模型在超出训练集上下文情况下的泛化能力。

Method: 提出了一种名为Mix3D的混合技术，通过将两个增强的3D场景结合，生成新的训练样本。这样可以让对象实例出现在新的非上下文环境中，增加模型对局部结构的理解能力。

Result: 实验结果表明，使用Mix3D训练的模型在多个室内（ScanNet, S3DIS）和室外（SemanticKITTI）数据集上表现出显著的性能提升。特别是与MinkowskiNet结合时，在ScanNet测试基准上的mIoU达到了78.1，超过了之前的所有最先进方法。

Conclusion: Mix3D是一种用于分割大规模3D场景的数据增强技术。通过结合两个增强场景来创建新的训练样本，使得模型能够同时依赖于局部结构和全局场景上下文进行语义推断。实验表明，使用Mix3D训练的模型在室内和室外数据集上都有显著的性能提升，并且可以与任何现有方法结合使用。

Abstract: We present Mix3D, a data augmentation technique for segmenting large-scale 3D
scenes. Since scene context helps reasoning about object semantics, current
works focus on models with large capacity and receptive fields that can fully
capture the global context of an input 3D scene. However, strong contextual
priors can have detrimental implications like mistaking a pedestrian crossing
the street for a car. In this work, we focus on the importance of balancing
global scene context and local geometry, with the goal of generalizing beyond
the contextual priors in the training set. In particular, we propose a "mixing"
technique which creates new training samples by combining two augmented scenes.
By doing so, object instances are implicitly placed into novel out-of-context
environments and therefore making it harder for models to rely on scene context
alone, and instead infer semantics from local structure as well. We perform
detailed analysis to understand the importance of global context, local
structures and the effect of mixing scenes. In experiments, we show that models
trained with Mix3D profit from a significant performance boost on indoor
(ScanNet, S3DIS) and outdoor datasets (SemanticKITTI). Mix3D can be trivially
used with any existing method, e.g., trained with Mix3D, MinkowskiNet
outperforms all prior state-of-the-art methods by a significant margin on the
ScanNet test benchmark 78.1 mIoU. Code is available at:
https://nekrasov.dev/mix3d/

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning](https://arxiv.org/abs/2505.24850)
*Shuyao Xu,Cheng Peng,Jiangxuan Long,Weidi Xu,Wei Chu,Yuan Qi*

Main category: cs.LG

TL;DR: This paper introduces REDI, a framework for enhancing LLM reasoning performance by leveraging both positive and negative reasoning traces in an offline setting.


<details>
  <summary>Details</summary>
Motivation: The motivation is to utilize both positive and negative distilled reasoning traces to maximize LLM reasoning performance in an offline setting, as current practices typically discard incorrect reasoning examples.

Method: The paper proposes a two-stage framework called Reinforcement Distillation (REDI). The first stage involves learning from positive traces via Supervised Fine-Tuning (SFT), while the second stage refines the model using both positive and negative traces through the REDI objective, a reference-free loss function.

Result: The Qwen-REDI-1.5B model achieves an 83.1% score on MATH-500 (pass@1) and matches or surpasses the performance of DeepSeek-R1-Distill-Qwen-1.5B across various mathematical reasoning benchmarks.

Conclusion: The paper concludes that the proposed REDI framework effectively leverages both positive and negative reasoning traces to enhance LLM performance in an offline setting.

Abstract: Recent advances in model distillation demonstrate that data from advanced
reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer
complex reasoning abilities to smaller, efficient student models. However,
standard practices employ rejection sampling, discarding incorrect reasoning
examples -- valuable, yet often underutilized data. This paper addresses the
critical question: How can both positive and negative distilled reasoning
traces be effectively leveraged to maximize LLM reasoning performance in an
offline setting? To this end, We propose Reinforcement Distillation (REDI), a
two-stage framework. Stage 1 learns from positive traces via Supervised
Fine-Tuning (SFT). Stage 2 further refines the model using both positive and
negative traces through our proposed REDI objective. This novel objective is a
simple, reference-free loss function that outperforms established methods like
DPO and SimPO in this distillation context. Our empirical evaluations
demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT
combined with DPO/SimPO on mathematical reasoning tasks. Notably, the
Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples
from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).
Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a
model post-trained on 800k proprietary data) across various mathematical
reasoning benchmarks, establishing a new state-of-the-art for 1.5B models
post-trained offline with openly available data.

</details>


### [8] [Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting](https://arxiv.org/abs/2505.24710)
*Wei Chen,Jiahao Zhang,Haipeng Zhu,Boyan Xu,Zhifeng Hao,Keli Zhang,Junjian Ye,Ruichu Cai*

Main category: cs.LG

TL;DR: 为了解决大型语言模型 (LLMs) 缺乏推理能力和难以适应新环境的问题，本文提出了一种名为 Causal-aware LLMs 的方法，将结构因果模型 (SCM) 集成到决策过程中，从而提高模型对环境的理解和决策效率。


<details>
  <summary>Details</summary>
Motivation: 由于预训练模型缺乏推理能力且难以适应新环境，限制了它们在复杂现实任务中的应用。因此，本文受到人类认知过程的启发，提出了整合结构因果模型的方法来提升 LLM 的决策能力。

Method: 提出了一种名为 Causal-aware LLMs 的新方法，该方法结合了结构因果模型 (SCM) 和大型语言模型 (LLM)，采用“学习-适应-行动”的范式进行决策。具体包括：1. 在学习阶段，使用 LLM 提取环境特定的因果实体及其关系，以初始化环境的结构因果模型；2. 在适应阶段，通过外部反馈更新结构因果模型；3. 在行动阶段，利用强化学习代理基于结构因果知识制定策略。

Result: 在开放世界游戏 Crafter 中的22个多样化任务上进行了实验，结果验证了所提出的 Causal-aware LLMs 方法的有效性，表明其能够更好地理解和适应环境，并实现更高效的决策。

Conclusion: 通过将结构因果模型（SCM）集成到决策过程中，Causal-aware LLMs 能够更准确地理解环境并做出更高效的决策。实验结果表明，该方法在开放世界游戏 Crafter 中的22个不同任务中均表现出色，验证了其有效性。

Abstract: Large language models (LLMs) have shown great potential in decision-making
due to the vast amount of knowledge stored within the models. However, these
pre-trained models are prone to lack reasoning abilities and are difficult to
adapt to new environments, further hindering their application to complex
real-world tasks. To address these challenges, inspired by the human cognitive
process, we propose Causal-aware LLMs, which integrate the structural causal
model (SCM) into the decision-making process to model, update, and utilize
structured knowledge of the environment in a ``learning-adapting-acting"
paradigm. Specifically, in the learning stage, we first utilize an LLM to
extract the environment-specific causal entities and their causal relations to
initialize a structured causal model of the environment. Subsequently,in the
adapting stage, we update the structured causal model through external feedback
about the environment, via an idea of causal intervention. Finally, in the
acting stage, Causal-aware LLMs exploit structured causal knowledge for more
efficient policy-making through the reinforcement learning agent. The above
processes are performed iteratively to learn causal knowledge, ultimately
enabling the causal-aware LLMs to achieve a more accurate understanding of the
environment and make more efficient decisions. Experimental results across 22
diverse tasks within the open-world game ``Crafter" validate the effectiveness
of our proposed method.

</details>


### [9] [AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning](https://arxiv.org/abs/2505.24298)
*Wei Fu,Jiaxuan Gao,Xujie Shen,Chen Zhu,Zhiyu Mei,Chuyi He,Shusheng Xu,Guo Wei,Jun Mei,Jiashu Wang,Tongkai Yang,Binhang Yuan,Yi Wu*

Main category: cs.LG

TL;DR: 本文介绍了一个高效的异步强化学习系统AReaL，它通过解耦生成与训练、提高GPU利用率，在数学和代码推理任务上取得了显著的训练加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模RL系统多为同步式，这导致了严重的系统效率低下，因为生成必须等待直到批次中所有rollouts完成才能进行模型更新，从而造成GPU资源的浪费。

Method: AReaL采用了异步RL架构，其中rollout工人持续生成新输出，而训练工人在收集到一批数据后立即更新模型。此外，AReaL还采用了控制数据陈旧性的方法和一种陈旧性增强的PPO变体以稳定训练过程。

Result: 在数学和代码推理基准上的大量实验表明，与使用相同数量GPU的最佳同步系统相比，AReaL实现了高达2.57倍的训练加速，并且最终性能得到了保持或提升。

Conclusion: AReaL是一个完全异步的RL系统，通过将生成与训练完全解耦，并采用一系列系统级优化，显著提高了GPU利用率和训练速度，同时保持或提升了模型性能。

Abstract: Reinforcement learning (RL) has become a trending paradigm for training large
language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs
requires massive parallelization and poses an urgent need for efficient
training systems. Most existing large-scale RL systems for LLMs are synchronous
by alternating generation and training in a batch setting, where the rollouts
in each training batch are generated by the same (or latest) model. This
stabilizes RL training but suffers from severe system-level inefficiency.
Generation must wait until the longest output in the batch is completed before
model update, resulting in GPU underutilization. We present AReaL, a
\emph{fully asynchronous} RL system that completely decouples generation from
training. Rollout workers in AReaL continuously generate new outputs without
waiting, while training workers update the model whenever a batch of data is
collected. AReaL also incorporates a collection of system-level optimizations,
leading to substantially higher GPU utilization. To stabilize RL training,
AReaL balances the workload of rollout and training workers to control data
staleness, and adopts a staleness-enhanced PPO variant to better handle
outdated training samples. Extensive experiments on math and code reasoning
benchmarks show that AReaL achieves \textbf{up to 2.57$\times$ training
speedup} compared to the best synchronous systems with the same number of GPUs
and matched or even improved final performance. The code of AReaL is available
at https://github.com/inclusionAI/AReaL/.

</details>


### [10] [CodeV-R1: Reasoning-Enhanced Verilog Generation](https://arxiv.org/abs/2505.24183)
*Yaoyu Zhu,Di Huang,Hanqi Lyu,Xiaoyun Zhang,Chongxiao Li,Wenxuan Shi,Yutong Wu,Jianan Mu,Jinghua Wang,Yang Zhao,Pengwei Jin,Shuyao Cheng,Shengwen Liang,Xishan Zhang,Rui Zhang,Zidong Du,Qi Guo,Xing Hu,Yunji Chen*

Main category: cs.LG

TL;DR: 本文介绍了一个用于Verilog代码生成的强化学习框架CodeV-R1，解决了电子设计自动化中的三大挑战，并在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 虽然基于可验证奖励的强化学习（RLVR）已经在软件编程和数学问题等任务上取得了突破，但在电子设计自动化（EDA）领域，尤其是在从自然语言生成硬件描述语言如Verilog的任务中，仍然面临多个挑战：缺乏自动化的准确验证环境、缺少高质量的自然语言与代码配对数据，以及RLVR的计算成本过高。因此，论文旨在解决这些问题，推动RLVR在EDA领域的应用。

Method: 论文引入了一个名为CodeV-R1的RLVR框架，包含三个关键部分：一是开发了一个基于规则的测试平台生成器，用于进行等效性检查；二是提出了一个双向数据合成方法，结合开源Verilog代码与大语言模型生成的自然语言描述，利用测试平台验证一致性并过滤低质量样本；三是采用两阶段的训练策略，包括推理能力冷启动的蒸馏阶段和使用自适应DAPO算法的强化学习阶段，以降低训练成本。

Result: 论文提出的模型CodeV-R1-7B在两个基准测试VerilogEval v2和RTLLM v1.1上分别达到了68.6%和72.9%的pass@1得分，超过了现有最先进的方法12~20%，并且其性能甚至优于671B参数的DeepSeek-R1模型。这一结果表明，在有效解决EDA领域挑战的同时，该模型在资源效率和性能方面均表现出色。

Conclusion: 论文提出了一种基于强化学习的Verilog代码生成框架CodeV-R1，成功解决了电子设计自动化领域中缺乏自动验证环境、高质量数据稀缺和计算成本高昂的关键问题。通过规则测试平台生成、双向数据合成方法以及创新的“distill-then-RL”训练流程，模型在基准测试中表现优异，超越了现有最先进方法，并计划开放模型和数据集以促进相关领域的研究发展。

Abstract: Large language models (LLMs) trained via reinforcement learning with
verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,
automatable verification, such as software programming and mathematical
problems. Extending RLVR to electronic design automation (EDA), especially
automatically generating hardware description languages (HDLs) like Verilog
from natural-language (NL) specifications, however, poses three key challenges:
the lack of automated and accurate verification environments, the scarcity of
high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To
this end, we introduce CodeV-R1, an RLVR framework for training Verilog
generation LLMs. First, we develop a rule-based testbench generator that
performs robust equivalence checking against golden references. Second, we
propose a round-trip data synthesis method that pairs open-source Verilog
snippets with LLM-generated NL descriptions, verifies code-NL-code consistency
via the generated testbench, and filters out inequivalent examples to yield a
high-quality dataset. Third, we employ a two-stage "distill-then-RL" training
pipeline: distillation for the cold start of reasoning abilities, followed by
adaptive DAPO, our novel RLVR algorithm that can reduce training cost by
adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves
68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,
surpassing prior state-of-the-art by 12~20%, while matching or even exceeding
the performance of 671B DeepSeek-R1. We will release our model, training
pipeline, and dataset to facilitate research in EDA and LLM communities.

</details>


### [11] [LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Trainin](https://arxiv.org/abs/2505.24034)
*Bo Wu,Sid Wang,Yunhao Tang,Jia Ding,Eryk Helenowski,Liang Tan,Tengyu Xu,Tushar Gowda,Zhengxing Chen,Chen Zhu,Xiaocheng Tang,Yundi Qian,Beibei Zhu,Rui Hou*

Main category: cs.LG

TL;DR: 本文提出了高效的强化学习框架 LlamaRL，适用于大规模语言模型训练，通过异步设计和技术优化显著提升了训练速度和扩展能力。


<details>
  <summary>Details</summary>
Motivation: 为了应对大规模语言模型（LLM）在延迟和内存需求上的高要求，开发一种能够可靠管理具有数百到数千亿参数的策略模型的高效强化学习框架变得尤为具有挑战性。

Method: 该论文提出了基于原生 PyTorch 的完全分布式、异步 RL 框架 LlamaRL，并结合了多种最佳实践技术，如共置模型卸载、异步离策略训练和分布式直接内存访问用于权重同步。

Result: 实验表明，与 DeepSpeed-Chat 类系统相比，LlamaRL 在 405B 参数的策略模型上实现了最高 10.7 倍的速度提升，并且随着模型规模增加，效率优势进一步扩大。

Conclusion: LlamaRL 是一个为大规模语言模型设计的高效强化学习框架，其异步设计在理论上被证明能严格加速强化学习过程。

Abstract: Reinforcement Learning (RL) has become the most effective post-training
approach for improving the capabilities of Large Language Models (LLMs). In
practice, because of the high demands on latency and memory, it is particularly
challenging to develop an efficient RL framework that reliably manages policy
models with hundreds to thousands of billions of parameters.
  In this paper, we present LlamaRL, a fully distributed, asynchronous RL
framework optimized for efficient training of large-scale LLMs with various
model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a
handful to thousands of devices. LlamaRL introduces a streamlined,
single-controller architecture built entirely on native PyTorch, enabling
modularity, ease of use, and seamless scalability to thousands of GPUs. We also
provide a theoretical analysis of LlamaRL's efficiency, including a formal
proof that its asynchronous design leads to strict RL speed-up. Empirically, by
leveraging best practices such as colocated model offloading, asynchronous
off-policy training, and distributed direct memory access for weight
synchronization, LlamaRL achieves significant efficiency gains -- up to 10.7x
speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy
model. Furthermore, the efficiency advantage continues to grow with increasing
model scale, demonstrating the framework's suitability for future large-scale
RL training.

</details>


### [12] [Thompson Sampling in Online RLHF with General Function Approximation](https://arxiv.org/abs/2505.23927)
*Songtao Feng,Jie Fu*

Main category: cs.LG

TL;DR: 本文提出了一个受Thompson抽样启发的无模型后验抽样算法，用于解决在线强化学习从人类反馈问题，并给出了其理论保证。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习从人类反馈（RLHF）在将大型语言模型与人类偏好对齐方面取得了显著成果，但研究RLHF算法的统计效率仍具有重要的理论意义。

Method: 采用Bellman eluder（BE）维度作为函数类的复杂性度量，设计了一种无模型后验抽样算法，并给出了O(√T)的遗憾界。

Result: 该研究为在线RLHF提供了一个理论上高效的算法，并在分析中首次建立了基于MLE的平方Bellman误差界的集中型不等式。

Conclusion: 本文通过设计一种受Thompson抽样启发的无模型后验抽样算法，为在线RLHF提供了理论保证，并首次基于最大似然估计（MLE）泛化界建立了平方Bellman误差界的集中型不等式，这在获得eluder型遗憾界方面具有重要作用，并可能引起独立兴趣。

Abstract: Reinforcement learning from human feedback (RLHF) has achieved great
empirical success in aligning large language models (LLMs) with human
preference, and it is of great importance to study the statistical efficiency
of RLHF algorithms from a theoretical perspective. In this work, we consider
the online RLHF setting where the preference data is revealed during the
learning process and study action value function approximation. We design a
model-free posterior sampling algorithm for online RLHF inspired by Thompson
sampling and provide its theoretical guarantee. Specifically, we adopt Bellman
eluder (BE) dimension as the complexity measure of the function class and
establish $O(\sqrt{T})$ regret bound for the proposed algorithm with other
multiplicative factor depending on the horizon, BE dimension and the
$log$-bracketing number of the function class. Further, in the analysis, we
first establish the concentration-type inequality of the squared Bellman error
bound based on the maximum likelihood estimator (MLE) generalization bound,
which plays the crucial rules in obtaining the eluder-type regret bound and may
be of independent interest.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [13] [A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains](https://arxiv.org/abs/2402.00559)
*Alon Jacovi,Yonatan Bitton,Bernd Bohnet,Jonathan Herzig,Or Honovich,Michael Tseng,Michael Collins,Roee Aharoni,Mor Geva*

Main category: cs.CL

TL;DR: This paper introduces REVEAL, a new dataset for evaluating automatic verification of complex reasoning in language models, highlighting current limitations in verifying logical correctness and contradiction detection.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the lack of fine-grained step-level datasets necessary for thoroughly evaluating automatic verification methods for reasoning chains. The absence of such datasets hinders progress in improving the correctness of reasoning in language models.

Method: The authors introduce REVEAL, a dataset designed to evaluate automatic verifiers of Chain-of-Thought reasoning by providing fine-grained step-level labels for relevance, attribution, and logical correctness. This dataset is built across multiple datasets and state-of-the-art language models.

Result: REVEAL provides comprehensive labels for each reasoning step, enabling detailed evaluation of automatic verifiers. Evaluation using REVEAL reveals that current verifiers struggle significantly with verifying logical correctness and detecting contradictions in reasoning chains.

Conclusion: The paper concludes that automatic verifiers face significant challenges in verifying reasoning chains, particularly in assessing logical correctness and identifying contradictions within complex Chain-of-Thought reasoning.

Abstract: Prompting language models to provide step-by-step answers (e.g.,
"Chain-of-Thought") is the prominent approach for complex reasoning tasks,
where more accurate reasoning chains typically improve downstream task
performance. Recent literature discusses automatic methods to verify reasoning
to evaluate and improve their correctness. However, no fine-grained step-level
datasets are available to enable thorough evaluation of such verification
methods, hindering progress in this direction. We introduce REVEAL: Reasoning
Verification Evaluation, a dataset to benchmark automatic verifiers of complex
Chain-of-Thought reasoning in open-domain question-answering settings. REVEAL
includes comprehensive labels for the relevance, attribution to evidence
passages, and logical correctness of each reasoning step in a language model's
answer, across a variety of datasets and state-of-the-art language models.
Evaluation on REVEAL shows that verifiers struggle at verifying reasoning
chains - in particular, verifying logical correctness and detecting
contradictions. Available at https://reveal-dataset.github.io/ .

</details>


### [14] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 本文探讨了通过重复采样提升大语言模型问题解决覆盖率的现象，提出了一种基于答案频率的基线方法，并发现其在某些情况下优于传统采样方法。


<details>
  <summary>Details</summary>
Motivation: 研究者观察到，随着样本数量的增加，大语言模型在解决问题时的覆盖率也随之上升。他们推测这种现象部分归因于标准评估基准的答案分布偏向于少数常见答案。因此，他们希望探索是否存在一种更有效的替代方法来衡量和提升模型的覆盖率。

Method: 文章的方法是定义一个基线，根据训练集中答案的频率进行枚举，并在两个领域（数学推理和事实知识）中进行实验。实验比较了该基线与重复模型采样的性能，同时测试了一种混合策略，即通过少量模型采样结合枚举猜测剩余答案的方式。

Result: 实验结果显示，提出的基线方法在某些大语言模型上优于重复采样方法，而对于其他模型，则与混合策略表现相当。混合策略仅使用少量模型样本（如10次），其余答案通过枚举猜测获得，仍然能够达到相似的覆盖率。这表明重复采样在某些场景下的优势可能被高估了。

Conclusion: 文章的结论是，通过重复采样来扩展大语言模型（LLM）推理计算可以提高问题解决的覆盖率，但这种提升部分依赖于评估基准答案分布的偏差。作者提出了一种基于训练集中答案出现频率的基线方法，并发现该方法在某些情况下优于重复采样，而在其他情况下则与混合策略表现相当。这表明在衡量重复采样对覆盖率的提升时，需要考虑提示无关的猜测因素。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [15] [A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs](https://arxiv.org/abs/2505.23816)
*Trenton Chang,Tobias Schnabel,Adith Swaminathan,Jenna Wiens*

Main category: cs.CL

TL;DR: 这篇论文探讨了大型语言模型（LLMs）在实现用户目标对齐方面的挑战，提出了一个新的评估框架来衡量模型的可引导性，并发现当前的LLMs在这方面仍然存在问题，即使是最先进的对齐策略也难以完全解决。


<details>
  <summary>Details</summary>
Motivation: 论文的主要动机是探讨当前大型语言模型是否能够可靠地生成符合多样化用户目标的输出（即steerability）。虽然已有许多方法试图修改LLM的行为，但尚不清楚这些模型本身是否已经足够可控，还是需要更多的干预。

Method: 论文提出了一种基于多维目标空间的评估框架，用于系统地衡量LLMs的可引导性。该方法将用户目标和模型输出建模为文本属性的向量，并应用于文本重写任务中。同时，作者还测试了多种提升steerability的方法，如提示工程、最佳N选1采样和强化学习微调。

Result: 研究结果显示，目前的LLMs在steerability方面表现不佳，尤其是在处理罕见用户目标时容易出现覆盖不足、过度调整以及副作用等问题。尽管尝试了多种改善策略，但副作用仍然是一个显著的问题。

Conclusion: 论文得出的结论是，尽管当前的大型语言模型（LLMs）在推理和指令遵循方面表现出色，但它们在满足多样化的用户目标方面仍存在困难，即‘steerability’问题。此外，现有的改进策略可能不足以解决这些问题，因此需要进一步的研究和干预措施。

Abstract: Despite advances in large language models (LLMs) on reasoning and
instruction-following benchmarks, it remains unclear whether they can reliably
produce outputs aligned with a broad variety of user goals, a concept we refer
to as steerability. The abundance of methods proposed to modify LLM behavior
makes it unclear whether current LLMs are already steerable, or require further
intervention. In particular, LLMs may exhibit (i) poor coverage, where rare
user goals are underrepresented; (ii) miscalibration, where models overshoot
requests; and (iii) side effects, where changes to one dimension of text
inadvertently affect others. To systematically evaluate these failures, we
introduce a framework based on a multi-dimensional goal space that models user
goals and LLM outputs as vectors with dimensions corresponding to text
attributes (e.g., reading difficulty). Applied to a text-rewriting task, we
find that current LLMs struggle with steerability, as side effects are
persistent. Interventions to improve steerability, such as prompt engineering,
best-of-$N$ sampling, and reinforcement learning fine-tuning, have varying
effectiveness, yet side effects remain problematic. Our findings suggest that
even strong LLMs struggle with steerability, and existing alignment strategies
may be insufficient. We open-source our steerability evaluation framework at
https://github.com/MLD3/steerability.

</details>


### [16] [ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2505.23723)
*Zexi Liu,Jingyi Chai,Xinyu Zhu,Shuo Tang,Rui Ye,Bo Zhang,Lei Bai,Siheng Chen*

Main category: cs.CL

TL;DR: 文章提出了一个基于学习的代理ML范式和训练框架，使大型语言模型能够通过在线强化学习在自主机器学习任务中进行交互实验并学习，从而实现更高效的训练和卓越的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前大多数基于大型语言模型（LLM）的代理方法严重依赖手动提示工程，无法根据不同的实验经验进行自适应和优化。因此，文章旨在探索一种基于学习的代理ML范式，使LLM代理能够通过在线强化学习（RL）在ML任务上进行交互实验并学习。

Method: 文章提出了一种新的代理ML训练框架，包含三个关键组件：（1）探索增强微调，使LLM代理能够生成多样化的动作以增强RL探索；（2）逐步RL，能够在单个动作步骤上进行训练，加速经验收集并提高训练效率；（3）代理ML特定奖励模块，将各种ML反馈信号统一为一致的奖励用于RL优化。

Result: 利用提出的框架，作者训练了一个名为ML-Agent的代理，该代理基于7B大小的Qwen-2.5 LLM驱动，用于自主ML。结果表明，尽管仅训练了9个ML任务，但7B大小的ML-Agent在性能上超过了671B大小的DeepSeek-R1代理。此外，ML-Agent还实现了持续的性能提升，并展示了出色的跨任务泛化能力。

Conclusion: 文章的结论是，提出的基于学习的代理ML范式和训练框架能够显著提高大型语言模型在自主机器学习任务中的性能。尽管仅训练了9个ML任务，但7B大小的ML-Agent在性能上超过了671B大小的DeepSeek-R1代理，并且展示了持续的性能提升和跨任务的泛化能力。

Abstract: The emergence of large language model (LLM)-based agents has significantly
advanced the development of autonomous machine learning (ML) engineering.
However, most existing approaches rely heavily on manual prompt engineering,
failing to adapt and optimize based on diverse experimental experiences.
Focusing on this, for the first time, we explore the paradigm of learning-based
agentic ML, where an LLM agent learns through interactive experimentation on ML
tasks using online reinforcement learning (RL). To realize this, we propose a
novel agentic ML training framework with three key components: (1)
exploration-enriched fine-tuning, which enables LLM agents to generate diverse
actions for enhanced RL exploration; (2) step-wise RL, which enables training
on a single action step, accelerating experience collection and improving
training efficiency; (3) an agentic ML-specific reward module, which unifies
varied ML feedback signals into consistent rewards for RL optimization.
Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM
for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our
7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it
achieves continuous performance improvements and demonstrates exceptional
cross-task generalization capabilities.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [17] [Model-Based AI planning and Execution Systems for Robotics](https://arxiv.org/abs/2505.04493)
*Or Wertheim,Ronen I. Brafman*

Main category: cs.RO

TL;DR: 这篇论文介绍了基于模型的机器人任务级控制系统的最新进展，包括设计选择、解决方案和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 尽管基于模型的规划和执行系统在机器人领域已有较长历史，但与现代机器人平台集成的通用系统直到最近才开始出现，这激发了对这一领域的深入研究。

Method: 通过回顾和分析现有文献中的方法和技术，探讨不同设计选择对系统性能的影响。

Result: 论文展示了多种不同的解决方案，并讨论了它们如何解决设计问题，以及它们各自的优缺点。

Conclusion: 论文总结了现有的基于模型的机器人任务级控制系统的设计选择和解决方案，同时指出了未来发展的方向。

Abstract: Model-based planning and execution systems offer a principled approach to
building flexible autonomous robots that can perform diverse tasks by
automatically combining a host of basic skills. This idea is almost as old as
modern robotics. Yet, while diverse general-purpose reasoning architectures
have been proposed since, general-purpose systems that are integrated with
modern robotic platforms have emerged only recently, starting with the
influential ROSPlan system. Since then, a growing number of model-based systems
for robot task-level control have emerged. In this paper, we consider the
diverse design choices and issues existing systems attempt to address, the
different solutions proposed so far, and suggest avenues for future
development.

</details>


### [18] [Hardware Design and Learning-Based Software Architecture of Musculoskeletal Wheeled Robot Musashi-W for Real-World Applications](https://arxiv.org/abs/2403.11729)
*Kento Kawaharazuka,Akihiro Miki,Masahiro Bando,Temma Suzuki,Yoshimoto Ribayashi,Yasunori Toshimitsu,Yuya Nagamatsu,Kei Okada,and Masayuki Inaba*

Main category: cs.RO

TL;DR: 本文介绍了一款名为 Musashi-W 的肌骨骼轮式机器人，它通过结合多种技术实现了真实世界任务的有效执行。


<details>
  <summary>Details</summary>
Motivation: 尽管已有许多肌骨骼人形机器人，但由于柔性身体的双足行走难度，它们在真实世界任务中的应用仍然有限。因此，研究者开发了 Musashi-W 以探索更实用的应用。

Method: 开发了一个肌骨骼轮式机器人 Musashi-W，并构建了其软件系统，结合了静态和动态身体模式学习、反射控制和视觉识别。

Result: Musashi-W 成功完成了多种任务，包括通过人工教学进行清洁、考虑肌肉添加搬运重物以及通过可变刚度动态布料操作来布置餐桌。

Conclusion: 通过结合静态和动态身体模式学习、反射控制和视觉识别，Musashi-W 的硬件和软件系统能够充分利用肌骨骼上肢的优势，适用于真实世界的任务。

Abstract: Various musculoskeletal humanoids have been developed so far. While these
humanoids have the advantage of their flexible and redundant bodies that mimic
the human body, they are still far from being applied to real-world tasks. One
of the reasons for this is the difficulty of bipedal walking in a flexible
body. Thus, we developed a musculoskeletal wheeled robot, Musashi-W, by
combining a wheeled base and musculoskeletal upper limbs for real-world
applications. Also, we constructed its software system by combining static and
dynamic body schema learning, reflex control, and visual recognition. We show
that the hardware and software of Musashi-W can make the most of the advantages
of the musculoskeletal upper limbs, through several tasks of cleaning by human
teaching, carrying a heavy object considering muscle addition, and setting a
table through dynamic cloth manipulation with variable stiffness.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [19] [Casper DPM: Cascaded Perceptual Dynamic Projection Mapping onto Hands](https://arxiv.org/abs/2409.04397)
*Yotam Erel,Or Kozlovsky-Mordenfeld,Daisuke Iwai,Kosuke Sato,Amit H. Bermano*

Main category: cs.GR

TL;DR: 文章提出了一种新技术，通过结合3D手部姿态估计和2D校正步骤，实现低延迟、高精度的动态投影，显著提升了用户体验和任务表现。


<details>
  <summary>Details</summary>
Motivation: 动机是为了克服准确且快速计算人体手部姿态和形状的挑战，因为手部具有高度关节化和可变形的特点，同时减少感知到的运动到光子延迟。

Method: 文章的方法是结合一个较慢的3D手部姿态粗略估计与高速的2D校正步骤来改进投影效果，并利用完整的3D重建技术实现任意纹理的应用。

Result: 研究结果显示，使用这种新方法，用户对延迟相关的伪影更加不敏感，并且在相关任务中表现得更快、更容易完成，相较于直接从3D姿态估计渲染帧的简单方法有明显优势。

Conclusion: 文章的结论是，通过结合较慢的3D手部姿态粗略估计与高速的2D校正步骤，可以有效降低感知到的运动到光子延迟，并且提高投影对齐度和表面覆盖面积。这种方法让用户对于延迟相关的伪影不那么敏感，从而能够更快速、更轻松地完成任务。

Abstract: We present a technique for dynamically projecting 3D content onto human hands
with short perceived motion-to-photon latency. Computing the pose and shape of
human hands accurately and quickly is a challenging task due to their
articulated and deformable nature. We combine a slower 3D coarse estimation of
the hand pose with high speed 2D correction steps which improve the alignment
of the projection to the hands, increase the projected surface area, and reduce
perceived latency. Since our approach leverages a full 3D reconstruction of the
hands, any arbitrary texture or reasonably performant effect can be applied,
which was not possible before. We conducted two user studies to assess the
benefits of using our method. The results show subjects are less sensitive to
latency artifacts and perform faster and with more ease a given associated task
over the naive approach of directly projecting rendered frames from the 3D pose
estimation. We demonstrate several novel use cases and applications.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [20] [Prices and preferences in the electric vehicle market](https://arxiv.org/abs/2403.00458)
*Chung Yi See,Vasco Rato Santos,Lucas Woodley,Megan Yeo,Daniel Palmer,Shuheng Zhang,and Ashley Nunes*

Main category: econ.EM

TL;DR: 本研究探讨了电动汽车价格较高的原因，发现其价格主要受配置和性能影响，而不是单纯因为电池成本。这种偏好导致了较低的燃油经济性，减少了排放效益。


<details>
  <summary>Details</summary>
Motivation: 尽管电动汽车比汽油车污染少，但其采用率受到采购价格较高的挑战。现有讨论强调电动汽车电池成本是这一价格差异的主要原因，并且通常认为只有电池成本下降才能实现广泛采用。

Method: 通过收集2011年至2023年间电动汽车属性和市场条件的数据进行分析。

Result: 研究发现：首先，电动汽车的价格主要受标准配置的设施、附加功能和经销商安装的配件数量的影响，其次才是电动汽车的马力。第二，电动汽车的续航里程与价格呈负相关，这意味着续航焦虑可能没有现有讨论中那样重要。第三，电池容量与电动汽车价格正相关，因为更多的容量意味着提供更多的马力。第四，这些偏好导致了燃油经济性较低的车辆出现，这使得预期的生命周期排放效益减少了至少3.26%。

Conclusion: 这篇论文得出的结论是，电动汽车的高价主要反映了消费者对功能丰富和更强大车辆的偏好，而这些偏好导致了较低的燃油经济性，从而减少了预期的生命周期排放效益。

Abstract: Although electric vehicles are less polluting than gasoline powered vehicles,
adoption is challenged by higher procurement prices. Existing discourse
emphasizes EV battery costs as being principally responsible for this price
differential and widespread adoption is routinely conditioned upon battery
costs declining. We scrutinize such reasoning by sourcing data on EV attributes
and market conditions between 2011 and 2023. Our findings are fourfold. First,
EV prices are influenced principally by the number of amenities, additional
features, and dealer-installed accessories sold as standard on an EV, and to a
lesser extent, by EV horsepower. Second, EV range is negatively correlated with
EV price implying that range anxiety concerns may be less consequential than
existing discourse suggests. Third, battery capacity is positively correlated
with EV price, due to more capacity being synonymous with the delivery of more
horsepower. Collectively, this suggests that higher procurement prices for EVs
reflects consumer preference for vehicles that are feature dense and more
powerful. Fourth and finally, accommodating these preferences have produced
vehicles with lower fuel economy, a shift that reduces envisioned lifecycle
emissions benefits by at least 3.26 percent, subject to the battery pack
chemistry leveraged and the carbon intensity of the electrical grid. These
findings warrant attention as decarbonization efforts increasingly emphasize
electrification as a pathway for complying with domestic and international
climate agreements.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [21] [Accelerating ab initio melting property calculations with machine learning: Application to the high entropy alloy TaVCrW](https://arxiv.org/abs/2408.08654)
*Li-Fang Zhu,Fritz Koermann,Qing Chen,Malin Selleby,Joerg Neugebauer,and Blazej Grabowski*

Main category: cond-mat.mtrl-sci

TL;DR: 这项研究开发了一种新的高效计算方法，通过机器学习潜力辅助密度泛函理论计算，显著降低了计算材料熔化性质所需的资源，且结果准确可靠。


<details>
  <summary>Details</summary>
Motivation: 材料的熔化性质对于设计新型高性能、高熔点耐火材料至关重要。然而，由于实验测量这些性质极具挑战性，因此需要一种成本效益更高的理论预测方法。

Method: 该研究采用了一种基于密度泛函理论的自由能微扰计算方法，并利用了专门设计的机器学习潜力来替代成本高昂的热力学积分。

Result: 通过将机器学习潜力应用于多组分合金的相空间再现，该方法成功地替换了传统的热力学积分步骤，实现了总体计算资源的80%节约。对高熵合金TaVCrW的熔化性质进行了准确计算，包括熔点、熔融熵和熔融焓以及熔点处的体积变化，同时计算了固态和液态TaVCrW的热容量。

Conclusion: 综上所述，本文提出了一种高效的基于密度泛函理论的方法，结合了机器学习潜力，用于计算材料的熔化性质。这种方法在资源消耗上比现有方法平均节省了80%，并且在高熵合金TaVCrW上的应用结果与calphad外推值合理一致。

Abstract: Melting properties are critical for designing novel materials, especially for
discovering high-performance, high-melting refractory materials. Experimental
measurements of these properties are extremely challenging due to their high
melting temperatures. Complementary theoretical predictions are, therefore,
indispensable. The conventional free energy approach using density functional
theory (DFT) has been a gold standard for such purposes because of its high
accuracy. However,it generally involves expensive thermodynamic integration
using ab initio molecular dynamic simulations. The high computational cost
makes high-throughput calculations infeasible. Here, we propose a highly
efficient DFT-based method aided by a specially designed machine learning
potential. As the machine learning potential can closely reproduce the ab
initio phase space, even for multi-component alloys, the costly thermodynamic
integration can be fully substituted with more efficient free energy
perturbation calculations. The method achieves overall savings of computational
resources by 80% compared to current alternatives. We apply the method to the
high-entropy alloy TaVCrW and calculate its melting properties, including
melting temperature, entropy and enthalpy of fusion, and volume change at the
melting point. Additionally, the heat capacities of solid and liquid TaVCrW are
calculated. The results agree reasonably with the calphad extrapolated values.

</details>
