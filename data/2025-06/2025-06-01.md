<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 3]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: This position paper critically surveys recent empirical developments on human-AI collaboration, proposes a new conceptual architecture to integrate these studies, and serves as both a review and a reference for future research.


<details>
  <summary>Details</summary>
Motivation: We observe a lack of a unifying theoretical framework that can coherently integrate these varied studies, especially when tackling open-ended, complex tasks. To address this, we propose a novel conceptual architecture.

Method: The paper's structure allows it to be read from any section, serving equally as a critical review of technical implementations and as a forward-looking reference for designing or extending human-AI symbioses.

Result: By mapping existing contributions, from symbolic AI techniques and connectionist LLM-based agents to hybrid organizational practices, onto this proposed framework (Hierarchical Exploration-Exploitation Net), our approach facilitates revision of legacy methods and inspires new work that fuses qualitative and quantitative paradigms.

Conclusion: Together, these insights offer a stepping stone toward deeper co-evolution of human cognition and AI capability.

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [2] [Artificial Expert Intelligence through PAC-reasoning](https://arxiv.org/abs/2412.02441)
*Shai Shalev-Shwartz,Amnon Shashua,Gal Beniamini,Yoav Levine,Or Sharir,Noam Wies,Ido Ben-Shaul,Tomer Nussbaum,Shir Granot Peled*

Main category: cs.AI

TL;DR: 本文介绍了一种新型人工智能框架Artificial Expert Intelligence (AEI)，它结合领域专业知识与精确推理能力，以提升现有AI在复杂问题上的适应性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的AI系统在预定义任务上表现出色，但在适应性和新颖问题解决方面存在不足，而AEI旨在弥补这一缺陷，实现更接近顶级人类专家水平的智能。

Method: AEI 引入了“可能近似正确（PAC）推理”范式，并提出了称为System 3的精确推理类型，以控制推理精度并实现误差有界的推理过程。

Result: AEI 成功地将科学方法的严谨性融入到推理过程中，提供了一个具有理论支持的框架，可在推理时学习并控制结果的精度。

Conclusion: AEI 提出了一个超越AGI和狭义AI局限性的框架，通过结合领域专业知识与精确推理能力，为复杂问题求解提供了可靠的理论保证和实践机制。

Abstract: Artificial Expert Intelligence (AEI) seeks to transcend the limitations of
both Artificial General Intelligence (AGI) and narrow AI by integrating
domain-specific expertise with critical, precise reasoning capabilities akin to
those of top human experts. Existing AI systems often excel at predefined tasks
but struggle with adaptability and precision in novel problem-solving. To
overcome this, AEI introduces a framework for ``Probably Approximately Correct
(PAC) Reasoning". This paradigm provides robust theoretical guarantees for
reliably decomposing complex problems, with a practical mechanism for
controlling reasoning precision. In reference to the division of human thought
into System 1 for intuitive thinking and System 2 for reflective
reasoning~\citep{tversky1974judgment}, we refer to this new type of reasoning
as System 3 for precise reasoning, inspired by the rigor of the scientific
method. AEI thus establishes a foundation for error-bounded, inference-time
learning.

</details>


### [3] [Decomposing Elements of Problem Solving: What "Math" Does RL Teach?](https://arxiv.org/abs/2505.22756)
*Tian Qin,Core Francisco Park,Mujin Kwun,Aaron Walsman,Eran Malach,Nikhil Anand,Hidenori Tanaka,David Alvarez-Melis*

Main category: cs.AI

TL;DR: 本研究探讨了强化学习（RL）如何影响大型语言模型（LLM）的推理能力，发现RL主要提升了已知问题的执行鲁棒性，但在新问题上受限于规划能力不足。


<details>
  <summary>Details</summary>
Motivation: 数学推理任务已成为评估大语言模型（LLM）推理能力的重要基准，尤其是使用强化学习（RL）方法如GRPO取得了显著的性能提升。然而，仅凭准确率指标无法对这些能力进行细粒度评估，也无法揭示哪些问题解决技能已经被内化。因此，本文旨在更好地理解RL训练如何影响LLM的问题解决能力。

Method: 该研究通过分解问题求解为计划、执行和验证三个基本能力来评估LLM的能力。作者使用GRPO等强化学习方法进行实验，并构建了一个最小的合成解决方案树导航任务作为数学问题求解的类比。

Result: 研究表明，GRPO主要增强了执行技能，改善了模型已经知道如何解决的问题的执行鲁棒性。但同时发现，RL训练的模型在处理全新的问题时遇到了困难，这是由于规划技能的不足导致的“覆盖墙”。通过合成的任务设置，研究者确认了RL主要提升的是执行鲁棒性，并且找到了RL可能通过更好的探索和泛化来克服覆盖墙的条件。

Conclusion: 这篇论文总结了RL在增强LLM推理能力中的作用，并揭示了一些关键的限制因素。研究发现，RL主要提高了执行的鲁棒性，但在面对全新的问题时，由于规划能力不足而遇到了“覆盖墙”。通过改进探索和泛化到新的解决路径，RL有可能克服这一障碍。

Abstract: Mathematical reasoning tasks have become prominent benchmarks for assessing
the reasoning capabilities of LLMs, especially with reinforcement learning (RL)
methods such as GRPO showing significant performance gains. However, accuracy
metrics alone do not support fine-grained assessment of capabilities and fail
to reveal which problem-solving skills have been internalized. To better
understand these capabilities, we propose to decompose problem solving into
fundamental capabilities: Plan (mapping questions to sequences of steps),
Execute (correctly performing solution steps), and Verify (identifying the
correctness of a solution). Empirically, we find that GRPO mainly enhances the
execution skill-improving execution robustness on problems the model already
knows how to solve-a phenomenon we call temperature distillation. More
importantly, we show that RL-trained models struggle with fundamentally new
problems, hitting a 'coverage wall' due to insufficient planning skills. To
explore RL's impact more deeply, we construct a minimal, synthetic
solution-tree navigation task as an analogy for mathematical problem-solving.
This controlled setup replicates our empirical findings, confirming RL
primarily boosts execution robustness. Importantly, in this setting, we
identify conditions under which RL can potentially overcome the coverage wall
through improved exploration and generalization to new solution paths. Our
findings provide insights into the role of RL in enhancing LLM reasoning,
expose key limitations, and suggest a path toward overcoming these barriers.
Code is available at https://github.com/cfpark00/RL-Wall.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [MvDeCor: Multi-view Dense Correspondence Learning for Fine-grained 3D Segmentation](https://arxiv.org/abs/2208.08580)
*Gopal Sharma,Kangxue Yin,Subhransu Maji,Evangelos Kalogerakis,Or Litany,Sanja Fidler*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose to utilize self-supervised techniques in the 2D domain for
fine-grained 3D shape segmentation tasks. This is inspired by the observation
that view-based surface representations are more effective at modeling
high-resolution surface details and texture than their 3D counterparts based on
point clouds or voxel occupancy. Specifically, given a 3D shape, we render it
from multiple views, and set up a dense correspondence learning task within the
contrastive learning framework. As a result, the learned 2D representations are
view-invariant and geometrically consistent, leading to better generalization
when trained on a limited number of labeled shapes compared to alternatives
that utilize self-supervision in 2D or 3D alone. Experiments on textured
(RenderPeople) and untextured (PartNet) 3D datasets show that our method
outperforms state-of-the-art alternatives in fine-grained part segmentation.
The improvements over baselines are greater when only a sparse set of views is
available for training or when shapes are textured, indicating that MvDeCor
benefits from both 2D processing and 3D geometric reasoning.

</details>


### [5] [Mix3D: Out-of-Context Data Augmentation for 3D Scenes](https://arxiv.org/abs/2110.02210)
*Alexey Nekrasov,Jonas Schult,Or Litany,Bastian Leibe,Francis Engelmann*

Main category: cs.CV

TL;DR: Mix3D is a novel data augmentation method that improves 3D scene segmentation by balancing global context and local geometry, achieving strong results on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Current models for 3D scene segmentation focus on capturing global context, which can lead to errors when contextual priors mislead the model (e.g., mistaking a pedestrian for a car). The authors aim to balance global context with local geometric details to improve generalization.

Method: The paper proposes a 'mixing' technique where new training samples are created by combining two augmented scenes. This approach forces models to rely less on global context and more on local structures to infer semantics.

Result: Models trained with Mix3D achieve significant performance improvements, such as MinkowskiNet outperforming prior state-of-the-art methods by a large margin on the ScanNet test benchmark with 78.1 mIoU.

Conclusion: Mix3D is an effective data augmentation technique that balances global scene context and local geometry, leading to improved performance in 3D scene segmentation tasks across both indoor and outdoor datasets.

Abstract: We present Mix3D, a data augmentation technique for segmenting large-scale 3D
scenes. Since scene context helps reasoning about object semantics, current
works focus on models with large capacity and receptive fields that can fully
capture the global context of an input 3D scene. However, strong contextual
priors can have detrimental implications like mistaking a pedestrian crossing
the street for a car. In this work, we focus on the importance of balancing
global scene context and local geometry, with the goal of generalizing beyond
the contextual priors in the training set. In particular, we propose a "mixing"
technique which creates new training samples by combining two augmented scenes.
By doing so, object instances are implicitly placed into novel out-of-context
environments and therefore making it harder for models to rely on scene context
alone, and instead infer semantics from local structure as well. We perform
detailed analysis to understand the importance of global context, local
structures and the effect of mixing scenes. In experiments, we show that models
trained with Mix3D profit from a significant performance boost on indoor
(ScanNet, S3DIS) and outdoor datasets (SemanticKITTI). Mix3D can be trivially
used with any existing method, e.g., trained with Mix3D, MinkowskiNet
outperforms all prior state-of-the-art methods by a significant margin on the
ScanNet test benchmark 78.1 mIoU. Code is available at:
https://nekrasov.dev/mix3d/

</details>


### [6] [cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning](https://arxiv.org/abs/2505.22914)
*Maksim Kolodiazhnyi,Denis Tarasov,Dmitrii Zhemchuzhnikov,Alexander Nikulin,Ilya Zisman,Anna Vorontsova,Anton Konushin,Vladislav Kurenkov,Danila Rukhovich*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉-语言模型和强化学习的多模态CAD重建方法，在多个数据集上实现了最先进的性能，特别是在真实世界数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的CAD重建方法通常只关注单一输入模态，限制了其通用性和鲁棒性。通过利用视觉-语言模型的最新进展，本文旨在开发一种能够同时处理多种输入模态的CAD重建模型，以提升设计应用的可访问性。

Method: 受大语言模型训练范式的启发，采用两阶段流程：首先在大规模程序生成数据上进行监督微调(SFT)，然后使用在线反馈进行强化学习微调。首次探索了针对CAD任务的大语言模型的强化学习微调，并展示了在线强化学习算法如Group Relative Preference Optimization (GRPO) 超过离线方法的表现。

Result: 在DeepCAD基准测试中，提出的SFT模型在所有三种输入模态上均优于现有单模态方法。经过RL微调后，该模型在三个具有挑战性的数据集中达到了新的最先进水平，包括一个真实世界数据集。

Conclusion: 论文提出了一种多模态CAD重建模型，能够同时处理点云、图像和文本三种输入模态，并通过监督微调和强化学习微调的方法，取得了优于现有单模态方法的表现。最终在多个数据集中达到了新的最优结果，尤其是在真实世界数据集上的表现尤为突出。

Abstract: Computer-Aided Design (CAD) plays a central role in engineering and
manufacturing, making it possible to create precise and editable 3D models.
Using a variety of sensor or user-provided data as inputs for CAD
reconstruction can democratize access to design applications. However, existing
methods typically focus on a single input modality, such as point clouds,
images, or text, which limits their generalizability and robustness. Leveraging
recent advances in vision-language models (VLM), we propose a multi-modal CAD
reconstruction model that simultaneously processes all three input modalities.
Inspired by large language model (LLM) training paradigms, we adopt a two-stage
pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated
data, followed by reinforcement learning (RL) fine-tuning using online
feedback, obtained programatically. Furthermore, we are the first to explore RL
fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such
as Group Relative Preference Optimization (GRPO) outperform offline
alternatives. In the DeepCAD benchmark, our SFT model outperforms existing
single-modal approaches in all three input modalities simultaneously. More
importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three
challenging datasets, including a real-world one.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model](https://arxiv.org/abs/2505.23579)
*Adibvafa Fallahpour,Andrew Magnuson,Purav Gupta,Shihao Ma,Jack Naimer,Arnav Shah,Haonan Duan,Omar Ibrahim,Hani Goodarzi,Chris J. Maddison,Bo Wang*

Main category: cs.LG

TL;DR: BioReason is a pioneering architecture that integrates a DNA foundation model with a Large Language Model to enable multi-step, interpretable biological reasoning from genomic data.


<details>
  <summary>Details</summary>
Motivation: Unlocking deep, interpretable biological reasoning from complex genomic data is a major AI challenge hindering scientific discovery. Current DNA foundation models struggle with multi-step reasoning and lack transparent explanations.

Method: BioReason integrates a DNA foundation model with a Large Language Model (LLM) through supervised fine-tuning and targeted reinforcement learning.

Result: BioReason demonstrates an average 15% performance gain over strong single-modality baselines on biological reasoning benchmarks including KEGG-based disease pathway prediction where accuracy improves from 88% to 97%, and variant effect prediction.

Conclusion: BioReason offers a transformative approach for AI in biology by enabling deeper mechanistic insights and accelerating testable hypothesis generation from genomic data.

Abstract: Unlocking deep, interpretable biological reasoning from complex genomic data
is a major AI challenge hindering scientific discovery. Current DNA foundation
models, despite strong sequence representation, struggle with multi-step
reasoning and lack inherent transparent, biologically intuitive explanations.
We introduce BioReason, a pioneering architecture that, for the first time,
deeply integrates a DNA foundation model with a Large Language Model (LLM).
This novel connection enables the LLM to directly process and reason with
genomic information as a fundamental input, fostering a new form of multimodal
biological understanding. BioReason's sophisticated multi-step reasoning is
developed through supervised fine-tuning and targeted reinforcement learning,
guiding the system to generate logical, biologically coherent deductions. On
biological reasoning benchmarks including KEGG-based disease pathway prediction
- where accuracy improves from 88% to 97% - and variant effect prediction,
BioReason demonstrates an average 15% performance gain over strong
single-modality baselines. BioReason reasons over unseen biological entities
and articulates decision-making through interpretable, step-by-step biological
traces, offering a transformative approach for AI in biology that enables
deeper mechanistic insights and accelerates testable hypothesis generation from
genomic data. Data, code, and checkpoints are publicly available at
https://github.com/bowang-lab/BioReason

</details>


### [8] [Diversity-Aware Policy Optimization for Large Language Model Reasoning](https://arxiv.org/abs/2505.23433)
*Jian Yao,Ran Cheng,Xingyu Wu,Jibin Wu,Kay Chen Tan*

Main category: cs.LG

TL;DR: 本文系统研究了强化学习训练中多样性对大型语言模型（LLM）推理能力的影响，并提出了一种新的多样性感知策略优化方法，以显式促进训练过程中的多样性。


<details>
  <summary>Details</summary>
Motivation: 尽管多样性在强化学习（RL）中起着关键作用，但其对大型语言模型（LLM）推理的影响仍未得到充分研究。为了填补这一空白，文章进行了系统的调查，并提出了一种新颖的方法来增强LLM在推理任务中的表现。

Method: 文章提出了一种新的多样性感知策略优化方法。具体而言，设计了一个词元级别的多样性指标，并将其重新制定为一个实际的目标函数，然后选择性地将其应用于正样本。这种方法被整合进R1-zero训练框架中。

Result: 通过对12个LLM的评估，观察到解决方案多样性和Potential at k（一个量化LLM推理潜力的新指标）之间存在显著正相关。所提出的多样性感知策略优化方法在四个数学推理基准测试中平均提升了3.5%的表现。

Conclusion: 文章总结指出，通过引入一种新的多样性感知策略优化方法，可以在提升大型语言模型（LLM）的推理能力方面取得显著成效。实验结果表明，该方法在多个数学推理基准测试中表现优异，同时生成了更多样化和稳健的解决方案。

Abstract: The reasoning capabilities of large language models (LLMs) have advanced
rapidly, particularly following the release of DeepSeek R1, which has inspired
a surge of research into data quality and reinforcement learning (RL)
algorithms. Despite the pivotal role diversity plays in RL, its influence on
LLM reasoning remains largely underexplored. To bridge this gap, this work
presents a systematic investigation into the impact of diversity in RL-based
training for LLM reasoning, and proposes a novel diversity-aware policy
optimization method. Across evaluations on 12 LLMs, we observe a strong
positive correlation between the solution diversity and Potential at k (a novel
metric quantifying an LLM's reasoning potential) in high-performing models.
This finding motivates our method to explicitly promote diversity during RL
training. Specifically, we design a token-level diversity and reformulate it
into a practical objective, then we selectively apply it to positive samples.
Integrated into the R1-zero training framework, our method achieves a 3.5
percent average improvement across four mathematical reasoning benchmarks,
while generating more diverse and robust solutions.

</details>


### [9] [Towards Reward Fairness in RLHF: From a Resource Allocation Perspective](https://arxiv.org/abs/2505.23349)
*Sheng Ouyang,Yulan Hu,Ge Chen,Qingyang Li,Fuzheng Zhang,Yong Liu*

Main category: cs.LG

TL;DR: 这篇文章提出了一种通用的强化学习方法，用于缓解奖励信号中的不公平性问题，而无需明确处理每种具体的偏差类型。通过将偏好学习视为资源分配问题，并采用公平性正则化和公平性系数等技术，该方法在实际应用中展现了提升大型语言模型与人类偏好对齐公平性的能力。


<details>
  <summary>Details</summary>
Motivation: 奖励通常被视为人类偏好的代理，在基于人类反馈的强化学习（RLHF）中起着关键作用。然而，如果这些奖励本身存在各种偏差，则可能导致大型语言模型（LLMs）的对齐效果不佳。因此，如何减轻奖励中的偏差、提高其公平性成为研究的核心问题。

Method: 文章提出了从资源分配的角度解决奖励公平性问题的方法，而无需专门针对每种类型的偏差进行设计。具体来说，作者引入了两种方法：公平性正则化（Fairness Regularization）和公平性系数（Fairness Coefficient），分别应用于验证和强化学习场景，以获得公平的奖励模型和策略模型。

Result: 实验表明，所提出的方法能够在多个场景下有效缓解奖励的不公平问题，从而更公平地对齐大型语言模型与人类偏好。

Conclusion: 本文通过将偏好学习建模为资源分配问题，并引入公平性正则化和公平性系数两种方法，成功地在不针对特定偏差的情况下提高了奖励的公平性。实验结果表明，所提出的方法能够以更加公平的方式使大型语言模型与人类偏好保持一致。

Abstract: Rewards serve as proxies for human preferences and play a crucial role in
Reinforcement Learning from Human Feedback (RLHF). However, if these rewards
are inherently imperfect, exhibiting various biases, they can adversely affect
the alignment of large language models (LLMs). In this paper, we collectively
define the various biases present in rewards as the problem of reward
unfairness. We propose a bias-agnostic method to address the issue of reward
fairness from a resource allocation perspective, without specifically designing
for each type of bias, yet effectively mitigating them. Specifically, we model
preference learning as a resource allocation problem, treating rewards as
resources to be allocated while considering the trade-off between utility and
fairness in their distribution. We propose two methods, Fairness Regularization
and Fairness Coefficient, to achieve fairness in rewards. We apply our methods
in both verification and reinforcement learning scenarios to obtain a fairness
reward model and a policy model, respectively. Experiments conducted in these
scenarios demonstrate that our approach aligns LLMs with human preferences in a
more fair manner.

</details>


### [10] [Accelerating RLHF Training with Reward Variance Increase](https://arxiv.org/abs/2505.23247)
*Zonglin Yang,Zhexuan Gu,Houduo Qi,Yancheng Yuan*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的奖励调整方法及其高效求解算法，并将其应用于GRPO算法中，从而显著提高了强化学习从人类反馈训练的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO方法虽然在许多大语言模型应用中取得了成功，但其训练效率仍然面临挑战。研究表明初始策略模型的奖励方差越大，RLHF训练越快。受此启发，作者试图通过提高奖励方差来加速训练过程。

Method: 作者提出了一种奖励调整模型，旨在增加奖励方差，同时保持相对偏好和奖励期望不变。为了解决该模型带来的非凸优化问题，他们设计了一个O(n log n)的高效算法来寻找全局解，并将这一方法集成到GRPO算法中，形成了GRPOVI算法。

Result: 实验结果表明，GRPOVI算法相比原始GRPO算法可以显著提高RLHF训练的效率。此外，作者还提供了一个间接解释，说明为何在DeepSeek-R1中基于规则的奖励机制在实践中是有效的。

Conclusion: 该论文提出了一种新的奖励调整模型，能够有效加速基于群体相对策略优化（GRPO）的强化学习从人类反馈（RLHF）训练过程。通过设计一个高效的算法来解决非凸优化问题，他们开发了更高效的GRPOVI算法，并在实验中展示了其相对于原始GRPO算法在RLHF训练效率方面的显著提升。

Abstract: Reinforcement learning from human feedback (RLHF) is an essential technique
for ensuring that large language models (LLMs) are aligned with human values
and preferences during the post-training phase. As an effective RLHF approach,
group relative policy optimization (GRPO) has demonstrated success in many
LLM-based applications. However, efficient GRPO-based RLHF training remains a
challenge. Recent studies reveal that a higher reward variance of the initial
policy model leads to faster RLHF training. Inspired by this finding, we
propose a practical reward adjustment model to accelerate RLHF training by
provably increasing the reward variance and preserving the relative preferences
and reward expectation. Our reward adjustment method inherently poses a
nonconvex optimization problem, which is NP-hard to solve in general. To
overcome the computational challenges, we design a novel $O(n \log n)$
algorithm to find a global solution of the nonconvex reward adjustment model by
explicitly characterizing the extreme points of the feasible set. As an
important application, we naturally integrate this reward adjustment model into
the GRPO algorithm, leading to a more efficient GRPO with reward variance
increase (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we
provide an indirect explanation for the empirical effectiveness of GRPO with
rule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment
results demonstrate that the GRPOVI algorithm can significantly improve the
RLHF training efficiency compared to the original GRPO algorithm.

</details>


### [11] [LLM-ODDR: A Large Language Model Framework for Joint Order Dispatching and Driver Repositioning](https://arxiv.org/abs/2505.22695)
*Tengfei Lyu,Siyuan Feng,Hao Liu,Hai Yang*

Main category: cs.LG

TL;DR: LLM-ODDR is a new ride-hailing decision-making framework using Large Language Models to improve order dispatching and driver repositioning by addressing fairness, adaptability, and interpretability shortcomings of traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches to ride-hailing order dispatching and driver repositioning often neglect driver income fairness, interpretability, and adaptability to real-world dynamics. The authors aim to address these gaps by introducing a novel framework that leverages Large Language Models (LLMs) for more effective and interpretable decision-making.

Method: The LLM-ODDR framework includes three components: (1) Multi-objective-guided Order Value Refinement to evaluate orders based on multiple objectives; (2) Fairness-aware Order Dispatching to balance platform revenue and driver income fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning to optimize idle vehicle placement using historical patterns and projected supply. A fine-tuned model, JointDR-GPT, is also developed with domain-specific knowledge.

Result: Experiments on real-world Manhattan taxi datasets show that the LLM-ODDR framework significantly outperforms traditional combinatorial optimization, rule-based heuristics, and reinforcement learning methods in terms of effectiveness, adaptability under anomalous conditions, and interpretability of decisions.

Conclusion: LLM-ODDR represents the first application of Large Language Models as decision-making agents in ride-hailing order dispatching and driver repositioning tasks. It outperforms traditional methods in effectiveness, adaptability, and interpretability, providing foundational insights for integrating advanced language models into intelligent transportation systems.

Abstract: Ride-hailing platforms face significant challenges in optimizing order
dispatching and driver repositioning operations in dynamic urban environments.
Traditional approaches based on combinatorial optimization, rule-based
heuristics, and reinforcement learning often overlook driver income fairness,
interpretability, and adaptability to real-world dynamics. To address these
gaps, we propose LLM-ODDR, a novel framework leveraging Large Language Models
(LLMs) for joint Order Dispatching and Driver Repositioning (ODDR) in
ride-hailing services. LLM-ODDR framework comprises three key components: (1)
Multi-objective-guided Order Value Refinement, which evaluates orders by
considering multiple objectives to determine their overall value; (2)
Fairness-aware Order Dispatching, which balances platform revenue with driver
income fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning,
which optimizes idle vehicle placement based on historical patterns and
projected supply. We also develop JointDR-GPT, a fine-tuned model optimized for
ODDR tasks with domain knowledge. Extensive experiments on real-world datasets
from Manhattan taxi operations demonstrate that our framework significantly
outperforms traditional methods in terms of effectiveness, adaptability to
anomalous conditions, and decision interpretability. To our knowledge, this is
the first exploration of LLMs as decision-making agents in ride-hailing ODDR
tasks, establishing foundational insights for integrating advanced language
models within intelligent transportation systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 这篇论文探讨了在大型语言模型中通过重复抽样来提高覆盖率的方法，并提出了一个基准来更准确地评估这种方法在不同场景下的效果。


<details>
  <summary>Details</summary>
Motivation: 作者推测，在大型语言模型中通过重复抽样增加覆盖范围的部分原因是由于标准评估基准的答案分布偏向于一小部分常见答案。

Method: 定义了一种基线方法，根据训练集中答案的普遍性列举答案，并进行了涵盖数学推理和事实知识两个领域的实验。

Result: 实验结果显示，这种基准方法在某些LLMs上优于重复模型抽样，而对于其他LLMs，则与混合策略的效果相当，后者通过使用仅10个模型样本来获取k个答案，并通过枚举猜测剩余的k-10次尝试。

Conclusion: 该论文提出了一种基准方法，用于更准确地评估在重复抽样中覆盖范围的改进程度，超出了与提示无关的猜测。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [13] [A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains](https://arxiv.org/abs/2402.00559)
*Alon Jacovi,Yonatan Bitton,Bernd Bohnet,Jonathan Herzig,Or Honovich,Michael Tseng,Michael Collins,Roee Aharoni,Mor Geva*

Main category: cs.CL

TL;DR: 该论文推出了REVEAL数据集，用于评估自动验证复杂思维链推理的效果，揭示了现有验证器在验证逻辑正确性和检测矛盾方面的不足，并提供了全面的步骤级标注以推动研究进展。


<details>
  <summary>Details</summary>
Motivation: 尽管提示语言模型提供逐步答案（如“思维链”）已成为复杂推理任务的主要方法，并且更准确的推理链通常能提升下游任务的表现，但目前缺乏细粒度的步骤级数据集来评估自动验证方法。这阻碍了该领域的进展。

Method: 论文提出了一种新的数据集REVEAL，包含多方面的详细标注，用于评估自动验证模型在开放领域问答任务中的表现。这些标注覆盖了语言模型答案中每个推理步骤的相关性、证据归属和逻辑正确性。

Result: 通过对REVEAL数据集的评估发现，现有的验证器在验证复杂的推理链时存在困难，尤其是在判断逻辑正确性和识别矛盾方面。这表明需要进一步改进自动验证技术。

Conclusion: 该论文介绍了REVEAL数据集，这是一个用于评估自动验证复杂思维链推理的基准数据集。通过引入全面的步骤级标签，包括相关性、证据段落的归属和逻辑正确性，REVEAL旨在推动自动验证方法的发展。实验结果表明，当前的验证器在验证推理链方面仍面临挑战，特别是在验证逻辑正确性和检测矛盾方面。

Abstract: Prompting language models to provide step-by-step answers (e.g.,
"Chain-of-Thought") is the prominent approach for complex reasoning tasks,
where more accurate reasoning chains typically improve downstream task
performance. Recent literature discusses automatic methods to verify reasoning
to evaluate and improve their correctness. However, no fine-grained step-level
datasets are available to enable thorough evaluation of such verification
methods, hindering progress in this direction. We introduce REVEAL: Reasoning
Verification Evaluation, a dataset to benchmark automatic verifiers of complex
Chain-of-Thought reasoning in open-domain question-answering settings. REVEAL
includes comprehensive labels for the relevance, attribution to evidence
passages, and logical correctness of each reasoning step in a language model's
answer, across a variety of datasets and state-of-the-art language models.
Evaluation on REVEAL shows that verifiers struggle at verifying reasoning
chains - in particular, verifying logical correctness and detecting
contradictions. Available at https://reveal-dataset.github.io/ .

</details>


### [14] [ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2505.23723)
*Zexi Liu,Jingyi Chai,Xinyu Zhu,Shuo Tang,Rui Ye,Bo Zhang,Lei Bai,Siheng Chen*

Main category: cs.CL

TL;DR: 本文介绍了一种基于大语言模型的新型机器学习代理框架，通过在线强化学习实现自主学习。提出的框架包括探索增强微调、逐步强化学习和代理专用奖励模块，最终训练出的小型ML-Agent在性能上超过了更大规模的模型，并展现了卓越的泛化能力和持续优化潜力。


<details>
  <summary>Details</summary>
Motivation: 目前大多数基于大语言模型（LLM）的代理依赖手动提示工程，缺乏根据多样化实验经验进行适应和优化的能力。因此，作者希望通过引入基于学习的方法，实现一个能够通过交互式实验进行自主学习的LLM代理，从而推动自动化机器学习的发展。

Method: 文章提出了一种新的代理机器学习训练框架，包含三个关键组成部分：增强探索的微调、逐步强化学习以及特定于代理机器学习的奖励模块。这些方法旨在提高强化学习的探索性、加速经验收集并优化训练效率，同时统一多样的机器学习反馈信号以进行强化学习优化。

Result: 利用提出的框架，研究人员成功训练了一个7B规模的Qwen-2.5 LLM驱动的ML-Agent。尽管仅在9个机器学习任务上进行了训练，该模型在多个指标上超越了671B规模的DeepSeek-R1代理，并且展示了出色的跨任务泛化能力和持续性能提升。

Conclusion: 本文首次探索了基于学习的代理机器学习范式，通过交互式实验和在线强化学习使LLM代理能够自主学习。提出的框架不仅提高了训练效率，还实现了卓越的跨任务泛化能力。尽管规模较小，但ML-Agent在多个任务上优于更大规模的模型，并展示了持续性能改进的能力。

Abstract: The emergence of large language model (LLM)-based agents has significantly
advanced the development of autonomous machine learning (ML) engineering.
However, most existing approaches rely heavily on manual prompt engineering,
failing to adapt and optimize based on diverse experimental experiences.
Focusing on this, for the first time, we explore the paradigm of learning-based
agentic ML, where an LLM agent learns through interactive experimentation on ML
tasks using online reinforcement learning (RL). To realize this, we propose a
novel agentic ML training framework with three key components: (1)
exploration-enriched fine-tuning, which enables LLM agents to generate diverse
actions for enhanced RL exploration; (2) step-wise RL, which enables training
on a single action step, accelerating experience collection and improving
training efficiency; (3) an agentic ML-specific reward module, which unifies
varied ML feedback signals into consistent rewards for RL optimization.
Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM
for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our
7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it
achieves continuous performance improvements and demonstrates exceptional
cross-task generalization capabilities.

</details>


### [15] [Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation](https://arxiv.org/abs/2505.23657)
*Hongxiang Zhang,Hao Chen,Tianyi Zhang,Muhao Chen*

Main category: cs.CL

TL;DR: 本文介绍了一种新的解码策略 ActLCD，它通过强化学习和奖励感知分类器优化事实性，有效减少了大型语言模型中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的解码方法已经改进了大型语言模型的事实性，但它们仍然容易产生幻觉，尤其是在较长的上下文中。

Method: 提出了一种名为 Active Layer-Contrastive Decoding (ActLCD) 的解码策略，利用强化学习决定何时在生成过程中应用对比层。

Result: 实验表明，ActLCD 在五个基准测试中均超过了最先进的方法，显示了其在不同生成场景中减少幻觉的有效性。

Conclusion: ActLCD 是一种新的解码策略，通过将解码视为序列决策问题，使用强化学习策略和奖励感知分类器来优化事实性，超过了现有方法的效果。

Abstract: Recent decoding methods improve the factuality of large language
models~(LLMs) by refining how the next token is selected during generation.
These methods typically operate at the token level, leveraging internal
representations to suppress superficial patterns. Nevertheless, LLMs remain
prone to hallucinations, especially over longer contexts. In this paper, we
propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy
that actively decides when to apply contrasting layers during generation. By
casting decoding as a sequential decision-making problem, ActLCD employs a
reinforcement learning policy guided by a reward-aware classifier to optimize
factuality beyond the token level. Our experiments demonstrate that ActLCD
surpasses state-of-the-art methods across five benchmarks, showcasing its
effectiveness in mitigating hallucinations in diverse generation scenarios.

</details>


### [16] [ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind](https://arxiv.org/abs/2505.22961)
*Peixuan Han,Zijia Liu,Jiaxuan You*

Main category: cs.CL

TL;DR: 本文提出了一种新的说服代理ToMAP，通过结合理论心智模块和强化学习框架，显著提升了大语言模型在说服任务中的效果。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型在模拟对手思维和生成多样化说服策略方面存在局限性，而人类在这方面具有较强的能力。

Method: 设计了一个增强说服者代理的方法，包括预测对手立场的模块和一个强化学习框架，以利用对手相关信息生成更有效的论点。

Result: 实验表明，尽管参数量仅为3B，ToMAP的表现优于GPT-4o等更大的基线模型，在多个说服任务中取得了39.4%的相对提升。

Conclusion: ToMAP通过引入两个理论心智模块，显著提高了大语言模型在说服任务中的表现，展示了更高效、更具逻辑性和对手意识的说服能力。

Abstract: Large language models (LLMs) have shown promising potential in persuasion,
but existing works on training LLM persuaders are still preliminary. Notably,
while humans are skilled in modeling their opponent's thoughts and opinions
proactively and dynamically, current LLMs struggle with such Theory of Mind
(ToM) reasoning, resulting in limited diversity and opponent awareness. To
address this limitation, we introduce Theory of Mind Augmented Persuader
(ToMAP), a novel approach for building more flexible persuader agents by
incorporating two theory of mind modules that enhance the persuader's awareness
and analysis of the opponent's mental state. Specifically, we begin by
prompting the persuader to consider possible objections to the target central
claim, and then use a text encoder paired with a trained MLP classifier to
predict the opponent's current stance on these counterclaims. Our carefully
designed reinforcement learning schema enables the persuader learns how to
analyze opponent-related information and utilize it to generate more effective
arguments. Experiments show that the ToMAP persuader, while containing only 3B
parameters, outperforms much larger baselines, like GPT-4o, with a relative
gain of 39.4% across multiple persuadee models and diverse corpora. Notably,
ToMAP exhibits complex reasoning chains and reduced repetition during training,
which leads to more diverse and effective arguments. The opponent-aware feature
of ToMAP also makes it suitable for long conversations and enables it to employ
more logical and opponent-aware strategies. These results underscore our
method's effectiveness and highlight its potential for developing more
persuasive language agents. Code is available at:
https://github.com/ulab-uiuc/ToMAP.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [17] [Model-Based AI planning and Execution Systems for Robotics](https://arxiv.org/abs/2505.04493)
*Or Wertheim,Ronen I. Brafman*

Main category: cs.RO

TL;DR: 这篇论文综述了现代机器人中基于模型的任务级控制系统的发展，讨论了设计选择、现有解决方案，并建议了未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: 尽管基于模型的规划与执行系统的理念历史悠久，但直到最近才出现能够与现代机器人平台有效集成的通用系统，如ROSPlan。因此，需要对现有系统的多样性进行总结，并为未来研究提供指导。

Method: 文章通过回顾现代机器人学中基于模型的规划与执行系统的发展历程，分析了不同系统的设计选择、解决的问题及提出的解决方案，并展望了未来可能的研究方向。

Result: 文章提供了对现有基于模型的机器人任务控制系统的全面综述，包括其设计选择、解决的关键问题以及所采用的不同方法，并提出了未来发展的潜在方向。

Conclusion: 这篇文章总结了基于模型的机器人任务级控制系统的发展现状，并探讨了现有系统在设计选择和解决方案方面的多样性。作者指出未来研究可以进一步优化规划与执行框架，以提升通用性与集成能力。

Abstract: Model-based planning and execution systems offer a principled approach to
building flexible autonomous robots that can perform diverse tasks by
automatically combining a host of basic skills. This idea is almost as old as
modern robotics. Yet, while diverse general-purpose reasoning architectures
have been proposed since, general-purpose systems that are integrated with
modern robotic platforms have emerged only recently, starting with the
influential ROSPlan system. Since then, a growing number of model-based systems
for robot task-level control have emerged. In this paper, we consider the
diverse design choices and issues existing systems attempt to address, the
different solutions proposed so far, and suggest avenues for future
development.

</details>


### [18] [Hardware Design and Learning-Based Software Architecture of Musculoskeletal Wheeled Robot Musashi-W for Real-World Applications](https://arxiv.org/abs/2403.11729)
*Kento Kawaharazuka,Akihiro Miki,Masahiro Bando,Temma Suzuki,Yoshimoto Ribayashi,Yasunori Toshimitsu,Yuya Nagamatsu,Kei Okada,and Masayuki Inaba*

Main category: cs.RO

TL;DR: Researchers developed Musashi-W, a wheeled musculoskeletal robot with advanced software integration, enabling it to perform practical tasks like cleaning, carrying heavy loads, and manipulating cloth.


<details>
  <summary>Details</summary>
Motivation: Despite the flexibility and redundancy of musculoskeletal humanoids, they struggle with real-world applications, especially bipedal walking. This research aims to overcome these challenges by developing a more applicable design using a wheeled platform combined with musculoskeletal arms.

Method: The researchers developed Musashi-W, a musculoskeletal wheeled robot integrating a wheeled base with musculoskeletal upper limbs. Its software system combines static and dynamic body schema learning, reflex control, and visual recognition to optimize performance.

Result: Musashi-W demonstrated its effectiveness through various tasks like cleaning via human teaching, carrying heavy objects with added muscle support, and dynamically manipulating cloth for table setting, showcasing its versatility and adaptability.

Conclusion: Musashi-W is a successful combination of hardware and software advancements that enable practical applications of musculoskeletal robotics, particularly in tasks requiring flexibility, strength, and interaction with complex environments.

Abstract: Various musculoskeletal humanoids have been developed so far. While these
humanoids have the advantage of their flexible and redundant bodies that mimic
the human body, they are still far from being applied to real-world tasks. One
of the reasons for this is the difficulty of bipedal walking in a flexible
body. Thus, we developed a musculoskeletal wheeled robot, Musashi-W, by
combining a wheeled base and musculoskeletal upper limbs for real-world
applications. Also, we constructed its software system by combining static and
dynamic body schema learning, reflex control, and visual recognition. We show
that the hardware and software of Musashi-W can make the most of the advantages
of the musculoskeletal upper limbs, through several tasks of cleaning by human
teaching, carrying a heavy object considering muscle addition, and setting a
table through dynamic cloth manipulation with variable stiffness.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [19] [Casper DPM: Cascaded Perceptual Dynamic Projection Mapping onto Hands](https://arxiv.org/abs/2409.04397)
*Yotam Erel,Or Kozlovsky-Mordenfeld,Daisuke Iwai,Kosuke Sato,Amit H. Bermano*

Main category: cs.GR

TL;DR: 文章介绍了一种将3D内容动态投射到人手的新技术，通过结合3D手部姿态粗略估计与高速2D校正步骤，有效减少了感知延迟，提高了投影的对齐性和用户体验。


<details>
  <summary>Details</summary>
Motivation: 由于人手具有高度关节化和可变形的特点，准确且快速地计算其姿态和形状是一项挑战。因此，作者提出了一种新的技术来改进3D内容动态投影到人手上的体验。

Method: 文章的方法是将3D手部姿态的粗略估计与高速2D校正步骤相结合，从而优化投影效果并减少延迟。此外，该方法利用完整的3D手部重建，可以应用任意纹理或性能合理的特效。

Result: 研究结果显示，使用该方法后，受试者对延迟伪影的敏感度降低，在相关任务中表现得更快、更轻松。此外，还演示了多种创新用例和应用。

Conclusion: 文章的结论是，通过结合较慢的3D手部姿态粗略估计与高速2D校正步骤，能够显著提高投影对手部的对齐性、增加投影表面积，并减少感知延迟。这种方法使得用户在执行任务时更加流畅和快速，同时展示了多个新颖的应用场景。

Abstract: We present a technique for dynamically projecting 3D content onto human hands
with short perceived motion-to-photon latency. Computing the pose and shape of
human hands accurately and quickly is a challenging task due to their
articulated and deformable nature. We combine a slower 3D coarse estimation of
the hand pose with high speed 2D correction steps which improve the alignment
of the projection to the hands, increase the projected surface area, and reduce
perceived latency. Since our approach leverages a full 3D reconstruction of the
hands, any arbitrary texture or reasonably performant effect can be applied,
which was not possible before. We conducted two user studies to assess the
benefits of using our method. The results show subjects are less sensitive to
latency artifacts and perform faster and with more ease a given associated task
over the naive approach of directly projecting rendered frames from the 3D pose
estimation. We demonstrate several novel use cases and applications.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [20] [Prices and preferences in the electric vehicle market](https://arxiv.org/abs/2403.00458)
*Chung Yi See,Vasco Rato Santos,Lucas Woodley,Megan Yeo,Daniel Palmer,Shuheng Zhang,and Ashley Nunes*

Main category: econ.EM

TL;DR: 该论文指出电动车的高价格主要反映消费者对功能丰富和更强大车辆的偏好，而非仅由电池成本决定，而这种偏好可能降低燃油经济性和减排效益。


<details>
  <summary>Details</summary>
Motivation: 电动车虽然比汽油车污染少，但由于较高的采购价格，其普及受到挑战。本文旨在审视现有讨论中关于电池成本是价格差异主要原因的观点，并进一步探究影响电动车价格的其他因素。

Method: 通过分析2011年至2023年间的电动车属性和市场状况数据，探讨影响电动车价格的因素及其与电池成本、车辆特征之间的关系。

Result: 首先，电动车的价格主要受标准配置的设施、附加功能和经销商安装的配件数量影响；其次，续航里程与价格呈负相关；第三，电池容量与价格正相关，因为更大的容量意味着更多的马力输出；最后，这种消费偏好导致了较低的燃油经济性，减少了预期的减排效益。

Conclusion: 研究发现，电动车的较高价格主要反映了消费者对功能丰富和更强大车辆的偏好，但这种偏好导致了较低的燃油经济性，从而减少了预期的生命周期排放效益。这提示在强调电气化作为脱碳路径的同时，需要关注其对实际减排目标的影响。

Abstract: Although electric vehicles are less polluting than gasoline powered vehicles,
adoption is challenged by higher procurement prices. Existing discourse
emphasizes EV battery costs as being principally responsible for this price
differential and widespread adoption is routinely conditioned upon battery
costs declining. We scrutinize such reasoning by sourcing data on EV attributes
and market conditions between 2011 and 2023. Our findings are fourfold. First,
EV prices are influenced principally by the number of amenities, additional
features, and dealer-installed accessories sold as standard on an EV, and to a
lesser extent, by EV horsepower. Second, EV range is negatively correlated with
EV price implying that range anxiety concerns may be less consequential than
existing discourse suggests. Third, battery capacity is positively correlated
with EV price, due to more capacity being synonymous with the delivery of more
horsepower. Collectively, this suggests that higher procurement prices for EVs
reflects consumer preference for vehicles that are feature dense and more
powerful. Fourth and finally, accommodating these preferences have produced
vehicles with lower fuel economy, a shift that reduces envisioned lifecycle
emissions benefits by at least 3.26 percent, subject to the battery pack
chemistry leveraged and the carbon intensity of the electrical grid. These
findings warrant attention as decarbonization efforts increasingly emphasize
electrification as a pathway for complying with domestic and international
climate agreements.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [21] [Accelerating ab initio melting property calculations with machine learning: Application to the high entropy alloy TaVCrW](https://arxiv.org/abs/2408.08654)
*Li-Fang Zhu,Fritz Koermann,Qing Chen,Malin Selleby,Joerg Neugebauer,and Blazej Grabowski*

Main category: cond-mat.mtrl-sci

TL;DR: 这篇论文介绍了一种结合密度泛函理论和机器学习的新方法，用于高效准确地预测高熵合金等材料的熔化性质，计算资源消耗减少了80%。


<details>
  <summary>Details</summary>
Motivation: 熔化性质对于设计新型高性能耐火材料至关重要，但实验测量因其高温特性而极具挑战性，因此需要一种高效且准确的理论预测方法。

Method: 本研究通过专门设计的机器学习势函数替代传统的热力学积分方法，利用自由能微扰计算显著提高了计算效率，并将其应用于高熵合金的熔化性质预测。

Result: 作者应用新方法对高熵合金TaVCrW进行了熔化温度、熔融熵和焓变以及体积变化的计算，并得到了固态和液态的热容数据，这些结果与现有的calphad外推值相符。

Conclusion: 该论文提出了一种高效的基于密度泛函理论（DFT）的方法，结合机器学习势函数来预测材料的熔化性质。这种方法在资源消耗上比现有方法节省了80%，并成功应用于高熵合金TaVCrW的熔化性质计算，结果与calphad外推值基本一致。

Abstract: Melting properties are critical for designing novel materials, especially for
discovering high-performance, high-melting refractory materials. Experimental
measurements of these properties are extremely challenging due to their high
melting temperatures. Complementary theoretical predictions are, therefore,
indispensable. The conventional free energy approach using density functional
theory (DFT) has been a gold standard for such purposes because of its high
accuracy. However,it generally involves expensive thermodynamic integration
using ab initio molecular dynamic simulations. The high computational cost
makes high-throughput calculations infeasible. Here, we propose a highly
efficient DFT-based method aided by a specially designed machine learning
potential. As the machine learning potential can closely reproduce the ab
initio phase space, even for multi-component alloys, the costly thermodynamic
integration can be fully substituted with more efficient free energy
perturbation calculations. The method achieves overall savings of computational
resources by 80% compared to current alternatives. We apply the method to the
high-entropy alloy TaVCrW and calculate its melting properties, including
melting temperature, entropy and enthalpy of fusion, and volume change at the
melting point. Additionally, the heat capacities of solid and liquid TaVCrW are
calculated. The results agree reasonably with the calphad extrapolated values.

</details>
