<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 2]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: This position paper critically surveys recent empirical developments in human-AI agent collaboration, identifying a lack of a unifying theoretical framework for integrating diverse studies on complex tasks. A new conceptual architecture is proposed to interlink multi-agent coordination, knowledge management, and control mechanisms, facilitating revisions of existing methods and inspiring innovative research blending qualitative and quantitative approaches.


<details>
  <summary>Details</summary>
Motivation: We observe a lack of a unifying theoretical framework that can coherently integrate these varied studies, especially when tackling open-ended, complex tasks.

Method: By mapping existing contributions, from symbolic AI techniques and connectionist LLM-based agents to hybrid organizational practices, onto this proposed framework (Hierarchical Exploration-Exploitation Net), our approach facilitates revision of legacy methods and inspires new work that fuses qualitative and quantitative paradigms.

Result: To address this, we propose a novel conceptual architecture: one that systematically interlinks the technical details of multi-agent coordination, knowledge management, cybernetic feedback loops, and higher-level control mechanisms.

Conclusion: Together, these insights offer a stepping stone toward deeper co-evolution of human cognition and AI capability.

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [2] [Artificial Expert Intelligence through PAC-reasoning](https://arxiv.org/abs/2412.02441)
*Shai Shalev-Shwartz,Amnon Shashua,Gal Beniamini,Yoav Levine,Or Sharir,Noam Wies,Ido Ben-Shaul,Tomer Nussbaum,Shir Granot Peled*

Main category: cs.AI

TL;DR: 本文提出了“人工专家智能（AEI）”概念，结合领域专业知识与精确推理，以突破传统人工智能在复杂问题上的限制。


<details>
  <summary>Details</summary>
Motivation: 现有的人工智能系统在预定义任务上表现出色，但在新颖的问题解决中缺乏适应性和精确性。为此，本文提出AEI 框架，旨在超越通用人工智能（AGI）和狭义AI的局限性，将领域专业知识与类人专家的批判性和精确推理能力相结合。

Method: 文章引入了一种名为“系统3”的精确推理方法，这种方法结合了领域专业知识与严密的科学方法，以实现比现有AI系统更高的适应性和精确性。

Result: AEI 成功地将类似于人类专家的精确推理能力整合到 AI 系统中，从而提升了系统在复杂问题上的可靠性和可控性。

Conclusion: AEI 提出了一个新框架，即“可能近似正确（PAC）推理”，通过提供可靠的理论保证和控制推理精度的机制，为解决复杂问题奠定了基础，并为错误边界内的推理时学习建立了基础。

Abstract: Artificial Expert Intelligence (AEI) seeks to transcend the limitations of
both Artificial General Intelligence (AGI) and narrow AI by integrating
domain-specific expertise with critical, precise reasoning capabilities akin to
those of top human experts. Existing AI systems often excel at predefined tasks
but struggle with adaptability and precision in novel problem-solving. To
overcome this, AEI introduces a framework for ``Probably Approximately Correct
(PAC) Reasoning". This paradigm provides robust theoretical guarantees for
reliably decomposing complex problems, with a practical mechanism for
controlling reasoning precision. In reference to the division of human thought
into System 1 for intuitive thinking and System 2 for reflective
reasoning~\citep{tversky1974judgment}, we refer to this new type of reasoning
as System 3 for precise reasoning, inspired by the rigor of the scientific
method. AEI thus establishes a foundation for error-bounded, inference-time
learning.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models](https://arxiv.org/abs/2506.01413)
*Yulei Qin,Gang Li,Zongyi Li,Zihan Xu,Yuchen Shi,Zhekai Lin,Xiao Cui,Ke Li,Xing Sun*

Main category: cs.CV

TL;DR: 本文提出了一种通过激励推理来提升大模型处理复杂指令的新方法，效果优于更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在遵循包含多种约束结构（如并行、链式和分支结构）的复杂指令时面临挑战。传统的链式思维方法由于其浅层的推理模式而无法有效解决这些问题。

Method: 首先基于现有分类法对复杂指令进行分解，并提出一种可重复的数据获取方法。然后利用具有可验证的以规则为中心的奖励信号的强化学习来培养特定于指令跟随的推理能力。此外，还采用样本对比和专家行为克隆来加强链式思维（CoT）并促进分布转移。

Result: 提出的系统性方法显著提升了1.5B参数模型的性能，在多个基准测试中平均提高了11.74%，并且表现接近8B参数模型。

Conclusion: 论文提出了一种新的方法，通过激励推理来增强大型语言模型在处理复杂指令方面的能力。经过七个综合基准的广泛评估验证了该方法的有效性，并且使用1.5B参数的模型达到了与8B参数模型相当的性能。

Abstract: Existing large language models (LLMs) face challenges of following complex
instructions, especially when multiple constraints are present and organized in
paralleling, chaining, and branching structures. One intuitive solution, namely
chain-of-thought (CoT), is expected to universally improve capabilities of
LLMs. However, we find that the vanilla CoT exerts a negative impact on
performance due to its superficial reasoning pattern of simply paraphrasing the
instructions. It fails to peel back the compositions of constraints for
identifying their relationship across hierarchies of types and dimensions. To
this end, we propose a systematic method to boost LLMs in dealing with complex
instructions via incentivizing reasoning for test-time compute scaling. First,
we stem from the decomposition of complex instructions under existing
taxonomies and propose a reproducible data acquisition method. Second, we
exploit reinforcement learning (RL) with verifiable rule-centric reward signals
to cultivate reasoning specifically for instruction following. We address the
shallow, non-essential nature of reasoning under complex instructions via
sample-wise contrast for superior CoT enforcement. We also exploit behavior
cloning of experts to facilitate steady distribution shift from fast-thinking
LLMs to skillful reasoners. Extensive evaluations on seven comprehensive
benchmarks confirm the validity of the proposed method, where a 1.5B LLM
achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data
are available at https://github.com/yuleiqin/RAIF.

</details>


### [4] [MvDeCor: Multi-view Dense Correspondence Learning for Fine-grained 3D Segmentation](https://arxiv.org/abs/2208.08580)
*Gopal Sharma,Kangxue Yin,Subhransu Maji,Evangelos Kalogerakis,Or Litany,Sanja Fidler*

Main category: cs.CV

TL;DR: 提出了一种结合2D自监督技术和3D几何推理的新方法，用于细粒度3D形状分割任务。


<details>
  <summary>Details</summary>
Motivation: 基于视图的表面表示在建模高分辨率表面细节和纹理方面比基于点云或体素占用的3D对应表示更有效。

Method: 在对比学习框架下设置密集对应学习任务，通过多视角渲染3D形状并利用2D领域的自监督技术进行学习。

Result: 所提出的MvDeCor方法在RenderPeople和PartNet数据集上均取得了优于现有方法的性能。

Conclusion: 实验证明，MvDeCor在细粒度3D形状分割任务中优于现有技术，特别是在训练视图稀疏或形状具有纹理时效果更佳。

Abstract: We propose to utilize self-supervised techniques in the 2D domain for
fine-grained 3D shape segmentation tasks. This is inspired by the observation
that view-based surface representations are more effective at modeling
high-resolution surface details and texture than their 3D counterparts based on
point clouds or voxel occupancy. Specifically, given a 3D shape, we render it
from multiple views, and set up a dense correspondence learning task within the
contrastive learning framework. As a result, the learned 2D representations are
view-invariant and geometrically consistent, leading to better generalization
when trained on a limited number of labeled shapes compared to alternatives
that utilize self-supervision in 2D or 3D alone. Experiments on textured
(RenderPeople) and untextured (PartNet) 3D datasets show that our method
outperforms state-of-the-art alternatives in fine-grained part segmentation.
The improvements over baselines are greater when only a sparse set of views is
available for training or when shapes are textured, indicating that MvDeCor
benefits from both 2D processing and 3D geometric reasoning.

</details>


### [5] [Mix3D: Out-of-Context Data Augmentation for 3D Scenes](https://arxiv.org/abs/2110.02210)
*Alexey Nekrasov,Jonas Schult,Or Litany,Bastian Leibe,Francis Engelmann*

Main category: cs.CV

TL;DR: Mix3D introduces a mixing technique for 3D scene segmentation that improves model performance by balancing global context and local geometry, achieving state-of-the-art results on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Current 3D scene segmentation models focus on capturing global context, which can lead to errors due to over-reliance on contextual priors. The work aims to balance global context with local geometry to generalize beyond these priors.

Method: The paper proposes a 'mixing' technique where new training samples are created by combining two augmented scenes, placing object instances into novel out-of-context environments. This makes it harder for models to rely solely on scene context and encourages inference from local structures.

Result: Models trained with Mix3D showed significant performance boosts on indoor (ScanNet, S3DIS) and outdoor (SemanticKITTI) datasets. For example, MinkowskiNet trained with Mix3D achieved 78.1 mIoU on the ScanNet test benchmark, outperforming all prior state-of-the-art methods.

Conclusion: Mix3D is a data augmentation technique that balances global scene context and local geometry to improve the segmentation of large-scale 3D scenes. It can be easily integrated with existing methods and shows significant performance improvements on both indoor and outdoor datasets.

Abstract: We present Mix3D, a data augmentation technique for segmenting large-scale 3D
scenes. Since scene context helps reasoning about object semantics, current
works focus on models with large capacity and receptive fields that can fully
capture the global context of an input 3D scene. However, strong contextual
priors can have detrimental implications like mistaking a pedestrian crossing
the street for a car. In this work, we focus on the importance of balancing
global scene context and local geometry, with the goal of generalizing beyond
the contextual priors in the training set. In particular, we propose a "mixing"
technique which creates new training samples by combining two augmented scenes.
By doing so, object instances are implicitly placed into novel out-of-context
environments and therefore making it harder for models to rely on scene context
alone, and instead infer semantics from local structure as well. We perform
detailed analysis to understand the importance of global context, local
structures and the effect of mixing scenes. In experiments, we show that models
trained with Mix3D profit from a significant performance boost on indoor
(ScanNet, S3DIS) and outdoor datasets (SemanticKITTI). Mix3D can be trivially
used with any existing method, e.g., trained with Mix3D, MinkowskiNet
outperforms all prior state-of-the-art methods by a significant margin on the
ScanNet test benchmark 78.1 mIoU. Code is available at:
https://nekrasov.dev/mix3d/

</details>


### [6] [MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning](https://arxiv.org/abs/2505.24871)
*Yiqing Liang,Jielin Qiu,Wenhao Ding,Zuxin Liu,James Tompkin,Mengdi Xu,Mengzhou Xia,Zhengzhong Tu,Laixi Shi,Jiacheng Zhu*

Main category: cs.CV

TL;DR: 本文介绍了一种针对多模态大语言模型的强化学习框架，通过优化数据混合策略，显著提升了模型的推理和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于多模态大语言模型（MLLMs）需要处理广泛且异构的视觉-语言任务，传统的单数据集训练方式难以满足其复杂需求，因此引入了能够解决多领域目标冲突并提高模型性能的最优数据混合策略。

Method: 构建了一个多模态RLVR框架，结合了包含多种可验证视觉-语言问题的数据集，并提出了一个数据混合策略，该策略通过预测不同数据分布下的强化学习微调结果来优化最佳混合方案。

Result: 多领域RLVR训练结合混合预测策略显著增强了MLLM的通用推理能力，在多个基准测试中取得了更高的准确率。

Conclusion: 论文提出了一种系统性的多模态大语言模型RLVR后训练框架，通过数据混合策略优化提升模型的泛化能力和推理能力。实验结果显示，与均匀数据混合方法相比，所提出的最佳混合策略在分布外基准测试中平均提高了5.24%的准确性，并相对于预微调基线提升了20.74%。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for post-training large language models (LLMs), achieving
state-of-the-art performance on tasks with structured, verifiable answers.
Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but
is complicated by the broader, heterogeneous nature of vision-language tasks
that demand nuanced visual, logical, and spatial capabilities. As such,
training MLLMs using RLVR on multiple datasets could be beneficial but creates
challenges with conflicting objectives from interaction among diverse datasets,
highlighting the need for optimal dataset mixture strategies to improve
generalization and reasoning. We introduce a systematic post-training framework
for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation
and benchmark implementation. Specifically, (1) We developed a multimodal RLVR
framework for multi-dataset post-training by curating a dataset that contains
different verifiable vision-language problems and enabling multi-domain online
RL learning with different verifiable rewards; (2) We proposed a data mixture
strategy that learns to predict the RL fine-tuning outcome from the data
mixture distribution, and consequently optimizes the best mixture.
Comprehensive experiments showcase that multi-domain RLVR training, when
combined with mixture prediction strategies, can significantly boost MLLM
general reasoning capacities. Our best mixture improves the post-trained
model's accuracy on out-of-distribution benchmarks by an average of 5.24%
compared to the same model post-trained with uniform data mixture, and by a
total of 20.74% compared to the pre-finetuning baseline.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Alignment as Distribution Learning: Your Preference Model is Explicitly a Language Model](https://arxiv.org/abs/2506.01523)
*Jihun Yun,Juno Kim,Jongho Park,Junhyuck Kim,Jongha Jon Ryu,Jaewoong Cho,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: 这篇论文介绍了一种新的基于分布学习的语言模型对齐方法，通过成对偏好反馈解决了传统RLHF和DPO的缺陷，在理论上和实践上都取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的RLHF及其变体（如DPO）缺乏理论支持，并可能导致退化的确定性解。因此需要一种新的方法从根本上解决这些问题。

Method: 将对齐问题重新定义为从成对偏好反馈中进行分布学习，提出了三种理论上有保证的学习目标：偏好最大似然估计、偏好蒸馏和反向KL最小化。

Result: 论文理论上证明了所提出的三种方法具有强的非渐近收敛性O(1/n)，并且实验表明这些方法在性能上优于或匹配现有技术。

Conclusion: 该论文提出了一种基于分布学习的新型对齐框架，通过成对偏好反馈来改进大语言模型的输出质量。与传统的RLHF和DPO方法相比，新方法在多个任务和模型上表现更优或相当，并且理论上避免了退化和奖励过拟合问题。

Abstract: Alignment via reinforcement learning from human feedback (RLHF) has become
the dominant paradigm for controlling the quality of outputs from large
language models (LLMs). However, when viewed as `loss + regularization,' the
standard RLHF objective lacks theoretical justification and incentivizes
degenerate, deterministic solutions, an issue that variants such as Direct
Policy Optimization (DPO) also inherit. In this paper, we rethink alignment by
framing it as \emph{distribution learning} from pairwise preference feedback by
explicitly modeling how information about the target language model bleeds
through the preference data. This explicit modeling leads us to propose three
principled learning objectives: preference maximum likelihood estimation,
preference distillation, and reverse KL minimization. We theoretically show
that all three approaches enjoy strong non-asymptotic $O(1/n)$ convergence to
the target language model, naturally avoiding degeneracy and reward
overfitting. Finally, we empirically demonstrate that our distribution learning
framework, especially preference distillation, consistently outperforms or
matches the performances of RLHF and DPO across various tasks and models.

</details>


### [8] [Incentivizing LLMs to Self-Verify Their Answers](https://arxiv.org/abs/2506.01369)
*Fuxiang Zhang,Jiacheng Xu,Chaojie Wang,Ce Cui,Yang Liu,Bo An*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习框架，让大型语言模型在没有外部帮助的情况下自我验证答案，从而提高复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时扩展方法通常依赖外部奖励模型来指导生成过程，但在特定推理任务上后训练的模型只能带来有限的性能提升，其根本原因是生成器与奖励模型之间的分布差异。为了解决这一问题，作者提出了一个自我验证的框架。

Method: 通过结合答案生成和验证过程，在单一的强化学习过程中训练模型，使其能够自我验证答案的正确性。这种方法基于Qwen2.5-Math-7B和DeepSeek-R1-Distill-Qwen-1.5B模型进行实现。

Result: 实验显示，所提出的自我验证模型在多个数学推理基准任务上表现出色，不仅改善了后训练性能，还能在测试阶段有效扩展模型性能。此外，该模型可在推理过程中自行验证生成内容，而无需依赖外部验证工具。

Conclusion: 文章总结提出了一种基于自我验证的强化学习框架，使大语言模型能够在无需外部验证器的情况下评估自身答案的正确性，并在推理阶段实现有效的性能扩展。实验结果表明，该方法不仅提升了后训练性能，还增强了测试时的扩展能力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in complex
reasoning tasks through both post-training and test-time scaling laws. While
prevalent test-time scaling approaches are often realized by using external
reward models to guide the model generation process, we find only marginal
gains can be acquired when scaling a model post-trained on specific reasoning
tasks. We identify that the limited improvement stems from distribution
discrepancies between the specific post-trained generator and the general
reward model. To address this, we propose a framework that incentivizes LLMs to
self-verify their own answers. By unifying answer generation and verification
within a single reinforcement learning (RL) process, we train models that can
effectively assess the correctness of their own solutions. The trained model
can further scale its performance during inference time by verifying its
generations, without the need for external verifiers. We train our
self-verification models based on Qwen2.5-Math-7B and
DeepSeek-R1-Distill-Qwen-1.5B, demonstrating its capabilities across varying
reasoning context lengths. Experiments on multiple mathematical reasoning
benchmarks show that our models can not only improve post-training performance
but also enable effective test-time scaling. Our code is available at
https://github.com/mansicer/self-verification.

</details>


### [9] [Generalizable LLM Learning of Graph Synthetic Data with Reinforcement Learning](https://arxiv.org/abs/2506.00845)
*Yizhuo Zhang,Heng Wang,Shangbin Feng,Zhaoxuan Tan,Xinyun Liu,Yulia Tsvetkov*

Main category: cs.LG

TL;DR: 通过强化学习提升大模型在隐含图结构任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 需要让大模型从合成图数据中泛化到具有隐含图结构的真实任务，而不是仅仅记忆特定模式。

Method: 设计基于解决方案和过程的奖励机制，使用GRPO和DPO等强化学习算法对大模型进行训练，并在合成数据和真实世界任务中评估性能。

Result: 实验表明，与基线设置相比，所提出的强化学习方法在5个数据集上平均提升了12.9%，尤其过程奖励优于解决方案奖励。

Conclusion: 强化学习方法，尤其是过程奖励机制，在提升大模型处理隐含图结构任务的能力方面表现出色，但可组合性和解释性仍然是挑战。

Abstract: Previous research has sought to enhance the graph reasoning capabilities of
LLMs by supervised fine-tuning on synthetic graph data. While these led to
specialized LLMs better at solving graph algorithm problems, we don't need LLMs
for shortest path: we need generalization from synthetic graph data to
real-world tasks with implicit graph structures. In this work, we propose to
unlock generalizable learning of graph synthetic data with reinforcement
learning. We first design solution-based and process-based rewards for
synthetic graph problems: instead of rigid memorizing response patterns in
direct fine-tuning, we posit that RL would help LLMs grasp the essentials
underlying graph reasoning and alleviate overfitting. We employ RL algorithms
such as GRPO and DPO, aligning both off-the-shelf LLMs and LLMs fine-tuned on
synthetic graph data. We then compare them against existing settings on both
in-domain synthetic tasks and out-of-domain real-world tasks with implicit
graph structures such as multi-hop QA, structured planning, and more. Extensive
experiments demonstrate that our RL recipe leads to statistically significant
improvement on 5 datasets, with an average gain of 12.9\% over baseline
settings. Further analysis reveals that process-based rewards consistently
outperform solution-based rewards, mixing synthetic and real-world task data
yields potential gains, while compositionality and explainable intermediate
steps remains a critical challenge even after RL.

</details>


### [10] [ORAN-GUIDE: RAG-Driven Prompt Learning for LLM-Augmented Reinforcement Learning in O-RAN Network Slicing](https://arxiv.org/abs/2506.00576)
*Fatemeh Lotfi,Hossein Rajoli,Fatemeh Afghah*

Main category: cs.LG

TL;DR: 本文提出了ORAN-GUIDE，一种结合双大型语言模型（LLM）和多智能体强化学习（MARL）的新方法，用于提升开放无线接入网络（O-RAN）中的动态资源管理和控制效率。


<details>
  <summary>Details</summary>
Motivation: 由于先进无线网络需要支持高度动态和异构的服务需求，传统的深度强化学习（DRL）在处理原始、非结构化输入（如射频特征、QoS指标和流量趋势）时存在困难，限制了其在部分可观测和不断变化的环境中的策略泛化和决策效率。因此，需要一种能够更好提取任务相关语义信息的方法。

Method: 提出了一种名为ORAN-GUIDE的双LLM框架，利用领域特定语言模型ORANSight生成结构化的上下文感知提示，并与冻结的GPT编码器结合，为深度强化学习代理提供高层语义表示。这一设计采用了针对无线系统技术决策优化的检索增强生成（RAG）风格流程。

Result: ORAN-GUIDE在样本效率、策略收敛速度以及性能泛化方面均优于标准MARL和单LLM基线方法，证明了其在无线网络动态资源管理中的有效性。

Conclusion: ORAN-GUIDE通过结合双LLM框架和多智能体强化学习（MARL），显著提升了无线网络环境中动态资源分配的效率和泛化能力。实验结果表明，该方法在样本效率、策略收敛性和性能泛化方面优于传统MARL和单LLM基线方法。

Abstract: Advanced wireless networks must support highly dynamic and heterogeneous
service demands. Open Radio Access Network (O-RAN) architecture enables this
flexibility by adopting modular, disaggregated components, such as the RAN
Intelligent Controller (RIC), Centralized Unit (CU), and Distributed Unit (DU),
that can support intelligent control via machine learning (ML). While deep
reinforcement learning (DRL) is a powerful tool for managing dynamic resource
allocation and slicing, it often struggles to process raw, unstructured input
like RF features, QoS metrics, and traffic trends. These limitations hinder
policy generalization and decision efficiency in partially observable and
evolving environments. To address this, we propose \textit{ORAN-GUIDE}, a
dual-LLM framework that enhances multi-agent RL (MARL) with task-relevant,
semantically enriched state representations. The architecture employs a
domain-specific language model, ORANSight, pretrained on O-RAN control and
configuration data, to generate structured, context-aware prompts. These
prompts are fused with learnable tokens and passed to a frozen GPT-based
encoder that outputs high-level semantic representations for DRL agents. This
design adopts a retrieval-augmented generation (RAG) style pipeline tailored
for technical decision-making in wireless systems. Experimental results show
that ORAN-GUIDE improves sample efficiency, policy convergence, and performance
generalization over standard MARL and single-LLM baselines.

</details>


### [11] [Prompt-Tuned LLM-Augmented DRL for Dynamic O-RAN Network Slicing](https://arxiv.org/abs/2506.00574)
*Fatemeh Lotfi,Hossein Rajoli,Fatemeh Afghah*

Main category: cs.LG

TL;DR: 本文提出了一种结合大型语言模型与深度强化学习的新方法PA-MRL，通过可学习提示优化状态表示，在O-RAN切片中实现了更高效和自适应的资源管理。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习在动态无线网络环境中因分散和不断变化的反馈而面临决策挑战，而大型语言模型能够将非结构化的网络反馈转化为有意义的潜在表示，从而改善状态表示并提高决策效率。

Method: 提出了一种基于情境化适应的方法，利用任务特定的可学习提示优化状态表示，结合大型语言模型（如ORANSight）来增强深度强化学习框架，从而避免完全微调模型。

Result: 所提出的Prompt-Augmented Multi-agent RL (PA-MRL) 框架在O-RAN切片场景下加速了收敛过程，提升了资源分配效率，并在性能上优于现有方法。

Conclusion: 通过将可学习提示集成到基于LLM的DRL框架中，本文提出的PA-MRL方法在O-RAN切片中实现了更快速、可扩展和自适应的资源分配，并且实验结果表明其收敛速度更快且优于其他基线方法。

Abstract: Modern wireless networks must adapt to dynamic conditions while efficiently
managing diverse service demands. Traditional deep reinforcement learning (DRL)
struggles in these environments, as scattered and evolving feedback makes
optimal decision-making challenging. Large Language Models (LLMs) offer a
solution by structuring unorganized network feedback into meaningful latent
representations, helping RL agents recognize patterns more effectively. For
example, in O-RAN slicing, concepts like SNR, power levels and throughput are
semantically related, and LLMs can naturally cluster them, providing a more
interpretable state representation. To leverage this capability, we introduce a
contextualization-based adaptation method that integrates learnable prompts
into an LLM-augmented DRL framework. Instead of relying on full model
fine-tuning, we refine state representations through task-specific prompts that
dynamically adjust to network conditions. Utilizing ORANSight, an LLM trained
on O-RAN knowledge, we develop Prompt-Augmented Multi agent RL (PA-MRL)
framework. Learnable prompts optimize both semantic clustering and RL
objectives, allowing RL agents to achieve higher rewards in fewer iterations
and adapt more efficiently. By incorporating prompt-augmented learning, our
approach enables faster, more scalable, and adaptive resource allocation in
O-RAN slicing. Experimental results show that it accelerates convergence and
outperforms other baselines.

</details>


### [12] [RLAE: Reinforcement Learning-Assisted Ensemble for LLMs](https://arxiv.org/abs/2506.00439)
*Yuqian Fu,Yuanheng Zhu,Jiajun Chai,Guojun Yin,Wei Lin,Qichao Zhang,Dongbin Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习的大语言模型集成新框架RLAE，能够根据输入上下文和生成状态动态调整模型权重，显著提升了模型性能与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）集成方法通常依赖于固定的加权策略，无法适应LLM能力的动态、上下文相关特性。因此需要一种能够动态调整模型集成权重的方法，以提升性能表现。

Method: 通过将集成过程建模为马尔可夫决策过程（MDP），引入一个强化学习代理（RL Agent）来动态调整模型权重，使用单智能体和多智能体强化学习算法（如PPO和MAPPO）实现RLAE。

Result: 在多种任务上的广泛评估表明，RLAE比现有方法提高了高达3.3%的准确率，且具有更好的泛化能力和更低的推理时间延迟。

Conclusion: RLAE方法不仅在不同任务上展现出更强的泛化能力，而且无需重新训练即可适应不同任务，同时实现了更低的时间延迟。这为LLM的集成提供了一个更有效的框架。

Abstract: Ensembling large language models (LLMs) can effectively combine diverse
strengths of different models, offering a promising approach to enhance
performance across various tasks. However, existing methods typically rely on
fixed weighting strategies that fail to adapt to the dynamic, context-dependent
characteristics of LLM capabilities. In this work, we propose Reinforcement
Learning-Assisted Ensemble for LLMs (RLAE), a novel framework that reformulates
LLM ensemble through the lens of a Markov Decision Process (MDP). Our approach
introduces a RL agent that dynamically adjusts ensemble weights by considering
both input context and intermediate generation states, with the agent being
trained using rewards that directly correspond to the quality of final outputs.
We implement RLAE using both single-agent and multi-agent reinforcement
learning algorithms ($\text{RLAE}_\text{PPO}$ and $\text{RLAE}_\text{MAPPO}$ ),
demonstrating substantial improvements over conventional ensemble methods.
Extensive evaluations on a diverse set of tasks show that RLAE outperforms
existing approaches by up to $3.3\%$ accuracy points, offering a more effective
framework for LLM ensembling. Furthermore, our method exhibits superior
generalization capabilities across different tasks without the need for
retraining, while simultaneously achieving lower time latency.

</details>


### [13] [Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning](https://arxiv.org/abs/2505.24850)
*Shuyao Xu,Cheng Peng,Jiangxuan Long,Weidi Xu,Wei Chu,Yuan Qi*

Main category: cs.LG

TL;DR: 文章提出了一种新的模型蒸馏方法RE-DI，通过充分利用正向和负向推理数据显著提升小型模型在数学推理任务上的性能，实验显示其效果达到甚至超越了使用更多专有数据训练的模型。


<details>
  <summary>Details</summary>
Motivation: 现有模型蒸馏方法通常采用拒绝采样，丢弃错误的推理示例，但这些数据其实包含有价值的信息。如何有效利用这些正向和负向的推理数据来最大化模型在离线设置下的推理能力是一个关键问题。

Method: 文章提出了一种名为Reinforcement Distillation (RE-DI) 的两阶段框架。第一阶段通过监督式微调(SFT)学习正向推理示例；第二阶段则通过一种新的无参考损失函数RE-DI目标，同时利用正向和负向推理示例进一步优化模型。

Result: 实验结果显示，RE-DI优于传统的拒绝采样SFT以及结合DPO/SimPO的方法。Qwen-RE-DI-1.5B模型仅使用来自Open-R1数据集的131k个正向和负向示例进行微调，在MATH-500测试中获得了83.1%的成绩，并在多个数学推理基准测试中表现优异。

Conclusion: 文章提出了一种新的两阶段框架RE-DI，通过有效利用正向和负向推理数据提升小型模型在数学推理任务中的表现。实验结果表明，使用RE-DI训练的Qwen-RE-DI-1.5B模型在MATH-500测试中取得了83.1%的成绩，达到了与更大规模模型相当或更优的表现，成为基于公开数据离线微调1.5B模型的新标杆。

Abstract: Recent advances in model distillation demonstrate that data from advanced
reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer
complex reasoning abilities to smaller, efficient student models. However,
standard practices employ rejection sampling, discarding incorrect reasoning
examples -- valuable, yet often underutilized data. This paper addresses the
critical question: How can both positive and negative distilled reasoning
traces be effectively leveraged to maximize LLM reasoning performance in an
offline setting? To this end, We propose Reinforcement Distillation (REDI), a
two-stage framework. Stage 1 learns from positive traces via Supervised
Fine-Tuning (SFT). Stage 2 further refines the model using both positive and
negative traces through our proposed REDI objective. This novel objective is a
simple, reference-free loss function that outperforms established methods like
DPO and SimPO in this distillation context. Our empirical evaluations
demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT
combined with DPO/SimPO on mathematical reasoning tasks. Notably, the
Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples
from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).
Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a
model post-trained on 800k proprietary data) across various mathematical
reasoning benchmarks, establishing a new state-of-the-art for 1.5B models
post-trained offline with openly available data.

</details>


### [14] [Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting](https://arxiv.org/abs/2505.24710)
*Wei Chen,Jiahao Zhang,Haipeng Zhu,Boyan Xu,Zhifeng Hao,Keli Zhang,Junjian Ye,Ruichu Cai*

Main category: cs.LG

TL;DR: 本文提出了 Causal-aware LLMs，通过将结构因果模型引入决策过程，提高了大型语言模型的理解与决策能力，并在多个任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于预训练的大型语言模型在推理能力和适应新环境方面存在不足，限制了其在复杂现实任务中的应用。因此，需要一种能够建模、更新和利用环境结构化知识的方法来提升模型的决策能力。

Method: 提出了一种名为 Causal-aware LLMs 的方法，结合了大型语言模型和结构因果模型，在「学习-适应-行动」范式下进行决策。具体包括：在学习阶段利用 LLM 提取环境的因果关系；在适应阶段通过外部反馈更新因果模型；在行动阶段使用强化学习代理基于因果知识制定策略。

Result: 在开放世界游戏 Crafter 中的 22 个不同任务上验证了所提方法的有效性，实验结果表明 Causal-aware LLMs 在理解和决策方面具有更好的性能。

Conclusion: 通过整合结构因果模型（SCM）到决策过程中，Causal-aware LLMs 能够更准确地理解环境并做出更高效的决策，实验结果验证了该方法的有效性。

Abstract: Large language models (LLMs) have shown great potential in decision-making
due to the vast amount of knowledge stored within the models. However, these
pre-trained models are prone to lack reasoning abilities and are difficult to
adapt to new environments, further hindering their application to complex
real-world tasks. To address these challenges, inspired by the human cognitive
process, we propose Causal-aware LLMs, which integrate the structural causal
model (SCM) into the decision-making process to model, update, and utilize
structured knowledge of the environment in a ``learning-adapting-acting"
paradigm. Specifically, in the learning stage, we first utilize an LLM to
extract the environment-specific causal entities and their causal relations to
initialize a structured causal model of the environment. Subsequently,in the
adapting stage, we update the structured causal model through external feedback
about the environment, via an idea of causal intervention. Finally, in the
acting stage, Causal-aware LLMs exploit structured causal knowledge for more
efficient policy-making through the reinforcement learning agent. The above
processes are performed iteratively to learn causal knowledge, ultimately
enabling the causal-aware LLMs to achieve a more accurate understanding of the
environment and make more efficient decisions. Experimental results across 22
diverse tasks within the open-world game ``Crafter" validate the effectiveness
of our proposed method.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 本文探讨了通过重复抽样提升大型语言模型（LLMs）推理覆盖率的现象，并提出一种基于训练集答案频率的基线方法。研究表明，这种基线在某些模型上优于重复抽样，或与结合少量抽样与枚举猜测的混合策略表现相当，从而揭示了重复抽样对覆盖率提升的实际影响。


<details>
  <summary>Details</summary>
Motivation: 文章的动机源于观察到通过增加大型语言模型（LLMs）的推理计算量（例如重复抽样），可以提高问题解决的覆盖率（即解决问题的比例）。然而，这种改进的原因尚不明确，尤其是考虑到标准评估基准的答案分布可能偏向于少数常见答案。因此，作者希望探索是否存在一种更有效的方式来衡量和提升覆盖率，而不仅仅是依赖于增加采样次数。

Method: 文章的方法包括构建一个基于训练集中答案频率的基线策略，并将其与传统的重复抽样方法进行比较。实验涵盖了数学推理和事实知识两个领域，使用不同大型语言模型（LLMs）进行测试。此外，还引入了一种混合策略，即通过少量的真实模型抽样（如10次）并结合枚举猜测剩余答案的方式，来模拟重复抽样的效果。

Result: 文章的结果显示，在某些大型语言模型（LLMs）上，基于训练集答案频率的基线策略优于传统的重复抽样方法。而在其他模型上，该基线的表现与一种混合策略相当：即仅使用少量（如10次）模型抽样，其余答案通过枚举猜测得到。这表明，在一些情况下，重复抽样带来的覆盖率提升可能并不显著优于简单的猜测策略，尤其是在答案分布高度偏斜的情况下。

Conclusion: 文章的结论指出，重复抽样带来的覆盖率提升部分归因于标准评估基准的答案分布偏向于少量常见答案。通过定义一个基于训练集中答案出现频率的基线方法，实验表明该基线在某些大型语言模型（LLMs）上优于重复抽样方法，而在其他模型上，与结合少量模型抽样和枚举猜测的混合策略效果相当。这表明该基线方法能够更准确地衡量重复抽样在提升覆盖率方面的实际贡献，排除了与提示无关的猜测因素。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [16] [A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains](https://arxiv.org/abs/2402.00559)
*Alon Jacovi,Yonatan Bitton,Bernd Bohnet,Jonathan Herzig,Or Honovich,Michael Tseng,Michael Collins,Roee Aharoni,Mor Geva*

Main category: cs.CL

TL;DR: 本文提出了REVEAL数据集，旨在帮助评估和改进开放领域问答任务中复杂思维链推理的自动验证方法。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏细粒度的步骤级数据集来全面评估自动验证推理链的方法，特别是在验证逻辑正确性和检测矛盾方面存在困难。

Method: 作者引入了一个新的数据集REVEAL，其中包含对语言模型答案中每一步推理的相关性、证据归属和逻辑正确性的全面标签。这些标签是在多个数据集和最先进的语言模型上生成的。

Result: 在REVEAL上的评估显示，现有的验证器在验证推理链时表现不佳，尤其是在验证逻辑正确性和检测矛盾方面。

Conclusion: 本文介绍了REVEAL数据集，该数据集用于评估开放领域问答设置中复杂思维链推理的自动验证器。通过这个数据集，研究人员可以更好地评估和改进自动验证方法在逻辑正确性和矛盾检测方面的能力。

Abstract: Prompting language models to provide step-by-step answers (e.g.,
"Chain-of-Thought") is the prominent approach for complex reasoning tasks,
where more accurate reasoning chains typically improve downstream task
performance. Recent literature discusses automatic methods to verify reasoning
to evaluate and improve their correctness. However, no fine-grained step-level
datasets are available to enable thorough evaluation of such verification
methods, hindering progress in this direction. We introduce REVEAL: Reasoning
Verification Evaluation, a dataset to benchmark automatic verifiers of complex
Chain-of-Thought reasoning in open-domain question-answering settings. REVEAL
includes comprehensive labels for the relevance, attribution to evidence
passages, and logical correctness of each reasoning step in a language model's
answer, across a variety of datasets and state-of-the-art language models.
Evaluation on REVEAL shows that verifiers struggle at verifying reasoning
chains - in particular, verifying logical correctness and detecting
contradictions. Available at https://reveal-dataset.github.io/ .

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [17] [Casper DPM: Cascaded Perceptual Dynamic Projection Mapping onto Hands](https://arxiv.org/abs/2409.04397)
*Yotam Erel,Or Kozlovsky-Mordenfeld,Daisuke Iwai,Kosuke Sato,Amit H. Bermano*

Main category: cs.GR

TL;DR: 本文介绍了一种新的动态投影技术，通过结合3D粗略估计和高速2D修正，显著改善了3D内容在人手上的投影效果和用户体验。


<details>
  <summary>Details</summary>
Motivation: 由于人手具有复杂的关节和可变形特性，准确且快速地计算其姿态和形状是一项挑战。为了解决这个问题，研究旨在开发一种高效且精确的方法来改进3D内容在人手上的投影体验。

Method: 研究提出了一种动态将3D内容投射到人手上的技术，该方法结合了较慢的3D手势粗略估计与快速的2D修正步骤，以提高投影对齐精度、增加投影表面积，并减少感知延迟。

Result: 通过用户实验验证了所提方法的优势，结果表明用户对延迟伪影的敏感性降低，并且相比直接使用3D姿态估计渲染帧的传统方法，用户能够更快、更轻松地完成相关任务。

Conclusion: 文章的结论是，通过结合较慢的手势3D粗略估计与高速2D修正步骤，可以有效减少投影延迟感并提升用户完成任务的速度和舒适度。这种方法还支持任意纹理或效果的应用，从而实现了之前无法实现的新用途和应用领域。

Abstract: We present a technique for dynamically projecting 3D content onto human hands
with short perceived motion-to-photon latency. Computing the pose and shape of
human hands accurately and quickly is a challenging task due to their
articulated and deformable nature. We combine a slower 3D coarse estimation of
the hand pose with high speed 2D correction steps which improve the alignment
of the projection to the hands, increase the projected surface area, and reduce
perceived latency. Since our approach leverages a full 3D reconstruction of the
hands, any arbitrary texture or reasonably performant effect can be applied,
which was not possible before. We conducted two user studies to assess the
benefits of using our method. The results show subjects are less sensitive to
latency artifacts and perform faster and with more ease a given associated task
over the naive approach of directly projecting rendered frames from the 3D pose
estimation. We demonstrate several novel use cases and applications.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [18] [Prices and preferences in the electric vehicle market](https://arxiv.org/abs/2403.00458)
*Chung Yi See,Vasco Rato Santos,Lucas Woodley,Megan Yeo,Daniel Palmer,Shuheng Zhang,and Ashley Nunes*

Main category: econ.EM

TL;DR: This paper challenges the belief that high EV prices are mainly due to battery costs, showing instead that consumer preferences for feature-rich and powerful vehicles play a major role, which also reduces the expected environmental benefits of EVs.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to challenge the prevailing discourse that battery costs are the main driver of high EV prices and to explore how consumer preferences and vehicle features impact EV pricing, fuel economy, and emissions.

Method: The authors analyzed data on EV attributes and market conditions from 2011 to 2023 to examine the factors influencing EV pricing and their implications on range, power, and emissions.

Result: Four key findings were identified: (1) EV prices are mainly influenced by standard amenities and features, not just battery costs; (2) EV range is negatively correlated with price, suggesting range anxiety may be less of a concern; (3) battery capacity positively affects price due to its link with horsepower; and (4) consumer preferences for powerful and feature-rich EVs have reduced potential emissions benefits by at least 3.26 percent.

Conclusion: The paper concludes that the higher procurement prices of electric vehicles (EVs) are primarily due to consumer preferences for feature-dense and more powerful vehicles, rather than battery costs alone. Additionally, these preferences have led to lower fuel economy, reducing the expected lifecycle emissions benefits of EVs by at least 3.26 percent, depending on battery chemistry and grid carbon intensity.

Abstract: Although electric vehicles are less polluting than gasoline powered vehicles,
adoption is challenged by higher procurement prices. Existing discourse
emphasizes EV battery costs as being principally responsible for this price
differential and widespread adoption is routinely conditioned upon battery
costs declining. We scrutinize such reasoning by sourcing data on EV attributes
and market conditions between 2011 and 2023. Our findings are fourfold. First,
EV prices are influenced principally by the number of amenities, additional
features, and dealer-installed accessories sold as standard on an EV, and to a
lesser extent, by EV horsepower. Second, EV range is negatively correlated with
EV price implying that range anxiety concerns may be less consequential than
existing discourse suggests. Third, battery capacity is positively correlated
with EV price, due to more capacity being synonymous with the delivery of more
horsepower. Collectively, this suggests that higher procurement prices for EVs
reflects consumer preference for vehicles that are feature dense and more
powerful. Fourth and finally, accommodating these preferences have produced
vehicles with lower fuel economy, a shift that reduces envisioned lifecycle
emissions benefits by at least 3.26 percent, subject to the battery pack
chemistry leveraged and the carbon intensity of the electrical grid. These
findings warrant attention as decarbonization efforts increasingly emphasize
electrification as a pathway for complying with domestic and international
climate agreements.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [19] [Accelerating ab initio melting property calculations with machine learning: Application to the high entropy alloy TaVCrW](https://arxiv.org/abs/2408.08654)
*Li-Fang Zhu,Fritz Koermann,Qing Chen,Malin Selleby,Joerg Neugebauer,and Blazej Grabowski*

Main category: cond-mat.mtrl-sci

TL;DR: 文章介绍了一种高效计算材料熔化性质的新方法，结合了密度泛函理论和机器学习潜力，显著降低了计算成本，并成功应用于高熵合金TaVCrW的研究。


<details>
  <summary>Details</summary>
Motivation: 材料设计中，特别是发现高性能、高熔点难熔材料时，熔化性质至关重要。然而，由于这些材料的熔点很高，实验测量非常具有挑战性，因此需要互补的理论预测方法。传统基于密度泛函理论（DFT）的自由能方法虽然准确但计算成本高昂，限制了其在高通量计算中的应用。

Method: 本文提出了一种高效的基于密度泛函理论（DFT）的方法，结合了专门设计的机器学习势函数。通过使用机器学习势函数替代传统的从头算分子动力学模拟，将昂贵的热力学积分替换为更高效的自由能微扰计算。

Result: 该方法相比现有替代方案，在计算资源上总体节省了80%。它被成功应用于高熵合金TaVCrW的熔化性质计算，包括熔点、熔融熵和焓变等关键参数。此外，还计算了固态和液态TaVCrW的热容，结果与calphad外推值合理一致。

Conclusion: 综上所述，这种方法在计算资源上实现了显著节省，并成功应用于高熵合金TaVCrW的熔化性质计算，包括熔点、熔融熵和焓变等关键参数，结果与calphad外推值合理一致。此外，还计算了固态和液态TaVCrW的热容。

Abstract: Melting properties are critical for designing novel materials, especially for
discovering high-performance, high-melting refractory materials. Experimental
measurements of these properties are extremely challenging due to their high
melting temperatures. Complementary theoretical predictions are, therefore,
indispensable. The conventional free energy approach using density functional
theory (DFT) has been a gold standard for such purposes because of its high
accuracy. However,it generally involves expensive thermodynamic integration
using ab initio molecular dynamic simulations. The high computational cost
makes high-throughput calculations infeasible. Here, we propose a highly
efficient DFT-based method aided by a specially designed machine learning
potential. As the machine learning potential can closely reproduce the ab
initio phase space, even for multi-component alloys, the costly thermodynamic
integration can be fully substituted with more efficient free energy
perturbation calculations. The method achieves overall savings of computational
resources by 80% compared to current alternatives. We apply the method to the
high-entropy alloy TaVCrW and calculate its melting properties, including
melting temperature, entropy and enthalpy of fusion, and volume change at the
melting point. Additionally, the heat capacities of solid and liquid TaVCrW are
calculated. The results agree reasonably with the calphad extrapolated values.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [20] [Model-Based AI planning and Execution Systems for Robotics](https://arxiv.org/abs/2505.04493)
*Or Wertheim,Ronen I. Brafman*

Main category: cs.RO

TL;DR: 该论文综述了基于模型的机器人任务级控制系统的发展，分析了现有设计及其挑战，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管基于模型的规划与执行系统理念历史悠久，但结合现代机器人平台的通用系统近期才开始发展，因此需要总结现有研究并指出未来方向。

Method: 本文通过回顾和分析现有文献，探讨不同系统设计尝试解决的问题及提出的解决方案。

Result: 文中介绍了多种基于模型的机器人控制系统的不同设计方法，并讨论了它们的优势和挑战。

Conclusion: 文章总结了现有的基于模型的机器人任务级控制系统的设计选择和问题，并提出了未来发展的方向。

Abstract: Model-based planning and execution systems offer a principled approach to
building flexible autonomous robots that can perform diverse tasks by
automatically combining a host of basic skills. This idea is almost as old as
modern robotics. Yet, while diverse general-purpose reasoning architectures
have been proposed since, general-purpose systems that are integrated with
modern robotic platforms have emerged only recently, starting with the
influential ROSPlan system. Since then, a growing number of model-based systems
for robot task-level control have emerged. In this paper, we consider the
diverse design choices and issues existing systems attempt to address, the
different solutions proposed so far, and suggest avenues for future
development.

</details>


### [21] [Hardware Design and Learning-Based Software Architecture of Musculoskeletal Wheeled Robot Musashi-W for Real-World Applications](https://arxiv.org/abs/2403.11729)
*Kento Kawaharazuka,Akihiro Miki,Masahiro Bando,Temma Suzuki,Yoshimoto Ribayashi,Yasunori Toshimitsu,Yuya Nagamatsu,Kei Okada,and Masayuki Inaba*

Main category: cs.RO

TL;DR: 本文介绍了一种新型肌肉骨骼轮式机器人Musashi-W及其软件系统，旨在解决现有仿人机器人在现实应用中的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的肌肉骨骼仿人机器人具有模仿人体灵活和冗余的优势，但在现实世界任务中的应用仍然困难，主要由于双足行走的挑战。

Method: 开发了一个名为Musashi-W的肌肉骨骼轮式机器人，并构建了其软件系统以实现高效的物理任务处理。

Result: Musashi-W成功完成了多种任务，包括清洁、搬运重物和动态布料操作等，展示了其硬件和软件系统的有效性。

Conclusion: Musashi-W通过结合轮式基础和肌肉骨骼上肢以及整合静态和动态身体模式学习、反射控制和视觉识别的软件系统，展示了其在现实世界任务中的潜力。

Abstract: Various musculoskeletal humanoids have been developed so far. While these
humanoids have the advantage of their flexible and redundant bodies that mimic
the human body, they are still far from being applied to real-world tasks. One
of the reasons for this is the difficulty of bipedal walking in a flexible
body. Thus, we developed a musculoskeletal wheeled robot, Musashi-W, by
combining a wheeled base and musculoskeletal upper limbs for real-world
applications. Also, we constructed its software system by combining static and
dynamic body schema learning, reflex control, and visual recognition. We show
that the hardware and software of Musashi-W can make the most of the advantages
of the musculoskeletal upper limbs, through several tasks of cleaning by human
teaching, carrying a heavy object considering muscle addition, and setting a
table through dynamic cloth manipulation with variable stiffness.

</details>
