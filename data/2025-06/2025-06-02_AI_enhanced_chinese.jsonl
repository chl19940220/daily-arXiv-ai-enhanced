{"id": "2402.00559", "keyword": "Chain of Thoughts", "pdf": "https://arxiv.org/pdf/2402.00559", "abs": "https://arxiv.org/abs/2402.00559", "authors": ["Alon Jacovi", "Yonatan Bitton", "Bernd Bohnet", "Jonathan Herzig", "Or Honovich", "Michael Tseng", "Michael Collins", "Roee Aharoni", "Mor Geva"], "title": "A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains", "categories": ["cs.CL"], "comment": "Accepted to ACL 2024", "summary": "Prompting language models to provide step-by-step answers (e.g.,\n\"Chain-of-Thought\") is the prominent approach for complex reasoning tasks,\nwhere more accurate reasoning chains typically improve downstream task\nperformance. Recent literature discusses automatic methods to verify reasoning\nto evaluate and improve their correctness. However, no fine-grained step-level\ndatasets are available to enable thorough evaluation of such verification\nmethods, hindering progress in this direction. We introduce REVEAL: Reasoning\nVerification Evaluation, a dataset to benchmark automatic verifiers of complex\nChain-of-Thought reasoning in open-domain question-answering settings. REVEAL\nincludes comprehensive labels for the relevance, attribution to evidence\npassages, and logical correctness of each reasoning step in a language model's\nanswer, across a variety of datasets and state-of-the-art language models.\nEvaluation on REVEAL shows that verifiers struggle at verifying reasoning\nchains - in particular, verifying logical correctness and detecting\ncontradictions. Available at https://reveal-dataset.github.io/ .", "keywords": ["Chain of Thoughts", "Reasoning"], "AI": {"tldr": "This paper introduces REVEAL, a new dataset for evaluating automatic verification of complex reasoning in language models, highlighting current limitations in verifying logical correctness and contradiction detection.", "motivation": "The motivation stems from the lack of fine-grained step-level datasets necessary for thoroughly evaluating automatic verification methods for reasoning chains. The absence of such datasets hinders progress in improving the correctness of reasoning in language models.", "method": "The authors introduce REVEAL, a dataset designed to evaluate automatic verifiers of Chain-of-Thought reasoning by providing fine-grained step-level labels for relevance, attribution, and logical correctness. This dataset is built across multiple datasets and state-of-the-art language models.", "result": "REVEAL provides comprehensive labels for each reasoning step, enabling detailed evaluation of automatic verifiers. Evaluation using REVEAL reveals that current verifiers struggle significantly with verifying logical correctness and detecting contradictions in reasoning chains.", "conclusion": "The paper concludes that automatic verifiers face significant challenges in verifying reasoning chains, particularly in assessing logical correctness and identifying contradictions within complex Chain-of-Thought reasoning."}}
{"id": "2410.15466", "keyword": "LLM reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing.", "keywords": ["LLM reasoning", "Reasoning"], "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u91cd\u590d\u91c7\u6837\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u95ee\u9898\u89e3\u51b3\u8986\u76d6\u7387\u7684\u73b0\u8c61\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b54\u6848\u9891\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u5176\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4f20\u7edf\u91c7\u6837\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u8005\u89c2\u5bdf\u5230\uff0c\u968f\u7740\u6837\u672c\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u95ee\u9898\u65f6\u7684\u8986\u76d6\u7387\u4e5f\u968f\u4e4b\u4e0a\u5347\u3002\u4ed6\u4eec\u63a8\u6d4b\u8fd9\u79cd\u73b0\u8c61\u90e8\u5206\u5f52\u56e0\u4e8e\u6807\u51c6\u8bc4\u4f30\u57fa\u51c6\u7684\u7b54\u6848\u5206\u5e03\u504f\u5411\u4e8e\u5c11\u6570\u5e38\u89c1\u7b54\u6848\u3002\u56e0\u6b64\uff0c\u4ed6\u4eec\u5e0c\u671b\u63a2\u7d22\u662f\u5426\u5b58\u5728\u4e00\u79cd\u66f4\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6cd5\u6765\u8861\u91cf\u548c\u63d0\u5347\u6a21\u578b\u7684\u8986\u76d6\u7387\u3002", "method": "\u6587\u7ae0\u7684\u65b9\u6cd5\u662f\u5b9a\u4e49\u4e00\u4e2a\u57fa\u7ebf\uff0c\u6839\u636e\u8bad\u7ec3\u96c6\u4e2d\u7b54\u6848\u7684\u9891\u7387\u8fdb\u884c\u679a\u4e3e\uff0c\u5e76\u5728\u4e24\u4e2a\u9886\u57df\uff08\u6570\u5b66\u63a8\u7406\u548c\u4e8b\u5b9e\u77e5\u8bc6\uff09\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u3002\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u8be5\u57fa\u7ebf\u4e0e\u91cd\u590d\u6a21\u578b\u91c7\u6837\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6d4b\u8bd5\u4e86\u4e00\u79cd\u6df7\u5408\u7b56\u7565\uff0c\u5373\u901a\u8fc7\u5c11\u91cf\u6a21\u578b\u91c7\u6837\u7ed3\u5408\u679a\u4e3e\u731c\u6d4b\u5269\u4f59\u7b54\u6848\u7684\u65b9\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u63d0\u51fa\u7684\u57fa\u7ebf\u65b9\u6cd5\u5728\u67d0\u4e9b\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u4f18\u4e8e\u91cd\u590d\u91c7\u6837\u65b9\u6cd5\uff0c\u800c\u5bf9\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5219\u4e0e\u6df7\u5408\u7b56\u7565\u8868\u73b0\u76f8\u5f53\u3002\u6df7\u5408\u7b56\u7565\u4ec5\u4f7f\u7528\u5c11\u91cf\u6a21\u578b\u6837\u672c\uff08\u598210\u6b21\uff09\uff0c\u5176\u4f59\u7b54\u6848\u901a\u8fc7\u679a\u4e3e\u731c\u6d4b\u83b7\u5f97\uff0c\u4ecd\u7136\u80fd\u591f\u8fbe\u5230\u76f8\u4f3c\u7684\u8986\u76d6\u7387\u3002\u8fd9\u8868\u660e\u91cd\u590d\u91c7\u6837\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u7684\u4f18\u52bf\u53ef\u80fd\u88ab\u9ad8\u4f30\u4e86\u3002", "conclusion": "\u6587\u7ae0\u7684\u7ed3\u8bba\u662f\uff0c\u901a\u8fc7\u91cd\u590d\u91c7\u6837\u6765\u6269\u5c55\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u8ba1\u7b97\u53ef\u4ee5\u63d0\u9ad8\u95ee\u9898\u89e3\u51b3\u7684\u8986\u76d6\u7387\uff0c\u4f46\u8fd9\u79cd\u63d0\u5347\u90e8\u5206\u4f9d\u8d56\u4e8e\u8bc4\u4f30\u57fa\u51c6\u7b54\u6848\u5206\u5e03\u7684\u504f\u5dee\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bad\u7ec3\u96c6\u4e2d\u7b54\u6848\u51fa\u73b0\u9891\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u8be5\u65b9\u6cd5\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8e\u91cd\u590d\u91c7\u6837\uff0c\u800c\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u5219\u4e0e\u6df7\u5408\u7b56\u7565\u8868\u73b0\u76f8\u5f53\u3002\u8fd9\u8868\u660e\u5728\u8861\u91cf\u91cd\u590d\u91c7\u6837\u5bf9\u8986\u76d6\u7387\u7684\u63d0\u5347\u65f6\uff0c\u9700\u8981\u8003\u8651\u63d0\u793a\u65e0\u5173\u7684\u731c\u6d4b\u56e0\u7d20\u3002"}}
{"id": "2505.00018", "keyword": "LLM Agent", "pdf": "https://arxiv.org/pdf/2505.00018", "abs": "https://arxiv.org/abs/2505.00018", "authors": ["Ju Wu", "Calvin K. L. Or"], "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability.", "keywords": ["LLM Agent"], "AI": {"tldr": "This position paper critically surveys recent empirical developments in human-AI agent collaboration and proposes a new conceptual architecture to bridge persistent gaps.", "motivation": "We observe a lack of a unifying theoretical framework that can coherently integrate these varied studies, especially when tackling open-ended, complex tasks.", "method": "We propose a novel conceptual architecture: one that systematically interlinks the technical details of multi-agent coordination, knowledge management, cybernetic feedback loops, and higher-level control mechanisms.", "result": "The paper's structure allows it to be read from any section, serving equally as a critical review of technical implementations and as a forward-looking reference for designing or extending human-AI symbioses.", "conclusion": "Together, these insights offer a stepping stone toward deeper co-evolution of human cognition and AI capability."}}
{"id": "2505.04493", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2505.04493", "abs": "https://arxiv.org/abs/2505.04493", "authors": ["Or Wertheim", "Ronen I. Brafman"], "title": "Model-Based AI planning and Execution Systems for Robotics", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Model-based planning and execution systems offer a principled approach to\nbuilding flexible autonomous robots that can perform diverse tasks by\nautomatically combining a host of basic skills. This idea is almost as old as\nmodern robotics. Yet, while diverse general-purpose reasoning architectures\nhave been proposed since, general-purpose systems that are integrated with\nmodern robotic platforms have emerged only recently, starting with the\ninfluential ROSPlan system. Since then, a growing number of model-based systems\nfor robot task-level control have emerged. In this paper, we consider the\ndiverse design choices and issues existing systems attempt to address, the\ndifferent solutions proposed so far, and suggest avenues for future\ndevelopment.", "keywords": ["Reasoning"], "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u57fa\u4e8e\u6a21\u578b\u7684\u673a\u5668\u4eba\u4efb\u52a1\u7ea7\u63a7\u5236\u7cfb\u7edf\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5305\u62ec\u8bbe\u8ba1\u9009\u62e9\u3001\u89e3\u51b3\u65b9\u6848\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u548c\u6267\u884c\u7cfb\u7edf\u5728\u673a\u5668\u4eba\u9886\u57df\u5df2\u6709\u8f83\u957f\u5386\u53f2\uff0c\u4f46\u4e0e\u73b0\u4ee3\u673a\u5668\u4eba\u5e73\u53f0\u96c6\u6210\u7684\u901a\u7528\u7cfb\u7edf\u76f4\u5230\u6700\u8fd1\u624d\u5f00\u59cb\u51fa\u73b0\uff0c\u8fd9\u6fc0\u53d1\u4e86\u5bf9\u8fd9\u4e00\u9886\u57df\u7684\u6df1\u5165\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u56de\u987e\u548c\u5206\u6790\u73b0\u6709\u6587\u732e\u4e2d\u7684\u65b9\u6cd5\u548c\u6280\u672f\uff0c\u63a2\u8ba8\u4e0d\u540c\u8bbe\u8ba1\u9009\u62e9\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86\u591a\u79cd\u4e0d\u540c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b83\u4eec\u5982\u4f55\u89e3\u51b3\u8bbe\u8ba1\u95ee\u9898\uff0c\u4ee5\u53ca\u5b83\u4eec\u5404\u81ea\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u73b0\u6709\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u673a\u5668\u4eba\u4efb\u52a1\u7ea7\u63a7\u5236\u7cfb\u7edf\u7684\u8bbe\u8ba1\u9009\u62e9\u548c\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u7684\u65b9\u5411\u3002"}}
{"id": "2412.02441", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2412.02441", "abs": "https://arxiv.org/abs/2412.02441", "authors": ["Shai Shalev-Shwartz", "Amnon Shashua", "Gal Beniamini", "Yoav Levine", "Or Sharir", "Noam Wies", "Ido Ben-Shaul", "Tomer Nussbaum", "Shir Granot Peled"], "title": "Artificial Expert Intelligence through PAC-reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Artificial Expert Intelligence (AEI) seeks to transcend the limitations of\nboth Artificial General Intelligence (AGI) and narrow AI by integrating\ndomain-specific expertise with critical, precise reasoning capabilities akin to\nthose of top human experts. Existing AI systems often excel at predefined tasks\nbut struggle with adaptability and precision in novel problem-solving. To\novercome this, AEI introduces a framework for ``Probably Approximately Correct\n(PAC) Reasoning\". This paradigm provides robust theoretical guarantees for\nreliably decomposing complex problems, with a practical mechanism for\ncontrolling reasoning precision. In reference to the division of human thought\ninto System 1 for intuitive thinking and System 2 for reflective\nreasoning~\\citep{tversky1974judgment}, we refer to this new type of reasoning\nas System 3 for precise reasoning, inspired by the rigor of the scientific\nmethod. AEI thus establishes a foundation for error-bounded, inference-time\nlearning.", "keywords": ["Reasoning"], "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Artificial Expert Intelligence (AEI)\uff0c\u5b83\u7ed3\u5408\u4e86\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4e0e\u7c7b\u4f3c\u9876\u7ea7\u4eba\u7c7b\u4e13\u5bb6\u7684\u7cbe\u786e\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u6846\u67b6System 3\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709AI\u5728\u9002\u5e94\u6027\u548c\u7cbe\u786e\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684AI\u7cfb\u7edf\u867d\u7136\u5728\u9884\u5b9a\u4e49\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9002\u5e94\u6027\u548c\u89e3\u51b3\u65b0\u9896\u95ee\u9898\u7684\u7cbe\u786e\u6027\u65b9\u9762\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u8d85\u8d8aAGI\u548c\u72ed\u4e49AI\u9650\u5236\u7684\u65b0\u65b9\u6cd5\uff0c\u5373Artificial Expert Intelligence\uff08AEI\uff09\u3002", "method": "AEI\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a\u201cProbably Approximately Correct (PAC) Reasoning\u201d\u7684\u65b0\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u53c2\u8003\u4e86\u4eba\u7c7b\u601d\u7ef4\u4e2dSystem 1\u548cSystem 2\u7684\u533a\u5206\uff0c\u5f3a\u8c03\u7cbe\u786e\u63a8\u7406\u7c7b\u4f3c\u4e8e\u79d1\u5b66\u65b9\u6cd5\u7684\u4e25\u8c28\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u65b0\u7684\u63a8\u7406\u6846\u67b6\u2014\u2014System 3\uff0c\u5176\u57fa\u4e8e\u4e25\u683c\u7684\u79d1\u5b66\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u9760\u7684\u95ee\u9898\u5206\u89e3\u5e76\u63a7\u5236\u63a8\u7406\u7cbe\u5ea6\uff0c\u4ece\u800c\u63a8\u52a8\u8bef\u5dee\u6709\u754c\u7684\u63a8\u7406\u65f6\u95f4\u5b66\u4e60\u7684\u53d1\u5c55\u3002", "conclusion": "AEI\u901a\u8fc7\u7ed3\u5408\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4e0e\u7cbe\u786e\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u95ee\u9898\u6c42\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u6846\u67b6\uff0c\u5e76\u4e14\u5728\u7406\u8bba\u4e0a\u4fdd\u8bc1\u4e86\u5206\u89e3\u95ee\u9898\u7684\u53ef\u9760\u6027\u4ee5\u53ca\u63a7\u5236\u63a8\u7406\u7cbe\u5ea6\u7684\u673a\u5236\u3002\u8fd9\u79cd\u7cfb\u7edf\u88ab\u79f0\u4f5cSystem 3\uff0c\u5b83\u4e3a\u8bef\u5dee\u6709\u754c\u7684\u63a8\u7406\u65f6\u95f4\u5b66\u4e60\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2409.04397", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2409.04397", "abs": "https://arxiv.org/abs/2409.04397", "authors": ["Yotam Erel", "Or Kozlovsky-Mordenfeld", "Daisuke Iwai", "Kosuke Sato", "Amit H. Bermano"], "title": "Casper DPM: Cascaded Perceptual Dynamic Projection Mapping onto Hands", "categories": ["cs.GR"], "comment": "Project page: https://yoterel.github.io/casper-project-page/", "summary": "We present a technique for dynamically projecting 3D content onto human hands\nwith short perceived motion-to-photon latency. Computing the pose and shape of\nhuman hands accurately and quickly is a challenging task due to their\narticulated and deformable nature. We combine a slower 3D coarse estimation of\nthe hand pose with high speed 2D correction steps which improve the alignment\nof the projection to the hands, increase the projected surface area, and reduce\nperceived latency. Since our approach leverages a full 3D reconstruction of the\nhands, any arbitrary texture or reasonably performant effect can be applied,\nwhich was not possible before. We conducted two user studies to assess the\nbenefits of using our method. The results show subjects are less sensitive to\nlatency artifacts and perform faster and with more ease a given associated task\nover the naive approach of directly projecting rendered frames from the 3D pose\nestimation. We demonstrate several novel use cases and applications.", "keywords": ["Reasoning"], "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6280\u672f\uff0c\u901a\u8fc7\u7ed3\u54083D\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u548c2D\u6821\u6b63\u6b65\u9aa4\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u7cbe\u5ea6\u7684\u52a8\u6001\u6295\u5f71\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u52a8\u673a\u662f\u4e3a\u4e86\u514b\u670d\u51c6\u786e\u4e14\u5feb\u901f\u8ba1\u7b97\u4eba\u4f53\u624b\u90e8\u59ff\u6001\u548c\u5f62\u72b6\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u624b\u90e8\u5177\u6709\u9ad8\u5ea6\u5173\u8282\u5316\u548c\u53ef\u53d8\u5f62\u7684\u7279\u70b9\uff0c\u540c\u65f6\u51cf\u5c11\u611f\u77e5\u5230\u7684\u8fd0\u52a8\u5230\u5149\u5b50\u5ef6\u8fdf\u3002", "method": "\u6587\u7ae0\u7684\u65b9\u6cd5\u662f\u7ed3\u5408\u4e00\u4e2a\u8f83\u6162\u76843D\u624b\u90e8\u59ff\u6001\u7c97\u7565\u4f30\u8ba1\u4e0e\u9ad8\u901f\u76842D\u6821\u6b63\u6b65\u9aa4\u6765\u6539\u8fdb\u6295\u5f71\u6548\u679c\uff0c\u5e76\u5229\u7528\u5b8c\u6574\u76843D\u91cd\u5efa\u6280\u672f\u5b9e\u73b0\u4efb\u610f\u7eb9\u7406\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528\u8fd9\u79cd\u65b0\u65b9\u6cd5\uff0c\u7528\u6237\u5bf9\u5ef6\u8fdf\u76f8\u5173\u7684\u4f2a\u5f71\u66f4\u52a0\u4e0d\u654f\u611f\uff0c\u5e76\u4e14\u5728\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u5f97\u66f4\u5feb\u3001\u66f4\u5bb9\u6613\u5b8c\u6210\uff0c\u76f8\u8f83\u4e8e\u76f4\u63a5\u4ece3D\u59ff\u6001\u4f30\u8ba1\u6e32\u67d3\u5e27\u7684\u7b80\u5355\u65b9\u6cd5\u6709\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "\u6587\u7ae0\u7684\u7ed3\u8bba\u662f\uff0c\u901a\u8fc7\u7ed3\u5408\u8f83\u6162\u76843D\u624b\u90e8\u59ff\u6001\u7c97\u7565\u4f30\u8ba1\u4e0e\u9ad8\u901f\u76842D\u6821\u6b63\u6b65\u9aa4\uff0c\u53ef\u4ee5\u6709\u6548\u964d\u4f4e\u611f\u77e5\u5230\u7684\u8fd0\u52a8\u5230\u5149\u5b50\u5ef6\u8fdf\uff0c\u5e76\u4e14\u63d0\u9ad8\u6295\u5f71\u5bf9\u9f50\u5ea6\u548c\u8868\u9762\u8986\u76d6\u9762\u79ef\u3002\u8fd9\u79cd\u65b9\u6cd5\u8ba9\u7528\u6237\u5bf9\u4e8e\u5ef6\u8fdf\u76f8\u5173\u7684\u4f2a\u5f71\u4e0d\u90a3\u4e48\u654f\u611f\uff0c\u4ece\u800c\u80fd\u591f\u66f4\u5feb\u901f\u3001\u66f4\u8f7b\u677e\u5730\u5b8c\u6210\u4efb\u52a1\u3002"}}
{"id": "2505.24871", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.24871", "abs": "https://arxiv.org/abs/2505.24871", "authors": ["Yiqing Liang", "Jielin Qiu", "Wenhao Ding", "Zuxin Liu", "James Tompkin", "Mengdi Xu", "Mengzhou Xia", "Zhengzhong Tu", "Laixi Shi", "Jiacheng Zhu"], "title": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Project Webpage: https://modomodo-rl.github.io/", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for post-training large language models (LLMs), achieving\nstate-of-the-art performance on tasks with structured, verifiable answers.\nApplying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but\nis complicated by the broader, heterogeneous nature of vision-language tasks\nthat demand nuanced visual, logical, and spatial capabilities. As such,\ntraining MLLMs using RLVR on multiple datasets could be beneficial but creates\nchallenges with conflicting objectives from interaction among diverse datasets,\nhighlighting the need for optimal dataset mixture strategies to improve\ngeneralization and reasoning. We introduce a systematic post-training framework\nfor Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation\nand benchmark implementation. Specifically, (1) We developed a multimodal RLVR\nframework for multi-dataset post-training by curating a dataset that contains\ndifferent verifiable vision-language problems and enabling multi-domain online\nRL learning with different verifiable rewards; (2) We proposed a data mixture\nstrategy that learns to predict the RL fine-tuning outcome from the data\nmixture distribution, and consequently optimizes the best mixture.\nComprehensive experiments showcase that multi-domain RLVR training, when\ncombined with mixture prediction strategies, can significantly boost MLLM\ngeneral reasoning capacities. Our best mixture improves the post-trained\nmodel's accuracy on out-of-distribution benchmarks by an average of 5.24%\ncompared to the same model post-trained with uniform data mixture, and by a\ntotal of 20.74% compared to the pre-finetuning baseline.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u6df7\u5408\u7b56\u7565\uff0c\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7531\u4e8e\u591a\u6a21\u6001\u4efb\u52a1\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u6df7\u5408\u7b56\u7565\u6765\u89e3\u51b3\u4e0d\u540c\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u51b2\u7a81\u76ee\u6807\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001RLVR\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u9884\u6d4b\u548c\u4f18\u5316\u6570\u636e\u6df7\u5408\u6548\u679c\u7684\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u6570\u636e\u6df7\u5408\u9884\u6d4b\u7b56\u7565\u7684\u591a\u9886\u57dfRLVR\u8bad\u7ec3\u663e\u8457\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u4e865.24%\u81f320.74%\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u6570\u636e\u6df7\u5408\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5206\u5e03\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2408.08654", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2408.08654", "abs": "https://arxiv.org/abs/2408.08654", "authors": ["Li-Fang Zhu", "Fritz Koermann", "Qing Chen", "Malin Selleby", "Joerg Neugebauer", "and Blazej Grabowski"], "title": "Accelerating ab initio melting property calculations with machine learning: Application to the high entropy alloy TaVCrW", "categories": ["cond-mat.mtrl-sci"], "comment": "14 pages, 6 figures", "summary": "Melting properties are critical for designing novel materials, especially for\ndiscovering high-performance, high-melting refractory materials. Experimental\nmeasurements of these properties are extremely challenging due to their high\nmelting temperatures. Complementary theoretical predictions are, therefore,\nindispensable. The conventional free energy approach using density functional\ntheory (DFT) has been a gold standard for such purposes because of its high\naccuracy. However,it generally involves expensive thermodynamic integration\nusing ab initio molecular dynamic simulations. The high computational cost\nmakes high-throughput calculations infeasible. Here, we propose a highly\nefficient DFT-based method aided by a specially designed machine learning\npotential. As the machine learning potential can closely reproduce the ab\ninitio phase space, even for multi-component alloys, the costly thermodynamic\nintegration can be fully substituted with more efficient free energy\nperturbation calculations. The method achieves overall savings of computational\nresources by 80% compared to current alternatives. We apply the method to the\nhigh-entropy alloy TaVCrW and calculate its melting properties, including\nmelting temperature, entropy and enthalpy of fusion, and volume change at the\nmelting point. Additionally, the heat capacities of solid and liquid TaVCrW are\ncalculated. The results agree reasonably with the calphad extrapolated values.", "keywords": ["Reasoning"], "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u6548\u8ba1\u7b97\u65b9\u6cd5\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6f5c\u529b\u8f85\u52a9\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u8ba1\u7b97\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6750\u6599\u7194\u5316\u6027\u8d28\u6240\u9700\u7684\u8d44\u6e90\uff0c\u4e14\u7ed3\u679c\u51c6\u786e\u53ef\u9760\u3002", "motivation": "\u6750\u6599\u7684\u7194\u5316\u6027\u8d28\u5bf9\u4e8e\u8bbe\u8ba1\u65b0\u578b\u9ad8\u6027\u80fd\u3001\u9ad8\u7194\u70b9\u8010\u706b\u6750\u6599\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5b9e\u9a8c\u6d4b\u91cf\u8fd9\u4e9b\u6027\u8d28\u6781\u5177\u6311\u6218\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6210\u672c\u6548\u76ca\u66f4\u9ad8\u7684\u7406\u8bba\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u7684\u81ea\u7531\u80fd\u5fae\u6270\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u4e86\u4e13\u95e8\u8bbe\u8ba1\u7684\u673a\u5668\u5b66\u4e60\u6f5c\u529b\u6765\u66ff\u4ee3\u6210\u672c\u9ad8\u6602\u7684\u70ed\u529b\u5b66\u79ef\u5206\u3002", "result": "\u901a\u8fc7\u5c06\u673a\u5668\u5b66\u4e60\u6f5c\u529b\u5e94\u7528\u4e8e\u591a\u7ec4\u5206\u5408\u91d1\u7684\u76f8\u7a7a\u95f4\u518d\u73b0\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u66ff\u6362\u4e86\u4f20\u7edf\u7684\u70ed\u529b\u5b66\u79ef\u5206\u6b65\u9aa4\uff0c\u5b9e\u73b0\u4e86\u603b\u4f53\u8ba1\u7b97\u8d44\u6e90\u768480%\u8282\u7ea6\u3002\u5bf9\u9ad8\u71b5\u5408\u91d1TaVCrW\u7684\u7194\u5316\u6027\u8d28\u8fdb\u884c\u4e86\u51c6\u786e\u8ba1\u7b97\uff0c\u5305\u62ec\u7194\u70b9\u3001\u7194\u878d\u71b5\u548c\u7194\u878d\u7113\u4ee5\u53ca\u7194\u70b9\u5904\u7684\u4f53\u79ef\u53d8\u5316\uff0c\u540c\u65f6\u8ba1\u7b97\u4e86\u56fa\u6001\u548c\u6db2\u6001TaVCrW\u7684\u70ed\u5bb9\u91cf\u3002", "conclusion": "\u7efc\u4e0a\u6240\u8ff0\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u673a\u5668\u5b66\u4e60\u6f5c\u529b\uff0c\u7528\u4e8e\u8ba1\u7b97\u6750\u6599\u7684\u7194\u5316\u6027\u8d28\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u8d44\u6e90\u6d88\u8017\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747\u8282\u7701\u4e8680%\uff0c\u5e76\u4e14\u5728\u9ad8\u71b5\u5408\u91d1TaVCrW\u4e0a\u7684\u5e94\u7528\u7ed3\u679c\u4e0ecalphad\u5916\u63a8\u503c\u5408\u7406\u4e00\u81f4\u3002"}}
{"id": "2505.24850", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.24850", "abs": "https://arxiv.org/abs/2505.24850", "authors": ["Shuyao Xu", "Cheng Peng", "Jiangxuan Long", "Weidi Xu", "Wei Chu", "Yuan Qi"], "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "comment": "27 pages, 10 figures. Code available at\n  https://github.com/Tim-Siu/reinforcement-distillation", "summary": "Recent advances in model distillation demonstrate that data from advanced\nreasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer\ncomplex reasoning abilities to smaller, efficient student models. However,\nstandard practices employ rejection sampling, discarding incorrect reasoning\nexamples -- valuable, yet often underutilized data. This paper addresses the\ncritical question: How can both positive and negative distilled reasoning\ntraces be effectively leveraged to maximize LLM reasoning performance in an\noffline setting? To this end, We propose Reinforcement Distillation (REDI), a\ntwo-stage framework. Stage 1 learns from positive traces via Supervised\nFine-Tuning (SFT). Stage 2 further refines the model using both positive and\nnegative traces through our proposed REDI objective. This novel objective is a\nsimple, reference-free loss function that outperforms established methods like\nDPO and SimPO in this distillation context. Our empirical evaluations\ndemonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT\ncombined with DPO/SimPO on mathematical reasoning tasks. Notably, the\nQwen-REDI-1.5B model, post-trained on just 131k positive and negative examples\nfrom the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).\nIts performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a\nmodel post-trained on 800k proprietary data) across various mathematical\nreasoning benchmarks, establishing a new state-of-the-art for 1.5B models\npost-trained offline with openly available data.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "This paper introduces REDI, a framework for enhancing LLM reasoning performance by leveraging both positive and negative reasoning traces in an offline setting.", "motivation": "The motivation is to utilize both positive and negative distilled reasoning traces to maximize LLM reasoning performance in an offline setting, as current practices typically discard incorrect reasoning examples.", "method": "The paper proposes a two-stage framework called Reinforcement Distillation (REDI). The first stage involves learning from positive traces via Supervised Fine-Tuning (SFT), while the second stage refines the model using both positive and negative traces through the REDI objective, a reference-free loss function.", "result": "The Qwen-REDI-1.5B model achieves an 83.1% score on MATH-500 (pass@1) and matches or surpasses the performance of DeepSeek-R1-Distill-Qwen-1.5B across various mathematical reasoning benchmarks.", "conclusion": "The paper concludes that the proposed REDI framework effectively leverages both positive and negative reasoning traces to enhance LLM performance in an offline setting."}}
{"id": "2403.11729", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2403.11729", "abs": "https://arxiv.org/abs/2403.11729", "authors": ["Kento Kawaharazuka", "Akihiro Miki", "Masahiro Bando", "Temma Suzuki", "Yoshimoto Ribayashi", "Yasunori Toshimitsu", "Yuya Nagamatsu", "Kei Okada", "and Masayuki Inaba"], "title": "Hardware Design and Learning-Based Software Architecture of Musculoskeletal Wheeled Robot Musashi-W for Real-World Applications", "categories": ["cs.RO"], "comment": "Accepted at Humanoids2022", "summary": "Various musculoskeletal humanoids have been developed so far. While these\nhumanoids have the advantage of their flexible and redundant bodies that mimic\nthe human body, they are still far from being applied to real-world tasks. One\nof the reasons for this is the difficulty of bipedal walking in a flexible\nbody. Thus, we developed a musculoskeletal wheeled robot, Musashi-W, by\ncombining a wheeled base and musculoskeletal upper limbs for real-world\napplications. Also, we constructed its software system by combining static and\ndynamic body schema learning, reflex control, and visual recognition. We show\nthat the hardware and software of Musashi-W can make the most of the advantages\nof the musculoskeletal upper limbs, through several tasks of cleaning by human\nteaching, carrying a heavy object considering muscle addition, and setting a\ntable through dynamic cloth manipulation with variable stiffness.", "keywords": ["Reasoning"], "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u6b3e\u540d\u4e3a Musashi-W \u7684\u808c\u9aa8\u9abc\u8f6e\u5f0f\u673a\u5668\u4eba\uff0c\u5b83\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u6280\u672f\u5b9e\u73b0\u4e86\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u7684\u6709\u6548\u6267\u884c\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u8bb8\u591a\u808c\u9aa8\u9abc\u4eba\u5f62\u673a\u5668\u4eba\uff0c\u4f46\u7531\u4e8e\u67d4\u6027\u8eab\u4f53\u7684\u53cc\u8db3\u884c\u8d70\u96be\u5ea6\uff0c\u5b83\u4eec\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4ecd\u7136\u6709\u9650\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u5f00\u53d1\u4e86 Musashi-W \u4ee5\u63a2\u7d22\u66f4\u5b9e\u7528\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u808c\u9aa8\u9abc\u8f6e\u5f0f\u673a\u5668\u4eba Musashi-W\uff0c\u5e76\u6784\u5efa\u4e86\u5176\u8f6f\u4ef6\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u9759\u6001\u548c\u52a8\u6001\u8eab\u4f53\u6a21\u5f0f\u5b66\u4e60\u3001\u53cd\u5c04\u63a7\u5236\u548c\u89c6\u89c9\u8bc6\u522b\u3002", "result": "Musashi-W \u6210\u529f\u5b8c\u6210\u4e86\u591a\u79cd\u4efb\u52a1\uff0c\u5305\u62ec\u901a\u8fc7\u4eba\u5de5\u6559\u5b66\u8fdb\u884c\u6e05\u6d01\u3001\u8003\u8651\u808c\u8089\u6dfb\u52a0\u642c\u8fd0\u91cd\u7269\u4ee5\u53ca\u901a\u8fc7\u53ef\u53d8\u521a\u5ea6\u52a8\u6001\u5e03\u6599\u64cd\u4f5c\u6765\u5e03\u7f6e\u9910\u684c\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u9759\u6001\u548c\u52a8\u6001\u8eab\u4f53\u6a21\u5f0f\u5b66\u4e60\u3001\u53cd\u5c04\u63a7\u5236\u548c\u89c6\u89c9\u8bc6\u522b\uff0cMusashi-W \u7684\u786c\u4ef6\u548c\u8f6f\u4ef6\u7cfb\u7edf\u80fd\u591f\u5145\u5206\u5229\u7528\u808c\u9aa8\u9abc\u4e0a\u80a2\u7684\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u4efb\u52a1\u3002"}}
{"id": "2505.24710", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.24710", "abs": "https://arxiv.org/abs/2505.24710", "authors": ["Wei Chen", "Jiahao Zhang", "Haipeng Zhu", "Boyan Xu", "Zhifeng Hao", "Keli Zhang", "Junjian Ye", "Ruichu Cai"], "title": "Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by IJCAI 2025", "summary": "Large language models (LLMs) have shown great potential in decision-making\ndue to the vast amount of knowledge stored within the models. However, these\npre-trained models are prone to lack reasoning abilities and are difficult to\nadapt to new environments, further hindering their application to complex\nreal-world tasks. To address these challenges, inspired by the human cognitive\nprocess, we propose Causal-aware LLMs, which integrate the structural causal\nmodel (SCM) into the decision-making process to model, update, and utilize\nstructured knowledge of the environment in a ``learning-adapting-acting\"\nparadigm. Specifically, in the learning stage, we first utilize an LLM to\nextract the environment-specific causal entities and their causal relations to\ninitialize a structured causal model of the environment. Subsequently,in the\nadapting stage, we update the structured causal model through external feedback\nabout the environment, via an idea of causal intervention. Finally, in the\nacting stage, Causal-aware LLMs exploit structured causal knowledge for more\nefficient policy-making through the reinforcement learning agent. The above\nprocesses are performed iteratively to learn causal knowledge, ultimately\nenabling the causal-aware LLMs to achieve a more accurate understanding of the\nenvironment and make more efficient decisions. Experimental results across 22\ndiverse tasks within the open-world game ``Crafter\" validate the effectiveness\nof our proposed method.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLMs) \u7f3a\u4e4f\u63a8\u7406\u80fd\u529b\u548c\u96be\u4ee5\u9002\u5e94\u65b0\u73af\u5883\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Causal-aware LLMs \u7684\u65b9\u6cd5\uff0c\u5c06\u7ed3\u6784\u56e0\u679c\u6a21\u578b (SCM) \u96c6\u6210\u5230\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u5bf9\u73af\u5883\u7684\u7406\u89e3\u548c\u51b3\u7b56\u6548\u7387\u3002", "motivation": "\u7531\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7f3a\u4e4f\u63a8\u7406\u80fd\u529b\u4e14\u96be\u4ee5\u9002\u5e94\u65b0\u73af\u5883\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u672c\u6587\u53d7\u5230\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u6574\u5408\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u7684\u65b9\u6cd5\u6765\u63d0\u5347 LLM \u7684\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Causal-aware LLMs \u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u7ed3\u6784\u56e0\u679c\u6a21\u578b (SCM) \u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM)\uff0c\u91c7\u7528\u201c\u5b66\u4e60-\u9002\u5e94-\u884c\u52a8\u201d\u7684\u8303\u5f0f\u8fdb\u884c\u51b3\u7b56\u3002\u5177\u4f53\u5305\u62ec\uff1a1. \u5728\u5b66\u4e60\u9636\u6bb5\uff0c\u4f7f\u7528 LLM \u63d0\u53d6\u73af\u5883\u7279\u5b9a\u7684\u56e0\u679c\u5b9e\u4f53\u53ca\u5176\u5173\u7cfb\uff0c\u4ee5\u521d\u59cb\u5316\u73af\u5883\u7684\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff1b2. \u5728\u9002\u5e94\u9636\u6bb5\uff0c\u901a\u8fc7\u5916\u90e8\u53cd\u9988\u66f4\u65b0\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff1b3. \u5728\u884c\u52a8\u9636\u6bb5\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u57fa\u4e8e\u7ed3\u6784\u56e0\u679c\u77e5\u8bc6\u5236\u5b9a\u7b56\u7565\u3002", "result": "\u5728\u5f00\u653e\u4e16\u754c\u6e38\u620f Crafter \u4e2d\u768422\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684 Causal-aware LLMs \u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u5176\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u548c\u9002\u5e94\u73af\u5883\uff0c\u5e76\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u51b3\u7b56\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u96c6\u6210\u5230\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0cCausal-aware LLMs \u80fd\u591f\u66f4\u51c6\u786e\u5730\u7406\u89e3\u73af\u5883\u5e76\u505a\u51fa\u66f4\u9ad8\u6548\u7684\u51b3\u7b56\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5f00\u653e\u4e16\u754c\u6e38\u620f Crafter \u4e2d\u768422\u4e2a\u4e0d\u540c\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2403.00458", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2403.00458", "abs": "https://arxiv.org/abs/2403.00458", "authors": ["Chung Yi See", "Vasco Rato Santos", "Lucas Woodley", "Megan Yeo", "Daniel Palmer", "Shuheng Zhang", "and Ashley Nunes"], "title": "Prices and preferences in the electric vehicle market", "categories": ["econ.EM"], "comment": "Main paper: 5 tables, 2 figures", "summary": "Although electric vehicles are less polluting than gasoline powered vehicles,\nadoption is challenged by higher procurement prices. Existing discourse\nemphasizes EV battery costs as being principally responsible for this price\ndifferential and widespread adoption is routinely conditioned upon battery\ncosts declining. We scrutinize such reasoning by sourcing data on EV attributes\nand market conditions between 2011 and 2023. Our findings are fourfold. First,\nEV prices are influenced principally by the number of amenities, additional\nfeatures, and dealer-installed accessories sold as standard on an EV, and to a\nlesser extent, by EV horsepower. Second, EV range is negatively correlated with\nEV price implying that range anxiety concerns may be less consequential than\nexisting discourse suggests. Third, battery capacity is positively correlated\nwith EV price, due to more capacity being synonymous with the delivery of more\nhorsepower. Collectively, this suggests that higher procurement prices for EVs\nreflects consumer preference for vehicles that are feature dense and more\npowerful. Fourth and finally, accommodating these preferences have produced\nvehicles with lower fuel economy, a shift that reduces envisioned lifecycle\nemissions benefits by at least 3.26 percent, subject to the battery pack\nchemistry leveraged and the carbon intensity of the electrical grid. These\nfindings warrant attention as decarbonization efforts increasingly emphasize\nelectrification as a pathway for complying with domestic and international\nclimate agreements.", "keywords": ["Reasoning"], "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u7535\u52a8\u6c7d\u8f66\u4ef7\u683c\u8f83\u9ad8\u7684\u539f\u56e0\uff0c\u53d1\u73b0\u5176\u4ef7\u683c\u4e3b\u8981\u53d7\u914d\u7f6e\u548c\u6027\u80fd\u5f71\u54cd\uff0c\u800c\u4e0d\u662f\u5355\u7eaf\u56e0\u4e3a\u7535\u6c60\u6210\u672c\u3002\u8fd9\u79cd\u504f\u597d\u5bfc\u81f4\u4e86\u8f83\u4f4e\u7684\u71c3\u6cb9\u7ecf\u6d4e\u6027\uff0c\u51cf\u5c11\u4e86\u6392\u653e\u6548\u76ca\u3002", "motivation": "\u5c3d\u7ba1\u7535\u52a8\u6c7d\u8f66\u6bd4\u6c7d\u6cb9\u8f66\u6c61\u67d3\u5c11\uff0c\u4f46\u5176\u91c7\u7528\u7387\u53d7\u5230\u91c7\u8d2d\u4ef7\u683c\u8f83\u9ad8\u7684\u6311\u6218\u3002\u73b0\u6709\u8ba8\u8bba\u5f3a\u8c03\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\u6210\u672c\u662f\u8fd9\u4e00\u4ef7\u683c\u5dee\u5f02\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u5e76\u4e14\u901a\u5e38\u8ba4\u4e3a\u53ea\u6709\u7535\u6c60\u6210\u672c\u4e0b\u964d\u624d\u80fd\u5b9e\u73b0\u5e7f\u6cdb\u91c7\u7528\u3002", "method": "\u901a\u8fc7\u6536\u96c62011\u5e74\u81f32023\u5e74\u95f4\u7535\u52a8\u6c7d\u8f66\u5c5e\u6027\u548c\u5e02\u573a\u6761\u4ef6\u7684\u6570\u636e\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u9996\u5148\uff0c\u7535\u52a8\u6c7d\u8f66\u7684\u4ef7\u683c\u4e3b\u8981\u53d7\u6807\u51c6\u914d\u7f6e\u7684\u8bbe\u65bd\u3001\u9644\u52a0\u529f\u80fd\u548c\u7ecf\u9500\u5546\u5b89\u88c5\u7684\u914d\u4ef6\u6570\u91cf\u7684\u5f71\u54cd\uff0c\u5176\u6b21\u624d\u662f\u7535\u52a8\u6c7d\u8f66\u7684\u9a6c\u529b\u3002\u7b2c\u4e8c\uff0c\u7535\u52a8\u6c7d\u8f66\u7684\u7eed\u822a\u91cc\u7a0b\u4e0e\u4ef7\u683c\u5448\u8d1f\u76f8\u5173\uff0c\u8fd9\u610f\u5473\u7740\u7eed\u822a\u7126\u8651\u53ef\u80fd\u6ca1\u6709\u73b0\u6709\u8ba8\u8bba\u4e2d\u90a3\u6837\u91cd\u8981\u3002\u7b2c\u4e09\uff0c\u7535\u6c60\u5bb9\u91cf\u4e0e\u7535\u52a8\u6c7d\u8f66\u4ef7\u683c\u6b63\u76f8\u5173\uff0c\u56e0\u4e3a\u66f4\u591a\u7684\u5bb9\u91cf\u610f\u5473\u7740\u63d0\u4f9b\u66f4\u591a\u7684\u9a6c\u529b\u3002\u7b2c\u56db\uff0c\u8fd9\u4e9b\u504f\u597d\u5bfc\u81f4\u4e86\u71c3\u6cb9\u7ecf\u6d4e\u6027\u8f83\u4f4e\u7684\u8f66\u8f86\u51fa\u73b0\uff0c\u8fd9\u4f7f\u5f97\u9884\u671f\u7684\u751f\u547d\u5468\u671f\u6392\u653e\u6548\u76ca\u51cf\u5c11\u4e86\u81f3\u5c113.26%\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u5f97\u51fa\u7684\u7ed3\u8bba\u662f\uff0c\u7535\u52a8\u6c7d\u8f66\u7684\u9ad8\u4ef7\u4e3b\u8981\u53cd\u6620\u4e86\u6d88\u8d39\u8005\u5bf9\u529f\u80fd\u4e30\u5bcc\u548c\u66f4\u5f3a\u5927\u8f66\u8f86\u7684\u504f\u597d\uff0c\u800c\u8fd9\u4e9b\u504f\u597d\u5bfc\u81f4\u4e86\u8f83\u4f4e\u7684\u71c3\u6cb9\u7ecf\u6d4e\u6027\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u9884\u671f\u7684\u751f\u547d\u5468\u671f\u6392\u653e\u6548\u76ca\u3002"}}
{"id": "2505.24298", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.24298", "abs": "https://arxiv.org/abs/2505.24298", "authors": ["Wei Fu", "Jiaxuan Gao", "Xujie Shen", "Chen Zhu", "Zhiyu Mei", "Chuyi He", "Shusheng Xu", "Guo Wei", "Jun Mei", "Jiashu Wang", "Tongkai Yang", "Binhang Yuan", "Yi Wu"], "title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has become a trending paradigm for training large\nlanguage models (LLMs), particularly for reasoning tasks. Effective RL for LLMs\nrequires massive parallelization and poses an urgent need for efficient\ntraining systems. Most existing large-scale RL systems for LLMs are synchronous\nby alternating generation and training in a batch setting, where the rollouts\nin each training batch are generated by the same (or latest) model. This\nstabilizes RL training but suffers from severe system-level inefficiency.\nGeneration must wait until the longest output in the batch is completed before\nmodel update, resulting in GPU underutilization. We present AReaL, a\n\\emph{fully asynchronous} RL system that completely decouples generation from\ntraining. Rollout workers in AReaL continuously generate new outputs without\nwaiting, while training workers update the model whenever a batch of data is\ncollected. AReaL also incorporates a collection of system-level optimizations,\nleading to substantially higher GPU utilization. To stabilize RL training,\nAReaL balances the workload of rollout and training workers to control data\nstaleness, and adopts a staleness-enhanced PPO variant to better handle\noutdated training samples. Extensive experiments on math and code reasoning\nbenchmarks show that AReaL achieves \\textbf{up to 2.57$\\times$ training\nspeedup} compared to the best synchronous systems with the same number of GPUs\nand matched or even improved final performance. The code of AReaL is available\nat https://github.com/inclusionAI/AReaL/.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edfAReaL\uff0c\u5b83\u901a\u8fc7\u89e3\u8026\u751f\u6210\u4e0e\u8bad\u7ec3\u3001\u63d0\u9ad8GPU\u5229\u7528\u7387\uff0c\u5728\u6570\u5b66\u548c\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8bad\u7ec3\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21RL\u7cfb\u7edf\u591a\u4e3a\u540c\u6b65\u5f0f\uff0c\u8fd9\u5bfc\u81f4\u4e86\u4e25\u91cd\u7684\u7cfb\u7edf\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u4e3a\u751f\u6210\u5fc5\u987b\u7b49\u5f85\u76f4\u5230\u6279\u6b21\u4e2d\u6240\u6709rollouts\u5b8c\u6210\u624d\u80fd\u8fdb\u884c\u6a21\u578b\u66f4\u65b0\uff0c\u4ece\u800c\u9020\u6210GPU\u8d44\u6e90\u7684\u6d6a\u8d39\u3002", "method": "AReaL\u91c7\u7528\u4e86\u5f02\u6b65RL\u67b6\u6784\uff0c\u5176\u4e2drollout\u5de5\u4eba\u6301\u7eed\u751f\u6210\u65b0\u8f93\u51fa\uff0c\u800c\u8bad\u7ec3\u5de5\u4eba\u5728\u6536\u96c6\u5230\u4e00\u6279\u6570\u636e\u540e\u7acb\u5373\u66f4\u65b0\u6a21\u578b\u3002\u6b64\u5916\uff0cAReaL\u8fd8\u91c7\u7528\u4e86\u63a7\u5236\u6570\u636e\u9648\u65e7\u6027\u7684\u65b9\u6cd5\u548c\u4e00\u79cd\u9648\u65e7\u6027\u589e\u5f3a\u7684PPO\u53d8\u4f53\u4ee5\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728\u6570\u5b66\u548c\u4ee3\u7801\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f7f\u7528\u76f8\u540c\u6570\u91cfGPU\u7684\u6700\u4f73\u540c\u6b65\u7cfb\u7edf\u76f8\u6bd4\uff0cAReaL\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.57\u500d\u7684\u8bad\u7ec3\u52a0\u901f\uff0c\u5e76\u4e14\u6700\u7ec8\u6027\u80fd\u5f97\u5230\u4e86\u4fdd\u6301\u6216\u63d0\u5347\u3002", "conclusion": "AReaL\u662f\u4e00\u4e2a\u5b8c\u5168\u5f02\u6b65\u7684RL\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u751f\u6210\u4e0e\u8bad\u7ec3\u5b8c\u5168\u89e3\u8026\uff0c\u5e76\u91c7\u7528\u4e00\u7cfb\u5217\u7cfb\u7edf\u7ea7\u4f18\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86GPU\u5229\u7528\u7387\u548c\u8bad\u7ec3\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.24238", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.24238", "abs": "https://arxiv.org/abs/2505.24238", "authors": ["Bowen Dong", "Minheng Ni", "Zitong Huang", "Guanglei Yang", "Wangmeng Zuo", "Lei Zhang"], "title": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal hallucination in multimodal large language models (MLLMs)\nrestricts the correctness of MLLMs. However, multimodal hallucinations are\nmulti-sourced and arise from diverse causes. Existing benchmarks fail to\nadequately distinguish between perception-induced hallucinations and\nreasoning-induced hallucinations. This failure constitutes a significant issue\nand hinders the diagnosis of multimodal reasoning failures within MLLMs. To\naddress this, we propose the {\\dataset} benchmark, which isolates reasoning\nhallucinations by constructing questions where input images are correctly\nperceived by MLLMs yet reasoning errors persist. {\\dataset} introduces\nmulti-granular evaluation metrics: accuracy, factuality, and LLMs hallucination\nscore for hallucination quantification. Our analysis reveals that (1) the model\nscale, data scale, and training stages significantly affect the degree of\nlogical, fabrication, and factual hallucinations; (2) current MLLMs show no\neffective improvement on spatial hallucinations caused by misinterpreted\nspatial relationships, indicating their limited visual reasoning capabilities;\nand (3) question types correlate with distinct hallucination patterns,\nhighlighting targeted challenges and potential mitigation strategies. To\naddress these challenges, we propose {\\method}, a method that combines\ncurriculum reinforcement fine-tuning to encourage models to generate\nlogic-consistent reasoning chains by stepwise reducing learning difficulty, and\ncollaborative hint inference to reduce reasoning complexity. {\\method}\nestablishes a baseline on {\\dataset}, and reduces the logical hallucinations in\noriginal base models.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2208.08580", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2208.08580", "abs": "https://arxiv.org/abs/2208.08580", "authors": ["Gopal Sharma", "Kangxue Yin", "Subhransu Maji", "Evangelos Kalogerakis", "Or Litany", "Sanja Fidler"], "title": "MvDeCor: Multi-view Dense Correspondence Learning for Fine-grained 3D Segmentation", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "project page: https://nv-tlabs.github.io/MvDeCor/", "summary": "We propose to utilize self-supervised techniques in the 2D domain for\nfine-grained 3D shape segmentation tasks. This is inspired by the observation\nthat view-based surface representations are more effective at modeling\nhigh-resolution surface details and texture than their 3D counterparts based on\npoint clouds or voxel occupancy. Specifically, given a 3D shape, we render it\nfrom multiple views, and set up a dense correspondence learning task within the\ncontrastive learning framework. As a result, the learned 2D representations are\nview-invariant and geometrically consistent, leading to better generalization\nwhen trained on a limited number of labeled shapes compared to alternatives\nthat utilize self-supervision in 2D or 3D alone. Experiments on textured\n(RenderPeople) and untextured (PartNet) 3D datasets show that our method\noutperforms state-of-the-art alternatives in fine-grained part segmentation.\nThe improvements over baselines are greater when only a sparse set of views is\navailable for training or when shapes are textured, indicating that MvDeCor\nbenefits from both 2D processing and 3D geometric reasoning.", "keywords": ["Reasoning"], "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86MvDeCor\uff0c\u4e00\u79cd\u7ed3\u54082D\u81ea\u76d1\u7763\u6280\u672f\u548c3D\u51e0\u4f55\u63a8\u7406\u7684\u7ec6\u7c92\u5ea63D\u5f62\u72b6\u5206\u5272\u65b9\u6cd5\uff0c\u5176\u5728\u591a\u79cd\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7531\u4e8e\u57fa\u4e8e\u89c6\u56fe\u7684\u8868\u9762\u8868\u793a\u5728\u5efa\u6a21\u9ad8\u5206\u8fa8\u7387\u8868\u9762\u7ec6\u8282\u548c\u7eb9\u7406\u65b9\u9762\u6bd4\u57fa\u4e8e\u70b9\u4e91\u6216\u4f53\u7d20\u76843D\u8868\u793a\u66f4\u6709\u6548\uff0c\u56e0\u6b64\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5229\u75282D\u9886\u57df\u7684\u81ea\u76d1\u7763\u6280\u672f\u6765\u63d0\u53473D\u5f62\u72b6\u5206\u5272\u7684\u6548\u679c\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u89c6\u89d2\u6e32\u67d3\u548c\u5bf9\u6bd4\u5b66\u4e60\u76842D\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e3D\u5f62\u72b6\u5206\u5272\u4efb\u52a1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4f5c\u8005\u901a\u8fc7\u591a\u4e2a\u89c6\u89d2\u6e32\u67d33D\u5f62\u72b6\uff0c\u5e76\u5728\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u5185\u8bbe\u7f6e\u5bc6\u96c6\u5bf9\u5e94\u5173\u7cfb\u5b66\u4e60\u4efb\u52a1\uff0c\u4ece\u800c\u83b7\u5f97\u89c6\u56fe\u4e0d\u53d8\u4e14\u51e0\u4f55\u4e00\u81f4\u76842D\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u4e0e\u4ec5\u4f7f\u75282D\u62163D\u81ea\u76d1\u7763\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cMvDeCor\u5728RenderPeople\uff08\u5e26\u7eb9\u7406\uff09\u548cPartNet\uff08\u4e0d\u5e26\u7eb9\u7406\uff09\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u66f4\u597d\uff0c\u5c24\u5176\u662f\u5728\u8bad\u7ec3\u89c6\u56fe\u7a00\u758f\u6216\u5f62\u72b6\u5e26\u6709\u7eb9\u7406\u65f6\u6548\u679c\u66f4\u663e\u8457\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMvDeCor\u5728\u7ec6\u7c92\u5ea63D\u5f62\u72b6\u5206\u5272\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u6216\u5f62\u72b6\u5177\u6709\u7eb9\u7406\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2505.24183", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.24183", "abs": "https://arxiv.org/abs/2505.24183", "authors": ["Yaoyu Zhu", "Di Huang", "Hanqi Lyu", "Xiaoyun Zhang", "Chongxiao Li", "Wenxuan Shi", "Yutong Wu", "Jianan Mu", "Jinghua Wang", "Yang Zhao", "Pengwei Jin", "Shuyao Cheng", "Shengwen Liang", "Xishan Zhang", "Rui Zhang", "Zidong Du", "Qi Guo", "Xing Hu", "Yunji Chen"], "title": "CodeV-R1: Reasoning-Enhanced Verilog Generation", "categories": ["cs.LG", "cs.AR", "cs.PL"], "comment": null, "summary": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while matching or even exceeding\nthe performance of 671B DeepSeek-R1. We will release our model, training\npipeline, and dataset to facilitate research in EDA and LLM communities.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8eVerilog\u4ee3\u7801\u751f\u6210\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6CodeV-R1\uff0c\u89e3\u51b3\u4e86\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\u4e2d\u7684\u4e09\u5927\u6311\u6218\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u867d\u7136\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u5df2\u7ecf\u5728\u8f6f\u4ef6\u7f16\u7a0b\u548c\u6570\u5b66\u95ee\u9898\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4f46\u5728\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\uff08EDA\uff09\u9886\u57df\uff0c\u5c24\u5176\u662f\u5728\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u786c\u4ef6\u63cf\u8ff0\u8bed\u8a00\u5982Verilog\u7684\u4efb\u52a1\u4e2d\uff0c\u4ecd\u7136\u9762\u4e34\u591a\u4e2a\u6311\u6218\uff1a\u7f3a\u4e4f\u81ea\u52a8\u5316\u7684\u51c6\u786e\u9a8c\u8bc1\u73af\u5883\u3001\u7f3a\u5c11\u9ad8\u8d28\u91cf\u7684\u81ea\u7136\u8bed\u8a00\u4e0e\u4ee3\u7801\u914d\u5bf9\u6570\u636e\uff0c\u4ee5\u53caRLVR\u7684\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63a8\u52a8RLVR\u5728EDA\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aCodeV-R1\u7684RLVR\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u90e8\u5206\uff1a\u4e00\u662f\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u89c4\u5219\u7684\u6d4b\u8bd5\u5e73\u53f0\u751f\u6210\u5668\uff0c\u7528\u4e8e\u8fdb\u884c\u7b49\u6548\u6027\u68c0\u67e5\uff1b\u4e8c\u662f\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u5411\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u5f00\u6e90Verilog\u4ee3\u7801\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u5229\u7528\u6d4b\u8bd5\u5e73\u53f0\u9a8c\u8bc1\u4e00\u81f4\u6027\u5e76\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u6837\u672c\uff1b\u4e09\u662f\u91c7\u7528\u4e24\u9636\u6bb5\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ec\u63a8\u7406\u80fd\u529b\u51b7\u542f\u52a8\u7684\u84b8\u998f\u9636\u6bb5\u548c\u4f7f\u7528\u81ea\u9002\u5e94DAPO\u7b97\u6cd5\u7684\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff0c\u4ee5\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u7684\u6a21\u578bCodeV-R1-7B\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5VerilogEval v2\u548cRTLLM v1.1\u4e0a\u5206\u522b\u8fbe\u5230\u4e8668.6%\u548c72.9%\u7684pass@1\u5f97\u5206\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd512~20%\uff0c\u5e76\u4e14\u5176\u6027\u80fd\u751a\u81f3\u4f18\u4e8e671B\u53c2\u6570\u7684DeepSeek-R1\u6a21\u578b\u3002\u8fd9\u4e00\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6709\u6548\u89e3\u51b3EDA\u9886\u57df\u6311\u6218\u7684\u540c\u65f6\uff0c\u8be5\u6a21\u578b\u5728\u8d44\u6e90\u6548\u7387\u548c\u6027\u80fd\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684Verilog\u4ee3\u7801\u751f\u6210\u6846\u67b6CodeV-R1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\u9886\u57df\u4e2d\u7f3a\u4e4f\u81ea\u52a8\u9a8c\u8bc1\u73af\u5883\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u5173\u952e\u95ee\u9898\u3002\u901a\u8fc7\u89c4\u5219\u6d4b\u8bd5\u5e73\u53f0\u751f\u6210\u3001\u53cc\u5411\u6570\u636e\u5408\u6210\u65b9\u6cd5\u4ee5\u53ca\u521b\u65b0\u7684\u201cdistill-then-RL\u201d\u8bad\u7ec3\u6d41\u7a0b\uff0c\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u8ba1\u5212\u5f00\u653e\u6a21\u578b\u548c\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2110.02210", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2110.02210", "abs": "https://arxiv.org/abs/2110.02210", "authors": ["Alexey Nekrasov", "Jonas Schult", "Or Litany", "Bastian Leibe", "Francis Engelmann"], "title": "Mix3D: Out-of-Context Data Augmentation for 3D Scenes", "categories": ["cs.CV"], "comment": "Accepted for publication at 3DV 2021. Camera-ready submission. Link\n  to code: https://github.com/kumuji/mix3d - Project page:\n  https://nekrasov.dev/mix3d/", "summary": "We present Mix3D, a data augmentation technique for segmenting large-scale 3D\nscenes. Since scene context helps reasoning about object semantics, current\nworks focus on models with large capacity and receptive fields that can fully\ncapture the global context of an input 3D scene. However, strong contextual\npriors can have detrimental implications like mistaking a pedestrian crossing\nthe street for a car. In this work, we focus on the importance of balancing\nglobal scene context and local geometry, with the goal of generalizing beyond\nthe contextual priors in the training set. In particular, we propose a \"mixing\"\ntechnique which creates new training samples by combining two augmented scenes.\nBy doing so, object instances are implicitly placed into novel out-of-context\nenvironments and therefore making it harder for models to rely on scene context\nalone, and instead infer semantics from local structure as well. We perform\ndetailed analysis to understand the importance of global context, local\nstructures and the effect of mixing scenes. In experiments, we show that models\ntrained with Mix3D profit from a significant performance boost on indoor\n(ScanNet, S3DIS) and outdoor datasets (SemanticKITTI). Mix3D can be trivially\nused with any existing method, e.g., trained with Mix3D, MinkowskiNet\noutperforms all prior state-of-the-art methods by a significant margin on the\nScanNet test benchmark 78.1 mIoU. Code is available at:\nhttps://nekrasov.dev/mix3d/", "keywords": ["Reasoning"], "AI": {"tldr": "Mix3D\u662f\u4e00\u79cd\u901a\u8fc7\u6df7\u5408\u4e24\u4e2a\u589e\u5f3a\u573a\u666f\u6765\u521b\u5efa\u65b0\u8bad\u7ec3\u6837\u672c\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u65e8\u5728\u5e73\u8861\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u5c40\u90e8\u51e0\u4f55\u7279\u5f81\uff0c\u4ece\u800c\u63d0\u9ad83D\u573a\u666f\u5206\u5272\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u76843D\u573a\u666f\u5206\u5272\u5de5\u4f5c\u8fc7\u4e8e\u4f9d\u8d56\u4e8e\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u4e00\u4e9b\u9519\u8bef\uff0c\u5982\u5c06\u884c\u4eba\u8bef\u8ba4\u4e3a\u6c7d\u8f66\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u5e73\u8861\u5168\u5c40\u573a\u666f\u4e0a\u4e0b\u6587\u548c\u5c40\u90e8\u51e0\u4f55\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u5728\u8d85\u51fa\u8bad\u7ec3\u96c6\u4e0a\u4e0b\u6587\u60c5\u51b5\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMix3D\u7684\u6df7\u5408\u6280\u672f\uff0c\u901a\u8fc7\u5c06\u4e24\u4e2a\u589e\u5f3a\u76843D\u573a\u666f\u7ed3\u5408\uff0c\u751f\u6210\u65b0\u7684\u8bad\u7ec3\u6837\u672c\u3002\u8fd9\u6837\u53ef\u4ee5\u8ba9\u5bf9\u8c61\u5b9e\u4f8b\u51fa\u73b0\u5728\u65b0\u7684\u975e\u4e0a\u4e0b\u6587\u73af\u5883\u4e2d\uff0c\u589e\u52a0\u6a21\u578b\u5bf9\u5c40\u90e8\u7ed3\u6784\u7684\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528Mix3D\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u591a\u4e2a\u5ba4\u5185\uff08ScanNet, S3DIS\uff09\u548c\u5ba4\u5916\uff08SemanticKITTI\uff09\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u7279\u522b\u662f\u4e0eMinkowskiNet\u7ed3\u5408\u65f6\uff0c\u5728ScanNet\u6d4b\u8bd5\u57fa\u51c6\u4e0a\u7684mIoU\u8fbe\u5230\u4e8678.1\uff0c\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u6240\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "Mix3D\u662f\u4e00\u79cd\u7528\u4e8e\u5206\u5272\u5927\u89c4\u6a213D\u573a\u666f\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002\u901a\u8fc7\u7ed3\u5408\u4e24\u4e2a\u589e\u5f3a\u573a\u666f\u6765\u521b\u5efa\u65b0\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u540c\u65f6\u4f9d\u8d56\u4e8e\u5c40\u90e8\u7ed3\u6784\u548c\u5168\u5c40\u573a\u666f\u4e0a\u4e0b\u6587\u8fdb\u884c\u8bed\u4e49\u63a8\u65ad\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528Mix3D\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5ba4\u5185\u548c\u5ba4\u5916\u6570\u636e\u96c6\u4e0a\u90fd\u6709\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u4efb\u4f55\u73b0\u6709\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u3002"}}
{"id": "2505.24034", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.24034", "abs": "https://arxiv.org/abs/2505.24034", "authors": ["Bo Wu", "Sid Wang", "Yunhao Tang", "Jia Ding", "Eryk Helenowski", "Liang Tan", "Tengyu Xu", "Tushar Gowda", "Zhengxing Chen", "Chen Zhu", "Xiaocheng Tang", "Yundi Qian", "Beibei Zhu", "Rui Hou"], "title": "LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Trainin", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement Learning (RL) has become the most effective post-training\napproach for improving the capabilities of Large Language Models (LLMs). In\npractice, because of the high demands on latency and memory, it is particularly\nchallenging to develop an efficient RL framework that reliably manages policy\nmodels with hundreds to thousands of billions of parameters.\n  In this paper, we present LlamaRL, a fully distributed, asynchronous RL\nframework optimized for efficient training of large-scale LLMs with various\nmodel sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a\nhandful to thousands of devices. LlamaRL introduces a streamlined,\nsingle-controller architecture built entirely on native PyTorch, enabling\nmodularity, ease of use, and seamless scalability to thousands of GPUs. We also\nprovide a theoretical analysis of LlamaRL's efficiency, including a formal\nproof that its asynchronous design leads to strict RL speed-up. Empirically, by\nleveraging best practices such as colocated model offloading, asynchronous\noff-policy training, and distributed direct memory access for weight\nsynchronization, LlamaRL achieves significant efficiency gains -- up to 10.7x\nspeed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy\nmodel. Furthermore, the efficiency advantage continues to grow with increasing\nmodel scale, demonstrating the framework's suitability for future large-scale\nRL training.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6 LlamaRL\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\uff0c\u901a\u8fc7\u5f02\u6b65\u8bbe\u8ba1\u548c\u6280\u672f\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u901f\u5ea6\u548c\u6269\u5c55\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5ef6\u8fdf\u548c\u5185\u5b58\u9700\u6c42\u4e0a\u7684\u9ad8\u8981\u6c42\uff0c\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u53ef\u9760\u7ba1\u7406\u5177\u6709\u6570\u767e\u5230\u6570\u5343\u4ebf\u53c2\u6570\u7684\u7b56\u7565\u6a21\u578b\u7684\u9ad8\u6548\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u53d8\u5f97\u5c24\u4e3a\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u539f\u751f PyTorch \u7684\u5b8c\u5168\u5206\u5e03\u5f0f\u3001\u5f02\u6b65 RL \u6846\u67b6 LlamaRL\uff0c\u5e76\u7ed3\u5408\u4e86\u591a\u79cd\u6700\u4f73\u5b9e\u8df5\u6280\u672f\uff0c\u5982\u5171\u7f6e\u6a21\u578b\u5378\u8f7d\u3001\u5f02\u6b65\u79bb\u7b56\u7565\u8bad\u7ec3\u548c\u5206\u5e03\u5f0f\u76f4\u63a5\u5185\u5b58\u8bbf\u95ee\u7528\u4e8e\u6743\u91cd\u540c\u6b65\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e DeepSpeed-Chat \u7c7b\u7cfb\u7edf\u76f8\u6bd4\uff0cLlamaRL \u5728 405B \u53c2\u6570\u7684\u7b56\u7565\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad8 10.7 \u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u5e76\u4e14\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u52a0\uff0c\u6548\u7387\u4f18\u52bf\u8fdb\u4e00\u6b65\u6269\u5927\u3002", "conclusion": "LlamaRL \u662f\u4e00\u4e2a\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u7684\u9ad8\u6548\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5176\u5f02\u6b65\u8bbe\u8ba1\u5728\u7406\u8bba\u4e0a\u88ab\u8bc1\u660e\u80fd\u4e25\u683c\u52a0\u901f\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u3002"}}
{"id": "2505.23927", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.23927", "abs": "https://arxiv.org/abs/2505.23927", "authors": ["Songtao Feng", "Jie Fu"], "title": "Thompson Sampling in Online RLHF with General Function Approximation", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) has achieved great\nempirical success in aligning large language models (LLMs) with human\npreference, and it is of great importance to study the statistical efficiency\nof RLHF algorithms from a theoretical perspective. In this work, we consider\nthe online RLHF setting where the preference data is revealed during the\nlearning process and study action value function approximation. We design a\nmodel-free posterior sampling algorithm for online RLHF inspired by Thompson\nsampling and provide its theoretical guarantee. Specifically, we adopt Bellman\neluder (BE) dimension as the complexity measure of the function class and\nestablish $O(\\sqrt{T})$ regret bound for the proposed algorithm with other\nmultiplicative factor depending on the horizon, BE dimension and the\n$log$-bracketing number of the function class. Further, in the analysis, we\nfirst establish the concentration-type inequality of the squared Bellman error\nbound based on the maximum likelihood estimator (MLE) generalization bound,\nwhich plays the crucial rules in obtaining the eluder-type regret bound and may\nbe of independent interest.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53d7Thompson\u62bd\u6837\u542f\u53d1\u7684\u65e0\u6a21\u578b\u540e\u9a8c\u62bd\u6837\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4ece\u4eba\u7c7b\u53cd\u9988\u95ee\u9898\uff0c\u5e76\u7ed9\u51fa\u4e86\u5176\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u4ece\u4eba\u7c7b\u53cd\u9988\uff08RLHF\uff09\u5728\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4f46\u7814\u7a76RLHF\u7b97\u6cd5\u7684\u7edf\u8ba1\u6548\u7387\u4ecd\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u610f\u4e49\u3002", "method": "\u91c7\u7528Bellman eluder\uff08BE\uff09\u7ef4\u5ea6\u4f5c\u4e3a\u51fd\u6570\u7c7b\u7684\u590d\u6742\u6027\u5ea6\u91cf\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65e0\u6a21\u578b\u540e\u9a8c\u62bd\u6837\u7b97\u6cd5\uff0c\u5e76\u7ed9\u51fa\u4e86O(\u221aT)\u7684\u9057\u61be\u754c\u3002", "result": "\u8be5\u7814\u7a76\u4e3a\u5728\u7ebfRLHF\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u4e0a\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u5206\u6790\u4e2d\u9996\u6b21\u5efa\u7acb\u4e86\u57fa\u4e8eMLE\u7684\u5e73\u65b9Bellman\u8bef\u5dee\u754c\u7684\u96c6\u4e2d\u578b\u4e0d\u7b49\u5f0f\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u8bbe\u8ba1\u4e00\u79cd\u53d7Thompson\u62bd\u6837\u542f\u53d1\u7684\u65e0\u6a21\u578b\u540e\u9a8c\u62bd\u6837\u7b97\u6cd5\uff0c\u4e3a\u5728\u7ebfRLHF\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u9996\u6b21\u57fa\u4e8e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\uff08MLE\uff09\u6cdb\u5316\u754c\u5efa\u7acb\u4e86\u5e73\u65b9Bellman\u8bef\u5dee\u754c\u7684\u96c6\u4e2d\u578b\u4e0d\u7b49\u5f0f\uff0c\u8fd9\u5728\u83b7\u5f97eluder\u578b\u9057\u61be\u754c\u65b9\u9762\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u5e76\u53ef\u80fd\u5f15\u8d77\u72ec\u7acb\u5174\u8da3\u3002"}}
{"id": "2505.23816", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.23816", "abs": "https://arxiv.org/abs/2505.23816", "authors": ["Trenton Chang", "Tobias Schnabel", "Adith Swaminathan", "Jenna Wiens"], "title": "A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs", "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 8 figures. 26 pages of references and supplementary\n  material, 20 additional figures", "summary": "Despite advances in large language models (LLMs) on reasoning and\ninstruction-following benchmarks, it remains unclear whether they can reliably\nproduce outputs aligned with a broad variety of user goals, a concept we refer\nto as steerability. The abundance of methods proposed to modify LLM behavior\nmakes it unclear whether current LLMs are already steerable, or require further\nintervention. In particular, LLMs may exhibit (i) poor coverage, where rare\nuser goals are underrepresented; (ii) miscalibration, where models overshoot\nrequests; and (iii) side effects, where changes to one dimension of text\ninadvertently affect others. To systematically evaluate these failures, we\nintroduce a framework based on a multi-dimensional goal space that models user\ngoals and LLM outputs as vectors with dimensions corresponding to text\nattributes (e.g., reading difficulty). Applied to a text-rewriting task, we\nfind that current LLMs struggle with steerability, as side effects are\npersistent. Interventions to improve steerability, such as prompt engineering,\nbest-of-$N$ sampling, and reinforcement learning fine-tuning, have varying\neffectiveness, yet side effects remain problematic. Our findings suggest that\neven strong LLMs struggle with steerability, and existing alignment strategies\nmay be insufficient. We open-source our steerability evaluation framework at\nhttps://github.com/MLD3/steerability.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5b9e\u73b0\u7528\u6237\u76ee\u6807\u5bf9\u9f50\u65b9\u9762\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u8861\u91cf\u6a21\u578b\u7684\u53ef\u5f15\u5bfc\u6027\uff0c\u5e76\u53d1\u73b0\u5f53\u524d\u7684LLMs\u5728\u8fd9\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u95ee\u9898\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u5bf9\u9f50\u7b56\u7565\u4e5f\u96be\u4ee5\u5b8c\u5168\u89e3\u51b3\u3002", "motivation": "\u8bba\u6587\u7684\u4e3b\u8981\u52a8\u673a\u662f\u63a2\u8ba8\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u53ef\u9760\u5730\u751f\u6210\u7b26\u5408\u591a\u6837\u5316\u7528\u6237\u76ee\u6807\u7684\u8f93\u51fa\uff08\u5373steerability\uff09\u3002\u867d\u7136\u5df2\u6709\u8bb8\u591a\u65b9\u6cd5\u8bd5\u56fe\u4fee\u6539LLM\u7684\u884c\u4e3a\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u6a21\u578b\u672c\u8eab\u662f\u5426\u5df2\u7ecf\u8db3\u591f\u53ef\u63a7\uff0c\u8fd8\u662f\u9700\u8981\u66f4\u591a\u7684\u5e72\u9884\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u7ef4\u76ee\u6807\u7a7a\u95f4\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u8861\u91cfLLMs\u7684\u53ef\u5f15\u5bfc\u6027\u3002\u8be5\u65b9\u6cd5\u5c06\u7528\u6237\u76ee\u6807\u548c\u6a21\u578b\u8f93\u51fa\u5efa\u6a21\u4e3a\u6587\u672c\u5c5e\u6027\u7684\u5411\u91cf\uff0c\u5e76\u5e94\u7528\u4e8e\u6587\u672c\u91cd\u5199\u4efb\u52a1\u4e2d\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u8fd8\u6d4b\u8bd5\u4e86\u591a\u79cd\u63d0\u5347steerability\u7684\u65b9\u6cd5\uff0c\u5982\u63d0\u793a\u5de5\u7a0b\u3001\u6700\u4f73N\u90091\u91c7\u6837\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u76ee\u524d\u7684LLMs\u5728steerability\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u7f55\u89c1\u7528\u6237\u76ee\u6807\u65f6\u5bb9\u6613\u51fa\u73b0\u8986\u76d6\u4e0d\u8db3\u3001\u8fc7\u5ea6\u8c03\u6574\u4ee5\u53ca\u526f\u4f5c\u7528\u7b49\u95ee\u9898\u3002\u5c3d\u7ba1\u5c1d\u8bd5\u4e86\u591a\u79cd\u6539\u5584\u7b56\u7565\uff0c\u4f46\u526f\u4f5c\u7528\u4ecd\u7136\u662f\u4e00\u4e2a\u663e\u8457\u7684\u95ee\u9898\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7684\u7ed3\u8bba\u662f\uff0c\u5c3d\u7ba1\u5f53\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u7406\u548c\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u6ee1\u8db3\u591a\u6837\u5316\u7684\u7528\u6237\u76ee\u6807\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u5373\u2018steerability\u2019\u95ee\u9898\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7684\u6539\u8fdb\u7b56\u7565\u53ef\u80fd\u4e0d\u8db3\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u548c\u5e72\u9884\u63aa\u65bd\u3002"}}
{"id": "2505.23723", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.23723", "abs": "https://arxiv.org/abs/2505.23723", "authors": ["Zexi Liu", "Jingyi Chai", "Xinyu Zhu", "Shuo Tang", "Rui Ye", "Bo Zhang", "Lei Bai", "Siheng Chen"], "title": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The emergence of large language model (LLM)-based agents has significantly\nadvanced the development of autonomous machine learning (ML) engineering.\nHowever, most existing approaches rely heavily on manual prompt engineering,\nfailing to adapt and optimize based on diverse experimental experiences.\nFocusing on this, for the first time, we explore the paradigm of learning-based\nagentic ML, where an LLM agent learns through interactive experimentation on ML\ntasks using online reinforcement learning (RL). To realize this, we propose a\nnovel agentic ML training framework with three key components: (1)\nexploration-enriched fine-tuning, which enables LLM agents to generate diverse\nactions for enhanced RL exploration; (2) step-wise RL, which enables training\non a single action step, accelerating experience collection and improving\ntraining efficiency; (3) an agentic ML-specific reward module, which unifies\nvaried ML feedback signals into consistent rewards for RL optimization.\nLeveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM\nfor autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our\n7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it\nachieves continuous performance improvements and demonstrates exceptional\ncross-task generalization capabilities.", "keywords": ["LLM Reinforcement Learning"], "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u4ee3\u7406ML\u8303\u5f0f\u548c\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u4e3b\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u8fdb\u884c\u4ea4\u4e92\u5b9e\u9a8c\u5e76\u5b66\u4e60\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u5353\u8d8a\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u624b\u52a8\u63d0\u793a\u5de5\u7a0b\uff0c\u65e0\u6cd5\u6839\u636e\u4e0d\u540c\u7684\u5b9e\u9a8c\u7ecf\u9a8c\u8fdb\u884c\u81ea\u9002\u5e94\u548c\u4f18\u5316\u3002\u56e0\u6b64\uff0c\u6587\u7ae0\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u4ee3\u7406ML\u8303\u5f0f\uff0c\u4f7fLLM\u4ee3\u7406\u80fd\u591f\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728ML\u4efb\u52a1\u4e0a\u8fdb\u884c\u4ea4\u4e92\u5b9e\u9a8c\u5e76\u5b66\u4e60\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ee3\u7406ML\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\uff081\uff09\u63a2\u7d22\u589e\u5f3a\u5fae\u8c03\uff0c\u4f7fLLM\u4ee3\u7406\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u7684\u52a8\u4f5c\u4ee5\u589e\u5f3aRL\u63a2\u7d22\uff1b\uff082\uff09\u9010\u6b65RL\uff0c\u80fd\u591f\u5728\u5355\u4e2a\u52a8\u4f5c\u6b65\u9aa4\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u52a0\u901f\u7ecf\u9a8c\u6536\u96c6\u5e76\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff1b\uff083\uff09\u4ee3\u7406ML\u7279\u5b9a\u5956\u52b1\u6a21\u5757\uff0c\u5c06\u5404\u79cdML\u53cd\u9988\u4fe1\u53f7\u7edf\u4e00\u4e3a\u4e00\u81f4\u7684\u5956\u52b1\u7528\u4e8eRL\u4f18\u5316\u3002", "result": "\u5229\u7528\u63d0\u51fa\u7684\u6846\u67b6\uff0c\u4f5c\u8005\u8bad\u7ec3\u4e86\u4e00\u4e2a\u540d\u4e3aML-Agent\u7684\u4ee3\u7406\uff0c\u8be5\u4ee3\u7406\u57fa\u4e8e7B\u5927\u5c0f\u7684Qwen-2.5 LLM\u9a71\u52a8\uff0c\u7528\u4e8e\u81ea\u4e3bML\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u4ec5\u8bad\u7ec3\u4e869\u4e2aML\u4efb\u52a1\uff0c\u4f467B\u5927\u5c0f\u7684ML-Agent\u5728\u6027\u80fd\u4e0a\u8d85\u8fc7\u4e86671B\u5927\u5c0f\u7684DeepSeek-R1\u4ee3\u7406\u3002\u6b64\u5916\uff0cML-Agent\u8fd8\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5c55\u793a\u4e86\u51fa\u8272\u7684\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6587\u7ae0\u7684\u7ed3\u8bba\u662f\uff0c\u63d0\u51fa\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u4ee3\u7406ML\u8303\u5f0f\u548c\u8bad\u7ec3\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u4e3b\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u5c3d\u7ba1\u4ec5\u8bad\u7ec3\u4e869\u4e2aML\u4efb\u52a1\uff0c\u4f467B\u5927\u5c0f\u7684ML-Agent\u5728\u6027\u80fd\u4e0a\u8d85\u8fc7\u4e86671B\u5927\u5c0f\u7684DeepSeek-R1\u4ee3\u7406\uff0c\u5e76\u4e14\u5c55\u793a\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u548c\u8de8\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
