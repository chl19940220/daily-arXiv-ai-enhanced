{"id": "2505.00018", "keyword": "LLM Agent", "pdf": "https://arxiv.org/pdf/2505.00018", "abs": "https://arxiv.org/abs/2505.00018", "authors": ["Ju Wu", "Calvin K. L. Or"], "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability."}
{"id": "2410.15466", "keyword": "LLM reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing."}
{"id": "2402.00559", "keyword": "Chain of Thoughts", "pdf": "https://arxiv.org/pdf/2402.00559", "abs": "https://arxiv.org/abs/2402.00559", "authors": ["Alon Jacovi", "Yonatan Bitton", "Bernd Bohnet", "Jonathan Herzig", "Or Honovich", "Michael Tseng", "Michael Collins", "Roee Aharoni", "Mor Geva"], "title": "A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains", "categories": ["cs.CL"], "comment": "Accepted to ACL 2024", "summary": "Prompting language models to provide step-by-step answers (e.g.,\n\"Chain-of-Thought\") is the prominent approach for complex reasoning tasks,\nwhere more accurate reasoning chains typically improve downstream task\nperformance. Recent literature discusses automatic methods to verify reasoning\nto evaluate and improve their correctness. However, no fine-grained step-level\ndatasets are available to enable thorough evaluation of such verification\nmethods, hindering progress in this direction. We introduce REVEAL: Reasoning\nVerification Evaluation, a dataset to benchmark automatic verifiers of complex\nChain-of-Thought reasoning in open-domain question-answering settings. REVEAL\nincludes comprehensive labels for the relevance, attribution to evidence\npassages, and logical correctness of each reasoning step in a language model's\nanswer, across a variety of datasets and state-of-the-art language models.\nEvaluation on REVEAL shows that verifiers struggle at verifying reasoning\nchains - in particular, verifying logical correctness and detecting\ncontradictions. Available at https://reveal-dataset.github.io/ ."}
{"id": "2505.04493", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2505.04493", "abs": "https://arxiv.org/abs/2505.04493", "authors": ["Or Wertheim", "Ronen I. Brafman"], "title": "Model-Based AI planning and Execution Systems for Robotics", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Model-based planning and execution systems offer a principled approach to\nbuilding flexible autonomous robots that can perform diverse tasks by\nautomatically combining a host of basic skills. This idea is almost as old as\nmodern robotics. Yet, while diverse general-purpose reasoning architectures\nhave been proposed since, general-purpose systems that are integrated with\nmodern robotic platforms have emerged only recently, starting with the\ninfluential ROSPlan system. Since then, a growing number of model-based systems\nfor robot task-level control have emerged. In this paper, we consider the\ndiverse design choices and issues existing systems attempt to address, the\ndifferent solutions proposed so far, and suggest avenues for future\ndevelopment."}
{"id": "2412.02441", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2412.02441", "abs": "https://arxiv.org/abs/2412.02441", "authors": ["Shai Shalev-Shwartz", "Amnon Shashua", "Gal Beniamini", "Yoav Levine", "Or Sharir", "Noam Wies", "Ido Ben-Shaul", "Tomer Nussbaum", "Shir Granot Peled"], "title": "Artificial Expert Intelligence through PAC-reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Artificial Expert Intelligence (AEI) seeks to transcend the limitations of\nboth Artificial General Intelligence (AGI) and narrow AI by integrating\ndomain-specific expertise with critical, precise reasoning capabilities akin to\nthose of top human experts. Existing AI systems often excel at predefined tasks\nbut struggle with adaptability and precision in novel problem-solving. To\novercome this, AEI introduces a framework for ``Probably Approximately Correct\n(PAC) Reasoning\". This paradigm provides robust theoretical guarantees for\nreliably decomposing complex problems, with a practical mechanism for\ncontrolling reasoning precision. In reference to the division of human thought\ninto System 1 for intuitive thinking and System 2 for reflective\nreasoning~\\citep{tversky1974judgment}, we refer to this new type of reasoning\nas System 3 for precise reasoning, inspired by the rigor of the scientific\nmethod. AEI thus establishes a foundation for error-bounded, inference-time\nlearning."}
{"id": "2410.15466", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2410.15466", "abs": "https://arxiv.org/abs/2410.15466", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing."}
{"id": "2409.04397", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2409.04397", "abs": "https://arxiv.org/abs/2409.04397", "authors": ["Yotam Erel", "Or Kozlovsky-Mordenfeld", "Daisuke Iwai", "Kosuke Sato", "Amit H. Bermano"], "title": "Casper DPM: Cascaded Perceptual Dynamic Projection Mapping onto Hands", "categories": ["cs.GR"], "comment": "Project page: https://yoterel.github.io/casper-project-page/", "summary": "We present a technique for dynamically projecting 3D content onto human hands\nwith short perceived motion-to-photon latency. Computing the pose and shape of\nhuman hands accurately and quickly is a challenging task due to their\narticulated and deformable nature. We combine a slower 3D coarse estimation of\nthe hand pose with high speed 2D correction steps which improve the alignment\nof the projection to the hands, increase the projected surface area, and reduce\nperceived latency. Since our approach leverages a full 3D reconstruction of the\nhands, any arbitrary texture or reasonably performant effect can be applied,\nwhich was not possible before. We conducted two user studies to assess the\nbenefits of using our method. The results show subjects are less sensitive to\nlatency artifacts and perform faster and with more ease a given associated task\nover the naive approach of directly projecting rendered frames from the 3D pose\nestimation. We demonstrate several novel use cases and applications."}
{"id": "2506.01523", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2506.01523", "abs": "https://arxiv.org/abs/2506.01523", "authors": ["Jihun Yun", "Juno Kim", "Jongho Park", "Junhyuck Kim", "Jongha Jon Ryu", "Jaewoong Cho", "Kwang-Sung Jun"], "title": "Alignment as Distribution Learning: Your Preference Model is Explicitly a Language Model", "categories": ["cs.LG", "stat.ML"], "comment": "26 pages, 7 tables", "summary": "Alignment via reinforcement learning from human feedback (RLHF) has become\nthe dominant paradigm for controlling the quality of outputs from large\nlanguage models (LLMs). However, when viewed as `loss + regularization,' the\nstandard RLHF objective lacks theoretical justification and incentivizes\ndegenerate, deterministic solutions, an issue that variants such as Direct\nPolicy Optimization (DPO) also inherit. In this paper, we rethink alignment by\nframing it as \\emph{distribution learning} from pairwise preference feedback by\nexplicitly modeling how information about the target language model bleeds\nthrough the preference data. This explicit modeling leads us to propose three\nprincipled learning objectives: preference maximum likelihood estimation,\npreference distillation, and reverse KL minimization. We theoretically show\nthat all three approaches enjoy strong non-asymptotic $O(1/n)$ convergence to\nthe target language model, naturally avoiding degeneracy and reward\noverfitting. Finally, we empirically demonstrate that our distribution learning\nframework, especially preference distillation, consistently outperforms or\nmatches the performances of RLHF and DPO across various tasks and models."}
{"id": "2408.08654", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2408.08654", "abs": "https://arxiv.org/abs/2408.08654", "authors": ["Li-Fang Zhu", "Fritz Koermann", "Qing Chen", "Malin Selleby", "Joerg Neugebauer", "and Blazej Grabowski"], "title": "Accelerating ab initio melting property calculations with machine learning: Application to the high entropy alloy TaVCrW", "categories": ["cond-mat.mtrl-sci"], "comment": "14 pages, 6 figures", "summary": "Melting properties are critical for designing novel materials, especially for\ndiscovering high-performance, high-melting refractory materials. Experimental\nmeasurements of these properties are extremely challenging due to their high\nmelting temperatures. Complementary theoretical predictions are, therefore,\nindispensable. The conventional free energy approach using density functional\ntheory (DFT) has been a gold standard for such purposes because of its high\naccuracy. However,it generally involves expensive thermodynamic integration\nusing ab initio molecular dynamic simulations. The high computational cost\nmakes high-throughput calculations infeasible. Here, we propose a highly\nefficient DFT-based method aided by a specially designed machine learning\npotential. As the machine learning potential can closely reproduce the ab\ninitio phase space, even for multi-component alloys, the costly thermodynamic\nintegration can be fully substituted with more efficient free energy\nperturbation calculations. The method achieves overall savings of computational\nresources by 80% compared to current alternatives. We apply the method to the\nhigh-entropy alloy TaVCrW and calculate its melting properties, including\nmelting temperature, entropy and enthalpy of fusion, and volume change at the\nmelting point. Additionally, the heat capacities of solid and liquid TaVCrW are\ncalculated. The results agree reasonably with the calphad extrapolated values."}
{"id": "2506.01413", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2506.01413", "abs": "https://arxiv.org/abs/2506.01413", "authors": ["Yulei Qin", "Gang Li", "Zongyi Li", "Zihan Xu", "Yuchen Shi", "Zhekai Lin", "Xiao Cui", "Ke Li", "Xing Sun"], "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "10 pages of main body, 3 tables, 5 figures, 40 pages of appendix", "summary": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nare available at https://github.com/yuleiqin/RAIF."}
{"id": "2403.11729", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2403.11729", "abs": "https://arxiv.org/abs/2403.11729", "authors": ["Kento Kawaharazuka", "Akihiro Miki", "Masahiro Bando", "Temma Suzuki", "Yoshimoto Ribayashi", "Yasunori Toshimitsu", "Yuya Nagamatsu", "Kei Okada", "and Masayuki Inaba"], "title": "Hardware Design and Learning-Based Software Architecture of Musculoskeletal Wheeled Robot Musashi-W for Real-World Applications", "categories": ["cs.RO"], "comment": "Accepted at Humanoids2022", "summary": "Various musculoskeletal humanoids have been developed so far. While these\nhumanoids have the advantage of their flexible and redundant bodies that mimic\nthe human body, they are still far from being applied to real-world tasks. One\nof the reasons for this is the difficulty of bipedal walking in a flexible\nbody. Thus, we developed a musculoskeletal wheeled robot, Musashi-W, by\ncombining a wheeled base and musculoskeletal upper limbs for real-world\napplications. Also, we constructed its software system by combining static and\ndynamic body schema learning, reflex control, and visual recognition. We show\nthat the hardware and software of Musashi-W can make the most of the advantages\nof the musculoskeletal upper limbs, through several tasks of cleaning by human\nteaching, carrying a heavy object considering muscle addition, and setting a\ntable through dynamic cloth manipulation with variable stiffness."}
{"id": "2506.01369", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2506.01369", "abs": "https://arxiv.org/abs/2506.01369", "authors": ["Fuxiang Zhang", "Jiacheng Xu", "Chaojie Wang", "Ce Cui", "Yang Liu", "Bo An"], "title": "Incentivizing LLMs to Self-Verify Their Answers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in complex\nreasoning tasks through both post-training and test-time scaling laws. While\nprevalent test-time scaling approaches are often realized by using external\nreward models to guide the model generation process, we find only marginal\ngains can be acquired when scaling a model post-trained on specific reasoning\ntasks. We identify that the limited improvement stems from distribution\ndiscrepancies between the specific post-trained generator and the general\nreward model. To address this, we propose a framework that incentivizes LLMs to\nself-verify their own answers. By unifying answer generation and verification\nwithin a single reinforcement learning (RL) process, we train models that can\neffectively assess the correctness of their own solutions. The trained model\ncan further scale its performance during inference time by verifying its\ngenerations, without the need for external verifiers. We train our\nself-verification models based on Qwen2.5-Math-7B and\nDeepSeek-R1-Distill-Qwen-1.5B, demonstrating its capabilities across varying\nreasoning context lengths. Experiments on multiple mathematical reasoning\nbenchmarks show that our models can not only improve post-training performance\nbut also enable effective test-time scaling. Our code is available at\nhttps://github.com/mansicer/self-verification."}
{"id": "2403.00458", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2403.00458", "abs": "https://arxiv.org/abs/2403.00458", "authors": ["Chung Yi See", "Vasco Rato Santos", "Lucas Woodley", "Megan Yeo", "Daniel Palmer", "Shuheng Zhang", "and Ashley Nunes"], "title": "Prices and preferences in the electric vehicle market", "categories": ["econ.EM"], "comment": "Main paper: 5 tables, 2 figures", "summary": "Although electric vehicles are less polluting than gasoline powered vehicles,\nadoption is challenged by higher procurement prices. Existing discourse\nemphasizes EV battery costs as being principally responsible for this price\ndifferential and widespread adoption is routinely conditioned upon battery\ncosts declining. We scrutinize such reasoning by sourcing data on EV attributes\nand market conditions between 2011 and 2023. Our findings are fourfold. First,\nEV prices are influenced principally by the number of amenities, additional\nfeatures, and dealer-installed accessories sold as standard on an EV, and to a\nlesser extent, by EV horsepower. Second, EV range is negatively correlated with\nEV price implying that range anxiety concerns may be less consequential than\nexisting discourse suggests. Third, battery capacity is positively correlated\nwith EV price, due to more capacity being synonymous with the delivery of more\nhorsepower. Collectively, this suggests that higher procurement prices for EVs\nreflects consumer preference for vehicles that are feature dense and more\npowerful. Fourth and finally, accommodating these preferences have produced\nvehicles with lower fuel economy, a shift that reduces envisioned lifecycle\nemissions benefits by at least 3.26 percent, subject to the battery pack\nchemistry leveraged and the carbon intensity of the electrical grid. These\nfindings warrant attention as decarbonization efforts increasingly emphasize\nelectrification as a pathway for complying with domestic and international\nclimate agreements."}
{"id": "2506.00845", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2506.00845", "abs": "https://arxiv.org/abs/2506.00845", "authors": ["Yizhuo Zhang", "Heng Wang", "Shangbin Feng", "Zhaoxuan Tan", "Xinyun Liu", "Yulia Tsvetkov"], "title": "Generalizable LLM Learning of Graph Synthetic Data with Reinforcement Learning", "categories": ["cs.LG", "cs.CL", "I.2.7; I.2.2"], "comment": "9 pages, 3 figures, 3 tables. Experimental code and results are\n  publicly available at\n  https://anonymous.4open.science/r/Graph_RL-BF08/readme.md", "summary": "Previous research has sought to enhance the graph reasoning capabilities of\nLLMs by supervised fine-tuning on synthetic graph data. While these led to\nspecialized LLMs better at solving graph algorithm problems, we don't need LLMs\nfor shortest path: we need generalization from synthetic graph data to\nreal-world tasks with implicit graph structures. In this work, we propose to\nunlock generalizable learning of graph synthetic data with reinforcement\nlearning. We first design solution-based and process-based rewards for\nsynthetic graph problems: instead of rigid memorizing response patterns in\ndirect fine-tuning, we posit that RL would help LLMs grasp the essentials\nunderlying graph reasoning and alleviate overfitting. We employ RL algorithms\nsuch as GRPO and DPO, aligning both off-the-shelf LLMs and LLMs fine-tuned on\nsynthetic graph data. We then compare them against existing settings on both\nin-domain synthetic tasks and out-of-domain real-world tasks with implicit\ngraph structures such as multi-hop QA, structured planning, and more. Extensive\nexperiments demonstrate that our RL recipe leads to statistically significant\nimprovement on 5 datasets, with an average gain of 12.9\\% over baseline\nsettings. Further analysis reveals that process-based rewards consistently\noutperform solution-based rewards, mixing synthetic and real-world task data\nyields potential gains, while compositionality and explainable intermediate\nsteps remains a critical challenge even after RL."}
{"id": "2402.00559", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2402.00559", "abs": "https://arxiv.org/abs/2402.00559", "authors": ["Alon Jacovi", "Yonatan Bitton", "Bernd Bohnet", "Jonathan Herzig", "Or Honovich", "Michael Tseng", "Michael Collins", "Roee Aharoni", "Mor Geva"], "title": "A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains", "categories": ["cs.CL"], "comment": "Accepted to ACL 2024", "summary": "Prompting language models to provide step-by-step answers (e.g.,\n\"Chain-of-Thought\") is the prominent approach for complex reasoning tasks,\nwhere more accurate reasoning chains typically improve downstream task\nperformance. Recent literature discusses automatic methods to verify reasoning\nto evaluate and improve their correctness. However, no fine-grained step-level\ndatasets are available to enable thorough evaluation of such verification\nmethods, hindering progress in this direction. We introduce REVEAL: Reasoning\nVerification Evaluation, a dataset to benchmark automatic verifiers of complex\nChain-of-Thought reasoning in open-domain question-answering settings. REVEAL\nincludes comprehensive labels for the relevance, attribution to evidence\npassages, and logical correctness of each reasoning step in a language model's\nanswer, across a variety of datasets and state-of-the-art language models.\nEvaluation on REVEAL shows that verifiers struggle at verifying reasoning\nchains - in particular, verifying logical correctness and detecting\ncontradictions. Available at https://reveal-dataset.github.io/ ."}
{"id": "2506.00576", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2506.00576", "abs": "https://arxiv.org/abs/2506.00576", "authors": ["Fatemeh Lotfi", "Hossein Rajoli", "Fatemeh Afghah"], "title": "ORAN-GUIDE: RAG-Driven Prompt Learning for LLM-Augmented Reinforcement Learning in O-RAN Network Slicing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Advanced wireless networks must support highly dynamic and heterogeneous\nservice demands. Open Radio Access Network (O-RAN) architecture enables this\nflexibility by adopting modular, disaggregated components, such as the RAN\nIntelligent Controller (RIC), Centralized Unit (CU), and Distributed Unit (DU),\nthat can support intelligent control via machine learning (ML). While deep\nreinforcement learning (DRL) is a powerful tool for managing dynamic resource\nallocation and slicing, it often struggles to process raw, unstructured input\nlike RF features, QoS metrics, and traffic trends. These limitations hinder\npolicy generalization and decision efficiency in partially observable and\nevolving environments. To address this, we propose \\textit{ORAN-GUIDE}, a\ndual-LLM framework that enhances multi-agent RL (MARL) with task-relevant,\nsemantically enriched state representations. The architecture employs a\ndomain-specific language model, ORANSight, pretrained on O-RAN control and\nconfiguration data, to generate structured, context-aware prompts. These\nprompts are fused with learnable tokens and passed to a frozen GPT-based\nencoder that outputs high-level semantic representations for DRL agents. This\ndesign adopts a retrieval-augmented generation (RAG) style pipeline tailored\nfor technical decision-making in wireless systems. Experimental results show\nthat ORAN-GUIDE improves sample efficiency, policy convergence, and performance\ngeneralization over standard MARL and single-LLM baselines."}
{"id": "2208.08580", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2208.08580", "abs": "https://arxiv.org/abs/2208.08580", "authors": ["Gopal Sharma", "Kangxue Yin", "Subhransu Maji", "Evangelos Kalogerakis", "Or Litany", "Sanja Fidler"], "title": "MvDeCor: Multi-view Dense Correspondence Learning for Fine-grained 3D Segmentation", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "project page: https://nv-tlabs.github.io/MvDeCor/", "summary": "We propose to utilize self-supervised techniques in the 2D domain for\nfine-grained 3D shape segmentation tasks. This is inspired by the observation\nthat view-based surface representations are more effective at modeling\nhigh-resolution surface details and texture than their 3D counterparts based on\npoint clouds or voxel occupancy. Specifically, given a 3D shape, we render it\nfrom multiple views, and set up a dense correspondence learning task within the\ncontrastive learning framework. As a result, the learned 2D representations are\nview-invariant and geometrically consistent, leading to better generalization\nwhen trained on a limited number of labeled shapes compared to alternatives\nthat utilize self-supervision in 2D or 3D alone. Experiments on textured\n(RenderPeople) and untextured (PartNet) 3D datasets show that our method\noutperforms state-of-the-art alternatives in fine-grained part segmentation.\nThe improvements over baselines are greater when only a sparse set of views is\navailable for training or when shapes are textured, indicating that MvDeCor\nbenefits from both 2D processing and 3D geometric reasoning."}
{"id": "2506.00574", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2506.00574", "abs": "https://arxiv.org/abs/2506.00574", "authors": ["Fatemeh Lotfi", "Hossein Rajoli", "Fatemeh Afghah"], "title": "Prompt-Tuned LLM-Augmented DRL for Dynamic O-RAN Network Slicing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Modern wireless networks must adapt to dynamic conditions while efficiently\nmanaging diverse service demands. Traditional deep reinforcement learning (DRL)\nstruggles in these environments, as scattered and evolving feedback makes\noptimal decision-making challenging. Large Language Models (LLMs) offer a\nsolution by structuring unorganized network feedback into meaningful latent\nrepresentations, helping RL agents recognize patterns more effectively. For\nexample, in O-RAN slicing, concepts like SNR, power levels and throughput are\nsemantically related, and LLMs can naturally cluster them, providing a more\ninterpretable state representation. To leverage this capability, we introduce a\ncontextualization-based adaptation method that integrates learnable prompts\ninto an LLM-augmented DRL framework. Instead of relying on full model\nfine-tuning, we refine state representations through task-specific prompts that\ndynamically adjust to network conditions. Utilizing ORANSight, an LLM trained\non O-RAN knowledge, we develop Prompt-Augmented Multi agent RL (PA-MRL)\nframework. Learnable prompts optimize both semantic clustering and RL\nobjectives, allowing RL agents to achieve higher rewards in fewer iterations\nand adapt more efficiently. By incorporating prompt-augmented learning, our\napproach enables faster, more scalable, and adaptive resource allocation in\nO-RAN slicing. Experimental results show that it accelerates convergence and\noutperforms other baselines."}
{"id": "2110.02210", "keyword": "Reasoning", "pdf": "https://arxiv.org/pdf/2110.02210", "abs": "https://arxiv.org/abs/2110.02210", "authors": ["Alexey Nekrasov", "Jonas Schult", "Or Litany", "Bastian Leibe", "Francis Engelmann"], "title": "Mix3D: Out-of-Context Data Augmentation for 3D Scenes", "categories": ["cs.CV"], "comment": "Accepted for publication at 3DV 2021. Camera-ready submission. Link\n  to code: https://github.com/kumuji/mix3d - Project page:\n  https://nekrasov.dev/mix3d/", "summary": "We present Mix3D, a data augmentation technique for segmenting large-scale 3D\nscenes. Since scene context helps reasoning about object semantics, current\nworks focus on models with large capacity and receptive fields that can fully\ncapture the global context of an input 3D scene. However, strong contextual\npriors can have detrimental implications like mistaking a pedestrian crossing\nthe street for a car. In this work, we focus on the importance of balancing\nglobal scene context and local geometry, with the goal of generalizing beyond\nthe contextual priors in the training set. In particular, we propose a \"mixing\"\ntechnique which creates new training samples by combining two augmented scenes.\nBy doing so, object instances are implicitly placed into novel out-of-context\nenvironments and therefore making it harder for models to rely on scene context\nalone, and instead infer semantics from local structure as well. We perform\ndetailed analysis to understand the importance of global context, local\nstructures and the effect of mixing scenes. In experiments, we show that models\ntrained with Mix3D profit from a significant performance boost on indoor\n(ScanNet, S3DIS) and outdoor datasets (SemanticKITTI). Mix3D can be trivially\nused with any existing method, e.g., trained with Mix3D, MinkowskiNet\noutperforms all prior state-of-the-art methods by a significant margin on the\nScanNet test benchmark 78.1 mIoU. Code is available at:\nhttps://nekrasov.dev/mix3d/"}
{"id": "2506.00439", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2506.00439", "abs": "https://arxiv.org/abs/2506.00439", "authors": ["Yuqian Fu", "Yuanheng Zhu", "Jiajun Chai", "Guojun Yin", "Wei Lin", "Qichao Zhang", "Dongbin Zhao"], "title": "RLAE: Reinforcement Learning-Assisted Ensemble for LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Ensembling large language models (LLMs) can effectively combine diverse\nstrengths of different models, offering a promising approach to enhance\nperformance across various tasks. However, existing methods typically rely on\nfixed weighting strategies that fail to adapt to the dynamic, context-dependent\ncharacteristics of LLM capabilities. In this work, we propose Reinforcement\nLearning-Assisted Ensemble for LLMs (RLAE), a novel framework that reformulates\nLLM ensemble through the lens of a Markov Decision Process (MDP). Our approach\nintroduces a RL agent that dynamically adjusts ensemble weights by considering\nboth input context and intermediate generation states, with the agent being\ntrained using rewards that directly correspond to the quality of final outputs.\nWe implement RLAE using both single-agent and multi-agent reinforcement\nlearning algorithms ($\\text{RLAE}_\\text{PPO}$ and $\\text{RLAE}_\\text{MAPPO}$ ),\ndemonstrating substantial improvements over conventional ensemble methods.\nExtensive evaluations on a diverse set of tasks show that RLAE outperforms\nexisting approaches by up to $3.3\\%$ accuracy points, offering a more effective\nframework for LLM ensembling. Furthermore, our method exhibits superior\ngeneralization capabilities across different tasks without the need for\nretraining, while simultaneously achieving lower time latency."}
{"id": "2505.24871", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.24871", "abs": "https://arxiv.org/abs/2505.24871", "authors": ["Yiqing Liang", "Jielin Qiu", "Wenhao Ding", "Zuxin Liu", "James Tompkin", "Mengdi Xu", "Mengzhou Xia", "Zhengzhong Tu", "Laixi Shi", "Jiacheng Zhu"], "title": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Project Webpage: https://modomodo-rl.github.io/", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for post-training large language models (LLMs), achieving\nstate-of-the-art performance on tasks with structured, verifiable answers.\nApplying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but\nis complicated by the broader, heterogeneous nature of vision-language tasks\nthat demand nuanced visual, logical, and spatial capabilities. As such,\ntraining MLLMs using RLVR on multiple datasets could be beneficial but creates\nchallenges with conflicting objectives from interaction among diverse datasets,\nhighlighting the need for optimal dataset mixture strategies to improve\ngeneralization and reasoning. We introduce a systematic post-training framework\nfor Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation\nand benchmark implementation. Specifically, (1) We developed a multimodal RLVR\nframework for multi-dataset post-training by curating a dataset that contains\ndifferent verifiable vision-language problems and enabling multi-domain online\nRL learning with different verifiable rewards; (2) We proposed a data mixture\nstrategy that learns to predict the RL fine-tuning outcome from the data\nmixture distribution, and consequently optimizes the best mixture.\nComprehensive experiments showcase that multi-domain RLVR training, when\ncombined with mixture prediction strategies, can significantly boost MLLM\ngeneral reasoning capacities. Our best mixture improves the post-trained\nmodel's accuracy on out-of-distribution benchmarks by an average of 5.24%\ncompared to the same model post-trained with uniform data mixture, and by a\ntotal of 20.74% compared to the pre-finetuning baseline."}
{"id": "2505.24850", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.24850", "abs": "https://arxiv.org/abs/2505.24850", "authors": ["Shuyao Xu", "Cheng Peng", "Jiangxuan Long", "Weidi Xu", "Wei Chu", "Yuan Qi"], "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "comment": "27 pages, 10 figures. Code available at\n  https://github.com/Tim-Siu/reinforcement-distillation", "summary": "Recent advances in model distillation demonstrate that data from advanced\nreasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer\ncomplex reasoning abilities to smaller, efficient student models. However,\nstandard practices employ rejection sampling, discarding incorrect reasoning\nexamples -- valuable, yet often underutilized data. This paper addresses the\ncritical question: How can both positive and negative distilled reasoning\ntraces be effectively leveraged to maximize LLM reasoning performance in an\noffline setting? To this end, We propose Reinforcement Distillation (REDI), a\ntwo-stage framework. Stage 1 learns from positive traces via Supervised\nFine-Tuning (SFT). Stage 2 further refines the model using both positive and\nnegative traces through our proposed REDI objective. This novel objective is a\nsimple, reference-free loss function that outperforms established methods like\nDPO and SimPO in this distillation context. Our empirical evaluations\ndemonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT\ncombined with DPO/SimPO on mathematical reasoning tasks. Notably, the\nQwen-REDI-1.5B model, post-trained on just 131k positive and negative examples\nfrom the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).\nIts performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a\nmodel post-trained on 800k proprietary data) across various mathematical\nreasoning benchmarks, establishing a new state-of-the-art for 1.5B models\npost-trained offline with openly available data."}
{"id": "2505.24710", "keyword": "LLM Reinforcement Learning", "pdf": "https://arxiv.org/pdf/2505.24710", "abs": "https://arxiv.org/abs/2505.24710", "authors": ["Wei Chen", "Jiahao Zhang", "Haipeng Zhu", "Boyan Xu", "Zhifeng Hao", "Keli Zhang", "Junjian Ye", "Ruichu Cai"], "title": "Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by IJCAI 2025", "summary": "Large language models (LLMs) have shown great potential in decision-making\ndue to the vast amount of knowledge stored within the models. However, these\npre-trained models are prone to lack reasoning abilities and are difficult to\nadapt to new environments, further hindering their application to complex\nreal-world tasks. To address these challenges, inspired by the human cognitive\nprocess, we propose Causal-aware LLMs, which integrate the structural causal\nmodel (SCM) into the decision-making process to model, update, and utilize\nstructured knowledge of the environment in a ``learning-adapting-acting\"\nparadigm. Specifically, in the learning stage, we first utilize an LLM to\nextract the environment-specific causal entities and their causal relations to\ninitialize a structured causal model of the environment. Subsequently,in the\nadapting stage, we update the structured causal model through external feedback\nabout the environment, via an idea of causal intervention. Finally, in the\nacting stage, Causal-aware LLMs exploit structured causal knowledge for more\nefficient policy-making through the reinforcement learning agent. The above\nprocesses are performed iteratively to learn causal knowledge, ultimately\nenabling the causal-aware LLMs to achieve a more accurate understanding of the\nenvironment and make more efficient decisions. Experimental results across 22\ndiverse tasks within the open-world game ``Crafter\" validate the effectiveness\nof our proposed method."}
