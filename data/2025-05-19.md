<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.CL](#cs.CL) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: 本文综述了人类与AI智能体协作的最新实证进展，指出了技术成就与持续存在的差距，并提出了一种新的概念架构以系统整合多智能体协调、知识管理、反馈循环和高级控制机制。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个统一的理论框架来整合各种关于人类-AI协作的研究，特别是在处理开放性复杂任务时。

Method: 提出了一种新的概念架构（Hierarchical Exploration-Exploitation Net），将现有的贡献（包括符号AI技术、连接主义LLM智能体和混合组织实践）映射到该框架中，从而促进传统方法的改进并激发新的研究方向。

Result: 通过新框架，文章为修订传统方法和融合定性和定量范式提供了思路，并促进了人类认知与AI能力的共同进化。

Conclusion: 本文为设计或扩展人类-AI共生关系提供了批判性回顾和前瞻性参考，推动了人类认知与AI能力的深度协同进化。

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [2] [Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs](https://arxiv.org/abs/2505.11227)
*Zhangying Feng,Qianglong Chen,Ning Lu,Yongqian Li,Siqi Cheng,Shuangmu Peng,Duyu Tang,Shengcai Liu,Zhirui Zhang*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在推理能力发展方面，强化学习（RL）和过程奖励模型（PRMs）是主要方法。研究表明，纯RL训练可以提升推理能力而不依赖PRM，问题解决能力和过程监督能力在RL训练中协同进化。当前PRM表现不如简单基线，提出Self-PRM框架通过自奖励机制改进，但面临难题精度低的问题。整体发现表明，纯RL可能足以增强复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习（RL）和过程奖励模型（PRM）在提升大型语言模型推理能力中的作用，特别是评估纯RL训练是否可以独立提升推理能力而无需PRM的介入。

Method: 通过系统研究RL训练和PRM能力之间的关系，使用DeepSeek-R1等模型进行实验，比较问题解决能力和过程监督能力的发展，并引入Self-PRM框架以改进PRM表现。

Result: 发现问题解决能力和过程监督能力在RL训练中协同进化，当前PRM表现不如简单基线，Self-PRM框架能提高基准准确率，但在难题上精度较低。

Conclusion: 纯RL训练不仅提升了解决问题的能力，还内在地培养了稳健的PRM能力，PRM可能不是增强复杂推理的必要条件。

Abstract: The development of reasoning capabilities represents a critical frontier in
large language models (LLMs) research, where reinforcement learning (RL) and
process reward models (PRMs) have emerged as predominant methodological
frameworks. Contrary to conventional wisdom, empirical evidence from
DeepSeek-R1 demonstrates that pure RL training focused on mathematical
problem-solving can progressively enhance reasoning abilities without PRM
integration, challenging the perceived necessity of process supervision. In
this study, we conduct a systematic investigation of the relationship between
RL training and PRM capabilities. Our findings demonstrate that problem-solving
proficiency and process supervision capabilities represent complementary
dimensions of reasoning that co-evolve synergistically during pure RL training.
In particular, current PRMs underperform simple baselines like majority voting
when applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To
address this limitation, we propose Self-PRM, an introspective framework in
which models autonomously evaluate and rerank their generated solutions through
self-reward mechanisms. Although Self-PRM consistently improves the accuracy of
the benchmark (particularly with larger sample sizes), analysis exposes
persistent challenges: The approach exhibits low precision (<10\%) on difficult
problems, frequently misclassifying flawed solutions as valid. These analyses
underscore the need for continued RL scaling to improve reward alignment and
introspective accuracy. Overall, our findings suggest that PRM may not be
essential for enhancing complex reasoning, as pure RL not only improves
problem-solving skills but also inherently fosters robust PRM capabilities. We
hope these findings provide actionable insights for building more reliable and
self-aware complex reasoning models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Visual Planning: Let's Think Only with Images](https://arxiv.org/abs/2505.11409)
*Yi Xu,Chengzu Li,Han Zhou,Xingchen Wan,Caiqi Zhang,Anna Korhonen,Ivan Vulić*

Main category: cs.LG

TL;DR: 近期大型语言模型（LLMs）及其多模态扩展（MLLMs）在各种任务中显著提升了机器推理能力。然而，即使存在视觉信息，这些模型仍然主要依赖纯文本进行推理表达和结构化。本文提出了一种新范式——视觉规划（Visual Planning），通过纯粹的视觉表示进行规划，无需文本支持。我们引入了一种新的强化学习框架VPRL，利用GRPO对大型视觉模型进行后训练，在几个代表性视觉导航任务中取得了显著改进。实验结果表明，视觉规划范式优于所有仅基于文本推理的规划变体，为需要直观图像推理的任务开辟了新方向。


<details>
  <summary>Details</summary>
Motivation: 尽管当前的LLMs和MLLMs在许多任务上表现出色，但它们过于依赖纯文本作为推理媒介，即使在涉及空间和几何信息的任务中也是如此。这可能并非最自然或有效的推理方式，因此需要探索一种更适合视觉信息处理的新方法。

Method: 提出了视觉规划（Visual Planning）这一新范式，通过一系列图像编码逐步推理解析过程，类似于人类通过草图或可视化未来行动的方式。同时，引入了一种新的强化学习框架VPRL，并结合GRPO技术对大型视觉模型进行后训练，以提升其在视觉导航任务中的表现。

Result: 视觉规划范式在FrozenLake、Maze和MiniBehavior等任务中表现出色，优于所有仅基于文本推理的规划变体。

Conclusion: 视觉规划是一种可行且有前景的语言推理替代方案，特别适用于从直观图像推理中受益的任务，为未来的多模态推理研究提供了新思路。

Abstract: Recent advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have substantially enhanced machine reasoning across diverse
tasks. However, these models predominantly rely on pure text as the medium for
both expressing and structuring reasoning, even when visual information is
present. In this work, we argue that language may not always be the most
natural or effective modality for reasoning, particularly in tasks involving
spatial and geometrical information. Motivated by this, we propose a new
paradigm, Visual Planning, which enables planning through purely visual
representations, independent of text. In this paradigm, planning is executed
via sequences of images that encode step-by-step inference in the visual
domain, akin to how humans sketch or visualize future actions. We introduce a
novel reinforcement learning framework, Visual Planning via Reinforcement
Learning (VPRL), empowered by GRPO for post-training large vision models,
leading to substantial improvements in planning in a selection of
representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our
visual planning paradigm outperforms all other planning variants that conduct
reasoning in the text-only space. Our results establish Visual Planning as a
viable and promising alternative to language-based reasoning, opening new
avenues for tasks that benefit from intuitive, image-based inference.

</details>


### [4] [ShiQ: Bringing back Bellman to LLMs](https://arxiv.org/abs/2505.11081)
*Pierre Clavier,Nathan Grinsztajn,Raphael Avalos,Yannis Flet-Berliac,Irem Ergun,Omar D. Domingues,Eugene Tarassov,Olivier Pietquin,Pierre H. Richemond,Florian Strub,Matthieu Geist*

Main category: cs.LG

TL;DR: 本文研究了将Q学习方法应用于预训练大语言模型（LLMs）微调的可行性，提出了基于贝尔曼方程的理论损失函数，并开发了一种名为ShiQ的算法，该算法在单轮和多轮LLM设置中均表现出有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习中的直接策略优化被广泛用于LLM的微调，但Q学习方法在非LLM任务中取得了显著成功，其样本效率和离线学习能力对计算成本高的LLM特别有价值。因此，探索如何将Q学习方法有效应用于LLM具有重要意义。

Method: 作者从贝尔曼方程出发，推导出适应于LLM的理论损失函数，解决了直接应用Q学习更新模型logits无效的问题。基于此损失函数，提出了一种名为ShiQ（Shifted-Q）的实用算法，支持离策略、逐标记学习且易于实现。

Result: ShiQ算法在合成数据和真实世界基准测试（如UltraFeedback和BFCL-V3）中表现出色，证明了其在单轮和多轮LLM设置中的有效性。

Conclusion: 通过引入适应LLM的Q学习方法，本文为高效微调大语言模型提供了一种新途径，ShiQ算法展示了良好的性能和灵活性，未来可能进一步推动LLM强化学习的发展。

Abstract: The fine-tuning of pre-trained large language models (LLMs) using
reinforcement learning (RL) is generally formulated as direct policy
optimization. This approach was naturally favored as it efficiently improves a
pretrained LLM, seen as an initial policy. Another RL paradigm, Q-learning
methods, has received far less attention in the LLM community while
demonstrating major success in various non-LLM RL tasks. In particular,
Q-learning effectiveness comes from its sample efficiency and ability to learn
offline, which is particularly valuable given the high computational cost of
sampling with LLMs. However, naively applying a Q-learning-style update to the
model's logits is ineffective due to the specificity of LLMs. Our core
contribution is to derive theoretically grounded loss functions from Bellman
equations to adapt Q-learning methods to LLMs. To do so, we carefully adapt
insights from the RL literature to account for LLM-specific characteristics,
ensuring that the logits become reliable Q-value estimates. We then use this
loss to build a practical algorithm, ShiQ for Shifted-Q, that supports
off-policy, token-wise learning while remaining simple to implement. Finally,
we evaluate ShiQ on both synthetic data and real-world benchmarks, e.g.,
UltraFeedback and BFCL-V3, demonstrating its effectiveness in both single-turn
and multi-turn LLM settings

</details>


### [5] [ReaCritic: Large Reasoning Transformer-based DRL Critic-model Scaling For Heterogeneous Networks](https://arxiv.org/abs/2505.10992)
*Feiran You,Hongyang Du*

Main category: cs.LG

TL;DR: 在异构网络（HetNets）中，由于用户需求多样和无线环境动态变化，现有的深度强化学习（DRL）方法面临适应性限制。本文提出了一种基于大型推理变压器的批评家模型ReaCritic，通过水平和垂直推理增强DRL算法的泛化能力，显著提升收敛速度和最终性能。


<details>
  <summary>Details</summary>
Motivation: 现有的DRL方法在处理异构网络中的复杂决策时受到限制，尤其是批评家模型通常采用浅层架构，难以应对多任务复杂性。而大型语言模型（LLMs）的研究表明，生成中间推理步骤可以显著提高决策质量。

Method: ReaCritic是一种基于大型推理变压器的批评家模型扩展方案，它结合了水平推理（针对并行状态-动作输入）和垂直推理（通过深层变压器堆栈），与多种基于价值或演员-评论家结构的DRL算法兼容，从而增强其在动态无线环境中的泛化能力。

Result: 广泛的实验表明，ReaCritic在各种异构网络设置和标准OpenAI Gym控制任务中提升了DRL算法的收敛速度和最终性能。

Conclusion: ReaCritic为DRL算法提供了一种新的批评家模型设计思路，能够有效应对异构网络中的复杂性和动态变化，推动智能管理技术的发展。

Abstract: Heterogeneous Networks (HetNets) pose critical challenges for intelligent
management due to the diverse user requirements and time-varying wireless
conditions. These factors introduce significant decision complexity, which
limits the adaptability of existing Deep Reinforcement Learning (DRL) methods.
In many DRL algorithms, especially those involving value-based or actor-critic
structures, the critic component plays a key role in guiding policy learning by
estimating value functions. However, conventional critic models often use
shallow architectures that map observations directly to scalar estimates,
limiting their ability to handle multi-task complexity. In contrast, recent
progress in inference-time scaling of Large Language Models (LLMs) has shown
that generating intermediate reasoning steps can significantly improve decision
quality. Motivated by this, we propose ReaCritic, a large reasoning
transformer-based criticmodel scaling scheme that brings reasoning ability into
DRL. ReaCritic performs horizontal reasoning over parallel state-action inputs
and vertical reasoning through deep transformer stacks. It is compatible with a
broad range of value-based and actor-critic DRL algorithms and enhances
generalization in dynamic wireless environments. Extensive experiments
demonstrate that ReaCritic improves convergence speed and final performance
across various HetNet settings and standard OpenAI Gym control tasks.

</details>


### [6] [Group-in-Group Policy Optimization for LLM Agent Training](https://arxiv.org/abs/2505.10978)
*Lang Feng,Zhenghai Xue,Tingcong Liu,Bo An*

Main category: cs.LG

TL;DR: 近期基于群体的强化学习（RL）推动了前沿大语言模型（LLMs）在单一回合任务中的进展，如数学推理。然而，在长时域LLM代理训练中的可扩展性仍然有限。本文提出了Group-in-Group Policy Optimization (GiGPO)，一种新颖的RL算法，能够在保持群体RL优点的同时实现对LLM代理的细粒度信用分配。GiGPO通过两级结构估计相对优势：在剧集级别上计算宏观相对优势；在步骤级别上引入锚点状态分组机制以构建步级分组，从而实现微观相对优势估计。此方法无需辅助模型或额外展开，有效捕捉全局轨迹质量和局部步骤有效性。实验结果表明，GiGPO在ALFWorld和WebShop基准测试中分别比GRPO基线提高了12%和9%以上的性能，同时保持相同的GPU内存开销、相同的LLM展开，并且几乎没有额外的时间成本。


<details>
  <summary>Details</summary>
Motivation: 当前基于群体的强化学习虽然在单回合任务中取得了显著进展，但在长时域LLM代理训练中的应用受限。由于代理与环境交互跨越多个步骤，且通常产生稀疏或延迟奖励，导致跨个体步骤的信用分配更具挑战性。因此，需要一种新的算法来解决这一问题，同时保留群体RL的优点。

Method: 提出了一种名为Group-in-Group Policy Optimization (GiGPO)的新算法。该算法采用两级结构：1）在剧集级别，根据完整的轨迹组计算宏观相对优势；2）在步骤级别，引入锚点状态分组机制，通过识别轨迹间的重复环境状态，回顾性地构建步级分组，从而实现微观相对优势估计。这种方法无需依赖辅助模型或额外展开，能够有效捕捉全局轨迹质量和局部步骤有效性。

Result: 在ALFWorld和WebShop两个具有挑战性的代理基准测试中，GiGPO相较于GRPO基线实现了超过12%和9%的性能提升。同时，GiGPO保持了相同的GPU内存开销、相同的LLM展开，并且几乎没有额外的时间成本。

Conclusion: GiGPO是一种有效的新型RL算法，能够在长时域LLM代理训练中实现细粒度信用分配，同时保留群体RL的优点。实验结果表明，GiGPO在复杂环境中表现出色，为未来的LLM代理训练提供了有力支持。

Abstract: Recent advances in group-based reinforcement learning (RL) have driven
frontier large language models (LLMs) in single-turn tasks like mathematical
reasoning. However, their scalability to long-horizon LLM agent training
remains limited. Unlike static tasks, agent-environment interactions unfold
over many steps and often yield sparse or delayed rewards, making credit
assignment across individual steps significantly more challenging. In this
work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL
algorithm that achieves fine-grained credit assignment for LLM agents while
preserving the appealing properties of group-based RL: critic-free, low memory,
and stable convergence. GiGPO introduces a two-level structure for estimating
relative advantage: (i) At the episode-level, GiGPO computes macro relative
advantages based on groups of complete trajectories; (ii) At the step-level,
GiGPO introduces an anchor state grouping mechanism that retroactively
constructs step-level groups by identifying repeated environment states across
trajectories. Actions stemming from the same state are grouped together,
enabling micro relative advantage estimation. This hierarchical structure
effectively captures both global trajectory quality and local step
effectiveness without relying on auxiliary models or additional rollouts. We
evaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using
Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers
fine-grained per-step credit signals and achieves performance gains of > 12\%
on ALFWorld and > 9\% on WebShop over the GRPO baseline: all while maintaining
the same GPU memory overhead, identical LLM rollout, and incurring little to no
additional time cost.

</details>


### [7] [Improving the Data-efficiency of Reinforcement Learning by Warm-starting with LLM](https://arxiv.org/abs/2505.10861)
*Thang Duong,Minglai Yang,Chicheng Zhang*

Main category: cs.LG

TL;DR: 研究了大语言模型（LLM）在收集高质量数据以启动强化学习（RL）算法中的应用，提出算法LORO，在多个OpenAI Gym环境中表现优于基线算法。


<details>
  <summary>Details</summary>
Motivation: 在Markov Decision Process (MDP) 环境中，使用大语言模型生成脱策略数据集，以覆盖最优策略访问的状态-动作对，从而为强化学习提供高质量的初始数据。

Method: 利用LLM生成一个包含最优策略状态-动作的离策略数据集，然后通过RL算法在环境中探索并改进LLM建议的策略。提出了名为LORO的算法。

Result: 在CartPole和Pendulum等OpenAI Gym环境中，LORO的表现优于纯LLM、纯RL以及两者的简单结合，累计奖励是纯RL基线的4倍。

Conclusion: LORO算法能够收敛到最优策略，并且由于LLM的良好初始策略，具有较高的样本效率。

Abstract: We investigate the usage of Large Language Model (LLM) in collecting
high-quality data to warm-start Reinforcement Learning (RL) algorithms for
learning in some classical Markov Decision Process (MDP) environments. In this
work, we focus on using LLM to generate an off-policy dataset that sufficiently
covers state-actions visited by optimal policies, then later using an RL
algorithm to explore the environment and improve the policy suggested by the
LLM. Our algorithm, LORO, can both converge to an optimal policy and have a
high sample efficiency thanks to the LLM's good starting policy. On multiple
OpenAI Gym environments, such as CartPole and Pendulum, we empirically
demonstrate that LORO outperforms baseline algorithms such as pure LLM-based
policies, pure RL, and a naive combination of the two, achieving up to $4
\times$ the cumulative rewards of the pure RL baseline.

</details>


### [8] [Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs](https://arxiv.org/abs/2505.10425)
*Jingyao Wang,Wenwen Qiang,Zeen Song,Changwen Zheng,Hui Xiong*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）在复杂任务中表现出色，但现有方法忽视了推理效果和计算效率之间的权衡。本文提出了一种名为Learning to Think（L2T）的信息理论强化微调框架，使LLMs能够在使用更少token的情况下实现最佳推理。L2T通过快速估计奖励并优化模型，显著提升了推理效果和效率。



<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在复杂任务上表现出色，但现有的方法往往导致不必要的长推理链，浪费了token，未能有效平衡推理效果与计算效率。因此，需要一种方法来优化推理过程，减少token使用量同时保持或提高推理效果。

Method: L2T将每个查询-响应交互视为多个阶段的分层会话，并提出了一种通用密集过程奖励，量化了每阶段参数的信息增益，无需额外注释或任务特定评估器。通过基于PAC-Bayes界和Fisher信息矩阵的方法快速估计奖励，从而显著降低计算复杂度。通过立即奖励每阶段的贡献并惩罚过度更新，L2T利用强化学习优化模型以最大化每阶段的使用并实现有效更新。

Result: 实验证明，在各种推理基准和基础模型上，L2T在不同任务中都具有优势，显著提高了推理效果和效率。

Conclusion: L2T作为一种信息理论强化微调框架，成功实现了在更少token使用的情况下提升LLMs的推理效果和效率，为未来的研究提供了一个新的方向。

Abstract: Large language models (LLMs) excel at complex tasks thanks to advances in
reasoning abilities. However, existing methods overlook the trade-off between
reasoning effectiveness and computational efficiency, often encouraging
unnecessarily long reasoning chains and wasting tokens. To address this, we
propose Learning to Think (L2T), an information-theoretic reinforcement
fine-tuning framework for LLMs to make the models achieve optimal reasoning
with fewer tokens. Specifically, L2T treats each query-response interaction as
a hierarchical session of multiple episodes and proposes a universal dense
process reward, i.e., quantifies the episode-wise information gain in
parameters, requiring no extra annotations or task-specific evaluators. We
propose a method to quickly estimate this reward based on PAC-Bayes bounds and
the Fisher information matrix. Theoretical analyses show that it significantly
reduces computational complexity with high estimation accuracy. By immediately
rewarding each episode's contribution and penalizing excessive updates, L2T
optimizes the model via reinforcement learning to maximize the use of each
episode and achieve effective updates. Empirical results on various reasoning
benchmarks and base models demonstrate the advantage of L2T across different
tasks, boosting both reasoning effectiveness and efficiency.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [9] [Keep Guessing? When Considering Inference Scaling, Mind the Baselines](https://arxiv.org/abs/2410.15466)
*Gal Yona,Or Honovich,Omer Levy,Roee Aharoni*

Main category: cs.CL

TL;DR: 通过重复采样扩展大型语言模型（LLMs）的推理计算，随着样本数量的增加，覆盖率（解决问题的比例）持续提高。实验表明，在数学推理和事实知识两个领域，基于训练集中答案频率的基线方法在某些LLM上优于重复采样，而在其他LLM上与混合策略表现相当。这为评估重复采样在提示无关猜测之外的实际改进提供了更准确的方法。


<details>
  <summary>Details</summary>
Motivation: 研究者观察到，通过重复采样扩展大型语言模型（LLMs）的推理计算时，随着样本数量增加，覆盖率持续提高。他们推测这种提升部分是由于标准评估基准中的答案分布偏向于一小部分常见答案。

Method: 研究者定义了一个基线方法，该方法根据训练集中答案的频率枚举答案。他们在数学推理和事实知识两个领域进行了实验，比较了基线方法、重复采样以及混合策略的表现。

Result: 实验结果表明，对于某些LLM，基线方法优于重复采样；而对于其他LLM，其表现与混合策略相当，即通过少量模型采样结合答案枚举来获得答案。

Conclusion: 这种方法提供了一种更准确的测量方式，以评估重复采样在提示无关猜测之外对覆盖率的实际改进。

Abstract: Scaling inference compute in large language models (LLMs) through repeated
sampling consistently increases the coverage (fraction of problems solved) as
the number of samples increases. We conjecture that this observed improvement
is partially due to the answer distribution of standard evaluation benchmarks,
which is skewed towards a relatively small set of common answers. To test this
conjecture, we define a baseline that enumerates answers according to their
prevalence in the training set. Experiments spanning two domains --
mathematical reasoning and factual knowledge -- reveal that this baseline
outperforms repeated model sampling for some LLMs, while the coverage for
others is on par with that of a mixture strategy that obtains $k$ answers by
using only $10$ model samples and similarly guessing the remaining $k-10$
attempts via enumeration. Our baseline enables a more accurate measurement of
how much repeated sampling improves coverage in such settings beyond
prompt-agnostic guessing.

</details>


### [10] [HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages](https://arxiv.org/abs/2505.11475)
*Zhilin Wang,Jiaqi Zeng,Olivier Delalleau,Hoo-Chang Shin,Felipe Soares,Alexander Bukharin,Ellie Evans,Yi Dong,Oleksii Kuchaiev*

Main category: cs.CL

TL;DR: 本文介绍了HelpSteer3-Preference数据集，这是一个包含超过40,000个样本的高质量人类标注偏好数据集，用于训练基于人类反馈强化学习（RLHF）的语言模型。该数据集涵盖STEM、编码和多语言场景等多样化任务，并在RM-Bench和JudgeBench上显著提升了Reward Models的表现。


<details>
  <summary>Details</summary>
Motivation: 当前RLHF方法需要高质量和多样化的偏好数据集来提升语言模型的能力，但现有数据集无法完全满足需求，因此需要一个更高质量、更大规模的数据集。

Method: 创建了一个名为HelpSteer3-Preference的数据集，包含超过40,000个人类标注样本，覆盖多种实际应用场景（如STEM、编程、多语言）。使用该数据集训练Reward Models（RMs），并在基准测试中取得了显著优于现有模型的结果。此外，还展示了如何用此数据集训练生成式RMs以及通过RLHF对策略模型进行对齐。

Result: 使用HelpSteer3-Preference训练的Reward Models在RM-Bench上达到了82.4%的准确率，在JudgeBench上达到73.7%，比之前最佳结果提高了约10%。这表明新数据集能够显著提升模型性能。

Conclusion: HelpSteer3-Preference数据集为RLHF方法提供了高质量和多样化的偏好数据，推动了Reward Models和策略模型的性能提升，并为未来研究奠定了基础。

Abstract: Preference datasets are essential for training general-domain,
instruction-following language models with Reinforcement Learning from Human
Feedback (RLHF). Each subsequent data release raises expectations for future
data collection, meaning there is a constant need to advance the quality and
diversity of openly available preference data. To address this need, we
introduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0),
high-quality, human-annotated preference dataset comprising of over 40,000
samples. These samples span diverse real-world applications of large language
models (LLMs), including tasks relating to STEM, coding and multilingual
scenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that
achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This
represents a substantial improvement (~10% absolute) over the previously
best-reported results from existing RMs. We demonstrate HelpSteer3-Preference
can also be applied to train Generative RMs and how policy models can be
aligned with RLHF using our RMs. Dataset (CC-BY-4.0):
https://huggingface.co/datasets/nvidia/HelpSteer3#preference

</details>


### [11] [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320)
*Chenxi Whitehouse,Tianlu Wang,Ping Yu,Xian Li,Jason Weston,Ilia Kulikov,Swarnadeep Saha*

Main category: cs.CL

TL;DR: 本文提出了一种名为J1的强化学习方法，用于训练以大语言模型作为评判者的模型，通过可验证奖励激励思考并减少评判偏差。实验表明，在相同参数量下，J1优于其他8B或70B模型，包括从DeepSeek-R1蒸馏出的模型。此外，尽管训练的是较小的模型，J1在某些基准上还优于o1-mini和R1。


<details>
  <summary>Details</summary>
Motivation: AI的进步受限于评估质量，而强大的LLM-as-a-Judge模型被证明是核心解决方案。为了提升判断能力，需要更强的连贯推理能力，从而促使研究者探索最佳的训练方法。

Method: 作者引入了J1，一种强化学习方法，将可验证和不可验证的提示转化为带有可验证奖励的判断任务，激励模型进行深入思考并减少判断偏差。

Result: J1在8B和70B参数量下训练时，性能优于其他现有模型，包括从DeepSeek-R1蒸馏出的模型。此外，尽管训练的是较小的模型，J1在某些基准上还优于o1-mini和R1。

Conclusion: J1方法能够显著提高模型的判断能力，并且通过学习制定评估标准、与自动生成的参考答案对比以及重新评估模型响应的正确性，模型可以做出更好的判断。

Abstract: The progress of AI is bottlenecked by the quality of evaluation, and powerful
LLM-as-a-Judge models have proved to be a core solution. Improved judgment
ability is enabled by stronger chain-of-thought reasoning, motivating the need
to find the best recipes for training such models to think. In this work we
introduce J1, a reinforcement learning approach to training such models. Our
method converts both verifiable and non-verifiable prompts to judgment tasks
with verifiable rewards that incentivize thinking and mitigate judgment bias.
In particular, our approach outperforms all other existing 8B or 70B models
when trained at those sizes, including models distilled from DeepSeek-R1. J1
also outperforms o1-mini, and even R1 on some benchmarks, despite training a
smaller model. We provide analysis and ablations comparing Pairwise-J1 vs
Pointwise-J1 models, offline vs online training recipes, reward strategies,
seed prompts, and variations in thought length and content. We find that our
models make better judgments by learning to outline evaluation criteria,
comparing against self-generated reference answers, and re-evaluating the
correctness of model responses.

</details>


### [12] [Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning](https://arxiv.org/abs/2505.10182)
*Yoichi Ishibashi,Taro Yano,Masafumi Oyamada*

Main category: cs.CL

TL;DR: 本文探讨了通过合成数据进行持续预训练（Reasoning CPT）的方法，以改进大语言模型在推理能力方面的表现。研究表明，Reasoning CPT相比传统方法在多个领域中表现出更好的性能，尤其是在困难问题上的提升更为显著。此外，模型能够根据问题难度调整推理深度。


<details>
  <summary>Details</summary>
Motivation: 当前通过监督微调和强化学习改进大语言模型推理能力的方法主要局限于特定领域（如数学和编程），限制了训练数据的广度和可扩展性。因此，研究者探索了一种无需任务特定信号的持续预训练方法，即Reasoning CPT。

Method: 研究者基于文本是作者思考过程结果的前提，使用来自STEM和法律语料库的合成数据来重建隐藏的思维过程，并将其应用于Gemma2-9B模型。然后将此方法与标准CPT在MMLU基准上进行比较。

Result: Reasoning CPT在所有评估领域中均提升了模型性能，特别是在困难问题上的表现优于传统方法，最大提升可达8分。此外，模型学会了根据问题难度调整推理深度。

Conclusion: Reasoning CPT是一种有效的改进大语言模型推理能力的方法，其跨领域的迁移能力和适应问题难度的能力表明了该方法的潜力。

Abstract: Large Language Models (LLMs) have demonstrated significant improvements in
reasoning capabilities through supervised fine-tuning and reinforcement
learning. However, when training reasoning models, these approaches are
primarily applicable to specific domains such as mathematics and programming,
which imposes fundamental constraints on the breadth and scalability of
training data. In contrast, continual pretraining (CPT) offers the advantage of
not requiring task-specific signals. Nevertheless, how to effectively
synthesize training data for reasoning and how such data affect a wide range of
domains remain largely unexplored. This study provides a detailed evaluation of
Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden
thought processes underlying texts, based on the premise that texts are the
result of the author's thinking process. Specifically, we apply Reasoning CPT
to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and
Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis
reveals that Reasoning CPT consistently improves performance across all
evaluated domains. Notably, reasoning skills acquired in one domain transfer
effectively to others; the performance gap with conventional methods widens as
problem difficulty increases, with gains of up to 8 points on the most
challenging problems. Furthermore, models trained with hidden thoughts learn to
adjust the depth of their reasoning according to problem difficulty.

</details>
